This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-05-14T03:45:27.423Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

================================================================
Directory Structure
================================================================
api-memory-bank/
  api_examples/
    adapters/
      storage/
        gcs.py
    application/
      interfaces/
        storage.py
    infrastructure/
      config/
        container.py
    main.py
  api-active-context.md
  api-file-structure-comparison.md
  api-implementation-tasks.md
  api-overview.md
  api-prd.txt
  api-product-context.md
  api-progress.md
  api-project-brief.md
  api-system-patterns.md
  api-tech-context.md
tests/
  e2e/
    __init__.py
    test_deployment.py
    test_video_processing.py
  integration/
    ai/
      test_gemini.py
    api/
      test_health.py
    storage/
      test_gcs.py
    __init__.py
  mocks/
    __init__.py
    ai.py
    publishing.py
    storage.py
  unit/
    adapters/
      test_storage.py
    application/
      services/
        test_video_processor.py
    domain/
      test_enums.py
      test_job.py
      test_metadata.py
      test_video.py
  conftest.py
  README.md
video_processor/
  adapters/
    ai/
      __init__.py
      cache.py
      gemini.py
      parallel.py
      vertex_ai.py
    publishing/
      __init__.py
      youtube.py
    storage/
      __init__.py
      gcs.py
      local.py
  application/
    dtos/
      __init__.py
      job_dto.py
      metadata_dto.py
      video_dto.py
    interfaces/
      __init__.py
      ai.py
      messaging.py
      publishing.py
      repositories.py
      storage.py
    services/
      metadata.py
      subtitle.py
      transcription.py
      video_processor.py
    __init__.py
  config/
    __init__.py
    environment.py
    settings.py
  domain/
    models/
      __init__.py
      enums.py
      job.py
      metadata.py
      video.py
    __init__.py
    exceptions.py
    value_objects.py
  infrastructure/
    api/
      routes/
        __init__.py
        health.py
        videos.py
      schemas/
        __init__.py
        video.py
      __init__.py
      auth.py
      dependencies.py
      server.py
    config/
      container.py
      secrets.py
      settings.py
    messaging/
      __init__.py
      handlers.py
      pubsub.py
    repositories/
      __init__.py
      job_repository.py
      video_repository.py
    monitoring.py
  services/
    storage/
      __init__.py
      base.py
      factory.py
      gcs.py
      local.py
    __init__.py
  tests/
    unit/
      test_video_processor.py
    __init__.py
    conftest.py
    test_chapters_generation.py
    test_firestore_trigger_listener.py
    test_generate_youtube_token.py
    test_titles_generation.py
    test_vtt_generation.py
    test_youtube_uploader.py
  utils/
    __init__.py
    error_handling.py
    ffmpeg.py
    file_handling.py
    logging.py
    profiling.py
    youtube_auth.py
  __init__.py
  .gcloudignore
  .gitignore
  conftest.py
  pytest.ini
  README.md
  test_audio_processing.py
  test_process_video.py
  youtube_uploader_README.md
video_processor.egg-info/
  dependency_links.txt
  PKG-INFO
  requires.txt
  SOURCES.txt
  top_level.txt
.pre-commit-config.yaml
.python-version
cloudbuild.yaml
deploy.sh
Dockerfile
Dockerfile.mock
Makefile
pyproject.toml
python_settings.txt
README.md
setup.sh

================================================================
Files
================================================================

================
File: api-memory-bank/api_examples/adapters/storage/gcs.py
================
"""
Google Cloud Storage adapter implementation.

This module provides a concrete implementation of the StorageInterface
for Google Cloud Storage. It handles the specific details of interacting
with GCS while conforming to the interface expected by the application.
"""

import asyncio
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
from typing import Optional, Union

from google.cloud import storage
from google.cloud.exceptions import Forbidden, NotFound

from video_processor.application.interfaces.storage import (
    FileNotFoundError,
    StorageError,
    StorageInterface,
    StoragePermissionError,
)


class GCSStorageAdapter(StorageInterface):
    """
    Google Cloud Storage implementation of the storage interface.

    This adapter implements the StorageInterface abstract methods using
    the Google Cloud Storage client library.
    """

    def __init__(
        self,
        client: Optional[storage.Client] = None,
        executor: Optional[ThreadPoolExecutor] = None,
    ):
        """
        Initialize the GCS storage adapter.

        Args:
            client: Optional pre-configured storage client
            executor: Optional thread pool executor for async operations
        """
        self._client = client or storage.Client()
        self._executor = executor or ThreadPoolExecutor(max_workers=10)

    def _get_bucket(self, bucket_name: str) -> storage.Bucket:
        """Get a bucket by name."""
        return self._client.bucket(bucket_name)

    def _parse_gcs_path(self, path: str) -> tuple[str, str]:
        """
        Parse a GCS path into bucket name and blob name.

        Args:
            path: GCS path in format 'bucket_name/path/to/file'
                or 'gs://bucket_name/path/to/file'

        Returns:
            Tuple of (bucket_name, blob_name)

        Raises:
            ValueError: If path format is invalid
        """
        if path.startswith("gs://"):
            path = path[5:]

        parts = path.split("/", 1)
        if len(parts) < 2:
            raise ValueError(
                f"Invalid GCS path: {path}. Must be in format 'bucket_name/blob_name'"
            )

        bucket_name, blob_name = parts
        return bucket_name, blob_name

    async def upload_file(
        self,
        source_path: Union[str, Path],
        destination_path: str,
        content_type: Optional[str] = None,
    ) -> str:
        """Upload a file to GCS."""
        try:
            source_path = Path(source_path)
            if not source_path.exists():
                raise FileNotFoundError(f"Source file not found: {source_path}")

            bucket_name, blob_name = self._parse_gcs_path(destination_path)
            bucket = self._get_bucket(bucket_name)
            blob = bucket.blob(blob_name)

            if content_type:
                blob.content_type = content_type

            # Run in thread pool to avoid blocking
            def _upload():
                blob.upload_from_filename(str(source_path))
                return f"gs://{bucket_name}/{blob_name}"

            return await asyncio.get_event_loop().run_in_executor(
                self._executor, _upload
            )

        except NotFound:
            raise StorageError(f"Bucket not found: {bucket_name}")
        except Forbidden:
            raise StoragePermissionError(
                f"Permission denied uploading to {destination_path}"
            )
        except Exception as e:
            raise StorageError(f"Failed to upload file: {str(e)}")

    async def upload_from_string(
        self, content: str, destination_path: str, content_type: Optional[str] = None
    ) -> str:
        """Upload string content to GCS."""
        try:
            bucket_name, blob_name = self._parse_gcs_path(destination_path)
            bucket = self._get_bucket(bucket_name)
            blob = bucket.blob(blob_name)

            if content_type:
                blob.content_type = content_type

            # Run in thread pool to avoid blocking
            def _upload():
                blob.upload_from_string(content)
                return f"gs://{bucket_name}/{blob_name}"

            return await asyncio.get_event_loop().run_in_executor(
                self._executor, _upload
            )

        except NotFound:
            raise StorageError(f"Bucket not found: {bucket_name}")
        except Forbidden:
            raise StoragePermissionError(
                f"Permission denied uploading to {destination_path}"
            )
        except Exception as e:
            raise StorageError(f"Failed to upload content: {str(e)}")

    async def download_to_file(
        self, source_path: str, destination_path: Union[str, Path]
    ) -> Path:
        """Download a file from GCS to local filesystem."""
        try:
            bucket_name, blob_name = self._parse_gcs_path(source_path)
            bucket = self._get_bucket(bucket_name)
            blob = bucket.blob(blob_name)

            destination_path = Path(destination_path)

            # Ensure destination directory exists
            destination_path.parent.mkdir(parents=True, exist_ok=True)

            # Check if blob exists
            def _exists():
                return blob.exists()

            exists = await asyncio.get_event_loop().run_in_executor(
                self._executor, _exists
            )

            if not exists:
                raise FileNotFoundError(f"File not found in storage: {source_path}")

            # Download the file
            def _download():
                blob.download_to_filename(str(destination_path))
                return destination_path

            return await asyncio.get_event_loop().run_in_executor(
                self._executor, _download
            )

        except NotFound:
            raise StorageError(f"Bucket not found: {bucket_name}")
        except Forbidden:
            raise StoragePermissionError(
                f"Permission denied downloading from {source_path}"
            )
        except Exception as e:
            if isinstance(e, FileNotFoundError):
                raise
            raise StorageError(f"Failed to download file: {str(e)}")

    async def download_as_string(self, source_path: str) -> str:
        """Download a file from GCS as a string."""
        try:
            bucket_name, blob_name = self._parse_gcs_path(source_path)
            bucket = self._get_bucket(bucket_name)
            blob = bucket.blob(blob_name)

            # Check if blob exists
            def _exists():
                return blob.exists()

            exists = await asyncio.get_event_loop().run_in_executor(
                self._executor, _exists
            )

            if not exists:
                raise FileNotFoundError(f"File not found in storage: {source_path}")

            # Download the file as string
            def _download():
                return blob.download_as_text()

            return await asyncio.get_event_loop().run_in_executor(
                self._executor, _download
            )

        except NotFound:
            raise StorageError(f"Bucket not found: {bucket_name}")
        except Forbidden:
            raise StoragePermissionError(
                f"Permission denied downloading from {source_path}"
            )
        except Exception as e:
            if isinstance(e, FileNotFoundError):
                raise
            raise StorageError(f"Failed to download file as string: {str(e)}")

    async def download_as_bytes(self, source_path: str) -> bytes:
        """Download a file from GCS as bytes."""
        try:
            bucket_name, blob_name = self._parse_gcs_path(source_path)
            bucket = self._get_bucket(bucket_name)
            blob = bucket.blob(blob_name)

            # Check if blob exists
            def _exists():
                return blob.exists()

            exists = await asyncio.get_event_loop().run_in_executor(
                self._executor, _exists
            )

            if not exists:
                raise FileNotFoundError(f"File not found in storage: {source_path}")

            # Download the file as bytes
            def _download():
                return blob.download_as_bytes()

            return await asyncio.get_event_loop().run_in_executor(
                self._executor, _download
            )

        except NotFound:
            raise StorageError(f"Bucket not found: {bucket_name}")
        except Forbidden:
            raise StoragePermissionError(
                f"Permission denied downloading from {source_path}"
            )
        except Exception as e:
            if isinstance(e, FileNotFoundError):
                raise
            raise StorageError(f"Failed to download file as bytes: {str(e)}")

    async def get_signed_url(
        self, path: str, expires_after_seconds: int = 3600, method: str = "GET"
    ) -> str:
        """Generate a signed URL for a GCS file."""
        try:
            bucket_name, blob_name = self._parse_gcs_path(path)
            bucket = self._get_bucket(bucket_name)
            blob = bucket.blob(blob_name)

            # Check if blob exists for GET requests
            if method.upper() == "GET":

                def _exists():
                    return blob.exists()

                exists = await asyncio.get_event_loop().run_in_executor(
                    self._executor, _exists
                )

                if not exists:
                    raise FileNotFoundError(f"File not found in storage: {path}")

            # Generate signed URL
            def _generate_url():
                return blob.generate_signed_url(
                    expiration=expires_after_seconds, method=method, version="v4"
                )

            return await asyncio.get_event_loop().run_in_executor(
                self._executor, _generate_url
            )

        except NotFound:
            raise StorageError(f"Bucket not found: {bucket_name}")
        except Forbidden:
            raise StoragePermissionError(
                f"Permission denied generating signed URL for {path}"
            )
        except Exception as e:
            if isinstance(e, FileNotFoundError):
                raise
            raise StorageError(f"Failed to generate signed URL: {str(e)}")

    async def file_exists(self, path: str) -> bool:
        """Check if a file exists in GCS."""
        try:
            bucket_name, blob_name = self._parse_gcs_path(path)
            bucket = self._get_bucket(bucket_name)
            blob = bucket.blob(blob_name)

            def _exists():
                return blob.exists()

            return await asyncio.get_event_loop().run_in_executor(
                self._executor, _exists
            )

        except Exception:
            # Don't raise exceptions for existence checks
            return False

    async def delete_file(self, path: str) -> bool:
        """Delete a file from GCS."""
        try:
            bucket_name, blob_name = self._parse_gcs_path(path)
            bucket = self._get_bucket(bucket_name)
            blob = bucket.blob(blob_name)

            # Check if blob exists
            def _exists():
                return blob.exists()

            exists = await asyncio.get_event_loop().run_in_executor(
                self._executor, _exists
            )

            if not exists:
                return False

            # Delete the blob
            def _delete():
                blob.delete()
                return True

            return await asyncio.get_event_loop().run_in_executor(
                self._executor, _delete
            )

        except NotFound:
            return False
        except Forbidden:
            raise StoragePermissionError(f"Permission denied deleting {path}")
        except Exception as e:
            raise StorageError(f"Failed to delete file: {str(e)}")

    async def move_file(self, source_path: str, destination_path: str) -> str:
        """Move a file within GCS (copy and delete)."""
        # First copy the file
        destination_url = await self.copy_file(source_path, destination_path)

        # Then delete the original
        await self.delete_file(source_path)

        return destination_url

    async def copy_file(self, source_path: str, destination_path: str) -> str:
        """Copy a file within GCS."""
        try:
            source_bucket_name, source_blob_name = self._parse_gcs_path(source_path)
            dest_bucket_name, dest_blob_name = self._parse_gcs_path(destination_path)

            source_bucket = self._get_bucket(source_bucket_name)
            source_blob = source_bucket.blob(source_blob_name)

            # Check if source blob exists
            def _exists():
                return source_blob.exists()

            exists = await asyncio.get_event_loop().run_in_executor(
                self._executor, _exists
            )

            if not exists:
                raise FileNotFoundError(f"Source file not found: {source_path}")

            dest_bucket = self._get_bucket(dest_bucket_name)

            # Copy the blob
            def _copy():
                destination_blob = source_bucket.copy_blob(
                    source_blob, dest_bucket, dest_blob_name
                )
                return f"gs://{dest_bucket_name}/{dest_blob_name}"

            return await asyncio.get_event_loop().run_in_executor(self._executor, _copy)

        except NotFound:
            raise StorageError(
                f"Bucket not found: {source_bucket_name} or {dest_bucket_name}"
            )
        except Forbidden:
            raise StoragePermissionError(
                f"Permission denied copying {source_path} to {destination_path}"
            )
        except Exception as e:
            if isinstance(e, FileNotFoundError):
                raise
            raise StorageError(f"Failed to copy file: {str(e)}")

    async def list_files(self, prefix: str) -> list[str]:
        """List files in GCS with a given prefix."""
        try:
            bucket_name, prefix_path = self._parse_gcs_path(prefix)
            bucket = self._get_bucket(bucket_name)

            # List blobs with prefix
            def _list():
                blobs = bucket.list_blobs(prefix=prefix_path)
                return [f"gs://{bucket_name}/{blob.name}" for blob in blobs]

            return await asyncio.get_event_loop().run_in_executor(self._executor, _list)

        except NotFound:
            raise StorageError(f"Bucket not found: {bucket_name}")
        except Forbidden:
            raise StoragePermissionError(
                f"Permission denied listing files with prefix {prefix}"
            )
        except Exception as e:
            raise StorageError(f"Failed to list files: {str(e)}")

================
File: api-memory-bank/api_examples/application/interfaces/storage.py
================
"""
Storage interface for the video processing pipeline.

This module defines the abstract interface for storage operations required by the
application. It follows the dependency inversion principle by having core business
logic depend on abstractions rather than concrete implementations.
"""

from abc import ABC, abstractmethod
from pathlib import Path
from typing import Optional, Union


class StorageError(Exception):
    """Base exception for storage-related errors."""

    pass


class FileNotFoundError(StorageError):
    """Raised when a file cannot be found in storage."""

    pass


class StoragePermissionError(StorageError):
    """Raised when there's a permission issue with storage operations."""

    pass


class StorageInterface(ABC):
    """
    Abstract interface for storage operations.

    This interface defines the contract that any storage implementation
    must fulfill, regardless of whether it's cloud storage, local filesystem,
    or another storage mechanism.
    """

    @abstractmethod
    async def upload_file(
        self,
        source_path: Union[str, Path],
        destination_path: str,
        content_type: Optional[str] = None,
    ) -> str:
        """
        Upload a file from local filesystem to storage.

        Args:
            source_path: Local path to file to upload
            destination_path: Path in storage where file should be saved
            content_type: Optional MIME type of the file

        Returns:
            The URL or path to the uploaded file

        Raises:
            StorageError: If upload fails
        """
        pass

    @abstractmethod
    async def upload_from_string(
        self, content: str, destination_path: str, content_type: Optional[str] = None
    ) -> str:
        """
        Upload string content to storage.

        Args:
            content: String content to upload
            destination_path: Path in storage where content should be saved
            content_type: Optional MIME type of the content

        Returns:
            The URL or path to the uploaded content

        Raises:
            StorageError: If upload fails
        """
        pass

    @abstractmethod
    async def download_to_file(
        self, source_path: str, destination_path: Union[str, Path]
    ) -> Path:
        """
        Download a file from storage to local filesystem.

        Args:
            source_path: Path in storage to the file to download
            destination_path: Local path where file should be saved

        Returns:
            Path object pointing to the downloaded file

        Raises:
            FileNotFoundError: If source file doesn't exist
            StorageError: If download fails
        """
        pass

    @abstractmethod
    async def download_as_string(self, source_path: str) -> str:
        """
        Download a file from storage as a string.

        Args:
            source_path: Path in storage to the file to download

        Returns:
            Content of the file as a string

        Raises:
            FileNotFoundError: If source file doesn't exist
            StorageError: If download fails
        """
        pass

    @abstractmethod
    async def download_as_bytes(self, source_path: str) -> bytes:
        """
        Download a file from storage as bytes.

        Args:
            source_path: Path in storage to the file to download

        Returns:
            Content of the file as bytes

        Raises:
            FileNotFoundError: If source file doesn't exist
            StorageError: If download fails
        """
        pass

    @abstractmethod
    async def get_signed_url(
        self, path: str, expires_after_seconds: int = 3600, method: str = "GET"
    ) -> str:
        """
        Generate a signed URL for a file in storage.

        Args:
            path: Path in storage to the file
            expires_after_seconds: Number of seconds until URL expires
            method: HTTP method the URL should support (GET, PUT, etc.)

        Returns:
            A signed URL that can be used to access the file

        Raises:
            FileNotFoundError: If file doesn't exist
            StorageError: If URL generation fails
        """
        pass

    @abstractmethod
    async def file_exists(self, path: str) -> bool:
        """
        Check if a file exists in storage.

        Args:
            path: Path in storage to check

        Returns:
            True if file exists, False otherwise
        """
        pass

    @abstractmethod
    async def delete_file(self, path: str) -> bool:
        """
        Delete a file from storage.

        Args:
            path: Path in storage to the file to delete

        Returns:
            True if file was deleted, False if file didn't exist

        Raises:
            StoragePermissionError: If deletion fails due to permissions
            StorageError: If deletion fails for other reasons
        """
        pass

    @abstractmethod
    async def move_file(self, source_path: str, destination_path: str) -> str:
        """
        Move a file within storage.

        Args:
            source_path: Current path of the file in storage
            destination_path: New path for the file in storage

        Returns:
            The new path or URL to the moved file

        Raises:
            FileNotFoundError: If source file doesn't exist
            StorageError: If move fails
        """
        pass

    @abstractmethod
    async def copy_file(self, source_path: str, destination_path: str) -> str:
        """
        Copy a file within storage.

        Args:
            source_path: Path of the file to copy
            destination_path: Path where the copy should be saved

        Returns:
            The path or URL to the copied file

        Raises:
            FileNotFoundError: If source file doesn't exist
            StorageError: If copy fails
        """
        pass

    @abstractmethod
    async def list_files(self, prefix: str) -> list[str]:
        """
        List files in storage with a given prefix.

        Args:
            prefix: Prefix to filter files by

        Returns:
            List of paths matching the prefix
        """
        pass

================
File: api-memory-bank/api_examples/infrastructure/config/container.py
================
"""
Dependency injection container.

This module provides a dependency injection container that manages the
creation and lifecycle of service objects in the application. It helps to
decouple object creation from object usage, enabling better testability
and flexibility.
"""

import os
from concurrent.futures import ThreadPoolExecutor
from typing import Any, Dict, Type, TypeVar

from google.cloud import storage
from vertexai.preview.generative_models import GenerativeModel

from video_processor.adapters.ai.gemini import GeminiAIAdapter
from video_processor.adapters.publishing.youtube import YouTubeAdapter
from video_processor.adapters.storage.gcs import GCSStorageAdapter
from video_processor.adapters.storage.local import LocalStorageAdapter
from video_processor.application.interfaces.ai import AIServiceInterface
from video_processor.application.interfaces.publishing import PublishingInterface
from video_processor.application.interfaces.storage import StorageInterface
from video_processor.application.services.metadata import MetadataService
from video_processor.application.services.subtitle import SubtitleService
from video_processor.application.services.transcription import TranscriptionService
from video_processor.application.services.video_processor import VideoProcessorService
from video_processor.infrastructure.repositories.job_repository import JobRepository
from video_processor.infrastructure.repositories.video_repository import VideoRepository

# Type variable for generics
T = TypeVar("T")


class Container:
    """
    Dependency injection container.

    This container manages the creation, configuration, and lifetime of
    service objects, providing a central registry for application components.
    """

    def __init__(self):
        """Initialize an empty container."""
        self._services: Dict[Type, Any] = {}
        self._factories: Dict[Type, callable] = {}
        self._singletons: Dict[Type, bool] = {}

    def register(
        self, service_type: Type[T], factory: callable, singleton: bool = True
    ) -> None:
        """
        Register a service with the container.

        Args:
            service_type: The type/interface for the service
            factory: A factory function that creates an instance of the service
            singleton: Whether the service should be a singleton
        """
        self._factories[service_type] = factory
        self._singletons[service_type] = singleton

        # Clear existing instance if re-registering
        if service_type in self._services:
            del self._services[service_type]

    def get(self, service_type: Type[T]) -> T:
        """
        Get a service instance from the container.

        Args:
            service_type: The type/interface of the service to retrieve

        Returns:
            An instance of the requested service

        Raises:
            KeyError: If the service type is not registered
        """
        # Return existing instance for singletons
        if service_type in self._services and self._singletons.get(service_type, True):
            return self._services[service_type]

        # Create new instance using factory
        if service_type not in self._factories:
            raise KeyError(f"Service {service_type.__name__} not registered")

        factory = self._factories[service_type]
        instance = factory()

        # Cache singleton instances
        if self._singletons.get(service_type, True):
            self._services[service_type] = instance

        return instance

    def register_instance(self, service_type: Type[T], instance: T) -> None:
        """
        Register an existing instance with the container.

        Args:
            service_type: The type/interface for the service
            instance: An existing instance to use
        """
        self._services[service_type] = instance
        self._singletons[service_type] = True


def create_container(testing: bool = False, local_storage: bool = False) -> Container:
    """
    Create and configure a dependency injection container.

    Args:
        testing: Whether the container is for testing
        local_storage: Whether to use local storage instead of GCS

    Returns:
        A configured Container instance
    """
    container = Container()
    thread_pool = ThreadPoolExecutor(max_workers=10)

    # Register infrastructure services
    if testing:
        # Use mock implementations for testing
        from unittest.mock import MagicMock

        container.register_instance(storage.Client, MagicMock())
    else:
        # Use real implementations for production
        container.register(storage.Client, lambda: storage.Client())

    # Register storage adapter
    if local_storage or testing:
        local_dir = os.environ.get("LOCAL_OUTPUT_DIR", "./output")
        container.register(
            StorageInterface,
            lambda: LocalStorageAdapter(base_dir=local_dir, executor=thread_pool),
        )
    else:
        container.register(
            StorageInterface,
            lambda: GCSStorageAdapter(
                client=container.get(storage.Client), executor=thread_pool
            ),
        )

    # Register AI service adapter
    if testing:
        from unittest.mock import MagicMock

        mock_ai = MagicMock(spec=AIServiceInterface)
        mock_ai.generate_content.return_value = "Mock AI response"
        container.register_instance(AIServiceInterface, mock_ai)
    else:
        # Configure Vertex AI
        project_id = os.environ.get("GOOGLE_CLOUD_PROJECT", "default-project")
        region = os.environ.get("REGION", "us-east1")
        model_name = os.environ.get("MODEL", "gemini-2.0-flash-001")

        import vertexai

        vertexai.init(project=project_id, location=region)

        container.register(GenerativeModel, lambda: GenerativeModel(model_name))

        container.register(
            AIServiceInterface,
            lambda: GeminiAIAdapter(model=container.get(GenerativeModel)),
        )

    # Register YouTube adapter
    container.register(
        PublishingInterface,
        lambda: YouTubeAdapter(
            storage=container.get(StorageInterface),
            credentials_path=os.environ.get(
                "YOUTUBE_CREDENTIALS", "credentials/youtube_credentials.json"
            ),
        ),
    )

    # Register repositories
    container.register(
        JobRepository, lambda: JobRepository(storage=container.get(StorageInterface))
    )

    container.register(
        VideoRepository,
        lambda: VideoRepository(storage=container.get(StorageInterface)),
    )

    # Register application services
    container.register(
        TranscriptionService,
        lambda: TranscriptionService(ai_service=container.get(AIServiceInterface)),
    )

    container.register(
        SubtitleService,
        lambda: SubtitleService(ai_service=container.get(AIServiceInterface)),
    )

    container.register(
        MetadataService,
        lambda: MetadataService(ai_service=container.get(AIServiceInterface)),
    )

    # Register the main processor service
    container.register(
        VideoProcessorService,
        lambda: VideoProcessorService(
            storage=container.get(StorageInterface),
            transcription_service=container.get(TranscriptionService),
            subtitle_service=container.get(SubtitleService),
            metadata_service=container.get(MetadataService),
            publishing_service=container.get(PublishingInterface),
            job_repository=container.get(JobRepository),
            video_repository=container.get(VideoRepository),
        ),
    )

    return container

================
File: api-memory-bank/api_examples/main.py
================
"""
Main application entry point.

This module serves as the entry point for the video processing application.
It sets up the dependency injection container, configures services, and
provides the main Cloud Function handler for processing video events.
"""

import logging
import os
from typing import Any, Dict

import functions_framework

from video_processor.application.services.video_processor import VideoProcessorService
from video_processor.domain.models.job import JobStatus
from video_processor.infrastructure.config.container import create_container

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# Configure testing mode
TESTING_MODE = os.environ.get("TESTING_MODE", "false").lower() == "true"
LOCAL_OUTPUT = os.environ.get("LOCAL_OUTPUT", "false").lower() == "true" or TESTING_MODE

# Create and configure the dependency injection container
container = create_container(testing=TESTING_MODE, local_storage=LOCAL_OUTPUT)

# Get the video processor service from the container
video_processor_service = container.get(VideoProcessorService)


@functions_framework.cloud_event
async def process_video_event(cloud_event: Dict[str, Any]) -> Dict[str, Any]:
    """
    Cloud Function handler for processing video events.

    This function is triggered by GCS events when a new video is uploaded.
    It extracts metadata from the event and delegates to the video processor
    service for actual processing.

    Args:
        cloud_event: GCS event data

    Returns:
        A dictionary with processing results
    """
    try:
        logger.info(f"Received event: {cloud_event.id}")

        # Extract bucket and file information from the event
        bucket = cloud_event.data["bucket"]
        file_name = cloud_event.data["name"]

        logger.info(f"Processing video: gs://{bucket}/{file_name}")

        # Check if we should process this file
        if not await video_processor_service.should_process(bucket, file_name):
            logger.info(f"Skipping file {file_name} (not a video or already processed)")
            return {"status": "skipped", "file": file_name}

        # Start processing job
        job = await video_processor_service.create_job(bucket, file_name)
        job_id = job.id

        logger.info(f"Created processing job {job_id} for {file_name}")

        # Process the video asynchronously
        # In a production environment, this would typically be offloaded to a task queue
        # or background worker to avoid Cloud Function timeout limits
        processing_result = await video_processor_service.process(job_id)

        if processing_result.status == JobStatus.COMPLETED:
            logger.info(f"Successfully processed {file_name}")
            return {
                "status": "success",
                "job_id": job_id,
                "file": file_name,
                "outputs": processing_result.output_paths,
            }
        else:
            logger.error(f"Failed to process {file_name}: {processing_result.error}")
            return {
                "status": "error",
                "job_id": job_id,
                "file": file_name,
                "error": processing_result.error,
            }

    except Exception as e:
        logger.exception(f"Error processing event: {e}")
        return {"status": "error", "error": str(e)}


@functions_framework.http
async def process_video_http(request):
    """
    HTTP endpoint for triggering video processing.

    This function allows manual triggering of video processing via HTTP
    requests, primarily used for testing or administrative purposes.

    Args:
        request: Flask request object

    Returns:
        Flask response with processing results
    """
    try:
        # Extract bucket and file from request parameters
        request_json = request.get_json(silent=True)

        if not request_json:
            return {"error": "No JSON data provided"}, 400

        bucket = request_json.get("bucket")
        file_name = request_json.get("file")

        if not bucket or not file_name:
            return {"error": "Missing required parameters (bucket, file)"}, 400

        logger.info(f"HTTP trigger for processing: gs://{bucket}/{file_name}")

        # Create a synthetic cloud event and process it
        cloud_event = {
            "id": "manual-trigger",
            "data": {"bucket": bucket, "name": file_name},
        }

        result = await process_video_event(cloud_event)
        return result, 200

    except Exception as e:
        logger.exception(f"Error in HTTP handler: {e}")
        return {"status": "error", "error": str(e)}, 500


if __name__ == "__main__":
    """
    Local development entry point.

    This block is executed when the script is run directly (not as a Cloud Function).
    It provides a way to test the processing logic locally.
    """
    import asyncio
    import sys

    if len(sys.argv) < 3:
        print("Usage: python main.py <bucket_name> <file_name>")
        sys.exit(1)

    bucket = sys.argv[1]
    file_name = sys.argv[2]

    # Create synthetic event for local testing
    test_event = {"id": "local-test", "data": {"bucket": bucket, "name": file_name}}

    # Configure more verbose logging for local testing
    logging.basicConfig(level=logging.DEBUG)

    # Run the event handler
    result = asyncio.run(process_video_event(test_event))
    print(f"Processing result: {result}")

================
File: api-memory-bank/api-active-context.md
================
# Active Context: Backend

## Current Focus

### Primary Workstreams
- Clean architecture implementation
- API endpoints using FastAPI
- Cloud storage and AI service integrations
- Testing and validating the new architecture

### Current Priorities
1. Remove legacy code components
2. Validate the clean architecture implementation
3. Complete missing documentation
4. Conduct security audit of the new architecture

## Recent Changes

### Completed Work
- Implemented clean architecture with domain-driven design
- Created modular service adapters for storage, AI, and publishing
- Developed FastAPI endpoints for the new architecture
- Removed legacy monolithic components:
  - Removed `process_uploaded_video.py` (711 lines)
  - Removed `youtube_uploader.py` (535 lines)
  - Removed other legacy files and outdated tests
  - Eliminated old application structure (core, api folders)

### In Progress
- Testing the new architecture components
- Documenting the architecture and implementation
- Completing security review
- Creating developer onboarding materials

## Next Steps

### Immediate Tasks
1. Test the new architecture end-to-end
2. Complete security audit
3. Update README.md with new architecture documentation
4. Create architectural diagrams and guides

### Upcoming Work
1. Performance optimization for video processing
2. Additional monitoring and observability
3. Enhanced error handling and recovery mechanisms
4. Production deployment of the new architecture

## Active Decisions

### Architecture Decisions
- Adopted clean architecture with four distinct layers:
  - Domain layer: Core business entities and rules
  - Application layer: Business orchestration and interfaces
  - Adapters layer: External service implementations
  - Infrastructure layer: Framework and technical implementations
- Using FastAPI for API endpoints (replaced Flask)
- Dependency injection for service management
- Repository pattern for data persistence

### Technology Decisions
- Google Cloud Platform for infrastructure
- Google Cloud Storage for file storage
- Gemini and Vertex AI for AI capabilities
- FastAPI for API framework
- Pytest for comprehensive testing

## Project Insights

### Learnings
- Clean architecture significantly improves maintainability and testability
- Smaller, focused files with single responsibilities improve development workflow
- Interface-driven development enables easier testing and component swapping
- Dependency injection provides flexibility for different environments

### Challenges
- Ensuring complete legacy code removal without breaking functionality
- Maintaining backward compatibility during transition
- Coordinating multiple services with consistent error handling
- Proper validation of the new architecture before production deployment

## Implementation Preferences

### Code Style
- Use type hints consistently
- Follow modern Python practices (pyproject.toml, dependency management)
- Document public interfaces thoroughly
- Write comprehensive unit tests for each layer

### Pattern Usage
- Clean architecture with clear layer separation
- Dependency injection for testability
- Use of interfaces and adapter pattern
- Repository pattern for data persistence
- Domain-driven design for core business logic

================
File: api-memory-bank/api-file-structure-comparison.md
================
# File Structure Comparison: Current vs. Proposed

## Current Backend Structure

The current backend follows a partially modular architecture but has significant monolithic components:

```
backend/
├── video_processor/                           # Main application module
│   ├── core/                                  # Core application logic
│   │   ├── models/                            # Domain models
│   │   │   ├── __init__.py                    # (8 lines) Module exports
│   │   │   └── video_job.py                   # (167 lines) VideoJob and ProcessingStage models
│   │   ├── processors/                        # Processing components
│   │   │   ├── __init__.py                    # (16 lines) Module exports
│   │   │   ├── audio.py                       # (196 lines) Audio extraction and processing
│   │   │   ├── chapters.py                    # (154 lines) Chapter generation
│   │   │   ├── transcript.py                  # (132 lines) Transcript generation
│   │   │   └── video.py                       # (252 lines) Video processing
│   │   └── __init__.py                        # (4 lines) Module exports
│   ├── services/                              # External service integrations
│   │   ├── ai/                                # AI service integrations
│   │   ├── storage/                           # Cloud storage services
│   │   ├── youtube/                           # YouTube API integration
│   │   └── __init__.py                        # (4 lines) Module exports
│   ├── api/                                   # API definitions
│   │   ├── __init__.py                        # (8 lines) Module exports
│   │   ├── controllers.py                     # (139 lines) Request handlers
│   │   ├── routes.py                          # (99 lines) API route definitions
│   │   └── schemas.py                         # (64 lines) API data schemas
│   ├── utils/                                 # Utility functions
│   ├── config/                                # Configuration management
│   ├── tests/                                 # Test directory (nested)
│   ├── __init__.py                            # (2 lines) Package initialization
│   ├── process_uploaded_video.py              # (711 lines) Main processing logic (monolithic)
│   ├── youtube_uploader.py                    # (535 lines) YouTube uploading functionality
│   ├── generate_youtube_token.py              # (151 lines) Token generation for YouTube API
│   ├── firestore_trigger_listener.py          # (69 lines) Firestore event processing
│   ├── setup_youtube_secrets.py               # (205 lines) YouTube credential setup
│   ├── app.py                                 # (27 lines) Flask application entry point
│   ├── main.py                                # (103 lines) Entry point for cloud functions
│   ├── test_process_video.py                  # (72 lines) Tests for video processing
│   ├── test_audio_processing.py               # (95 lines) Tests for audio processing
│   ├── conftest.py                            # (59 lines) Pytest fixtures
│   ├── pytest.ini                             # (11 lines) Pytest configuration
│   ├── setup.py                               # (22 lines) Package setup
│   ├── README.md                              # (190 lines) Module documentation
│   ├── youtube_uploader_README.md             # (114 lines) YouTube uploader documentation
│   └── .gcloudignore                          # (9 lines) GCP deployment ignore patterns
├── tests/                                     # Test suite (root level)
│   ├── unit/                                  # Unit tests
│   ├── integration/                           # Integration tests
│   └── e2e/                                   # End-to-end tests
├── scripts/                                   # Utility scripts
├── docs/                                      # Documentation
├── requirements.txt                           # (31 lines) Dependencies
├── Dockerfile                                 # (27 lines) Container definition
├── Dockerfile.mock                            # (19 lines) Mock container for testing
└── deploy.sh                                  # (268 lines) Deployment script
```

## Key Issues with Current Structure

1. **Monolithic Files:**
   - `process_uploaded_video.py` (711 lines) handles multiple concerns:
     - Video and audio processing
     - AI content generation
     - Cloud storage management
     - Process orchestration
   - `youtube_uploader.py` (535 lines) combines too many YouTube API operations

2. **Mixed Responsibilities:**
   - Tests are split between the module-level and project-level directories
   - Configuration is spread across multiple files
   - No clear separation between domain logic and infrastructure

3. **Limited Modularity:**
   - Difficult to replace components with alternative implementations
   - Testing requires many mock objects
   - Heavy coupling between processing stages

## Proposed Backend Structure

The proposed architecture follows clean architecture principles with well-defined layers:

```
backend/
├── video_processor/                           # Main package
│   ├── domain/                                # Domain models and business logic
│   │   ├── models/                            # Domain entities
│   │   │   ├── __init__.py                    # Package initialization
│   │   │   ├── video.py                       # Video entity
│   │   │   ├── job.py                         # Processing job entity (from video_job.py)
│   │   │   └── metadata.py                    # Video metadata entity (from video_job.py)
│   │   ├── __init__.py                        # Package initialization
│   │   ├── exceptions.py                      # Domain-specific exceptions
│   │   └── value_objects.py                   # Value objects for domain
│   ├── application/                           # Application services and use cases
│   │   ├── services/                          # Application services
│   │   │   ├── __init__.py                    # Package initialization 
│   │   │   ├── video_processor.py             # Video processing orchestration (from process_uploaded_video.py)
│   │   │   ├── transcription.py               # Transcript generation service (from generate_transcript())
│   │   │   ├── subtitle.py                    # Subtitle generation service (from generate_vtt())
│   │   │   └── metadata.py                    # Metadata generation service (from existing generators)
│   │   ├── interfaces/                        # Service interfaces
│   │   │   ├── __init__.py                    # Package initialization
│   │   │   ├── storage.py                     # Storage service interface
│   │   │   ├── ai.py                          # AI service interface
│   │   │   └── publishing.py                  # Publishing service interface
│   │   ├── __init__.py                        # Package initialization
│   │   └── dtos/                              # Data Transfer Objects
│   │       ├── __init__.py                    # Package initialization
│   │       └── video_job_dto.py               # DTOs for API communication
│   ├── adapters/                              # External service adapters
│   │   ├── ai/                                # AI service adapters
│   │   │   ├── __init__.py                    # Package initialization
│   │   │   ├── vertex_ai.py                   # Vertex AI implementation 
│   │   │   └── gemini.py                      # Gemini API implementation (from process_uploaded_video.py)
│   │   ├── storage/                           # Storage adapters
│   │   │   ├── __init__.py                    # Package initialization
│   │   │   ├── gcs.py                         # Google Cloud Storage implementation (from process_uploaded_video.py)
│   │   │   └── local.py                       # Local filesystem implementation
│   │   ├── __init__.py                        # Package initialization
│   │   └── publishing/                        # Publishing adapters
│   │       ├── __init__.py                    # Package initialization
│   │       └── youtube.py                     # YouTube API implementation (from youtube_uploader.py)
│   ├── infrastructure/                        # Framework-specific code
│   │   ├── config/                            # Configuration management
│   │   │   ├── __init__.py                    # Package initialization
│   │   │   ├── settings.py                    # Application settings
│   │   │   └── container.py                   # Dependency injection container
│   │   ├── api/                               # API framework implementation
│   │   │   ├── __init__.py                    # Package initialization
│   │   │   ├── server.py                      # FastAPI server definition (replacing Flask app.py)
│   │   │   ├── routes/                        # API route handlers
│   │   │   │   ├── __init__.py                # Package initialization
│   │   │   │   ├── videos.py                  # Video-related endpoints (from api/routes.py)
│   │   │   │   └── health.py                  # Health and status endpoints
│   │   │   ├── schemas/                       # API schemas using Pydantic
│   │   │   │   ├── __init__.py                # Package initialization
│   │   │   │   └── video.py                   # Video-related schemas (from api/schemas.py)
│   │   │   └── dependencies.py                # FastAPI dependencies
│   │   ├── repositories/                      # Data repositories
│   │   │   ├── __init__.py                    # Package initialization
│   │   │   ├── job_repository.py              # Job persistence
│   │   │   └── video_repository.py            # Video metadata persistence
│   │   ├── __init__.py                        # Package initialization
│   │   └── messaging/                         # Messaging infrastructure
│   │       ├── __init__.py                    # Package initialization
│   │       └── pubsub.py                      # Google Pub/Sub integration
│   ├── utils/                                 # Utility functions and helpers
│   │   ├── __init__.py                        # Package initialization
│   │   ├── logging.py                         # Logging configuration
│   │   ├── ffmpeg.py                          # FFmpeg wrapper (from core/processors/audio.py)
│   │   └── profiling.py                       # Performance profiling tools
│   ├── __init__.py                            # Package initialization
│   └── main.py                                # Application entry point (from main.py)
├── tests/                                     # Test suite
│   ├── unit/                                  # Unit tests
│   │   ├── domain/                            # Tests for domain models
│   │   ├── application/                       # Tests for application services 
│   │   └── adapters/                          # Tests for adapters
│   ├── integration/                           # Integration tests
│   │   ├── api/                               # API integration tests
│   │   ├── storage/                           # Storage integration tests
│   │   └── ai/                                # AI services integration tests
│   ├── e2e/                                   # End-to-end tests
│   └── conftest.py                            # Test fixtures (from video_processor/conftest.py)
├── scripts/                                   # Utility scripts for development/deployment
│   ├── deploy.sh                              # Deployment script (improved from root deploy.sh)
│   └── generate_youtube_token.py              # Token generation script (from video_processor)
├── docs/                                      # Documentation
│   ├── be-prd.txt                             # Product Requirements Document
│   └── file-structure-comparison.md           # This file
├── requirements.txt                           # Dependencies
├── Dockerfile                                 # Container definition
├── pyproject.toml                             # Project configuration (replacing setup.py)
└── README.md                                  # Project documentation
```

## Key Benefits of Proposed Structure

1. **Clean Architecture:**
   - Clear separation of concerns between layers
   - Domain logic isolated from infrastructure details
   - Easier to reason about and maintain

2. **Improved Testability:**
   - Domain logic is free from external dependencies
   - Interfaces allow easy mocking of external services
   - Dedicated test directory structure aligns with application structure

3. **Enhanced Modularity:**
   - Dependency injection enables swapping implementations
   - Adapters isolate external service interactions
   - Repository pattern separates persistence concerns

4. **Better Maintainability:**
   - Smaller, focused files with single responsibilities
   - Clear boundaries between application components
   - Standardized structure that's easier to navigate

5. **Scalability:**
   - New features can be added without affecting existing components
   - Processing stages can be evolved independently
   - Multiple developers can work on different areas simultaneously

================
File: api-memory-bank/api-implementation-tasks.md
================
# Consolidated Implementation Tasks

## Project Configuration & Setup

- [x] **Create pyproject.toml**
  - Replace setup.py with modern Python project configuration
  - Include build-system, project metadata, dependencies, and tool configs

- [x] **Set up pre-commit hooks**
  - Configure black, ruff, mypy, and pytest hooks
  - Ensure consistent code quality before commits

- [x] **Create base directory structure**
  - Set up domain-driven design folder structure
  - Create necessary module directories
  
- [x] **Create package initialization files**
  - Add __init__.py files to all directories
  - Add version information to root __init__.py

## Domain Model

- [x] **Create Video entity**
  - Implement Video class with attributes and methods
  - Define model for video files and metadata

- [x] **Create VideoMetadata entity**
  - Implement metadata class for video content
  - Add fields for title, description, tags, etc.

- [x] **Implement VideoJob model**
  - Define job status states and workflow
  - Create job tracking and state management

- [x] **Create domain exceptions**
  - Define error hierarchy for domain operations
  - Implement specific exception types

- [x] **Implement value objects**
  - Create immutable domain entities
  - Define core domain concepts

## Application Services & Interfaces

- [x] **Define storage interface**
  - Create abstract interface for storage operations
  - Define methods for file operations

- [x] **Define AI service interface**
  - Create abstract interface for AI operations
  - Define methods for content generation

- [x] **Define publishing interface**
  - Create abstract interface for publishing operations
  - Define methods for video publishing

- [x] **Create data transfer objects**
  - Implement DTOs for API communication
  - Add serialization methods

- [x] **Create video processor service**
  - Implement main processing orchestration
  - Manage processing workflow

- [x] **Create transcription service**
  - Implement audio transcription logic
  - Add methods for extracting audio

- [x] **Create subtitle service**
  - Implement subtitle generation
  - Support different subtitle formats

- [x] **Create metadata service**
  - Implement AI-driven metadata generation
  - Generate titles, descriptions, and tags

## Adapters

- [x] **Implement GCS storage adapter**
  - Create Google Cloud Storage client
  - Implement file operations methods

- [x] **Implement local storage adapter**
  - Create filesystem adapter for local development
  - Implement compatible storage interface

- [x] **Implement Gemini AI adapter**
  - Create adapter for Google's Gemini API
  - Implement AI operations

- [x] **Implement Vertex AI adapter**
  - Create adapter for Google Vertex AI
  - Support alternative AI provider

- [x] **Implement YouTube adapter**
  - Create YouTube publishing client
  - Implement video upload and metadata management

## Infrastructure

- [x] **Create settings module**
  - Define configuration for all components
  - Load settings from environment variables

- [x] **Implement dependency injection**
  - Create container for service registration
  - Implement service provider registry

- [x] **Create job repository**
  - Implement Firestore-based job storage
  - Create CRUD operations for jobs

- [x] **Create video repository**
  - Implement video metadata storage
  - Add data persistence layer

- [x] **Set up FastAPI application**
  - Configure web API framework
  - Set up middleware and routers

- [x] **Create API schemas**
  - Define Pydantic models for requests/responses
  - Implement data validation

- [x] **Implement video routes**
  - Create API endpoints for video operations
  - Add request handlers

- [x] **Implement health check**
  - Add service health monitoring endpoints
  - Create status checking logic

- [x] **Set up API dependencies**
  - Configure dependency injection for routes
  - Connect services to API layer

- [x] **Implement message handling**
  - Create message broker interfaces
  - Implement Pub/Sub adapter

- [x] **Implement event handlers**
  - Create handlers for system events
  - Add message processing logic

## Testing

- [x] **Set up test configuration**
  - Create pytest fixtures and configuration
  - Set up test environment

- [x] **Create mock implementations**
  - Implement test doubles for external services
  - Add mockable service interfaces

- [x] **Implement domain model tests**
  - Test core domain entities
  - Verify business logic

- [x] **Implement service tests**
  - Test application services
  - Validate service workflows

- [x] **Implement adapter tests**
  - Test external service adapters
  - Verify integration points

- [x] **Implement API integration tests**
  - Test API endpoints
  - Verify request/response handling

- [x] **Implement storage integration tests**
  - Test against real storage services
  - Verify file operations

- [x] **Implement AI integration tests**
  - Test against real AI services
  - Verify content generation

- [x] **Create video processing E2E test**
  - Test complete processing flow
  - Verify end-to-end functionality

- [x] **Create deployment verification test**
  - Test deployed service health
  - Verify production environment

## Deployment & Configuration

- [x] **Update Dockerfile**
  - Refactor for new architecture
  - Implement multi-stage build for smaller image

- [x] **Configure Cloud Run deployment**
  - Implement CI/CD pipeline
  - Configure service accounts and permissions

## Utilities & Tools

- [x] **Create FFmpeg wrapper**
  - Implement media processing utilities
  - Add video/audio manipulation functions

- [x] **Set up logging configuration**
  - Implement structured logging
  - Configure environment-specific logging

- [x] **Create profiling tools**
  - Implement performance monitoring
  - Add timing and profiling utilities

- [x] **Create benchmarking script**
  - Add performance testing tools
  - Measure processing time for pipeline stages

## Performance & Security

- [x] **Optimize AI service usage**
  - Implement caching and optimization
  - Improve performance and reduce API costs

- [x] **Set up metrics collection**
  - Configure service metrics
  - Implement structured logging

- [ ] **Implement authentication & authorization**
  - Replace JWT-based auth with Supabase authentication
  - Implement Supabase client integration
  - Configure role-based permissions

- [x] **Implement secure credential management**
  - Integrate with Secret Manager
  - Remove hard-coded credentials

## Documentation

- [x] **Configure Swagger UI**
  - Set up via FastAPI
  - Add detailed endpoint descriptions

## Migration & Legacy Code Removal

- [x] **Remove Flask dependencies**
  - Remove Flask from pyproject.toml
  - Delete or refactor Flask-based implementation files
  - Update any Flask-dependent test fixtures

- [x] **Update testing utilities**
  - Refactor test helpers that use Flask
  - Create FastAPI-based test fixtures

- [x] **Refactor script utilities**
  - Update scripts that depend on Flask
  - Create FastAPI-compatible alternatives if needed

## Local Development Improvements

- [x] **Update Makefile**
  - Fix `install-dev` command to use pyproject.toml
  - Add commands for running FastAPI server locally
  - Add convenience commands for common development tasks

- [ ] **Enhance local development setup**
  - Ensure environment can run without cloud dependencies
  - Create clear documentation for local setup
  - Add configuration for local frontend integration testing

## Authentication Implementation

- [ ] **Set up Supabase project configuration**
  - Create Supabase environment variables
  - Configure Supabase connection settings
  - Set up appropriate auth settings in Supabase dashboard

- [ ] **Implement Supabase auth client**
  - Create auth client wrapper
  - Configure auth state management
  - Implement token refresh mechanisms

- [ ] **Replace existing JWT auth with Supabase**
  - Update auth.py to use Supabase
  - Implement Supabase-based auth middleware
  - Update dependencies.py to leverage Supabase auth

- [ ] **Implement Supabase auth routes**
  - Add login/logout endpoints 
  - Create user registration flow
  - Add password reset functionality

- [ ] **Configure role-based permissions**
  - Implement role management with Supabase
  - Set up permission checks in API endpoints
  - Create authorization policy enforcement

## Dependency Updates

- [ ] **Clean up dependencies**
  - Remove outdated/unused dependencies
  - Update dependency versions to latest stable
  - Resolve any dependency conflicts

- [x] **Add Supabase SDK**
  - Add supabase-py to project dependencies
  - Configure Supabase client for auth (configuration pending)

## Frontend Integration

- [ ] **Create frontend authentication flow**
  - Implement API endpoints for frontend auth
  - Configure CORS for frontend access
  - Test authentication flow with frontend

- [ ] **Create API documentation for frontend**
  - Document authentication process
  - Create API endpoint reference
  - Add examples for common operations

## Deployment Pipeline

- [ ] **Decide on deployment strategy**
  - Evaluate GitHub Actions vs Cloud Build
  - Define deployment workflow stages
  - Determine environment strategy (dev/staging/prod)

- [ ] **Configure GitHub-based deployment**
  - Set up GitHub Actions workflow
  - Configure secrets and environment variables
  - Create deployment verification steps

- [ ] **Set up automated testing in CI**
  - Integrate testing into deployment pipeline
  - Add code quality checks
  - Create deployment approval process if needed

## Remaining Tasks

- [ ] **Conduct security audit**
  - Review authentication mechanisms
  - Check secure handling of credentials
  - Verify proper access controls

- [ ] **Update README.md**
  - Document new architecture
  - Add setup and usage instructions
  - Include development workflow
  - Update installation instructions to use pyproject.toml

- [ ] **Create architectural guides**
  - Document component relationships
  - Add diagrams explaining architecture
  - Create onboarding guide for new developers

================
File: api-memory-bank/api-overview.md
================
# Video Processing Pipeline - Clean Architecture Overview

This document provides a brief overview of the clean architecture approach and the example implementations provided in the `api/docs/examples` directory.

## Clean Architecture Principles

Our implementation follows the clean architecture principles introduced by Robert C. Martin, which emphasize:

1. **Separation of Concerns**: Clear boundaries between different layers of the application
2. **Dependency Rule**: Dependencies always point inward, with the domain at the center
3. **Abstraction**: High-level modules don't depend on low-level modules; both depend on abstractions
4. **Testability**: Business logic is isolated from external concerns for better testing
5. **Framework Independence**: Core business logic doesn't depend on frameworks or external services

## Architecture Layers

Our implementation divides the application into the following layers:

### 1. Domain Layer

The domain layer contains the core business entities, value objects, and business rules. It has no dependencies on other layers or external frameworks.

Key components:
- Domain entities (e.g., `Video`, `Job`)
- Value objects
- Domain exceptions

### 2. Application Layer

The application layer contains the use cases and business logic that orchestrates the domain entities. It defines interfaces that will be implemented by the adapters layer.

Key components:
- Application services (e.g., `TranscriptionService`, `VideoProcessorService`)
- Service interfaces (e.g., `StorageInterface`, `AIServiceInterface`)
- Data Transfer Objects (DTOs)

### 3. Adapters Layer

The adapters layer contains implementations of the interfaces defined in the application layer. These adapters translate between the application layer and external systems.

Key components:
- Storage adapters (e.g., `GCSStorageAdapter`)
- AI service adapters (e.g., `GeminiAIAdapter`)
- Publishing adapters (e.g., `YouTubeAdapter`)

### 4. Infrastructure Layer

The infrastructure layer contains framework-specific code, configurations, and other technical details.

Key components:
- API implementation (e.g., FastAPI or Flask routes)
- Dependency injection container
- Database repositories
- Configuration management

## Example Implementations

The `api/docs/examples` directory contains sample implementations of key components in the new architecture:

### 1. Application Interfaces

The `application/interfaces/storage.py` file demonstrates a clean interface definition for storage operations:

- Defines a clear contract through abstract methods
- Uses domain-specific exceptions
- Is independent of any specific storage provider
- Provides comprehensive documentation for implementers

### 2. Adapter Implementation

The `adapters/storage/gcs.py` file shows how to implement the storage interface for Google Cloud Storage:

- Implements all methods defined in the interface
- Handles GCS-specific concerns (like path formats, authentication)
- Translates between domain exceptions and provider-specific exceptions
- Uses asynchronous programming with thread pooling for non-blocking I/O

### 3. Dependency Injection

The `infrastructure/config/container.py` file demonstrates a flexible dependency injection approach:

- Provides a container for managing service instances
- Supports both singleton and transient lifetimes
- Allows easy swapping of implementations (e.g., for testing)
- Centralizes configuration of all application services

### 4. Application Entry Point

The `main.py` file shows how all components come together in the application entry point:

- Sets up the dependency injection container
- Configures services based on environment variables
- Provides Cloud Function handlers for event processing
- Includes a local development entry point for testing

## Benefits of This Approach

1. **Maintainability**: Smaller, focused components with clear responsibilities
2. **Testability**: Business logic can be tested without external dependencies
3. **Flexibility**: Implementation details can be changed without affecting core logic
4. **Scalability**: Components can be evolved or replaced independently
5. **Clarity**: Clear dependencies and service boundaries

## Implementation Strategy

To migrate the existing monolithic code to this architecture:

1. Start by defining domain models and interfaces
2. Implement adapters for existing external services
3. Refactor business logic into application services
4. Set up the dependency injection container
5. Update the main entry point to use the new architecture

This can be done incrementally, allowing the application to continue functioning during the migration.

## Further Reading

For more details about the implementation plan and file structures:

- `api/docs/api-prd.txt` - Product Requirements Document
- `api/docs/file-structure-comparison.md` - Current vs. Proposed File Structure
- `api/docs/implementation-tasks.md` - Detailed Implementation Tasks

================
File: api-memory-bank/api-prd.txt
================
# VIDEO PROCESSING PIPELINE - PRODUCT REQUIREMENTS DOCUMENT

## 1. Overview

This Product Requirements Document (PRD) outlines the specifications for developing a modern, scalable video processing pipeline system that automates the conversion of raw video content into publishable assets complete with AI-generated metadata. The system utilizes Google Cloud Platform services, particularly Cloud Run and Vertex AI, to process videos, extract audio, and generate high-quality transcripts, subtitles, chapters, titles, and other metadata to streamline the content publishing workflow.

## 2. Project goals

### 2.1 Primary goals

- Create a scalable, modular backend architecture for processing video files
- Implement AI-powered content generation for video metadata
- Develop a flexible system that supports multiple content channels and platforms
- Establish a maintainable codebase with modern Python design patterns
- Provide a robust API for frontend integration
- Enable automated deployment to Google Cloud Platform

### 2.2 Success metrics

- Processing time < 10 minutes for videos up to 30 minutes in length
- 95% accuracy in AI-generated transcripts (compared to manual transcription)
- 99.5% system availability
- 100% of videos successfully processed with complete metadata
- < 5% error rate in automated metadata generation
- Ability to process at least 50 concurrent video jobs

## 3. Current architecture

The project currently follows a partially modular architecture with several key components already implemented:

```
backend/
├── video_processor/               # Main application module
│   ├── core/                      # Core application logic
│   │   ├── models/                # Domain models
│   │   │   └── video_job.py       # VideoJob and ProcessingStage models
│   │   └── processors/            # Processing components
│   │       ├── audio.py           # Audio extraction and processing
│   │       ├── chapters.py        # Chapter generation
│   │       ├── transcript.py      # Transcript generation
│   │       └── video.py           # Video processing
│   ├── services/                  # External service integrations
│   │   ├── ai/                    # AI service integrations
│   │   ├── storage/               # Cloud storage services
│   │   └── youtube/               # YouTube API integration
│   ├── api/                       # API definitions
│   │   ├── controllers.py         # Request handlers
│   │   ├── routes.py              # API route definitions
│   │   └── schemas.py             # API data schemas
│   ├── utils/                     # Utility functions
│   ├── config/                    # Configuration management
│   ├── process_uploaded_video.py  # Main processing logic (monolithic)
│   ├── youtube_uploader.py        # YouTube uploading functionality
│   └── main.py                    # Entry point for cloud functions
```

The current implementation primarily uses a monolithic approach in `process_uploaded_video.py` (711 lines), which handles multiple responsibilities:
- Video and audio processing (`extract_audio()` function)
- AI content generation (`generate_transcript()`, `generate_vtt()`, etc.)
- GCS bucket management (`write_blob()`, `move_processed_file()`)
- Process orchestration (`process_video_event()`)

This monolithic approach lacks clear separation of concerns, making it difficult to maintain, test, and extend the codebase.

## 4. Proposed architecture

We propose refactoring the existing codebase to follow a more modern Python architecture using a proper hexagonal/clean architecture approach:

```
backend/
├── video_processor/
│   ├── domain/                      # Domain models and business logic
│   │   ├── models/                  # Domain entities
│   │   │   ├── video.py             # Video entity
│   │   │   ├── job.py               # Processing job entity
│   │   │   └── metadata.py          # Video metadata entity
│   │   ├── exceptions.py            # Domain-specific exceptions
│   │   └── value_objects.py         # Value objects for domain
│   ├── application/                 # Application services and use cases
│   │   ├── services/                # Application services
│   │   │   ├── video_processor.py   # Video processing orchestration
│   │   │   ├── transcription.py     # Transcript generation service
│   │   │   ├── subtitle.py          # Subtitle generation service
│   │   │   └── metadata.py          # Metadata generation service
│   │   ├── interfaces/              # Service interfaces
│   │   │   ├── storage.py           # Storage service interface
│   │   │   ├── ai.py                # AI service interface
│   │   │   └── publishing.py        # Publishing service interface
│   │   └── dtos/                    # Data Transfer Objects
│   │       └── video_job_dto.py     # DTOs for API communication
│   ├── adapters/                    # External service adapters
│   │   ├── ai/                      # AI service adapters
│   │   │   ├── vertex_ai.py         # Vertex AI implementation
│   │   │   └── gemini.py            # Gemini API implementation
│   │   ├── storage/                 # Storage adapters
│   │   │   ├── gcs.py               # Google Cloud Storage implementation
│   │   │   └── local.py             # Local filesystem implementation
│   │   └── publishing/              # Publishing adapters
│   │       └── youtube.py           # YouTube API implementation
│   ├── infrastructure/              # Framework-specific code
│   │   ├── config/                  # Configuration management
│   │   │   ├── settings.py          # Application settings
│   │   │   └── container.py         # Dependency injection container
│   │   ├── api/                     # API framework implementation
│   │   │   ├── server.py            # FastAPI server definition
│   │   │   ├── routes/              # API route handlers
│   │   │   │   ├── videos.py        # Video-related endpoints
│   │   │   │   └── health.py        # Health and status endpoints
│   │   │   ├── schemas/             # API schemas using Pydantic
│   │   │   │   └── video.py         # Video-related schemas
│   │   │   └── dependencies.py      # FastAPI dependencies
│   │   ├── repositories/            # Data repositories
│   │   │   ├── job_repository.py    # Job persistence
│   │   │   └── video_repository.py  # Video metadata persistence
│   │   └── messaging/               # Messaging infrastructure
│   │       └── pubsub.py            # Google Pub/Sub integration
│   ├── utils/                       # Utility functions and helpers
│   │   ├── logging.py               # Logging configuration
│   │   ├── ffmpeg.py                # FFmpeg wrapper (from current audio.py)
│   │   └── profiling.py             # Performance profiling tools
│   └── main.py                      # Application entry point
├── tests/                           # Test suite
│   ├── unit/                        # Unit tests
│   ├── integration/                 # Integration tests
│   └── e2e/                         # End-to-end tests
└── scripts/                         # Utility scripts for development/deployment
```

## 5. Key features & requirements

### 5.1 Video processing pipeline

**REQ-VP-001: Video upload and storage**
- System must accept video uploads from GCS bucket triggers
- Reuse existing `bucket_name` and `file_name` handling from `process_uploaded_video.py`
- Refactor `should_process_file()` and `setup_output_paths()` into domain services

**REQ-VP-002: Audio extraction**
- Extract audio from video files for AI analysis
- Maintain existing functionality from `extract_audio()` in `process_uploaded_video.py`
- Refactor to follow adapter pattern in `utils/ffmpeg.py`

**REQ-VP-003: Transcript generation**
- Generate high-quality transcripts from audio
- Migrate logic from `generate_transcript()` to dedicated service
- Implement as adapter in `adapters/ai/gemini.py`

**REQ-VP-004: Subtitle generation**
- Create WebVTT format subtitles for videos
- Migrate logic from `generate_vtt()` to dedicated service
- Implement in `application/services/subtitle.py`

**REQ-VP-005: Content summarization**
- Generate show notes based on video content
- Migrate logic from `generate_shownotes()` to dedicated service
- Use dependency injection to allow different AI providers

**REQ-VP-006: Chapter generation**
- Create timestamped chapters for video content
- Migrate logic from `generate_chapters()` to dedicated service
- Use existing domain model structure from `core/processors/chapters.py`

**REQ-VP-007: Title suggestion**
- Generate optimized title variations for video
- Migrate logic from `generate_titles()` to dedicated service
- Implement in `application/services/metadata.py`

**REQ-VP-008: YouTube integration**
- Support publishing processed videos to YouTube
- Maintain functionality from `youtube_uploader.py`
- Refactor to adapter pattern in `adapters/publishing/youtube.py`

**REQ-VP-009: Processing job management**
- Track status of video processing through workflow
- Expand current `VideoJob` model from `core/models/video_job.py`
- Implement repository pattern for job persistence

### 5.2 API requirements

**REQ-API-001: Video processing API**
- API endpoint for triggering video processing
- Refactor existing API implementation in `api/routes.py` and `api/controllers.py`
- Migrate to FastAPI framework in `infrastructure/api/server.py`

**REQ-API-002: Job status API**
- Endpoint for checking video processing status
- Create new endpoint based on current job tracking
- Implement in `infrastructure/api/routes/videos.py`

**REQ-API-003: API documentation**
- OpenAPI documentation for all endpoints
- Automatic generation via FastAPI
- Example requests and responses

**REQ-API-004: Authentication & authorization**
- Secure API endpoints with authentication
- Token-based authentication for frontend access
- Role-based access control for administrative functions

**REQ-API-005: Health check endpoints**
- Service health monitoring endpoints
- Connectivity checks for dependent services
- Version information endpoint

### 5.3 Technical requirements

**REQ-TECH-001: Clean architecture**
- Refactor existing code following clean architecture principles
- Separate domain logic from infrastructure concerns
- Implement proper dependency injection

**REQ-TECH-002: Testing strategy**
- Comprehensive unit test coverage for domain and application layers
- Integration tests for adapters and infrastructure
- End-to-end tests for critical flows
- Maintain test fixtures in line with current `conftest.py`

**REQ-TECH-003: Containerization**
- Docker container configuration for local testing
- Utilize and expand existing `Dockerfile` and `docker-compose.yml`
- Multi-stage build for efficient deployment

**REQ-TECH-004: Configuration management**
- Environment-based configuration
- Secrets management using GCP Secret Manager
- Feature flags for progressive rollout

**REQ-TECH-005: Performance optimization**
- Benchmark processing stages
- Implement performance profiling
- Optimize AI service usage

**REQ-TECH-006: Monitoring & observability**
- Structured logging throughout application
- Performance metric collection
- Error reporting and alerting

**REQ-TECH-007: Continuous integration/deployment**
- Automated testing pipeline
- Deployment to GCP Cloud Run
- Infrastructure as code for deployment

### 5.4 Deployment requirements

**REQ-DEPLOY-001: Google Cloud Run deployment**
- Deploy as containerized service on Cloud Run
- Optimize existing `deploy.sh` script (268 lines)
- Configure service accounts and permissions

**REQ-DEPLOY-002: Cloud functions integration**
- Maintain Cloud Function triggers for GCS events
- Refactor `main.py` event handlers
- Optimize cold start performance

**REQ-DEPLOY-003: Scaling configuration**
- Configure auto-scaling parameters
- Resource allocation optimization
- Concurrency limits

**REQ-DEPLOY-004: Monitoring setup**
- Configure Cloud Monitoring
- Set up logging and alerting
- Performance dashboards

**REQ-DEPLOY-005: Cost optimization**
- Optimize resource usage
- Implement caching where appropriate
- Configure budget alerts

## 6. User stories

### 6.1 Content creator stories

**US-001: Video upload processing**
- As a content creator, I want to upload a video and have it automatically processed, so I can quickly publish it with minimal manual effort.
- *Acceptance criteria:*
  - Video is processed within 10 minutes of upload
  - Creator receives notification when processing is complete
  - All metadata (transcript, subtitles, etc.) is generated automatically
  - Implementation should refactor `process_video_event()` from `process_uploaded_video.py`

**US-002: Processing status tracking**
- As a content creator, I want to see the current status of my video processing, so I know when it will be ready.
- *Acceptance criteria:*
  - Status updates for each processing stage
  - Estimated completion time
  - Error notifications if processing fails
  - Progress indication for long-running processes
  - Leverage `ProcessingStage` and `ProcessingStatus` from `core/models/video_job.py`

**US-003: Metadata review and editing**
- As a content creator, I want to review and edit generated metadata before publishing, so I can ensure quality and accuracy.
- *Acceptance criteria:*
  - Interface to view all generated metadata
  - Editing capabilities for transcript, title, description
  - Preview of how metadata will appear on platforms
  - Changes are saved and used for publishing

**US-004: Platform-specific publishing**
- As a content creator, I want to publish my processed video to multiple platforms with optimized metadata, so I can maximize my audience reach.
- *Acceptance criteria:*
  - One-click publishing to YouTube
  - Platform-specific metadata optimization
  - Scheduling options for delayed publishing
  - Publishing status and confirmation
  - Extend existing YouTube integration in `youtube_uploader.py`

**US-005: Processing customization**
- As a content creator, I want to customize which processing steps are applied to my video, so I can control the output based on my needs.
- *Acceptance criteria:*
  - Options to enable/disable specific processing steps
  - Presets for common combinations of processing steps
  - Custom parameters for specific processors (e.g., subtitle style)
  - Settings are remembered for future uploads

### 6.2 Administrator stories

**US-006: System monitoring**
- As an administrator, I want to monitor the overall system performance and processing queue, so I can ensure the service is running optimally.
- *Acceptance criteria:*
  - Dashboard showing current processing queue
  - Resource utilization metrics
  - Error rate and common failure points
  - Throughput and processing time statistics

**US-007: Service configuration**
- As an administrator, I want to configure system parameters and resource allocation, so I can optimize performance and costs.
- *Acceptance criteria:*
  - Configuration interface for system parameters
  - Ability to adjust resource limits
  - Cost estimation based on configuration
  - Changes take effect without service restart

**US-008: User management**
- As an administrator, I want to manage user accounts and permissions, so I can control access to the system.
- *Acceptance criteria:*
  - User creation, deletion, and editing
  - Role assignment
  - Access control for sensitive operations
  - Audit log of administrative actions

**US-009: Error investigation**
- As an administrator, I want to investigate processing errors, so I can resolve issues and improve system reliability.
- *Acceptance criteria:*
  - Detailed error logs with context
  - Ability to reprocess failed videos
  - Trend analysis of common errors
  - Integration with monitoring tools

### 6.3 Developer stories

**US-010: API integration**
- As a developer, I want to integrate with the video processing API, so I can use its capabilities in other applications.
- *Acceptance criteria:*
  - Comprehensive API documentation
  - Authentication mechanism for API access
  - Examples and SDKs for common languages
  - Rate limiting and usage metrics

**US-011: Custom processor development**
- As a developer, I want to create custom processors for the pipeline, so I can extend its functionality for specific needs.
- *Acceptance criteria:*
  - Documentation for processor interface
  - Example processors to use as templates
  - Testing framework for custom processors
  - Deployment process for new processors

**US-012: Webhook integration**
- As a developer, I want to configure webhooks for processing events, so I can trigger actions in external systems.
- *Acceptance criteria:*
  - Configurable webhook endpoints for different event types
  - Event payload documentation
  - Retry mechanism for failed webhook deliveries
  - Webhook delivery logs

## 7. Technical architecture details

### 7.1 Domain model

**Video Job Entity**
- Currently implemented in `core/models/video_job.py`
- Will be refactored to `domain/models/job.py`
- Core attributes:
  - Job ID (unique identifier)
  - Status (pending, in-progress, completed, failed)
  - Current processing stage
  - Completed stages
  - Created/updated timestamps
  - Source video reference
  - Output references
  - Error information

**Video Entity**
- Currently partially implemented in processing logic
- Will be defined in `domain/models/video.py`
- Core attributes:
  - Video ID
  - File information (name, size, format)
  - Duration
  - Resolution
  - Source location
  - Processed state

**Video Metadata Entity**
- Currently implemented as `VideoMetadata` in `core/models/video_job.py`
- Will be refactored to `domain/models/metadata.py`
- Core attributes:
  - Title
  - Description
  - Keywords/tags
  - Chapters
  - Transcript
  - Subtitles
  - Thumbnails
  - Platform-specific metadata

### 7.2 Use cases & application services

**Video Processing Service**
- Orchestrates the processing workflow
- Manages state transitions for video jobs
- Coordinates between different processing services
- Will expand `process_video_event()` from `process_uploaded_video.py`

**Transcription Service**
- Handles audio analysis and transcript generation
- Refactors `generate_transcript()` from `process_uploaded_video.py`
- Interfaces with AI services

**Subtitle Generation Service**
- Creates timestamped subtitles from audio
- Refactors `generate_vtt()` from `process_uploaded_video.py`
- Formats output for different platforms

**Metadata Generation Service**
- Creates various metadata artifacts (titles, descriptions, etc.)
- Refactors `generate_titles()` and other generation functions
- Optimizes metadata for different platforms

**Publishing Service**
- Handles video publishing to platforms
- Will refactor logic from `youtube_uploader.py`
- Manages platform-specific requirements

### 7.3 Adapters & infrastructure

**AI Service Adapters**
- Abstract interface in `application/interfaces/ai.py`
- Vertex AI implementation in `adapters/ai/vertex_ai.py`
- Gemini implementation in `adapters/ai/gemini.py`
- Mock implementation for testing

**Storage Adapters**
- Abstract interface in `application/interfaces/storage.py`
- GCS implementation in `adapters/storage/gcs.py`
- Local filesystem implementation for testing
- Refactors storage interactions from `process_uploaded_video.py`

**Publishing Adapters**
- Abstract interface in `application/interfaces/publishing.py`
- YouTube implementation in `adapters/publishing/youtube.py`
- Refactors from existing `youtube_uploader.py`

**API Infrastructure**
- FastAPI server configuration
- Route handlers for all endpoints
- Request/response schemas using Pydantic
- Dependency injection setup

### 7.4 Dependency injection

**Container Configuration**
- Will be implemented in `infrastructure/config/container.py`
- Manages application service instantiation
- Configures service dependencies
- Allows swapping implementations based on environment

**Service Locator Pattern**
- Provides access to services throughout application
- Simplifies testing through mock injection
- Enforces proper separation of concerns

### 7.5 Testing strategy

**Unit Testing**
- Domain models and business logic
- Application services with mocked dependencies
- Expand existing tests in `test_process_video.py` and `test_audio_processing.py`

**Integration Testing**
- Storage adapters against emulators/test environments
- AI service adapters with simplified models
- API endpoints with test client

**End-to-End Testing**
- Complete processing workflow
- Cloud Function triggers
- Video processing and publishing

### 7.6 Benchmarking and profiling

**Performance Metrics**
- Processing time per stage
- Memory utilization
- API response times
- AI service latency

**Profiling Tools**
- Code instrumentation for timing
- Resource usage tracking
- Bottleneck identification

### 7.7 Deployment architecture

**GCP Services**
- Cloud Run for API and processing service
- Cloud Storage for video files and artifacts
- Cloud Functions for event triggers
- Pub/Sub for asynchronous communication
- Secret Manager for credentials
- Logging and Monitoring

## 8. Implementation phases

### 8.1 Phase 1: Refactor core architecture

**Sprint 1: Domain model and structure setup**
- Establish new project structure
- Refactor domain models from existing code
- Set up dependency injection container
- Implement basic testing framework

**Sprint 2: Service layer implementation**
- Refactor processing services from monolithic code
- Implement storage adapters
- Create AI service adapters
- Unit test coverage for core services

### 8.2 Phase 2: API and infrastructure

**Sprint 3: API implementation**
- Implement FastAPI server
- Create API schemas and routes
- Add authentication and authorization
- Develop API documentation

**Sprint 4: Deployment and monitoring**
- Configure Docker and Cloud Run deployment
- Set up monitoring and logging
- Implement health checks
- Create deployment automation

### 8.3 Phase 3: Enhanced functionality

**Sprint 5: Advanced metadata generation**
- Enhance AI-powered metadata generation
- Implement feedback loop for quality improvement
- Add customization options
- Optimize AI prompt engineering

**Sprint 6: Multi-platform publishing**
- Extend YouTube integration
- Add support for additional platforms
- Implement platform-specific optimizations
- Create publishing queue and scheduling

### 8.4 Phase 4: Frontend integration and polish

**Sprint 7: Frontend API integration**
- Finalize API contracts with frontend
- Implement real-time status updates
- Create comprehensive API examples
- Develop client SDK

**Sprint 8: Performance optimization and scaling**
- Optimize processing performance
- Configure auto-scaling
- Implement caching strategies
- Conduct load testing

## 9. Metrics and analytics

### 9.1 Key performance indicators (KPIs)

- Video processing throughput (videos/hour)
- Average processing time per video
- AI generation quality scores
- API response times
- Error rates by processing stage
- System availability percentage

### 9.2 User analytics

- Processing volume by user
- Feature usage statistics
- Platform publishing distribution
- User retention and engagement

### 9.3 Cost analytics

- Processing cost per video
- AI service usage and cost
- Storage utilization and cost
- Infrastructure cost breakdown

## 10. Migration plan

### 10.1 Current code migration

- The existing code in `backend/video_processor/process_uploaded_video.py` (711 lines) will be refactored into:
  - Domain models in `domain/models/`
  - Application services in `application/services/`
  - Adapters in `adapters/`
  
- The YouTube uploader code in `backend/video_processor/youtube_uploader.py` (535 lines) will be refactored into:
  - YouTube adapter in `adapters/publishing/youtube.py`
  - Publishing service in `application/services/publishing.py`
  
- The API code in `backend/video_processor/api/` will be migrated to FastAPI in `infrastructure/api/`

### 10.2 Database migration

- Any existing data will be migrated to the new structure
- Backward compatibility maintained during transition
- Data validation and cleanup during migration

### 10.3 Testing and verification

- Parallel running of old and new systems
- Comparison of outputs for validation
- Performance benchmarking comparison
- Gradual traffic migration

## 11. Risks and mitigations

**Risk: AI service reliability**
- *Impact:* Processing failures, poor quality metadata
- *Mitigation:* Fallback providers, retry mechanisms, quality monitoring

**Risk: Processing performance issues**
- *Impact:* Long processing times, capacity limitations
- *Mitigation:* Performance profiling, optimization, scaling configuration

**Risk: GCP service limits**
- *Impact:* Processing throttling, increased costs
- *Mitigation:* Quotas monitoring, graceful degradation, cost alerts

**Risk: API compatibility issues**
- *Impact:* Frontend integration failures
- *Mitigation:* Versioned API, comprehensive testing, backward compatibility

**Risk: Security vulnerabilities**
- *Impact:* Unauthorized access, data breaches
- *Mitigation:* Security review, access controls, regular updates

## 12. Glossary

**AI service:** Cloud-based artificial intelligence services used for content analysis and generation.

**Adapter:** A software component that translates between core business logic and external services.

**Clean architecture:** A software design philosophy that separates concerns into layers with dependencies pointing inward.

**Dependency injection:** A technique where object dependencies are provided from outside rather than created internally.

**Domain model:** Core business objects and logic independent of external concerns.

**FastAPI:** A modern, high-performance web framework for building APIs with Python based on standard type hints.

**GCP:** Google Cloud Platform, the cloud infrastructure provider for the system.

**Hexagonal architecture:** An architectural pattern that allows an application to be driven by users, programs, or tests equally.

**Processing stage:** A discrete step in the video processing pipeline.

**Repository pattern:** A design pattern that mediates between the domain and data mapping layers.

**Vertex AI:** Google Cloud's unified machine learning platform.

**WebVTT:** Web Video Text Tracks format, a subtitle format for HTML5 video.

================
File: api-memory-bank/api-product-context.md
================
# Product Context: Automations

## Problem Statement
Content creators, marketers, and businesses face significant challenges in producing high-quality video content efficiently. The current process is:
- Time-consuming: Manual editing and processing is labor-intensive
- Expensive: Requires specialized skills or outsourcing
- Inconsistent: Quality varies based on available resources
- Limited in scale: Difficult to produce large volumes of content

## Solution
Automations provides an end-to-end platform that streamlines and automates the video content creation process by:
- Automatically processing raw video footage
- Using AI to identify the most engaging segments
- Applying professional editing techniques programmatically
- Enabling one-click publishing to multiple platforms

## Target Users
1. **Content Creators**: YouTubers, streamers, and social media influencers
2. **Marketing Teams**: Businesses creating product demos and promotional content
3. **Educational Institutions**: Creating learning materials and presentations
4. **Media Companies**: Processing large volumes of video content

## User Experience Goals
- **Simplicity**: Intuitive interfaces that require minimal technical knowledge
- **Speed**: Significantly reduce time from raw footage to published content
- **Quality**: AI-assisted processing that enhances rather than diminishes quality
- **Control**: Appropriate balance between automation and user customization
- **Integration**: Seamless connection with existing tools and platforms

## Key Workflows
1. **Video Upload & Processing**
   - User uploads raw video footage
   - System automatically processes the content
   - AI identifies key segments and generates metadata

2. **Content Enhancement**
   - System suggests edits and enhancements
   - User can approve or modify suggestions
   - Final content is prepared for publishing

3. **Publishing & Distribution**
   - Content is formatted appropriately for target platforms
   - System handles the publishing process
   - Analytics are collected and presented to the user

## Business Model
- Subscription-based service with tiered pricing
- Usage limits based on processing time and storage
- Premium features for advanced customization and integration

## Success Metrics
- Reduction in time spent on video production
- Increase in content output per user
- Quality ratings of automated vs. manual processing
- User retention and subscription renewal rates

================
File: api-memory-bank/api-progress.md
================
# Progress Status: Backend

## What Works

### Infrastructure
- ✅ Modern project structure with pyproject.toml
- ✅ Pre-commit hooks for code quality
- ✅ Testing framework configuration
- ✅ FastAPI application setup

### Core Components
- ✅ Complete domain model implementation
- ✅ Application service interfaces and implementations
- ✅ Storage, AI, and publishing adapters
- ✅ Repository implementations
- ✅ Data transfer objects

### APIs
- ✅ FastAPI endpoint implementation
- ✅ API schemas with Pydantic
- ✅ Route handlers and dependencies
- ✅ Swagger UI documentation
- ✅ Health check endpoints

### Services
- ✅ Video processor service
- ✅ Transcription service
- ✅ Subtitle generation
- ✅ Metadata generation

## In Progress

### Testing and Validation
- 🔄 End-to-end validation of clean architecture
- 🔄 Performance testing of the new implementation
- 🔄 Security review and improvements

### Documentation
- 🔄 README.md updates for the new architecture
- 🔄 Architectural guides and diagrams
- 🔄 Developer onboarding documentation

### Final Components
- 🔄 Monitoring and observability enhancements
- 🔄 Extended error handling and recovery

## Not Started

### Advanced Features
- ❌ Advanced video processing algorithms
- ❌ Real-time video analysis
- ❌ Custom AI model training
- ❌ Video editing capabilities

### Integration Points
- ❌ Additional publishing platform integrations
- ❌ Advanced analytics and reporting
- ❌ Enhanced notification system

### DevOps
- ❌ Enhanced CI/CD pipeline
- ❌ Production deployment monitoring
- ❌ Advanced performance benchmarking

## Current Status

### Overall Progress
- **Project Phase**: Implementation Complete, Validation in Progress
- **Estimated Completion**: 85%
- **Key Milestone**: Clean architecture implementation and legacy code removal

### Current Status
The project has successfully implemented the new clean architecture, with all major components completed. Legacy monolithic code has been removed, and the codebase now follows domain-driven design principles with proper separation of concerns. The focus is now on testing and validating the new architecture, completing documentation, and preparing for production deployment.

### Recent Achievements
1. Successfully removed legacy monolithic components
2. Completed implementation of all architectural layers
3. Implemented FastAPI endpoints with proper schema validation
4. Created comprehensive adapter implementations for external services

### Known Issues
1. Documentation needs to be updated to reflect the new architecture
2. Security audit is pending
3. Additional testing needed to ensure complete functionality

## Key Decisions & Evolution

### Architectural Evolution
- Transitioned from monolithic code to clean architecture with four distinct layers
- Moved from Flask to FastAPI for improved performance and documentation
- Implemented proper dependency injection for service management
- Adopted domain-driven design for better business logic representation

### Technology Updates
- Shifted to pyproject.toml from setup.py for modern Python packaging
- Adopted FastAPI over Flask for API implementation
- Implemented comprehensive test structure aligned with architecture
- Created modular adapters for all external services

### Future Considerations
- Potential to further optimize video processing performance
- May enhance monitoring and observability
- Considering additional publishing integrations
- Might explore advanced AI capabilities for content analysis

================
File: api-memory-bank/api-project-brief.md
================
# Project Brief: Automations Backend

## Project Overview
This project serves as the backend for the Automations application, which focuses on automating video content creation and processing workflows. The backend provides API endpoints, processing capabilities, and integrations with various services.

## Core Requirements

1. **Video Processing**
   - Automated processing of video content
   - Extraction of key segments and metadata
   - Handling of various video formats and resolutions

2. **AI Integration**
   - Integration with AI services for content analysis
   - Smart recommendations and content enhancement
   - Automated tagging and categorization

3. **API Endpoints**
   - RESTful API for frontend communication
   - Authentication and authorization
   - Efficient data transfer and streaming capabilities

4. **Storage Solutions**
   - Secure video storage
   - Metadata management
   - Efficient retrieval systems

5. **Scalability**
   - Handle concurrent processing requests
   - Scale up/down based on demand
   - Efficient resource utilization

## Technical Goals

1. **Maintainability**
   - Clean code architecture with clear separation of concerns
   - Comprehensive testing with high coverage
   - Well-documented codebase with consistent patterns

2. **Performance**
   - Optimized video processing algorithms
   - Efficient database queries
   - Minimal latency for API responses

3. **Security**
   - Secure API endpoints with proper authentication
   - Data encryption for sensitive information
   - Compliance with relevant data protection regulations

4. **Extensibility**
   - Modular design to easily add new features
   - Well-defined interfaces for new integrations
   - Plugin architecture for custom processors

## Success Criteria
- Successfully process videos of various formats
- Provide accurate AI-based analysis and recommendations
- Maintain API response times within acceptable thresholds
- Scale to handle projected user load
- Secure handling of user data and content

================
File: api-memory-bank/api-system-patterns.md
================
# System Patterns: Backend Architecture

## Architecture Overview
The backend follows a clean architecture approach with distinct layers providing clear separation of concerns:

```
├── Domain Layer (Core Business Logic)
├── Application Layer (Use Cases, Services)
├── Adapters Layer (Implementation Details)
└── Infrastructure Layer (External Systems, Config)
```

## Design Patterns

### Clean Architecture
- **Domain**: Contains core business models and logic with no external dependencies
- **Application**: Orchestrates use cases and defines service interfaces
- **Adapters**: Implements interfaces defined by the application layer
- **Infrastructure**: Provides technical capabilities and external system integrations

### Repository Pattern
- Abstracts data storage operations behind domain-focused interfaces
- Provides domain-centric methods for data access
- Decouples business logic from data access implementation details

### Dependency Injection
- Interfaces defined in inner layers (application)
- Implementations provided by outer layers (adapters, infrastructure)
- Container configuration in infrastructure layer
- Enables easier testing and implementation flexibility

### Adapter Pattern
- Used for external service integration (storage, AI, publishing)
- Isolates external dependencies from core business logic
- Provides consistent interfaces for variable implementations

### Domain-Driven Design
- Rich domain models with business logic
- Value objects for immutable concepts
- Domain events for cross-boundary communication
- Bounded contexts for logical separation

## Component Relationships

### Video Processing Flow
```
Upload → Validation → Storage → Processor Service → 
Transcription Service → AI Analysis → Metadata Generation → 
Storage → Publishing Adapter → Notification
```

### API Request Flow
```
API Request → Authentication/Authorization → 
FastAPI Router → Dependency Injection → 
Application Service → Repository/Adapters → 
Domain Logic → Response Mapping → API Response
```

### Data Flow
```
Client → API Gateway → Service Layer → 
Adapter/Repository Layer → External Services → 
Adapter/Repository → Service Layer → Client
```

## Implementation Details

### Module Structure
- Vertically sliced by clean architecture layers
- Each layer contains components for all domain concepts
- Cross-cutting concerns handled through infrastructure layer

### Error Handling
- Domain-specific exceptions defined in domain layer
- Consistent error response format through FastAPI
- Error translation in adapter implementations
- Structured logging throughout the application

### Validation
- Input validation via Pydantic schemas at API boundaries
- Domain validation within entity methods
- Output validation before responses
- Consistent validation errors

## Technical Implementation

### Directory Structure
```
api/
├── video_processor/
│   ├── domain/
│   │   ├── models/
│   │   │   ├── video.py
│   │   │   ├── job.py
│   │   │   └── metadata.py
│   │   ├── exceptions.py
│   │   └── value_objects.py
│   ├── application/
│   │   ├── interfaces/
│   │   │   ├── storage.py
│   │   │   ├── ai.py
│   │   │   └── publishing.py
│   │   ├── dtos/
│   │   │   ├── video_dto.py 
│   │   │   └── job_dto.py
│   │   └── services/
│   │       ├── video_processor.py
│   │       ├── transcription.py
│   │       ├── subtitle.py
│   │       └── metadata.py
│   ├── adapters/
│   │   ├── ai/
│   │   │   ├── gemini.py
│   │   │   └── vertex_ai.py
│   │   ├── storage/
│   │   │   ├── gcs.py
│   │   │   └── local.py
│   │   └── publishing/
│   │       └── youtube.py
│   ├── infrastructure/
│   │   ├── api/
│   │   │   ├── routes/
│   │   │   │   ├── videos.py
│   │   │   │   └── health.py
│   │   │   ├── schemas/
│   │   │   │   └── video.py
│   │   │   ├── dependencies.py
│   │   │   └── server.py
│   │   ├── config/
│   │   │   ├── settings.py
│   │   │   └── container.py
│   │   ├── repositories/
│   │   │   ├── job_repository.py
│   │   │   └── video_repository.py
│   │   └── messaging/
│   │       └── pubsub.py
│   ├── utils/
│   │   ├── ffmpeg.py
│   │   ├── logging.py
│   │   └── profiling.py
│   ├── __init__.py
│   └── main.py
├── tests/
│   ├── unit/
│   │   ├── domain/
│   │   ├── application/
│   │   └── adapters/
│   ├── integration/
│   │   ├── api/
│   │   ├── storage/
│   │   └── ai/
│   └── e2e/
├── scripts/
├── docs/
├── pyproject.toml
└── Dockerfile
```

### Key Implementation Paths

1. **Video Processing Flow**:
   ```
   API Request → FastAPI Router → VideoController → 
   VideoProcessorService → StorageAdapter (upload) → 
   TranscriptionService → AIAdapter (analysis) → 
   MetadataService → StorageAdapter (metadata) → 
   YouTubeAdapter (publish) → Response
   ```

2. **Job Management Flow**:
   ```
   Job Creation → JobRepository → ProcessingQueue → 
   VideoProcessorService → JobStatusUpdate → 
   JobRepository → Notification
   ```

3. **API Integration Flow**:
   ```
   Client Request → Authentication → FastAPI Router → 
   Dependencies (DI Container) → Application Service → 
   Response Serialization → Client Response
   ```

================
File: api-memory-bank/api-tech-context.md
================
# Technical Context: Backend

## Core Technologies

### Programming Languages
- **Python 3.12+**: Primary backend language
- **TypeScript/JavaScript**: Used for cloud functions and some utilities

### Frameworks & Libraries
- **FastAPI**: Modern web framework for API endpoints
- **Pydantic**: Data validation and schema definition
- **Pytest**: Testing framework
- **FFmpeg**: Video processing and transformation
- **Dependency Injector**: For dependency management

### Infrastructure & Cloud Services
- **Google Cloud Platform**
  - Cloud Storage: For video and asset storage
  - Cloud Functions: For serverless processing
  - Vertex AI and Gemini AI: For AI/ML capabilities
  - Secret Manager: For managing secrets and credentials
  - Pub/Sub: For event-driven messaging
- **Docker**: For containerization and consistent environments

### Data Storage
- **Firestore**: For job and metadata persistence
- **Google Cloud Storage**: Object storage for video files and assets

### AI & ML
- **Google Gemini AI**: Primary AI model for content generation
- **Google Vertex AI**: Alternative AI platform for specialized analysis

## Development Environment

### Local Setup
- Python 3.12+ with virtual environment
- Docker for containerized services
- FFmpeg for local video processing
- Google Cloud SDK for cloud interaction
- Environment variables via .env files
- Pre-commit hooks for code quality

### Preferred Tools
- Visual Studio Code with Python extensions
- Postman or Swagger UI for API testing
- Docker Desktop for container management

### Environment Variables
Required in `.env` file:
- `GOOGLE_CLOUD_PROJECT`: GCP project ID
- `GOOGLE_APPLICATION_CREDENTIALS`: Path to service account key
- `STORAGE_BUCKET`: Main storage bucket name
- `AI_API_KEY`: API key for AI services
- `YOUTUBE_CLIENT_SECRET`: For YouTube publishing
- `FASTAPI_ENV`: Development or production environment

## Deployment & CI/CD

### Environments
- **Development**: For active development and testing
- **Staging**: For pre-production validation
- **Production**: Live environment

### CI/CD Pipelines
- **GitHub Actions**: For automated testing and deployment
- **Docker**: For consistent build and deployment artifacts
- **Cloud Build**: For GCP-native CI/CD pipelines

### Deployment Process
1. Code pushed to GitHub repository
2. Automated tests run in CI environment
3. Docker image built using multi-stage approach
4. Image pushed to Container Registry
5. Deployment to Cloud Run service
6. Post-deployment verification

## Technical Constraints

### Performance Requirements
- Video processing within 2x real-time duration
- API response times < 200ms for non-processing endpoints
- Support for videos up to 4K resolution

### Security Requirements
- HTTPS for all endpoints
- JWT-based authentication
- Role-based access control
- Encryption for sensitive data at rest and in transit
- Secret Manager for credential management

### Scalability Goals
- Support concurrent processing of multiple videos
- Handle peak loads during high-traffic periods
- Automatic scaling based on demand
- Clean architectural separation for independent scaling of components

================
File: tests/e2e/__init__.py
================
"""
End-to-end tests for the complete application flow.
"""

================
File: tests/e2e/test_deployment.py
================
"""End-to-end tests for verifying deployment."""
import os
import pytest
import requests
from typing import Optional


@pytest.fixture
def api_base_url() -> str:
    """Get the base URL of the deployed API from environment variable."""
    # Use environment variable or default to local development URL
    base_url = os.environ.get("API_BASE_URL", "http://localhost:8080")
    # Ensure URL doesn't end with a slash
    return base_url.rstrip("/")


@pytest.fixture
def api_key() -> Optional[str]:
    """Get API key for authentication from environment variable."""
    return os.environ.get("API_KEY")


@pytest.mark.e2e
class TestDeployment:
    """End-to-end tests for verifying deployment."""
    
    def test_health_endpoint(self, api_base_url):
        """Test that the health endpoint is responding."""
        response = requests.get(f"{api_base_url}/health")
        
        assert response.status_code == 200
        assert response.json()["status"] == "ok"
    
    def test_detailed_health_endpoint(self, api_base_url):
        """Test that the detailed health endpoint is responding."""
        response = requests.get(f"{api_base_url}/health/detailed")
        
        assert response.status_code == 200
        data = response.json()
        
        assert "status" in data
        assert "components" in data
        assert "version" in data
        
        # Check for expected components
        components = data["components"]
        assert "storage" in components
        assert "ai" in components
        assert "database" in components
    
    def test_api_documentation(self, api_base_url):
        """Test that the API documentation is accessible."""
        response = requests.get(f"{api_base_url}/docs")
        
        assert response.status_code == 200
        assert "text/html" in response.headers["Content-Type"]
    
    def test_openapi_schema(self, api_base_url):
        """Test that the OpenAPI schema is accessible."""
        response = requests.get(f"{api_base_url}/openapi.json")
        
        assert response.status_code == 200
        schema = response.json()
        
        assert "openapi" in schema
        assert "paths" in schema
        assert "components" in schema
        assert "/videos" in schema["paths"]
    
    @pytest.mark.skipif(not os.environ.get("API_KEY"), reason="API_KEY not set")
    def test_api_key_authentication(self, api_base_url, api_key):
        """Test that API key authentication is working."""
        # Attempt to access protected endpoint without API key
        response_without_key = requests.get(f"{api_base_url}/videos")
        assert response_without_key.status_code in (401, 403)
        
        # Attempt to access protected endpoint with API key
        headers = {"Authorization": f"Bearer {api_key}"}
        response_with_key = requests.get(f"{api_base_url}/videos", headers=headers)
        
        # Should get 200 or 204 if empty list
        assert response_with_key.status_code in (200, 204)
    
    @pytest.mark.skipif(not os.environ.get("API_KEY"), reason="API_KEY not set")
    def test_video_lifecycle(self, api_base_url, api_key):
        """Test basic video processing lifecycle.
        
        This test submits a video processing job and verifies that the API
        responds correctly at each stage. It doesn't test the full processing
        pipeline, just that the API endpoints work.
        """
        headers = {"Authorization": f"Bearer {api_key}"}
        
        # 1. Get pre-signed upload URL
        response = requests.post(
            f"{api_base_url}/videos/upload-url",
            json={"filename": "test_video.mp4"},
            headers=headers
        )
        assert response.status_code == 200
        upload_data = response.json()
        assert "upload_url" in upload_data
        assert "job_id" in upload_data
        
        job_id = upload_data["job_id"]
        
        # 2. Check job status (should be PENDING or similar)
        response = requests.get(
            f"{api_base_url}/videos/job/{job_id}",
            headers=headers
        )
        assert response.status_code == 200
        job_data = response.json()
        assert "status" in job_data
        assert job_data["status"] in ("PENDING", "UPLOADED", "PROCESSING", "FAILED", "COMPLETED")
        
        # 3. Verify job appears in jobs list
        response = requests.get(
            f"{api_base_url}/videos/jobs",
            headers=headers
        )
        assert response.status_code == 200
        jobs_data = response.json()
        assert isinstance(jobs_data, list)
        
        # Find our job in the list
        job_found = False
        for job in jobs_data:
            if job.get("id") == job_id:
                job_found = True
                break
        
        assert job_found, f"Job {job_id} not found in jobs list"

================
File: tests/e2e/test_video_processing.py
================
"""
End-to-end tests for the video processing pipeline.
"""

import os
import tempfile
import uuid

import pytest
from video_processor.adapters.storage.local import LocalStorageAdapter
from video_processor.application.services.video_processor import VideoProcessorService
from video_processor.domain.models.job import VideoJob
from video_processor.domain.models.video import Video

from tests.mocks.ai import MockAIAdapter


@pytest.fixture
def sample_video_path():
    """Create a sample video file for testing."""
    # In a real test, we would use a real video file
    # For this example, we'll create a dummy file
    with tempfile.NamedTemporaryFile(suffix=".mp4", delete=False) as temp:
        temp.write(b"dummy video content")
        temp_path = temp.name

    yield temp_path

    # Clean up
    if os.path.exists(temp_path):
        os.remove(temp_path)


@pytest.fixture
def output_dir():
    """Create a temporary output directory."""
    temp_dir = tempfile.mkdtemp()
    yield temp_dir

    # Clean up
    if os.path.exists(temp_dir):
        import shutil

        shutil.rmtree(temp_dir)


@pytest.mark.e2e
def test_end_to_end_video_processing(sample_video_path, output_dir):
    """Test the complete video processing flow from upload to completion."""
    # Skip this test in CI environments where real dependencies may not be available
    if os.environ.get("CI") == "true":
        pytest.skip("Skipping E2E test in CI environment")

    # We'll use real storage but mock AI to avoid API costs
    storage_adapter = LocalStorageAdapter(base_dir=output_dir)
    ai_adapter = MockAIAdapter()

    # Create a unique job ID
    job_id = f"test-job-{uuid.uuid4()}"

    # Create a video object
    video = Video(
        id=f"test-video-{uuid.uuid4()}",
        file_path=sample_video_path,
        file_name=os.path.basename(sample_video_path),
        file_size=os.path.getsize(sample_video_path),
    )

    # Create a job
    job = VideoJob(job_id=job_id, video=video)

    # Process the video
    try:
        # Create the video processor service
        processor = VideoProcessorService(
            storage_adapter=storage_adapter,
            ai_adapter=ai_adapter,
            local_output_dir=output_dir,
        )

        # This would normally be triggered by an API or event
        with pytest.raises(Exception):
            # This will likely fail since we're using a dummy file,
            # but it tests the flow up to the point of actual video processing
            processor.process_video(job)

        # In a real test with a real video file, we'd verify:
        # - Job status was updated
        # - Output files were created
        # - Metadata was generated

    except Exception as e:
        # We expect some exceptions with our dummy file
        print(f"Error in processing (expected): {str(e)}")

    # Verify the job was created and attempted processing
    assert job is not None

    print(f"Test completed for job {job_id}")


@pytest.mark.e2e
def test_minimal_processing_flow(output_dir):
    """
    Test a minimal processing flow with mocked components.
    This test avoids using real video files but still tests the service interactions.
    """
    # Set up components with mocks
    storage_adapter = LocalStorageAdapter(base_dir=output_dir)
    ai_adapter = MockAIAdapter()

    # Create test data
    video_id = f"test-video-{uuid.uuid4()}"
    job_id = f"test-job-{uuid.uuid4()}"

    # Create a video object directly (no real file needed)
    video = Video(
        id=video_id,
        file_path=os.path.join(output_dir, "test.mp4"),
        file_name="test.mp4",
        file_size=1024,
        duration=60.0,
        width=1920,
        height=1080,
    )

    # Write a dummy file
    with open(video.file_path, "wb") as f:
        f.write(b"dummy video content")

    # Create a job
    job = VideoJob(job_id=job_id, video=video)

    # Our test validates that:
    # 1. The components can be instantiated
    # 2. The basic flow executes without raising unhandled exceptions

    # This should pass even with a dummy file because we're using mocks
    assert job.video.id == video_id
    assert os.path.exists(video.file_path)

    print(f"Minimal test completed for job {job_id}")

================
File: tests/integration/ai/test_gemini.py
================
"""Integration tests for Gemini AI adapter."""
import os
import pytest
from typing import Optional

from video_processor.adapters.ai.gemini import GeminiAIAdapter
from video_processor.domain.exceptions import MetadataGenerationError


@pytest.fixture
def api_key() -> Optional[str]:
    """Get API key from environment variable."""
    return os.environ.get("GEMINI_API_KEY")


@pytest.fixture
def sample_transcript() -> str:
    """Provide a sample transcript for testing."""
    return """
    Hello and welcome to our video about the benefits of clean architecture. 
    Today, we're going to discuss how clean architecture helps with maintainability, 
    testing, and overall code quality. 
    
    First, let's talk about separation of concerns. Clean architecture divides your
    codebase into distinct layers, each with specific responsibilities. This makes
    it easier to understand and maintain the system.
    
    Second, we'll cover dependency inversion. By depending on abstractions rather
    than concrete implementations, we can easily swap out components, which is
    particularly useful for testing.
    
    Finally, we'll look at how clean architecture promotes testability by allowing
    us to test business logic in isolation from external dependencies.
    
    Thanks for watching, and don't forget to like and subscribe for more architecture videos!
    """


@pytest.fixture
def sample_audio_file(tmpdir) -> str:
    """Create a placeholder for an audio file path.
    
    In a real test, this would be a path to an actual audio file.
    For this integration test, we'll skip actual audio transcription
    and focus on the other AI functions that use text input.
    """
    return str(tmpdir.join("sample_audio.mp3"))


@pytest.fixture
def ai_adapter(api_key) -> Optional[GeminiAIAdapter]:
    """Create a Gemini AI adapter for testing if API key is available."""
    if not api_key:
        pytest.skip("GEMINI_API_KEY environment variable not set")
    
    return GeminiAIAdapter(api_key=api_key)


@pytest.mark.integration
class TestGeminiAIAdapter:
    """Integration tests for Gemini AI adapter."""
    
    def test_generate_metadata(self, ai_adapter, sample_transcript):
        """Test generating metadata from a transcript."""
        metadata = ai_adapter.generate_metadata(sample_transcript)
        
        # Verify that we get a dictionary with expected keys
        assert isinstance(metadata, dict)
        assert "title" in metadata
        assert "description" in metadata
        assert "tags" in metadata
        
        # Verify title is a reasonable length
        assert len(metadata["title"]) > 10
        assert len(metadata["title"]) < 100
        
        # Verify description has some content
        assert len(metadata["description"]) > 50
        
        # Verify tags are a list with reasonable content
        assert isinstance(metadata["tags"], list)
        assert len(metadata["tags"]) > 0
        
        # Verify content relevance (basic check)
        assert "architecture" in sample_transcript.lower()
        assert any("architecture" in tag.lower() for tag in metadata["tags"]) or \
               "architecture" in metadata["title"].lower() or \
               "architecture" in metadata["description"].lower()
    
    def test_generate_thumbnail_description(self, ai_adapter, sample_transcript):
        """Test generating a thumbnail description."""
        timestamp = 30.0  # 30 seconds into the video
        description = ai_adapter.generate_thumbnail_description(
            transcript=sample_transcript,
            timestamp=timestamp
        )
        
        # Verify we get a non-empty string
        assert isinstance(description, str)
        assert len(description) > 10
        
        # Should be related to the content
        assert "architecture" in description.lower() or \
               "clean" in description.lower() or \
               "code" in description.lower()
    
    def test_summarize_content(self, ai_adapter, sample_transcript):
        """Test summarizing content from a transcript."""
        max_length = 100
        summary = ai_adapter.summarize_content(
            transcript=sample_transcript,
            max_length=max_length
        )
        
        # Verify length constraints
        assert isinstance(summary, str)
        assert len(summary) <= max_length
        assert len(summary) > 20  # Should have reasonable content
        
        # Should be related to the content
        assert "architecture" in summary.lower() or \
               "clean" in summary.lower()
    
    def test_set_model(self, ai_adapter):
        """Test setting a different model."""
        # Store the original model
        original_model = ai_adapter.model_name
        
        # Set a new model
        new_model = "gemini-pro-vision"
        ai_adapter.set_model(new_model)
        
        # Verify the model was changed
        assert ai_adapter.model_name == new_model
        
        # Reset to original model
        ai_adapter.set_model(original_model)
    
    def test_invalid_api_key(self):
        """Test that an invalid API key raises an appropriate error."""
        invalid_adapter = GeminiAIAdapter(api_key="invalid_key")
        
        with pytest.raises(MetadataGenerationError):
            invalid_adapter.generate_metadata("This is a test transcript")

================
File: tests/integration/api/test_health.py
================
"""
Integration tests for API health endpoints.
"""

import pytest
from fastapi.testclient import TestClient
from video_processor.infrastructure.api.server import app


@pytest.fixture
def client():
    """Create a test client for the FastAPI app."""
    return TestClient(app)


def test_health_endpoint(client):
    """Test that the health endpoint returns a 200 status code."""
    response = client.get("/health")
    assert response.status_code == 200
    assert response.json() == {"status": "ok"}


def test_health_detailed_endpoint(client):
    """Test that the detailed health endpoint returns service status information."""
    response = client.get("/health/detailed")
    assert response.status_code == 200
    assert "status" in response.json()
    assert "services" in response.json()
    assert "version" in response.json()
    assert "uptime" in response.json()

================
File: tests/integration/storage/test_gcs.py
================
"""Integration tests for GCS storage adapter."""
import os
import pytest
import tempfile
import uuid
from pathlib import Path

from video_processor.adapters.storage.gcs import GCSStorageAdapter
from video_processor.domain.exceptions import StorageError


@pytest.fixture
def test_bucket_name():
    """Get test bucket name from environment or use a default for local testing."""
    return os.environ.get("TEST_GCS_BUCKET", "test-video-processor-bucket")


@pytest.fixture
def test_file_content():
    """Provide sample file content for testing."""
    return b"Test file content for GCS storage adapter integration test."


@pytest.fixture
def test_file_path():
    """Create a temporary file for testing."""
    with tempfile.NamedTemporaryFile(delete=False) as tmp:
        yield tmp.name
    
    # Cleanup after test
    if os.path.exists(tmp.name):
        os.unlink(tmp.name)


@pytest.fixture
def gcs_adapter(test_bucket_name):
    """Create a GCS adapter for testing."""
    return GCSStorageAdapter(bucket_name=test_bucket_name)


@pytest.mark.integration
class TestGCSStorageAdapter:
    """Integration tests for GCS storage adapter."""
    
    def setup_method(self):
        """Set up test environment."""
        self.test_files = []
    
    def teardown_method(self):
        """Clean up any test files created during tests."""
        for file_path in self.test_files:
            try:
                if os.path.exists(file_path):
                    os.unlink(file_path)
            except Exception:
                pass
                
    def test_upload_and_download(self, gcs_adapter, test_file_path, test_file_content):
        """Test uploading and downloading a file."""
        # Prepare test file
        with open(test_file_path, "wb") as f:
            f.write(test_file_content)
        
        # Generate unique destination path
        test_id = str(uuid.uuid4())
        destination_path = f"test/integration/{test_id}/test_file.txt"
        
        # Upload the file
        uploaded_path = gcs_adapter.upload_file(
            file_path=test_file_path,
            destination_path=destination_path
        )
        
        assert uploaded_path == destination_path
        
        # Download the file to a new location
        download_path = f"{test_file_path}_downloaded"
        self.test_files.append(download_path)
        
        downloaded_path = gcs_adapter.download_file(
            source_path=destination_path,
            destination_path=download_path
        )
        
        assert downloaded_path == download_path
        assert os.path.exists(download_path)
        
        # Verify content
        with open(download_path, "rb") as f:
            downloaded_content = f.read()
        
        assert downloaded_content == test_file_content
        
        # Clean up the remote file
        assert gcs_adapter.delete_file(destination_path)
    
    def test_get_public_url(self, gcs_adapter, test_bucket_name):
        """Test getting a public URL for a file."""
        test_path = "test/public/test_file.txt"
        url = gcs_adapter.get_public_url(test_path)
        
        assert url.startswith("https://storage.googleapis.com/")
        assert test_bucket_name in url
        assert test_path in url
    
    def test_get_signed_url(self, gcs_adapter, test_bucket_name):
        """Test getting a signed URL for a file."""
        test_path = "test/signed/test_file.txt"
        expiration = 3600  # 1 hour
        
        url = gcs_adapter.get_signed_url(test_path, expiration_seconds=expiration)
        
        assert url.startswith("https://storage.googleapis.com/")
        assert test_bucket_name in url
        assert test_path in url
        assert "Signature=" in url
        assert "Expires=" in url
    
    def test_upload_nonexistent_file(self, gcs_adapter):
        """Test uploading a non-existent file raises appropriate error."""
        with pytest.raises(StorageError):
            gcs_adapter.upload_file(
                file_path="/nonexistent/file.txt",
                destination_path="test/error/file.txt"
            )
    
    def test_download_nonexistent_file(self, gcs_adapter, test_file_path):
        """Test downloading a non-existent file raises appropriate error."""
        with pytest.raises(StorageError):
            gcs_adapter.download_file(
                source_path="test/nonexistent/file.txt",
                destination_path=f"{test_file_path}_nonexistent"
            )

================
File: tests/integration/__init__.py
================
"""
Integration tests for checking the interaction between components.
"""

================
File: tests/mocks/__init__.py
================
"""
Mock implementations for testing.
"""

================
File: tests/mocks/ai.py
================
"""
Mock implementation of the AI service interface for testing.
"""

from typing import Dict

from video_processor.application.interfaces.ai import AIServiceInterface


class MockAIAdapter(AIServiceInterface):
    """
    Mock implementation of AIServiceInterface for testing.

    Provides predetermined responses for AI operations without actually
    calling any AI services.
    """

    def __init__(self, model_name: str = "mock-model"):
        """Initialize the mock AI adapter."""
        self.model_name = model_name
        # Store call counts for verification in tests
        self.call_counts = {
            "generate_transcript": 0,
            "generate_metadata": 0,
            "generate_thumbnail_description": 0,
            "summarize_content": 0,
            "set_model": 0,
        }
        # Predefined responses
        self.responses = {
            "transcript": "This is a mock transcript of the video content. "
            "The speaker discusses video processing techniques.",
            "metadata": {
                "title": "How to Process Videos Effectively",
                "description": "A comprehensive guide to video processing using modern techniques.",
                "tags": ["video", "processing", "tutorial", "AI"],
                "show_notes": "00:00 Introduction\n00:30 Basic concepts\n01:45 Advanced techniques",
            },
            "thumbnail": "A person explaining video processing concepts with a whiteboard",
            "summary": "This video covers the fundamentals of video processing including "
            "transcoding, metadata extraction, and AI-assisted content generation.",
        }

    def set_model(self, model_name: str) -> None:
        """
        Set the AI model to use.

        Args:
            model_name: Name of the model to use
        """
        self.call_counts["set_model"] += 1
        self.model_name = model_name

    def generate_transcript(self, audio_file: str) -> str:
        """
        Generate a transcript from an audio file.

        Args:
            audio_file: Path to the audio file

        Returns:
            Generated transcript text
        """
        self.call_counts["generate_transcript"] += 1
        return self.responses["transcript"]

    def generate_metadata(self, transcript: str) -> Dict:
        """
        Generate metadata from a transcript.

        Args:
            transcript: Video transcript text

        Returns:
            Dictionary containing generated metadata
        """
        self.call_counts["generate_metadata"] += 1
        return self.responses["metadata"]

    def generate_thumbnail_description(self, transcript: str, timestamp: float) -> str:
        """
        Generate a description for a thumbnail at a specific timestamp.

        Args:
            transcript: Video transcript text
            timestamp: Timestamp in seconds

        Returns:
            Description for the thumbnail
        """
        self.call_counts["generate_thumbnail_description"] += 1
        return f"At {timestamp:.2f}s: {self.responses['thumbnail']}"

    def summarize_content(self, transcript: str, max_length: int = 500) -> str:
        """
        Generate a summary of the transcript content.

        Args:
            transcript: Video transcript text
            max_length: Maximum summary length

        Returns:
            Summarized content
        """
        self.call_counts["summarize_content"] += 1
        # Adjust summary length if needed
        if max_length < len(self.responses["summary"]):
            return self.responses["summary"][:max_length] + "..."
        return self.responses["summary"]

    def set_custom_response(self, response_type: str, response_data: any) -> None:
        """
        Set a custom response for a specific operation.
        Useful for testing specific scenarios.

        Args:
            response_type: Type of response to set (transcript, metadata, etc.)
            response_data: The response data to use
        """
        if response_type in self.responses:
            self.responses[response_type] = response_data

    def reset_call_counts(self) -> None:
        """Reset all call counters to zero."""
        for key in self.call_counts:
            self.call_counts[key] = 0

================
File: tests/mocks/publishing.py
================
"""
Mock implementation of the publishing interface for testing.
"""

from typing import Dict

from video_processor.application.interfaces.publishing import PublishingInterface


class MockPublishingAdapter(PublishingInterface):
    """
    Mock implementation of PublishingInterface for testing.

    Simulates video publishing operations without making actual API calls.
    """

    def __init__(self):
        """Initialize the mock publishing adapter."""
        self.uploaded_videos = {}  # Dictionary to track "uploaded" videos
        self.deleted_videos = set()  # Set to track "deleted" videos
        self.call_counts = {
            "upload_video": 0,
            "update_metadata": 0,
            "get_upload_status": 0,
            "delete_video": 0,
        }
        self.next_video_id = 1000  # Starting ID for mock uploads

    def upload_video(self, video_file: str, metadata: Dict) -> str:
        """
        Mock uploading a video with metadata.

        Args:
            video_file: Path to the video file
            metadata: Video metadata dictionary

        Returns:
            Platform-specific video ID
        """
        self.call_counts["upload_video"] += 1
        video_id = f"mock-video-{self.next_video_id}"
        self.next_video_id += 1

        # Store the video information
        self.uploaded_videos[video_id] = {
            "file_path": video_file,
            "metadata": metadata.copy(),
            "status": "published",
        }

        return video_id

    def update_metadata(self, video_id: str, metadata: Dict) -> bool:
        """
        Mock updating video metadata.

        Args:
            video_id: ID of the video to update
            metadata: Updated metadata dictionary

        Returns:
            Success status (boolean)
        """
        self.call_counts["update_metadata"] += 1

        if video_id in self.uploaded_videos:
            # Update the stored metadata
            self.uploaded_videos[video_id]["metadata"].update(metadata)
            return True
        return False

    def get_upload_status(self, video_id: str) -> str:
        """
        Mock checking video upload status.

        Args:
            video_id: ID of the video to check

        Returns:
            Status string (published, processing, failed, etc.)
        """
        self.call_counts["get_upload_status"] += 1

        if video_id in self.uploaded_videos:
            return self.uploaded_videos[video_id]["status"]
        return "not_found"

    def delete_video(self, video_id: str) -> bool:
        """
        Mock deleting a video.

        Args:
            video_id: ID of the video to delete

        Returns:
            Success status (boolean)
        """
        self.call_counts["delete_video"] += 1

        if video_id in self.uploaded_videos:
            # Mark as deleted but keep the record for test verification
            self.deleted_videos.add(video_id)
            self.uploaded_videos[video_id]["status"] = "deleted"
            return True
        return False

    def set_upload_status(self, video_id: str, status: str) -> None:
        """
        Set a specific upload status for a video.
        Useful for testing different scenarios.

        Args:
            video_id: ID of the video to update
            status: New status string
        """
        if video_id in self.uploaded_videos:
            self.uploaded_videos[video_id]["status"] = status

    def get_video_count(self) -> int:
        """
        Get the number of uploaded videos.

        Returns:
            Count of uploaded videos
        """
        return len(self.uploaded_videos)

    def get_deleted_count(self) -> int:
        """
        Get the number of deleted videos.

        Returns:
            Count of deleted videos
        """
        return len(self.deleted_videos)

    def reset(self) -> None:
        """Reset all mock state."""
        self.uploaded_videos.clear()
        self.deleted_videos.clear()
        for key in self.call_counts:
            self.call_counts[key] = 0
        self.next_video_id = 1000

================
File: tests/mocks/storage.py
================
"""
Mock implementation of the storage interface for testing.
"""

import os
import shutil
from typing import Dict

from video_processor.application.interfaces.storage import StorageInterface


class MockStorageAdapter(StorageInterface):
    """
    Mock implementation of StorageInterface for testing.

    This adapter simulates storage operations in memory without accessing
    any real storage system.
    """

    def __init__(self, base_url: str = "https://mock-storage.example.com"):
        """Initialize the mock storage adapter."""
        self.base_url = base_url
        self.files: Dict[str, bytes] = {}  # In-memory storage
        self.temp_dir = "/tmp/mock_storage"
        os.makedirs(self.temp_dir, exist_ok=True)

    def upload_file(self, file_path: str, destination_path: str) -> str:
        """
        Mock uploading a file to storage.

        Args:
            file_path: Local path to the file
            destination_path: Destination path in storage

        Returns:
            Full path of the uploaded file in storage
        """
        if os.path.exists(file_path):
            with open(file_path, "rb") as f:
                content = f.read()
                self.files[destination_path] = content
        else:
            # For testing, create some mock content if file doesn't exist
            self.files[destination_path] = b"Mock file content"

        return f"gs://mock-bucket/{destination_path}"

    def download_file(self, source_path: str, destination_path: str) -> str:
        """
        Mock downloading a file from storage.

        Args:
            source_path: Storage path of the file
            destination_path: Local destination path

        Returns:
            Local path of the downloaded file
        """
        # Create the directory structure if it doesn't exist
        os.makedirs(os.path.dirname(destination_path), exist_ok=True)

        # Write mock content to the destination file
        if source_path in self.files:
            content = self.files[source_path]
        else:
            content = b"Mock downloaded content"

        with open(destination_path, "wb") as f:
            f.write(content)

        return destination_path

    def delete_file(self, path: str) -> bool:
        """
        Mock deleting a file from storage.

        Args:
            path: Storage path of the file to delete

        Returns:
            True if successful, False otherwise
        """
        if path in self.files:
            del self.files[path]
            return True
        return False

    def get_public_url(self, path: str) -> str:
        """
        Get a public URL for a file.

        Args:
            path: Storage path of the file

        Returns:
            Public URL for the file
        """
        return f"{self.base_url}/{path}"

    def get_signed_url(self, path: str, expiration_seconds: int = 3600) -> str:
        """
        Get a signed URL for a file with expiration.

        Args:
            path: Storage path of the file
            expiration_seconds: Seconds until the URL expires

        Returns:
            Signed URL for the file
        """
        return (
            f"{self.base_url}/{path}?token=mock-signature&expires={expiration_seconds}"
        )

    def list_files(self, prefix: str) -> list:
        """
        List files with a given prefix.

        Args:
            prefix: Prefix to filter files by

        Returns:
            List of file paths matching the prefix
        """
        return [path for path in self.files.keys() if path.startswith(prefix)]

    def clear(self) -> None:
        """Clear all mock storage data."""
        self.files.clear()
        if os.path.exists(self.temp_dir):
            shutil.rmtree(self.temp_dir)
            os.makedirs(self.temp_dir, exist_ok=True)

================
File: tests/unit/adapters/test_storage.py
================
"""
Unit tests for the storage adapter implementations.
"""

import os
import tempfile

from tests.mocks.storage import MockStorageAdapter


def test_mock_storage_adapter_upload():
    """Test the mock storage adapter's upload functionality."""
    adapter = MockStorageAdapter()

    # Create a temporary file for testing
    with tempfile.NamedTemporaryFile(delete=False) as temp:
        temp.write(b"Test content")
        temp_path = temp.name

    try:
        # Test uploading the file
        result = adapter.upload_file(temp_path, "test/path.txt")

        assert result == "gs://mock-bucket/test/path.txt"
        assert "test/path.txt" in adapter.files
        assert adapter.files["test/path.txt"] == b"Test content"
    finally:
        # Clean up
        if os.path.exists(temp_path):
            os.remove(temp_path)


def test_mock_storage_adapter_download():
    """Test the mock storage adapter's download functionality."""
    adapter = MockStorageAdapter()

    # Add a file to the mock storage
    adapter.files["test/download.txt"] = b"Downloaded content"

    with tempfile.TemporaryDirectory() as temp_dir:
        dest_path = os.path.join(temp_dir, "downloaded.txt")

        # Test downloading the file
        result = adapter.download_file("test/download.txt", dest_path)

        assert result == dest_path
        assert os.path.exists(dest_path)

        # Verify the content
        with open(dest_path, "rb") as f:
            content = f.read()
            assert content == b"Downloaded content"


def test_mock_storage_adapter_delete():
    """Test the mock storage adapter's delete functionality."""
    adapter = MockStorageAdapter()

    # Add a file to the mock storage
    adapter.files["test/delete.txt"] = b"Content to delete"

    # Test deleting existing file
    result = adapter.delete_file("test/delete.txt")
    assert result is True
    assert "test/delete.txt" not in adapter.files

    # Test deleting non-existent file
    result = adapter.delete_file("nonexistent.txt")
    assert result is False


def test_mock_storage_adapter_urls():
    """Test the mock storage adapter's URL generation functionality."""
    adapter = MockStorageAdapter(base_url="https://test-storage.example.org")

    # Test public URL
    public_url = adapter.get_public_url("test/file.txt")
    assert public_url == "https://test-storage.example.org/test/file.txt"

    # Test signed URL
    signed_url = adapter.get_signed_url("test/file.txt", 7200)
    assert "https://test-storage.example.org/test/file.txt" in signed_url
    assert "token=mock-signature" in signed_url
    assert "expires=7200" in signed_url


def test_mock_storage_adapter_list_files():
    """Test the mock storage adapter's file listing functionality."""
    adapter = MockStorageAdapter()

    # Add some files with different prefixes
    adapter.files["test/file1.txt"] = b"Content 1"
    adapter.files["test/file2.txt"] = b"Content 2"
    adapter.files["other/file3.txt"] = b"Content 3"

    # Test listing with prefix
    test_files = adapter.list_files("test/")
    assert len(test_files) == 2
    assert "test/file1.txt" in test_files
    assert "test/file2.txt" in test_files
    assert "other/file3.txt" not in test_files

    # Test listing with different prefix
    other_files = adapter.list_files("other/")
    assert len(other_files) == 1
    assert "other/file3.txt" in other_files


def test_mock_storage_adapter_clear():
    """Test the mock storage adapter's clearing functionality."""
    adapter = MockStorageAdapter()

    # Add some files
    adapter.files["test/file1.txt"] = b"Content 1"
    adapter.files["test/file2.txt"] = b"Content 2"

    assert len(adapter.files) == 2

    # Clear the adapter
    adapter.clear()

    assert len(adapter.files) == 0

================
File: tests/unit/application/services/test_video_processor.py
================
"""
Unit tests for the VideoProcessorService.
"""

from unittest.mock import MagicMock, patch

import pytest
from video_processor.application.services.video_processor import VideoProcessorService
from video_processor.domain.models.enums import ProcessingStage, ProcessingStatus
from video_processor.domain.models.job import VideoJob

from tests.mocks.ai import MockAIAdapter
from tests.mocks.storage import MockStorageAdapter


@pytest.fixture
def mock_services():
    """Create mock services needed by the VideoProcessorService."""
    mock_transcription = MagicMock()
    mock_transcription.generate_transcript.return_value = "Mocked transcript content"

    mock_subtitle = MagicMock()
    mock_subtitle.generate_vtt.return_value = (
        "WEBVTT\n\n00:00:00.000 --> 00:00:05.000\nMocked subtitle content"
    )
    mock_subtitle.generate_srt.return_value = (
        "1\n00:00:00,000 --> 00:00:05,000\nMocked subtitle content"
    )

    mock_metadata = MagicMock()
    mock_metadata.generate_title.return_value = "Generated Video Title"
    mock_metadata.generate_description.return_value = "Generated video description"
    mock_metadata.generate_tags.return_value = ["tag1", "tag2", "tag3"]

    return {
        "transcription": mock_transcription,
        "subtitle": mock_subtitle,
        "metadata": mock_metadata,
    }


@patch("video_processor.application.services.video_processor.extract_audio")
@patch("video_processor.application.services.video_processor.extract_frame")
@patch("video_processor.application.services.video_processor.get_video_metadata")
def test_video_processor_service_instantiation(
    mock_get_metadata, mock_extract_frame, mock_extract_audio
):
    """Test that the VideoProcessorService can be instantiated with dependencies."""
    storage = MockStorageAdapter()
    ai = MockAIAdapter()

    service = VideoProcessorService(
        storage_adapter=storage,
        ai_adapter=ai,
        output_bucket="test-bucket",
        local_output_dir="/tmp/test-output",
    )

    assert service._storage == storage
    assert service._ai == ai
    assert service._output_bucket == "test-bucket"
    assert service._local_output_dir == "/tmp/test-output"
    assert service._transcription_service is not None
    assert service._subtitle_service is not None
    assert service._metadata_service is not None


@patch("video_processor.application.services.video_processor.TranscriptionService")
@patch("video_processor.application.services.video_processor.SubtitleService")
@patch("video_processor.application.services.video_processor.MetadataService")
@patch("video_processor.application.services.video_processor.extract_audio")
@patch("video_processor.application.services.video_processor.extract_frame")
@patch("video_processor.application.services.video_processor.get_video_metadata")
@patch("tempfile.TemporaryDirectory")
def test_process_video_success_flow(
    mock_temp_dir,
    mock_get_metadata,
    mock_extract_frame,
    mock_extract_audio,
    MockMetadataService,
    MockSubtitleService,
    MockTranscriptionService,
    test_video,
    test_metadata,
):
    """Test the successful video processing flow."""
    # Configure mocks
    mock_temp_dir.return_value.__enter__.return_value = "/tmp/mock"
    mock_get_metadata.return_value = {
        "duration": 60.0,
        "resolution": (1920, 1080),
        "format": "mp4",
    }

    # Mock service instances
    mock_transcription = MockTranscriptionService.return_value
    mock_transcription.generate_transcript.return_value = "Mocked transcript content"

    mock_subtitle = MockSubtitleService.return_value
    mock_subtitle.generate_vtt.return_value = "WEBVTT content"
    mock_subtitle.generate_srt.return_value = "SRT content"

    mock_metadata = MockMetadataService.return_value
    mock_metadata.generate_title.return_value = "Generated Title"
    mock_metadata.generate_description.return_value = "Generated Description"
    mock_metadata.generate_tags.return_value = ["tag1", "tag2"]
    mock_metadata.generate_thumbnail_description.return_value = (
        "A person explaining concepts"
    )

    # Create test objects
    storage = MockStorageAdapter()
    ai = MockAIAdapter()

    job = VideoJob.create_new(test_video)
    job.metadata = test_metadata

    # Create service and process video
    service = VideoProcessorService(storage_adapter=storage, ai_adapter=ai)

    # Mock file paths to avoid file system operations
    with patch("os.path.exists", return_value=True):
        processed_job = service.process_video(job)

    # Verify job was updated correctly
    assert processed_job.status == ProcessingStatus.COMPLETED
    assert processed_job.error_message is None

    # Verify all stages were completed
    assert ProcessingStage.DOWNLOAD in processed_job.completed_stages
    assert ProcessingStage.EXTRACT_AUDIO in processed_job.completed_stages
    assert ProcessingStage.GENERATE_TRANSCRIPT in processed_job.completed_stages
    assert ProcessingStage.GENERATE_SUBTITLES in processed_job.completed_stages
    assert ProcessingStage.GENERATE_SHOWNOTES in processed_job.completed_stages
    assert ProcessingStage.UPLOAD_OUTPUTS in processed_job.completed_stages

    # Verify the core service calls were made
    assert mock_transcription.generate_transcript.called
    assert mock_subtitle.generate_vtt.called
    assert mock_subtitle.generate_srt.called
    assert mock_metadata.generate_title.called
    assert mock_metadata.generate_description.called
    assert mock_metadata.generate_tags.called

    # Verify storage operations
    assert len(storage.files) > 0


@patch("video_processor.application.services.video_processor.TranscriptionService")
@patch("video_processor.application.services.video_processor.SubtitleService")
@patch("video_processor.application.services.video_processor.MetadataService")
@patch("video_processor.application.services.video_processor.extract_audio")
@patch("video_processor.application.services.video_processor.extract_frame")
@patch("video_processor.application.services.video_processor.get_video_metadata")
@patch("tempfile.TemporaryDirectory")
def test_process_video_file_not_found(
    mock_temp_dir,
    mock_get_metadata,
    mock_extract_frame,
    mock_extract_audio,
    MockMetadataService,
    MockSubtitleService,
    MockTranscriptionService,
    test_video,
):
    """Test handling of file not found error."""
    # Configure mocks
    mock_temp_dir.return_value.__enter__.return_value = "/tmp/mock"

    # Create test objects
    storage = MockStorageAdapter()
    ai = MockAIAdapter()

    job = VideoJob.create_new(test_video)

    # Create service
    service = VideoProcessorService(storage_adapter=storage, ai_adapter=ai)

    # Mock file not existing
    with patch("os.path.exists", return_value=False):
        with pytest.raises(Exception) as excinfo:
            service.process_video(job)

        # Verify exception was raised
        assert "Video file not found" in str(excinfo.value)

    # Verify job status was updated
    assert job.status == ProcessingStatus.FAILED
    assert job.error_message is not None


@patch("video_processor.application.services.video_processor.TranscriptionService")
@patch("video_processor.application.services.video_processor.SubtitleService")
@patch("video_processor.application.services.video_processor.MetadataService")
@patch("video_processor.application.services.video_processor.extract_audio")
@patch("video_processor.application.services.video_processor.extract_frame")
@patch("video_processor.application.services.video_processor.get_video_metadata")
@patch("tempfile.TemporaryDirectory")
def test_process_video_transcript_error(
    mock_temp_dir,
    mock_get_metadata,
    mock_extract_frame,
    mock_extract_audio,
    MockMetadataService,
    MockSubtitleService,
    MockTranscriptionService,
    test_video,
):
    """Test handling of transcript generation error."""
    # Configure mocks
    mock_temp_dir.return_value.__enter__.return_value = "/tmp/mock"
    mock_get_metadata.return_value = {
        "duration": 60.0,
        "resolution": (1920, 1080),
        "format": "mp4",
    }

    # Mock transcript error
    mock_transcription = MockTranscriptionService.return_value
    mock_transcription.generate_transcript.side_effect = Exception(
        "Transcript generation failed"
    )

    # Create test objects
    storage = MockStorageAdapter()
    ai = MockAIAdapter()

    job = VideoJob.create_new(test_video)

    # Create service
    service = VideoProcessorService(storage_adapter=storage, ai_adapter=ai)

    # Run test
    with patch("os.path.exists", return_value=True):
        with pytest.raises(Exception) as excinfo:
            service.process_video(job)

        # Verify exception was raised
        assert "Transcript generation failed" in str(excinfo.value)

    # Verify job status was updated
    assert job.status == ProcessingStatus.FAILED
    assert job.error_message is not None
    assert "Transcript generation failed" in job.error_message

================
File: tests/unit/domain/test_enums.py
================
"""
Unit tests for domain enums.
"""

from video_processor.domain.models.enums import ProcessingStage, ProcessingStatus


def test_processing_stage_values():
    """Test the existence and order of processing stage enum values."""
    stages = list(ProcessingStage)

    # Verify all expected stages exist
    assert ProcessingStage.DOWNLOAD in stages
    assert ProcessingStage.EXTRACT_AUDIO in stages
    assert ProcessingStage.GENERATE_TRANSCRIPT in stages
    assert ProcessingStage.GENERATE_SUBTITLES in stages
    assert ProcessingStage.GENERATE_SHOWNOTES in stages
    assert ProcessingStage.GENERATE_CHAPTERS in stages
    assert ProcessingStage.GENERATE_TITLES in stages
    assert ProcessingStage.UPLOAD_OUTPUTS in stages
    assert ProcessingStage.UPLOAD_TO_YOUTUBE in stages
    assert ProcessingStage.COMPLETE in stages

    # Test that DOWNLOAD is the first stage
    assert stages[0] == ProcessingStage.DOWNLOAD

    # Test that COMPLETE is the last stage
    assert stages[-1] == ProcessingStage.COMPLETE


def test_processing_status_values():
    """Test the existence and properties of processing status enum values."""
    statuses = list(ProcessingStatus)

    # Verify all expected statuses exist
    assert ProcessingStatus.PENDING in statuses
    assert ProcessingStatus.IN_PROGRESS in statuses
    assert ProcessingStatus.COMPLETED in statuses
    assert ProcessingStatus.FAILED in statuses
    assert ProcessingStatus.PARTIAL in statuses

    # Test enum properties
    assert str(ProcessingStatus.PENDING) == "ProcessingStatus.PENDING"
    assert ProcessingStatus.PENDING.name == "PENDING"


def test_processing_stage_comparisons():
    """Test that enum stages can be properly compared."""
    # Test identity comparison
    assert ProcessingStage.DOWNLOAD is ProcessingStage.DOWNLOAD
    assert ProcessingStage.DOWNLOAD is not ProcessingStage.EXTRACT_AUDIO

    # Test equality comparison
    assert ProcessingStage.DOWNLOAD == ProcessingStage.DOWNLOAD
    assert ProcessingStage.DOWNLOAD != ProcessingStage.EXTRACT_AUDIO

    # Test ordering based on declaration order
    assert ProcessingStage.DOWNLOAD.value < ProcessingStage.EXTRACT_AUDIO.value
    assert (
        ProcessingStage.GENERATE_TRANSCRIPT.value > ProcessingStage.EXTRACT_AUDIO.value
    )
    assert ProcessingStage.COMPLETE.value > ProcessingStage.UPLOAD_TO_YOUTUBE.value

================
File: tests/unit/domain/test_job.py
================
"""
Unit tests for the VideoJob domain model.
"""

from datetime import datetime

import pytest
from video_processor.domain.models.enums import ProcessingStage, ProcessingStatus
from video_processor.domain.models.job import VideoJob
from video_processor.domain.models.metadata import VideoMetadata
from video_processor.domain.models.video import Video


@pytest.fixture
def sample_video():
    """Create a sample video for testing."""
    return Video(
        id="video123",
        file_path="/path/to/video.mp4",
        file_name="video.mp4",
        duration=60.0,
    )


@pytest.fixture
def sample_metadata():
    """Create sample metadata for testing."""
    return VideoMetadata(
        title="Test Video",
        description="This is a test video",
        tags=["test", "video"],
    )


def test_job_creation(sample_video):
    """Test that a VideoJob can be created with minimal attributes."""
    job = VideoJob(job_id="job123", video=sample_video)

    assert job.job_id == "job123"
    assert job.video.id == "video123"
    assert job.status == ProcessingStatus.PENDING
    assert job.current_stage == ProcessingStage.DOWNLOAD
    assert job.completed_stages == []
    assert job.error_message is None
    assert job.processed_path is None
    assert job.output_files == {}
    assert job.youtube_video_id is None
    assert isinstance(job.created_at, datetime)
    assert isinstance(job.updated_at, datetime)
    assert job.metadata.title == ""  # Default empty title


def test_create_new_job(sample_video):
    """Test creation of a new job from a video."""
    job = VideoJob.create_new(sample_video)

    assert job.job_id.startswith(f"job-{sample_video.id}-")
    assert job.video == sample_video
    assert job.metadata.title == "Video"  # From filename "video.mp4"
    assert job.status == ProcessingStatus.PENDING


def test_job_with_metadata(sample_video, sample_metadata):
    """Test job creation with metadata."""
    job = VideoJob(
        job_id="job123",
        video=sample_video,
        metadata=sample_metadata,
    )

    assert job.metadata.title == "Test Video"
    assert job.metadata.description == "This is a test video"
    assert job.metadata.tags == ["test", "video"]


def test_update_status(sample_video):
    """Test updating job status."""
    job = VideoJob(job_id="job123", video=sample_video)
    original_updated_at = job.updated_at

    # Small delay to ensure timestamp changes
    job.update_status(ProcessingStatus.IN_PROGRESS)

    assert job.status == ProcessingStatus.IN_PROGRESS
    assert job.updated_at > original_updated_at

    # Test with error message
    job.update_status(ProcessingStatus.FAILED, "Something went wrong")

    assert job.status == ProcessingStatus.FAILED
    assert job.error_message == "Something went wrong"


def test_move_to_stage(sample_video):
    """Test moving to a new processing stage."""
    job = VideoJob(job_id="job123", video=sample_video)
    original_updated_at = job.updated_at

    # Initial stage is DOWNLOAD and should be moved to completed
    job.move_to_stage(ProcessingStage.EXTRACT_AUDIO)

    assert job.current_stage == ProcessingStage.EXTRACT_AUDIO
    assert ProcessingStage.DOWNLOAD in job.completed_stages
    assert job.updated_at > original_updated_at

    # Move to next stage
    job.move_to_stage(ProcessingStage.GENERATE_TRANSCRIPT)

    assert job.current_stage == ProcessingStage.GENERATE_TRANSCRIPT
    assert ProcessingStage.EXTRACT_AUDIO in job.completed_stages
    assert len(job.completed_stages) == 2


def test_complete_current_stage(sample_video):
    """Test completing the current stage."""
    job = VideoJob(job_id="job123", video=sample_video)
    original_updated_at = job.updated_at

    job.complete_current_stage()

    assert ProcessingStage.DOWNLOAD in job.completed_stages
    assert job.current_stage == ProcessingStage.DOWNLOAD  # Hasn't changed
    assert job.updated_at > original_updated_at


def test_is_stage_completed(sample_video):
    """Test checking if a stage is completed."""
    job = VideoJob(job_id="job123", video=sample_video)

    assert not job.is_stage_completed(ProcessingStage.DOWNLOAD)

    job.complete_current_stage()

    assert job.is_stage_completed(ProcessingStage.DOWNLOAD)
    assert not job.is_stage_completed(ProcessingStage.EXTRACT_AUDIO)


def test_add_output_file(sample_video):
    """Test adding output files."""
    job = VideoJob(job_id="job123", video=sample_video)
    original_updated_at = job.updated_at

    job.add_output_file("transcript", "/path/to/transcript.txt")

    assert job.output_files["transcript"] == "/path/to/transcript.txt"
    assert job.updated_at > original_updated_at

    job.add_output_file("subtitles", "/path/to/subtitles.vtt")

    assert len(job.output_files) == 2
    assert job.output_files["subtitles"] == "/path/to/subtitles.vtt"


def test_fail(sample_video):
    """Test marking a job as failed."""
    job = VideoJob(job_id="job123", video=sample_video)
    original_updated_at = job.updated_at

    job.fail("Processing error")

    assert job.status == ProcessingStatus.FAILED
    assert job.error_message == "Processing error"
    assert job.updated_at > original_updated_at


def test_complete(sample_video):
    """Test marking a job as completed."""
    job = VideoJob(job_id="job123", video=sample_video)
    original_updated_at = job.updated_at

    job.complete()

    assert job.status == ProcessingStatus.COMPLETED
    assert job.current_stage == ProcessingStage.COMPLETE
    assert ProcessingStage.DOWNLOAD in job.completed_stages  # Current stage was added
    assert job.updated_at > original_updated_at
    assert job.is_complete() is True


def test_is_complete(sample_video):
    """Test checking if a job is complete."""
    job = VideoJob(job_id="job123", video=sample_video)

    assert job.is_complete() is False

    job.update_status(ProcessingStatus.COMPLETED)

    assert job.is_complete() is True


def test_to_dict(sample_video, sample_metadata):
    """Test conversion to dictionary."""
    job = VideoJob(
        job_id="job123",
        video=sample_video,
        metadata=sample_metadata,
        processed_path="/processed/video.mp4",
        youtube_video_id="yt12345",
    )

    job_dict = job.to_dict()

    assert job_dict["job_id"] == "job123"
    assert isinstance(job_dict["video"], dict)
    assert job_dict["video"]["id"] == "video123"
    assert isinstance(job_dict["created_at"], str)
    assert isinstance(job_dict["updated_at"], str)
    assert isinstance(job_dict["metadata"], dict)
    assert job_dict["metadata"]["title"] == "Test Video"
    assert job_dict["status"] == "PENDING"
    assert job_dict["current_stage"] == "DOWNLOAD"
    assert job_dict["completed_stages"] == []
    assert job_dict["processed_path"] == "/processed/video.mp4"
    assert job_dict["youtube_video_id"] == "yt12345"


def test_from_dict(sample_video, sample_metadata):
    """Test creation from dictionary."""
    # Create a timestamp for testing
    now = datetime.now()

    job_dict = {
        "job_id": "job123",
        "video": sample_video.to_dict(),
        "created_at": now.isoformat(),
        "updated_at": now.isoformat(),
        "metadata": sample_metadata.to_dict(),
        "status": "IN_PROGRESS",
        "current_stage": "GENERATE_TRANSCRIPT",
        "completed_stages": ["DOWNLOAD", "EXTRACT_AUDIO"],
        "error_message": None,
        "processed_path": "/processed/video.mp4",
        "output_files": {
            "transcript": "/path/to/transcript.txt",
            "subtitles": "/path/to/subtitles.vtt",
        },
        "youtube_video_id": "yt12345",
    }

    job = VideoJob.from_dict(job_dict)

    assert job.job_id == "job123"
    assert job.video.id == "video123"
    assert job.created_at.isoformat() == now.isoformat()
    assert job.updated_at.isoformat() == now.isoformat()
    assert job.metadata.title == "Test Video"
    assert job.status == ProcessingStatus.IN_PROGRESS
    assert job.current_stage == ProcessingStage.GENERATE_TRANSCRIPT
    assert len(job.completed_stages) == 2
    assert ProcessingStage.DOWNLOAD in job.completed_stages
    assert ProcessingStage.EXTRACT_AUDIO in job.completed_stages
    assert job.processed_path == "/processed/video.mp4"
    assert job.output_files["transcript"] == "/path/to/transcript.txt"
    assert job.output_files["subtitles"] == "/path/to/subtitles.vtt"
    assert job.youtube_video_id == "yt12345"

================
File: tests/unit/domain/test_metadata.py
================
"""
Unit tests for the VideoMetadata domain model.
"""

from video_processor.domain.models.metadata import VideoMetadata


def test_metadata_creation():
    """Test that a VideoMetadata object can be created with minimal attributes."""
    metadata = VideoMetadata(title="Test Video")

    assert metadata.title == "Test Video"
    assert metadata.description is None
    assert metadata.category_id == "22"  # Default category
    assert metadata.channel == "daily"  # Default channel
    assert metadata.tags == []  # Default empty list
    assert metadata.chapters == []  # Default empty list


def test_metadata_creation_with_attributes():
    """Test that a VideoMetadata object can be created with all attributes."""
    metadata = VideoMetadata(
        title="Test Video",
        description="A test video description",
        keywords="test, video, unit test",
        category_id="10",  # Music
        duration_seconds=300,
        width=1920,
        height=1080,
        channel="main",
        tags=["test", "video"],
        show_notes="These are the show notes",
        thumbnail_url="https://example.com/thumbnail.jpg",
        transcript="This is the transcript",
        chapters=[{"time": "00:00", "title": "Introduction"}],
    )

    assert metadata.title == "Test Video"
    assert metadata.description == "A test video description"
    assert metadata.keywords == "test, video, unit test"
    assert metadata.category_id == "10"
    assert metadata.duration_seconds == 300
    assert metadata.width == 1920
    assert metadata.height == 1080
    assert metadata.channel == "main"
    assert metadata.tags == ["test", "video"]
    assert metadata.show_notes == "These are the show notes"
    assert metadata.thumbnail_url == "https://example.com/thumbnail.jpg"
    assert metadata.transcript == "This is the transcript"
    assert metadata.chapters == [{"time": "00:00", "title": "Introduction"}]


def test_post_init_with_null_collections():
    """Test that __post_init__ initializes collections properly."""
    metadata = VideoMetadata(title="Test Video", tags=None, chapters=None)

    assert metadata.tags == []
    assert metadata.chapters == []


def test_to_dict():
    """Test conversion to dictionary."""
    metadata = VideoMetadata(
        title="Test Video",
        description="A test video description",
        tags=["tag1", "tag2"],
        thumbnail_url="https://example.com/thumbnail.jpg",
    )

    metadata_dict = metadata.to_dict()

    assert metadata_dict["title"] == "Test Video"
    assert metadata_dict["description"] == "A test video description"
    assert metadata_dict["tags"] == ["tag1", "tag2"]
    assert metadata_dict["thumbnail_url"] == "https://example.com/thumbnail.jpg"
    assert metadata_dict["category_id"] == "22"  # Default value
    assert metadata_dict["channel"] == "daily"  # Default value


def test_from_dict():
    """Test creation from dictionary."""
    metadata_dict = {
        "title": "Test Video",
        "description": "A test video description",
        "keywords": "test, video",
        "category_id": "10",
        "duration_seconds": 300,
        "width": 1920,
        "height": 1080,
        "channel": "main",
        "tags": ["tag1", "tag2"],
        "show_notes": "Test show notes",
        "thumbnail_url": "https://example.com/thumbnail.jpg",
        "transcript": "This is the transcript",
        "chapters": [{"time": "00:00", "title": "Introduction"}],
    }

    metadata = VideoMetadata.from_dict(metadata_dict)

    assert metadata.title == "Test Video"
    assert metadata.description == "A test video description"
    assert metadata.keywords == "test, video"
    assert metadata.category_id == "10"
    assert metadata.duration_seconds == 300
    assert metadata.width == 1920
    assert metadata.height == 1080
    assert metadata.channel == "main"
    assert metadata.tags == ["tag1", "tag2"]
    assert metadata.show_notes == "Test show notes"
    assert metadata.thumbnail_url == "https://example.com/thumbnail.jpg"
    assert metadata.transcript == "This is the transcript"
    assert metadata.chapters == [{"time": "00:00", "title": "Introduction"}]


def test_from_dict_with_missing_fields():
    """Test creation from dictionary with missing fields."""
    metadata_dict = {
        "title": "Test Video"
        # All other fields missing
    }

    metadata = VideoMetadata.from_dict(metadata_dict)

    assert metadata.title == "Test Video"
    assert metadata.description is None
    assert metadata.category_id == "22"  # Default
    assert metadata.channel == "daily"  # Default
    assert metadata.tags == []  # Default empty list
    assert metadata.chapters == []  # Default empty list

================
File: tests/unit/domain/test_video.py
================
"""
Unit tests for the Video domain model.
"""

from datetime import datetime

from video_processor.domain.models.video import Video


def test_video_creation():
    """Test that a Video object can be created with required attributes."""
    video = Video(
        id="test123",
        file_path="/path/to/video.mp4",
        file_name="video.mp4",
    )
    assert video.id == "test123"
    assert video.file_path == "/path/to/video.mp4"
    assert video.file_name == "video.mp4"
    assert video.created_at is not None
    assert isinstance(video.created_at, datetime)


def test_video_resolution():
    """Test the resolution property."""
    # Without dimensions
    video = Video(
        id="test123",
        file_path="/path/to/video.mp4",
        file_name="video.mp4",
    )
    assert video.resolution is None

    # With dimensions
    video = Video(
        id="test123",
        file_path="/path/to/video.mp4",
        file_name="video.mp4",
        width=1920,
        height=1080,
    )
    assert video.resolution == (1920, 1080)


def test_get_thumbnail_time():
    """Test thumbnail time calculation."""
    # Without duration
    video = Video(
        id="test123",
        file_path="/path/to/video.mp4",
        file_name="video.mp4",
    )
    assert video.get_thumbnail_time() == 0

    # With duration
    video = Video(
        id="test123",
        file_path="/path/to/video.mp4",
        file_name="video.mp4",
        duration=100.0,
    )
    assert video.get_thumbnail_time() == 20.0  # 20% of duration

    # Very short video
    video = Video(
        id="test123",
        file_path="/path/to/video.mp4",
        file_name="video.mp4",
        duration=3.0,
    )
    assert video.get_thumbnail_time() == 0.6  # 20% of 3 seconds


def test_get_file_extension():
    """Test file extension extraction."""
    video = Video(
        id="test123",
        file_path="/path/to/video.mp4",
        file_name="video.mp4",
    )
    assert video.get_file_extension() == "mp4"

    # No extension
    video = Video(
        id="test123",
        file_path="/path/to/video",
        file_name="video",
    )
    assert video.get_file_extension() == ""


def test_is_valid_video():
    """Test video validation based on extension."""
    # Valid extensions
    for ext in ["mp4", "mov", "avi", "mkv", "webm"]:
        video = Video(
            id="test123",
            file_path=f"/path/to/video.{ext}",
            file_name=f"video.{ext}",
        )
        assert video.is_valid_video() is True

    # Invalid extension
    video = Video(
        id="test123",
        file_path="/path/to/video.txt",
        file_name="video.txt",
    )
    assert video.is_valid_video() is False


def test_to_dict():
    """Test conversion to dictionary."""
    video = Video(
        id="test123",
        file_path="/path/to/video.mp4",
        file_name="video.mp4",
        file_size=1024,
        file_format="mp4",
        duration=60.0,
        width=1920,
        height=1080,
        bucket_name="my-bucket",
    )
    video_dict = video.to_dict()

    assert video_dict["id"] == "test123"
    assert video_dict["file_path"] == "/path/to/video.mp4"
    assert video_dict["file_name"] == "video.mp4"
    assert video_dict["file_size"] == 1024
    assert video_dict["file_format"] == "mp4"
    assert video_dict["duration"] == 60.0
    assert video_dict["width"] == 1920
    assert video_dict["height"] == 1080
    assert video_dict["bucket_name"] == "my-bucket"
    assert isinstance(video_dict["created_at"], str)


def test_from_dict():
    """Test creation from dictionary."""
    now = datetime.now()
    video_dict = {
        "id": "test123",
        "file_path": "/path/to/video.mp4",
        "file_name": "video.mp4",
        "file_size": 1024,
        "file_format": "mp4",
        "duration": 60.0,
        "width": 1920,
        "height": 1080,
        "created_at": now.isoformat(),
        "bucket_name": "my-bucket",
    }

    video = Video.from_dict(video_dict)

    assert video.id == "test123"
    assert video.file_path == "/path/to/video.mp4"
    assert video.file_name == "video.mp4"
    assert video.file_size == 1024
    assert video.file_format == "mp4"
    assert video.duration == 60.0
    assert video.width == 1920
    assert video.height == 1080
    assert video.bucket_name == "my-bucket"
    assert isinstance(video.created_at, datetime)

================
File: tests/conftest.py
================
"""
Pytest configuration file with common fixtures for testing.
"""

import os
import subprocess
import tempfile
from datetime import datetime
from unittest.mock import MagicMock, patch

import pytest
from google.cloud.storage import Client as StorageClient
from video_processor.application.interfaces.ai import AIServiceInterface
from video_processor.application.interfaces.publishing import PublishingInterface
from video_processor.application.interfaces.storage import StorageInterface
from video_processor.domain.models.enums import ProcessingStage, ProcessingStatus
from video_processor.domain.models.job import VideoJob
from video_processor.domain.models.metadata import VideoMetadata
from video_processor.domain.models.video import Video

# Set environment variables for testing
os.environ["GOOGLE_CLOUD_PROJECT"] = "test-project"


# Mock Google auth for all tests
@pytest.fixture(autouse=True, scope="session")
def mock_google_auth():
    """Mock Google Cloud authentication for all tests."""
    with patch("google.auth.default", return_value=(None, "test-project")):
        yield


@pytest.fixture
def test_settings():
    """Create test settings with testing mode enabled."""
    from video_processor.infrastructure.config.settings import Settings

    return Settings(
        project_id="test-project",
        region="test-region",
        testing_mode=True,
        gcs_upload_bucket="test-bucket",
        ai_model="test-model",
        default_privacy_status="unlisted",
    )


@pytest.fixture
def mock_storage_client():
    """Mock for Google Cloud Storage client."""
    mock_client = MagicMock(spec=StorageClient)
    mock_bucket = MagicMock()
    mock_blob = MagicMock()

    # Set up the chain of mocks
    mock_client.bucket.return_value = mock_bucket
    mock_bucket.blob.return_value = mock_blob
    mock_bucket.copy_blob.return_value = mock_blob
    mock_blob.exists.return_value = True

    return mock_client, mock_bucket, mock_blob


@pytest.fixture
def storage_adapter():
    """Mock for the StorageAdapter interface."""
    from video_processor.application.interfaces.storage import StorageInterface

    mock_adapter = MagicMock(spec=StorageInterface)

    # Configure methods to return appropriate values
    mock_adapter.upload_file.return_value = "processed/test_video.mp4"
    mock_adapter.download_file.return_value = "/tmp/test_video.mp4"
    mock_adapter.delete_file.return_value = True
    mock_adapter.get_public_url.return_value = (
        "https://storage.googleapis.com/test-bucket/test_video.mp4"
    )
    mock_adapter.get_signed_url.return_value = (
        "https://storage.googleapis.com/test-bucket/test_video.mp4?token=abc123"
    )

    return mock_adapter


@pytest.fixture
def ai_adapter():
    """Mock for AIServiceInterface."""
    from video_processor.application.interfaces.ai import AIServiceInterface

    mock_ai = MagicMock(spec=AIServiceInterface)

    # Configure methods to return appropriate values
    mock_ai.generate_transcript.return_value = "This is a test transcript."
    mock_ai.generate_metadata.return_value = {
        "title": "Test Video Title",
        "description": "This is a test video description.",
        "tags": ["test", "video", "example"],
    }
    mock_ai.generate_thumbnail_description.return_value = (
        "A person explaining a concept with a whiteboard"
    )
    mock_ai.summarize_content.return_value = "This is a summary of the video content."

    return mock_ai


@pytest.fixture
def publishing_adapter():
    """Mock for PublishingInterface."""
    from video_processor.application.interfaces.publishing import PublishingInterface

    mock_publishing = MagicMock(spec=PublishingInterface)

    # Configure methods to return appropriate values
    mock_publishing.upload_video.return_value = "test_video_id"
    mock_publishing.update_metadata.return_value = True
    mock_publishing.get_upload_status.return_value = "complete"
    mock_publishing.delete_video.return_value = True

    return mock_publishing


@pytest.fixture
def sample_audio_file():
    """Create a temporary WAV file for testing."""
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_file:
        temp_path = temp_file.name

    try:
        # Generate a simple test tone using ffmpeg
        subprocess.run(
            [
                "ffmpeg",
                "-y",  # Overwrite output files without asking
                "-f",
                "lavfi",  # Use libavfilter
                "-i",
                "sine=frequency=440:duration=1",  # Generate a 1-second 440Hz tone
                "-ar",
                "16000",  # Audio sample rate
                "-ac",
                "1",  # Mono audio
                temp_path,
            ],
            check=True,
            capture_output=True,
            text=True,
        )

        yield temp_path
    finally:
        # Clean up the temporary file
        if os.path.exists(temp_path):
            os.unlink(temp_path)


@pytest.fixture
def sample_video_file():
    """Create a temporary MP4 file for testing."""
    with tempfile.NamedTemporaryFile(suffix=".mp4", delete=False) as temp_file:
        temp_path = temp_file.name

    try:
        # Generate a simple test video using ffmpeg
        subprocess.run(
            [
                "ffmpeg",
                "-y",  # Overwrite output files without asking
                "-f",
                "lavfi",  # Use libavfilter
                "-i",
                "sine=frequency=440:duration=1",  # Generate a 1-second 440Hz tone
                "-f",
                "lavfi",  # Use libavfilter for video
                "-i",
                "color=c=blue:s=320x240:d=1",  # Generate a 1-second blue screen
                "-c:a",
                "aac",  # Audio codec
                "-c:v",
                "h264",  # Video codec
                temp_path,
            ],
            check=True,
            capture_output=True,
            text=True,
        )

        yield temp_path
    finally:
        # Clean up the temporary file
        if os.path.exists(temp_path):
            os.unlink(temp_path)


@pytest.fixture
def test_video():
    """Create a sample video for testing."""
    return Video(
        id="test123",
        file_path="/path/to/sample.mp4",
        file_name="sample.mp4",
        file_size=1024000,
        file_format="mp4",
        duration=60.0,
        width=1920,
        height=1080,
        bucket_name="test-bucket",
    )


@pytest.fixture
def test_metadata():
    """Create sample metadata for testing."""
    return VideoMetadata(
        title="Test Video Title",
        description="This is a test video description.",
        keywords="test, video, sample",
        category_id="22",  # People & Blogs
        duration_seconds=60,
        width=1920,
        height=1080,
        channel="daily",
        tags=["test", "video", "sample"],
        show_notes="These are test show notes.",
        thumbnail_url="https://example.com/thumbnail.jpg",
        transcript="This is a test transcript.",
        chapters=[
            {"time": "00:00", "title": "Introduction"},
            {"time": "00:30", "title": "Main Content"},
        ],
    )


@pytest.fixture
def test_job(test_video, test_metadata):
    """Create a sample job for testing."""
    return VideoJob(
        job_id="job123",
        video=test_video,
        created_at=datetime.now(),
        updated_at=datetime.now(),
        metadata=test_metadata,
        status=ProcessingStatus.IN_PROGRESS,
        current_stage=ProcessingStage.GENERATE_TRANSCRIPT,
        completed_stages=[ProcessingStage.DOWNLOAD, ProcessingStage.EXTRACT_AUDIO],
        error_message=None,
        processed_path="/processed/sample.mp4",
        output_files={
            "transcript": "/path/to/transcript.txt",
            "subtitles": "/path/to/subtitles.vtt",
        },
        youtube_video_id=None,
    )


@pytest.fixture
def mock_job_repository():
    """Mock for JobRepositoryInterface."""
    from video_processor.application.interfaces.repositories import (
        JobRepositoryInterface,
    )

    mock_repo = MagicMock(spec=JobRepositoryInterface)

    # Configure methods to return appropriate values
    mock_repo.get_by_id.return_value = test_job()
    mock_repo.save.return_value = "job_123"
    mock_repo.update.return_value = True
    mock_repo.delete.return_value = True

    return mock_repo


@pytest.fixture
def mock_cloud_event():
    """Create a mock Cloud Event for testing."""
    event = MagicMock()
    event.data = {
        "bucket": "test-bucket",
        "name": "daily-raw/test_video.mp4",
        "contentType": "video/mp4",
        "size": "1000000",
    }
    return event


@pytest.fixture
def mock_storage_adapter():
    """Create a mock storage adapter for testing."""
    mock_storage = MagicMock(spec=StorageInterface)

    # Configure default return values
    mock_storage.upload_file.return_value = "gs://test-bucket/uploaded/file.mp4"
    mock_storage.download_file.return_value = "/tmp/downloaded/file.mp4"
    mock_storage.delete_file.return_value = True
    mock_storage.get_public_url.return_value = (
        "https://storage.googleapis.com/test-bucket/file.mp4"
    )
    mock_storage.get_signed_url.return_value = (
        "https://storage.googleapis.com/test-bucket/file.mp4?token=abc123"
    )

    return mock_storage


@pytest.fixture
def mock_ai_adapter():
    """Create a mock AI adapter for testing."""
    mock_ai = MagicMock(spec=AIServiceInterface)

    # Configure default return values
    mock_ai.generate_transcript.return_value = "This is a test transcript."
    mock_ai.generate_metadata.return_value = {
        "title": "Generated Title",
        "description": "Generated description",
        "tags": ["ai", "generated", "tags"],
        "show_notes": "Generated show notes",
    }
    mock_ai.generate_thumbnail_description.return_value = (
        "A person explaining video processing"
    )
    mock_ai.summarize_content.return_value = "This is a summary of the video content."

    return mock_ai


@pytest.fixture
def mock_publishing_adapter():
    """Create a mock publishing adapter for testing."""
    mock_pub = MagicMock(spec=PublishingInterface)

    # Configure default return values
    mock_pub.upload_video.return_value = "yt12345"
    mock_pub.update_metadata.return_value = True
    mock_pub.get_upload_status.return_value = "published"
    mock_pub.delete_video.return_value = True

    return mock_pub

================
File: tests/README.md
================
# Video Processor Testing Guide

This directory contains the test suite for the Video Processing Pipeline. The tests follow a clean architecture approach, mirroring the structure of the main application.

## Test Structure

```
tests/
├── conftest.py            # Shared test fixtures and configurations
├── mocks/                 # Mock implementations of external dependencies
│   ├── ai.py              # Mock AI service adapter
│   ├── publishing.py      # Mock publishing adapter
│   └── storage.py         # Mock storage adapter
├── unit/                  # Unit tests for isolated components
│   ├── domain/            # Tests for domain models
│   ├── application/       # Tests for application services
│   └── adapters/          # Tests for adapter implementations
├── integration/           # Integration tests for component combinations
│   ├── api/               # API integration tests
│   ├── storage/           # Storage integration tests
│   └── ai/                # AI service integration tests
└── e2e/                   # End-to-end tests for complete workflows
```

## Running Tests

### Running All Tests

```bash
pytest
```

### Running Specific Test Types

```bash
# Run unit tests only
pytest tests/unit/

# Run integration tests only
pytest tests/integration/

# Run end-to-end tests only
pytest tests/e2e/
```

### Running Tests with Coverage

```bash
# Run tests with coverage report
pytest --cov=video_processor

# Generate HTML coverage report
pytest --cov=video_processor --cov-report=html
```

## Test Categories

### Unit Tests

Unit tests focus on testing individual components in isolation, using mock objects for dependencies. These tests ensure that each component behaves as expected on its own.

### Integration Tests

Integration tests verify that combinations of components work together correctly. These tests may use real external services in test environments.

### End-to-End Tests

End-to-end tests validate complete system workflows from start to finish. These tests ensure that the entire video processing pipeline functions correctly as a whole.

## Mock Objects

The `mocks/` directory contains mock implementations of external dependencies, such as storage, AI services, and publishing services. These mocks allow for reliable testing without requiring access to real external services.

## Test Fixtures

Common test fixtures are defined in `conftest.py`. These fixtures provide test data and mock objects that can be reused across multiple tests.

================
File: video_processor/adapters/ai/__init__.py
================
"""
AI service adapters for the video processing application.

This package contains implementations of AI service adapters for:
- Gemini AI: Google's Gemini AI model for content generation
- Vertex AI: Google Cloud Vertex AI for more complex AI operations
"""

================
File: video_processor/adapters/ai/cache.py
================
"""Caching layer for AI service responses."""

import hashlib
import json
import os
import time
from functools import wraps
from pathlib import Path
from typing import Any, Callable, Dict, Optional, TypeVar, Union, cast

from google.cloud import storage

from video_processor.infrastructure.monitoring import structured_log

# Type variable for decorator functions
F = TypeVar("F", bound=Callable[..., Any])

# Default cache location
DEFAULT_CACHE_DIR = Path(os.environ.get("AI_CACHE_DIR", "/tmp/ai_cache"))

# Cache invalidation time (default: 30 days)
DEFAULT_CACHE_TTL = int(
    os.environ.get("AI_CACHE_TTL", 60 * 60 * 24 * 30)
)  # 30 days in seconds

# Global cache statistics
_cache_stats = {
    "hits": 0,
    "misses": 0,
    "stores": 0,
    "errors": 0,
}


class AIResponseCache:
    """Cache for AI service responses."""

    def __init__(
        self,
        cache_dir: Optional[Union[str, Path]] = None,
        ttl: int = DEFAULT_CACHE_TTL,
        gcs_bucket: Optional[str] = None,
        prefix: str = "ai_cache",
    ):
        """Initialize cache.

        Args:
            cache_dir: Local directory for cache (default: /tmp/ai_cache)
            ttl: Cache TTL in seconds (default: 30 days)
            gcs_bucket: GCS bucket for remote cache (default: None)
            prefix: Prefix for remote cache keys (default: "ai_cache")
        """
        self.ttl = ttl
        self.gcs_bucket = gcs_bucket
        self.prefix = prefix

        # Set up local cache directory
        self.cache_dir = Path(cache_dir) if cache_dir else DEFAULT_CACHE_DIR
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # Set up GCS client if bucket specified
        self.gcs_client = None
        if gcs_bucket:
            try:
                self.gcs_client = storage.Client()
            except Exception as e:
                structured_log(
                    "error",
                    f"Failed to initialize GCS client for cache: {str(e)}",
                    {"error": str(e)},
                )

    def _generate_cache_key(self, prompt: str, **kwargs) -> str:
        """Generate a cache key from prompt and parameters.

        Args:
            prompt: Prompt text
            **kwargs: Additional parameters affecting the response

        Returns:
            Cache key
        """
        # Generate a stable cache key based on prompt and parameters
        key_data = {"prompt": prompt}

        # Add other parameters that affect the output
        for param_name in sorted(kwargs.keys()):
            param_value = kwargs[param_name]

            # Skip irrelevant parameters
            if param_name in ["api_key", "timeout"]:
                continue

            # Handle different parameter types
            if isinstance(param_value, (str, int, float, bool, type(None))):
                key_data[param_name] = param_value
            elif isinstance(param_value, (list, dict, tuple, set)):
                # Convert to JSON string for hashing
                key_data[param_name] = json.dumps(param_value, sort_keys=True)

        # Convert to string and hash
        key_str = json.dumps(key_data, sort_keys=True)
        return hashlib.sha256(key_str.encode("utf-8")).hexdigest()

    def _get_local_cache_path(self, cache_key: str) -> Path:
        """Get local file path for cache key.

        Args:
            cache_key: Cache key

        Returns:
            Path to cache file
        """
        return self.cache_dir / f"{cache_key}.json"

    def _get_gcs_cache_path(self, cache_key: str) -> str:
        """Get GCS object path for cache key.

        Args:
            cache_key: Cache key

        Returns:
            GCS object path
        """
        return f"{self.prefix}/{cache_key}.json"

    def get(self, prompt: str, **kwargs) -> Optional[Dict[str, Any]]:
        """Get cached response for prompt and parameters.

        Args:
            prompt: Prompt text
            **kwargs: Additional parameters affecting the response

        Returns:
            Cached response or None if not found
        """
        cache_key = self._generate_cache_key(prompt, **kwargs)

        # Try to get from local cache first
        local_path = self._get_local_cache_path(cache_key)
        if local_path.exists():
            try:
                with open(local_path, "r") as f:
                    cache_data = json.load(f)

                # Check if cache is expired
                if time.time() - cache_data.get("cached_at", 0) > self.ttl:
                    structured_log(
                        "info",
                        f"Cache expired for key {cache_key}",
                        {"cache_key": cache_key},
                    )
                    return None

                # Record cache hit
                _cache_stats["hits"] += 1

                structured_log(
                    "info",
                    f"Cache hit for key {cache_key}",
                    {"cache_key": cache_key},
                )

                return cache_data.get("response")

            except Exception as e:
                structured_log(
                    "error",
                    f"Failed to read from local cache: {str(e)}",
                    {"cache_key": cache_key, "error": str(e)},
                )
                _cache_stats["errors"] += 1

        # Try to get from GCS if available
        if self.gcs_client and self.gcs_bucket:
            try:
                bucket = self.gcs_client.bucket(self.gcs_bucket)
                blob = bucket.blob(self._get_gcs_cache_path(cache_key))

                if blob.exists():
                    cache_data = json.loads(blob.download_as_string())

                    # Check if cache is expired
                    if time.time() - cache_data.get("cached_at", 0) > self.ttl:
                        structured_log(
                            "info",
                            f"GCS cache expired for key {cache_key}",
                            {"cache_key": cache_key},
                        )
                        return None

                    # Store in local cache
                    try:
                        with open(local_path, "w") as f:
                            json.dump(cache_data, f)
                    except Exception as e:
                        structured_log(
                            "error",
                            f"Failed to store GCS cache locally: {str(e)}",
                            {"cache_key": cache_key, "error": str(e)},
                        )

                    # Record cache hit
                    _cache_stats["hits"] += 1

                    structured_log(
                        "info",
                        f"GCS cache hit for key {cache_key}",
                        {"cache_key": cache_key},
                    )

                    return cache_data.get("response")

            except Exception as e:
                structured_log(
                    "error",
                    f"Failed to read from GCS cache: {str(e)}",
                    {"cache_key": cache_key, "error": str(e)},
                )
                _cache_stats["errors"] += 1

        # Record cache miss
        _cache_stats["misses"] += 1

        return None

    def store(
        self,
        prompt: str,
        response: Dict[str, Any],
        metadata: Optional[Dict[str, Any]] = None,
        **kwargs,
    ) -> None:
        """Store response in cache.

        Args:
            prompt: Prompt text
            response: Response to cache
            metadata: Additional metadata to store
            **kwargs: Additional parameters affecting the response
        """
        cache_key = self._generate_cache_key(prompt, **kwargs)

        # Prepare cache data
        cache_data = {
            "prompt": prompt,
            "response": response,
            "cached_at": time.time(),
            "metadata": metadata or {},
            "parameters": {
                k: v for k, v in kwargs.items() if k not in ["api_key", "timeout"]
            },
        }

        # Store in local cache
        local_path = self._get_local_cache_path(cache_key)
        try:
            with open(local_path, "w") as f:
                json.dump(cache_data, f)
        except Exception as e:
            structured_log(
                "error",
                f"Failed to write to local cache: {str(e)}",
                {"cache_key": cache_key, "error": str(e)},
            )
            _cache_stats["errors"] += 1
            return

        # Store in GCS if available
        if self.gcs_client and self.gcs_bucket:
            try:
                bucket = self.gcs_client.bucket(self.gcs_bucket)
                blob = bucket.blob(self._get_gcs_cache_path(cache_key))
                blob.upload_from_string(json.dumps(cache_data))

                structured_log(
                    "info",
                    f"Stored response in GCS cache for key {cache_key}",
                    {"cache_key": cache_key},
                )

            except Exception as e:
                structured_log(
                    "error",
                    f"Failed to write to GCS cache: {str(e)}",
                    {"cache_key": cache_key, "error": str(e)},
                )
                _cache_stats["errors"] += 1
                return

        # Record cache store
        _cache_stats["stores"] += 1

        structured_log(
            "info",
            f"Stored response in cache for key {cache_key}",
            {"cache_key": cache_key},
        )


# Create a global cache instance
_global_cache = AIResponseCache(
    gcs_bucket=os.environ.get("AI_CACHE_BUCKET"),
)


def get_global_cache() -> AIResponseCache:
    """Get the global cache instance.

    Returns:
        Global cache instance
    """
    return _global_cache


def get_cache_stats() -> Dict[str, int]:
    """Get cache statistics.

    Returns:
        Cache statistics
    """
    return _cache_stats.copy()


def use_cache(func: F) -> F:
    """Decorator for caching AI service responses.

    Args:
        func: Function to decorate

    Returns:
        Decorated function
    """

    @wraps(func)
    def wrapper(*args, **kwargs):
        # Check if the first argument is a prompt or text
        prompt = None
        if args and isinstance(args[0], str):
            prompt = args[0]
        elif "prompt" in kwargs and isinstance(kwargs["prompt"], str):
            prompt = kwargs["prompt"]
        elif "text" in kwargs and isinstance(kwargs["text"], str):
            prompt = kwargs["text"]

        # If we have a prompt, try to get from cache
        if prompt:
            cache = get_global_cache()
            cached_response = cache.get(prompt, **kwargs)

            if cached_response is not None:
                return cached_response

        # If not in cache or no prompt, call the original function
        response = func(*args, **kwargs)

        # Store in cache if we have a prompt and response
        if prompt and response:
            cache = get_global_cache()
            cache.store(prompt, response, **kwargs)

        return response

    return cast(F, wrapper)


def clear_cache(cache_dir: Optional[Union[str, Path]] = None) -> None:
    """Clear the cache.

    Args:
        cache_dir: Local directory to clear (default: DEFAULT_CACHE_DIR)
    """
    cache_dir = Path(cache_dir) if cache_dir else DEFAULT_CACHE_DIR

    if cache_dir.exists():
        # Remove all cache files
        for file_path in cache_dir.glob("*.json"):
            try:
                file_path.unlink()
            except Exception as e:
                structured_log(
                    "error",
                    f"Failed to delete cache file {file_path}: {str(e)}",
                    {"file_path": str(file_path), "error": str(e)},
                )

        structured_log(
            "info",
            f"Cleared local cache in {cache_dir}",
            {"cache_dir": str(cache_dir)},
        )

================
File: video_processor/adapters/ai/gemini.py
================
"""
Gemini AI adapter implementation.

This module provides a concrete implementation of the AIServiceInterface
for Google's Gemini AI models, handling transcript and metadata generation.
"""

import json
import logging
import time
from typing import Dict, List, Optional

from vertexai.generative_models import GenerativeModel, Part

from video_processor.application.interfaces.ai import AIServiceInterface
from video_processor.domain.exceptions import (
    MetadataGenerationError,
    TranscriptionError,
)


class GeminiAIAdapter(AIServiceInterface):
    """
    Gemini AI implementation of AIServiceInterface.

    This adapter implements AI operations using Google's Gemini AI models.
    """

    def __init__(
        self, api_key: Optional[str] = None, model: str = "gemini-2.0-flash-001"
    ):
        """
        Initialize the Gemini AI Adapter.

        Args:
            api_key: Optional API key for Gemini (usually handled by environment)
            model: Model name to use (default: gemini-2.0-flash-001)
        """
        self._api_key = api_key
        self._model_name = model
        self._model = GenerativeModel(self._model_name)
        logging.info(f"Initialized Gemini AI adapter with model: {self._model_name}")

    def generate_transcript(self, audio_file: str) -> str:
        """
        Generate a transcript from an audio file.

        Args:
            audio_file: Path to the audio file

        Returns:
            The generated transcript text

        Raises:
            TranscriptionError: If transcript generation fails
        """
        try:
            # Create audio part from file
            audio_part = Part.from_uri(audio_file, mime_type="audio/wav")

            # Set prompt for transcript generation
            prompt = (
                "Generate a transcription of the audio, only extract speech "
                "and ignore background audio."
            )

            # Implement retry logic with exponential backoff
            max_retries = 3
            backoff = 1  # Initial backoff in seconds

            for attempt in range(max_retries):
                try:
                    response = self._model.generate_content(
                        [prompt, audio_part],
                        generation_config={
                            "temperature": 0.2
                        },  # Lower temp for accuracy
                    )

                    # Validate response
                    if not hasattr(response, "text") or not response.text:
                        raise TranscriptionError("Empty transcript generated")

                    return response.text.strip()
                except Exception as e:
                    if attempt == max_retries - 1:
                        # Last attempt failed, raise exception
                        raise
                    # Exponential backoff
                    logging.warning(
                        f"Transcript generation attempt {attempt+1} failed: {e}. Retrying..."
                    )
                    time.sleep(backoff)
                    backoff *= 2

        except Exception as e:
            error_msg = f"Failed to generate transcript: {str(e)}"
            logging.error(error_msg)
            raise TranscriptionError(error_msg) from e

    def generate_metadata(self, transcript: str) -> Dict:
        """
        Generate video metadata from a transcript.

        Args:
            transcript: Transcript text

        Returns:
            A dictionary containing generated metadata:
            {
                "title": str,
                "description": str,
                "tags": List[str],
                "show_notes": str,
                "chapters": List[Dict[str, str]]
            }

        Raises:
            MetadataGenerationError: If metadata generation fails
        """
        try:
            # Generate title and tags
            title_tags = self._generate_title_tags(transcript)

            # Generate show notes
            show_notes = self.summarize_content(transcript, max_length=2000)

            # Generate chapters
            chapters = self.generate_chapters(transcript)

            # Compose the complete metadata
            metadata = {
                "title": title_tags.get("Description", "Untitled Video"),
                "description": show_notes[:500],  # Shorter version for description
                "tags": title_tags.get("Keywords", "").split(","),
                "show_notes": show_notes,
                "chapters": chapters,
            }

            return metadata

        except Exception as e:
            error_msg = f"Failed to generate metadata: {str(e)}"
            logging.error(error_msg)
            raise MetadataGenerationError(error_msg) from e

    def generate_thumbnail_description(self, transcript: str, timestamp: float) -> str:
        """
        Generate a description for a thumbnail at a specific timestamp.

        Args:
            transcript: Transcript text
            timestamp: Time in seconds for the thumbnail

        Returns:
            A text description for the thumbnail image

        Raises:
            MetadataGenerationError: If description generation fails
        """
        try:
            # Format timestamp as timecode for readability
            minutes = int(timestamp // 60)
            seconds = int(timestamp % 60)
            timecode = f"{minutes:02d}:{seconds:02d}"

            # Create prompt for thumbnail description
            prompt = (
                f"Based on the transcript, describe what is likely happening in the video "
                f"at timestamp {timecode}. This will be used to create a thumbnail image. "
                f"Keep it brief (30 words max) and focus on visually descriptive elements. "
                f"Do not include the timestamp in your description. "
                f"Transcript: {transcript[:2000]}..."  # Limit transcript length
            )

            response = self._model.generate_content(
                prompt,
                generation_config={"temperature": 0.7, "max_output_tokens": 100},
            )

            return response.text.strip()

        except Exception as e:
            error_msg = f"Failed to generate thumbnail description: {str(e)}"
            logging.error(error_msg)
            raise MetadataGenerationError(error_msg) from e

    def summarize_content(self, transcript: str, max_length: int = 500) -> str:
        """
        Generate a summary of the content from a transcript.

        Args:
            transcript: Transcript text
            max_length: Maximum length of the summary in characters

        Returns:
            A summary of the content

        Raises:
            MetadataGenerationError: If summary generation fails
        """
        try:
            prompt = (
                f"Create detailed show notes for this video transcript. "
                f"Include key takeaways, any mentioned resources, and highlight "
                f"important points. Format with Markdown headings and bullet points. "
                f"Maximum length: {max_length} characters."
            )

            response = self._model.generate_content(
                [prompt, transcript],
                generation_config={
                    "temperature": 0.7,
                    "max_output_tokens": max(1024, max_length // 2),
                },
            )

            summary = response.text.strip()

            # Truncate if needed while preserving complete sentences
            if len(summary) > max_length:
                truncated = summary[:max_length]
                last_period = truncated.rfind(".")
                if (
                    last_period > max_length * 0.7
                ):  # Only truncate if we're not losing too much
                    summary = truncated[: last_period + 1]

            return summary

        except Exception as e:
            error_msg = f"Failed to generate content summary: {str(e)}"
            logging.error(error_msg)
            raise MetadataGenerationError(error_msg) from e

    def set_model(self, model_name: str) -> None:
        """
        Set the AI model to use for generation.

        Args:
            model_name: Name of the model to use

        Raises:
            ValueError: If the model name is invalid
        """
        valid_models = [
            "gemini-1.5-pro",
            "gemini-1.5-flash",
            "gemini-2.0-pro",
            "gemini-2.0-flash-001",
        ]

        if model_name not in valid_models:
            logging.warning(
                f"Model {model_name} not in known models: {valid_models}. Using anyway."
            )

        self._model_name = model_name
        self._model = GenerativeModel(self._model_name)
        logging.info(f"Switched to model: {self._model_name}")

    def generate_chapters(
        self, transcript: str, num_chapters: Optional[int] = None
    ) -> List[Dict]:
        """
        Generate chapters from a transcript.

        Args:
            transcript: Transcript text
            num_chapters: Optional number of chapters to generate,
                          or None to let the AI determine the optimal number

        Returns:
            A list of chapter dictionaries:
            [
                {"title": str, "start_time": float, "end_time": float},
                ...
            ]

        Raises:
            MetadataGenerationError: If chapter generation fails
        """
        try:
            # Construct the prompt
            chapter_prompt = (
                "Chapterize the video content by grouping the content into chapters "
                "and providing a summary for each chapter. "
            )

            if num_chapters:
                chapter_prompt += f"Create exactly {num_chapters} chapters. "

            chapter_prompt += (
                "Please only capture key events and highlights. "
                "Return the result ONLY as a valid JSON array of objects, "
                'where each object has the keys "title" (string), '
                '"start_time" (float, seconds from start), and '
                '"end_time" (float, seconds from start). '
                "Example JSON output format:\n"
                "[\n"
                '  {"title": "Introduction to the topic", "start_time": 0.0, "end_time": 120.5},\n'
                '  {"title": "First main point", "start_time": 120.5, "end_time": 300.0}\n'
                "]"
            )

            response = self._model.generate_content(
                [chapter_prompt, transcript],
                generation_config={
                    "temperature": 0.6,
                    "response_mime_type": "application/json",
                },
            )

            # Parse and validate the JSON response
            try:
                chapter_list = json.loads(response.text)

                # Basic validation
                if not isinstance(chapter_list, list):
                    raise ValueError("Response is not a list")

                for chapter in chapter_list:
                    if not isinstance(chapter, dict):
                        raise ValueError("Chapter is not a dictionary")
                    if not all(
                        key in chapter for key in ["title", "start_time", "end_time"]
                    ):
                        raise ValueError("Chapter missing required keys")

                return chapter_list

            except json.JSONDecodeError as e:
                logging.error(
                    f"Failed to parse chapters JSON: {e}. Raw response: {response.text}"
                )
                raise ValueError(f"Invalid JSON response: {e}")

        except Exception as e:
            error_msg = f"Failed to generate chapters: {str(e)}"
            logging.error(error_msg)
            raise MetadataGenerationError(error_msg) from e

    def _generate_title_tags(self, transcript: str) -> Dict[str, str]:
        """
        Generate title and tags as a dictionary from transcript.

        Args:
            transcript: Transcript text

        Returns:
            Dictionary with "Description" (title) and "Keywords" (comma-separated tags)

        Raises:
            MetadataGenerationError: If generation fails
        """
        try:
            prompt = (
                "Write a 40-character long intriguing title for this video "
                "and 10 comma-separated hashtags suitable for YouTube "
                "based on the transcript. Format the response strictly as a valid JSON object "
                "with two keys: 'Description' (containing the title, max 50 characters) "
                "and 'Keywords' (containing the comma-separated hashtags as a single string)."
            )

            response = self._model.generate_content(
                [prompt, transcript],
                generation_config={
                    "temperature": 0.8,
                    "response_mime_type": "application/json",
                },
            )

            # Parse the JSON response
            try:
                title_dict = json.loads(response.text)

                # Basic validation
                if not isinstance(title_dict, dict):
                    raise ValueError("Response is not a dictionary")

                if "Description" not in title_dict or "Keywords" not in title_dict:
                    raise ValueError("Response missing required keys")

                return title_dict

            except json.JSONDecodeError as e:
                logging.error(
                    f"Failed to parse title/tags JSON: {e}. Raw response: {response.text}"
                )
                # Fallback to default
                return {
                    "Description": "Untitled Video",
                    "Keywords": "video,content,media",
                }

        except Exception as e:
            error_msg = f"Failed to generate title and tags: {str(e)}"
            logging.error(error_msg)
            raise MetadataGenerationError(error_msg) from e

================
File: video_processor/adapters/ai/parallel.py
================
"""Parallel processing utilities for AI services."""

import asyncio
import concurrent.futures
import functools
import time
from typing import Any, Callable, Dict, List, Optional, TypeVar

from video_processor.infrastructure.monitoring import structured_log
from video_processor.utils.profiling import Timer

# Type variables
T = TypeVar("T")
R = TypeVar("R")


class ParallelProcessor:
    """Parallel processor for AI requests.

    This class provides methods for processing multiple AI requests in parallel,
    using either threading or asyncio.
    """

    def __init__(
        self,
        max_workers: Optional[int] = None,
        timeout: float = 60.0,
    ):
        """Initialize parallel processor.

        Args:
            max_workers: Maximum number of worker threads (default: None = CPU count)
            timeout: Timeout in seconds for each request (default: 60.0)
        """
        self.max_workers = max_workers
        self.timeout = timeout

    def map_threaded(self, func: Callable[[T], R], items: List[T], **kwargs) -> List[R]:
        """Process items in parallel using threads.

        Args:
            func: Function to call for each item
            items: List of items to process
            **kwargs: Additional keyword arguments to pass to func

        Returns:
            List of results in the same order as the input items
        """
        with Timer("parallel_map_threaded") as timer:
            with concurrent.futures.ThreadPoolExecutor(
                max_workers=self.max_workers
            ) as executor:
                # Wrap function to include kwargs
                if kwargs:
                    func_with_kwargs = functools.partial(func, **kwargs)
                else:
                    func_with_kwargs = func

                # Submit all tasks
                future_to_index = {
                    executor.submit(func_with_kwargs, item): i
                    for i, item in enumerate(items)
                }

                # Prepare results list with placeholders
                results = [None] * len(items)

                # Process completed futures as they complete
                for future in concurrent.futures.as_completed(
                    future_to_index, timeout=self.timeout
                ):
                    index = future_to_index[future]
                    try:
                        results[index] = future.result()
                    except Exception as exc:
                        structured_log(
                            "error",
                            f"Error processing item {index}: {exc}",
                            {"error": str(exc), "item_index": index},
                        )
                        results[index] = None

        structured_log(
            "info",
            f"Processed {len(items)} items in {timer.end_time - timer.start_time:.2f} seconds using threads",
            {
                "item_count": len(items),
                "duration": timer.end_time - timer.start_time,
                "throughput": len(items) / (timer.end_time - timer.start_time),
            },
        )

        return results

    async def map_async(
        self, func: Callable[[T], R], items: List[T], **kwargs
    ) -> List[R]:
        """Process items in parallel using asyncio.

        Note: This requires that the function `func` is a standard synchronous function,
        not a coroutine function. It will be run in a thread pool.

        Args:
            func: Function to call for each item (must be a regular function, not a coroutine)
            items: List of items to process
            **kwargs: Additional keyword arguments to pass to func

        Returns:
            List of results in the same order as the input items
        """
        with Timer("parallel_map_async") as timer:
            loop = asyncio.get_event_loop()

            # Prepare results list with placeholders
            results = [None] * len(items)

            # Create a semaphore to limit concurrency
            semaphore = asyncio.Semaphore(self.max_workers or 10)

            async def process_item(index: int, item: T) -> None:
                """Process a single item with semaphore limiting."""
                async with semaphore:
                    try:
                        # Run synchronous function in a thread pool
                        if kwargs:
                            result = await loop.run_in_executor(
                                None, functools.partial(func, **kwargs), item
                            )
                        else:
                            result = await loop.run_in_executor(None, func, item)

                        results[index] = result
                    except Exception as exc:
                        structured_log(
                            "error",
                            f"Error processing item {index}: {exc}",
                            {"error": str(exc), "item_index": index},
                        )
                        results[index] = None

            # Create tasks for all items
            tasks = [process_item(i, item) for i, item in enumerate(items)]

            # Wait for all tasks to complete
            await asyncio.gather(*tasks)

        structured_log(
            "info",
            f"Processed {len(items)} items in {timer.end_time - timer.start_time:.2f} seconds using asyncio",
            {
                "item_count": len(items),
                "duration": timer.end_time - timer.start_time,
                "throughput": len(items) / (timer.end_time - timer.start_time),
            },
        )

        return results


class AsyncBatcher:
    """Batches async operations and processes them together.

    This class is useful for batching together multiple similar AI requests,
    which can lead to better utilization of API quotas and network resources.
    """

    def __init__(
        self,
        batch_size: int = 10,
        max_wait_time: float = 0.5,
    ):
        """Initialize the batcher.

        Args:
            batch_size: Maximum number of items in a batch (default: 10)
            max_wait_time: Maximum time to wait for a batch to fill (default: 0.5 seconds)
        """
        self.batch_size = batch_size
        self.max_wait_time = max_wait_time
        self.batch: List[Dict[str, Any]] = []
        self.lock = asyncio.Lock()
        self.batch_event = asyncio.Event()
        self.batch_task = None
        self.processing = False

    async def add_item(
        self, item: Any, processor: Callable[[List[Any]], List[Any]]
    ) -> Any:
        """Add an item to the current batch and wait for processing.

        Args:
            item: Item to process
            processor: Function to process a batch of items

        Returns:
            The processed result for this item
        """
        # Create a future to get the result for this item
        result_future = asyncio.Future()

        async with self.lock:
            # Add the item and its result future to the batch
            batch_index = len(self.batch)
            self.batch.append(
                {
                    "item": item,
                    "future": result_future,
                    "added_at": time.time(),
                }
            )

            # Start the batch processor task if not already running
            if not self.processing:
                self.processing = True
                self.batch_task = asyncio.create_task(self._process_batch(processor))

            # If batch is full, trigger processing immediately
            if len(self.batch) >= self.batch_size:
                self.batch_event.set()

        # Wait for the result
        try:
            return await result_future
        except Exception as e:
            structured_log(
                "error",
                f"Error getting batch result: {str(e)}",
                {"error": str(e)},
            )
            raise

    async def _process_batch(self, processor: Callable[[List[Any]], List[Any]]) -> None:
        """Process the current batch.

        Args:
            processor: Function to process a batch of items
        """
        while True:
            # Wait for the batch to fill or timeout
            try:
                # Check if we should process the batch
                should_process = False

                async with self.lock:
                    if len(self.batch) > 0:
                        # Process if batch is full or oldest item has waited too long
                        if len(self.batch) >= self.batch_size:
                            should_process = True
                        else:
                            oldest_time = self.batch[0]["added_at"]
                            if time.time() - oldest_time >= self.max_wait_time:
                                should_process = True

                if should_process:
                    await self._do_process_batch(processor)
                else:
                    # Wait a bit and check again
                    await asyncio.sleep(0.1)

                # Reset event
                self.batch_event.clear()

                # Break if no items left
                async with self.lock:
                    if len(self.batch) == 0:
                        self.processing = False
                        break

            except Exception as e:
                structured_log(
                    "error",
                    f"Error in batch processor: {str(e)}",
                    {"error": str(e)},
                )

                # Reset processing flag after error
                async with self.lock:
                    self.processing = False
                    break

    async def _do_process_batch(
        self, processor: Callable[[List[Any]], List[Any]]
    ) -> None:
        """Process the current batch and resolve futures.

        Args:
            processor: Function to process a batch of items
        """
        current_batch = []
        futures = []

        # Get current batch under lock
        async with self.lock:
            current_batch = [item["item"] for item in self.batch]
            futures = [item["future"] for item in self.batch]
            self.batch = []

        # Process the batch
        try:
            with Timer("batch_processing") as timer:
                results = processor(current_batch)

            structured_log(
                "info",
                f"Processed batch of {len(current_batch)} items in {timer.end_time - timer.start_time:.2f} seconds",
                {
                    "batch_size": len(current_batch),
                    "duration": timer.end_time - timer.start_time,
                },
            )

            # Set results for each future
            for i, future in enumerate(futures):
                if i < len(results):
                    future.set_result(results[i])
                else:
                    future.set_exception(
                        IndexError(
                            "Batch processor returned fewer results than expected"
                        )
                    )

        except Exception as e:
            structured_log(
                "error",
                f"Error processing batch: {str(e)}",
                {"error": str(e), "batch_size": len(current_batch)},
            )

            # Set exception for all futures
            for future in futures:
                if not future.done():
                    future.set_exception(e)


# Create a default parallel processor
default_processor = ParallelProcessor()


def parallel_map(func: Callable[[T], R], items: List[T], **kwargs) -> List[R]:
    """Process items in parallel using threads.

    This is a convenience function that uses the default parallel processor.

    Args:
        func: Function to call for each item
        items: List of items to process
        **kwargs: Additional keyword arguments to pass to func

    Returns:
        List of results in the same order as the input items
    """
    return default_processor.map_threaded(func, items, **kwargs)


async def parallel_map_async(
    func: Callable[[T], R], items: List[T], **kwargs
) -> List[R]:
    """Process items in parallel using asyncio.

    This is a convenience function that uses the default parallel processor.

    Args:
        func: Function to call for each item (must be a regular function, not a coroutine)
        items: List of items to process
        **kwargs: Additional keyword arguments to pass to func

    Returns:
        List of results in the same order as the input items
    """
    return await default_processor.map_async(func, items, **kwargs)

================
File: video_processor/adapters/ai/vertex_ai.py
================
"""
Vertex AI adapter implementation.

This module provides a concrete implementation of the AIServiceInterface
for Google Cloud Vertex AI, offering advanced AI capabilities beyond basic Gemini models.
"""

import json
import logging
import time
from typing import Dict, List, Optional

from google.cloud import aiplatform
from vertexai.preview.generative_models import GenerativeModel, Part

from video_processor.application.interfaces.ai import AIServiceInterface
from video_processor.domain.exceptions import (
    MetadataGenerationError,
    TranscriptionError,
)


class VertexAIAdapter(AIServiceInterface):
    """
    Vertex AI implementation of AIServiceInterface.

    This adapter implements AI operations using Google's Vertex AI platform,
    which provides more advanced features and models compared to basic Gemini.
    """

    def __init__(
        self,
        project_id: Optional[str] = None,
        location: str = "us-central1",
        model: str = "gemini-2.0-pro",
    ):
        """
        Initialize the Vertex AI Adapter.

        Args:
            project_id: Google Cloud project ID (if None, will use default from environment)
            location: Google Cloud region to use
            model: Model name to use (default: gemini-2.0-pro)
        """
        self._project_id = project_id
        self._location = location
        self._model_name = model

        # Initialize Vertex AI client
        aiplatform.init(project=self._project_id, location=self._location)

        # Initialize the model
        self._model = GenerativeModel(self._model_name)

        logging.info(
            f"Initialized Vertex AI adapter with project: {self._project_id}, "
            f"location: {self._location}, model: {self._model_name}"
        )

    def generate_transcript(self, audio_file: str) -> str:
        """
        Generate a transcript from an audio file.

        Args:
            audio_file: Path to the audio file

        Returns:
            The generated transcript text

        Raises:
            TranscriptionError: If transcript generation fails
        """
        try:
            # Create audio part from file
            audio_part = Part.from_uri(audio_file, mime_type="audio/wav")

            # Set prompt for transcript generation
            prompt = (
                "Generate a detailed, accurate transcription of the audio, "
                "only extract speech and ignore background audio. "
                "Include speaker labels if multiple speakers are detected."
            )

            # Implement retry logic with exponential backoff
            max_retries = 3
            backoff = 1  # Initial backoff in seconds

            for attempt in range(max_retries):
                try:
                    response = self._model.generate_content(
                        [prompt, audio_part],
                        generation_config={
                            "temperature": 0.2,  # Lower temp for accuracy
                            "max_output_tokens": 8192,  # Allow longer transcripts
                        },
                    )

                    # Validate response
                    if not hasattr(response, "text") or not response.text:
                        raise TranscriptionError("Empty transcript generated")

                    return response.text.strip()
                except Exception as e:
                    if attempt == max_retries - 1:
                        # Last attempt failed, raise exception
                        raise
                    # Exponential backoff
                    logging.warning(
                        f"Transcript generation attempt {attempt+1} failed: {e}. Retrying..."
                    )
                    time.sleep(backoff)
                    backoff *= 2

        except Exception as e:
            error_msg = f"Failed to generate transcript: {str(e)}"
            logging.error(error_msg)
            raise TranscriptionError(error_msg) from e

    def generate_metadata(self, transcript: str) -> Dict:
        """
        Generate video metadata from a transcript.

        Args:
            transcript: Transcript text

        Returns:
            A dictionary containing generated metadata:
            {
                "title": str,
                "description": str,
                "tags": List[str],
                "show_notes": str,
                "chapters": List[Dict[str, str]]
            }

        Raises:
            MetadataGenerationError: If metadata generation fails
        """
        try:
            # Generate content with a single comprehensive prompt to reduce API calls
            prompt = (
                "Generate comprehensive metadata for a video based on this transcript. "
                "Return ONLY a valid JSON object with the following structure:\n"
                "{\n"
                '  "title": "An engaging, SEO-optimized title under 60 characters",\n'
                '  "description": "A compelling 2-3 sentence description of the video content",\n'
                '  "tags": ["tag1", "tag2", "tag3", ...],\n'  # List of 8-10 relevant tags
                '  "show_notes": "Detailed markdown-formatted notes with sections and bullet points",\n'
                '  "chapters": [\n'
                '    {"title": "Chapter title", "start_time": 0.0, "end_time": 120.5},\n'
                '    {"title": "Next chapter", "start_time": 120.5, "end_time": 300.0},\n'
                "    ...\n"
                "  ]\n"
                "}\n"
                "Ensure the JSON is valid and each field follows the format described."
            )

            response = self._model.generate_content(
                [prompt, transcript],
                generation_config={
                    "temperature": 0.7,
                    "max_output_tokens": 4096,
                    "response_mime_type": "application/json",
                },
            )

            # Parse and validate the JSON response
            try:
                metadata = json.loads(response.text)

                # Basic validation
                required_keys = [
                    "title",
                    "description",
                    "tags",
                    "show_notes",
                    "chapters",
                ]
                if not all(key in metadata for key in required_keys):
                    missing_keys = [key for key in required_keys if key not in metadata]
                    raise ValueError(
                        f"Missing required keys in metadata: {missing_keys}"
                    )

                # Additional validation for nested structures
                if not isinstance(metadata["tags"], list):
                    metadata["tags"] = (
                        metadata["tags"].split(",")
                        if isinstance(metadata["tags"], str)
                        else []
                    )

                if not isinstance(metadata["chapters"], list):
                    metadata["chapters"] = []

                return metadata

            except json.JSONDecodeError as e:
                logging.error(
                    f"Failed to parse metadata JSON: {e}. Raw response: {response.text}"
                )
                raise ValueError(f"Invalid JSON response: {e}")

        except Exception as e:
            error_msg = f"Failed to generate metadata: {str(e)}"
            logging.error(error_msg)
            raise MetadataGenerationError(error_msg) from e

    def generate_thumbnail_description(self, transcript: str, timestamp: float) -> str:
        """
        Generate a description for a thumbnail at a specific timestamp.

        Args:
            transcript: Transcript text
            timestamp: Time in seconds for the thumbnail

        Returns:
            A text description for the thumbnail image

        Raises:
            MetadataGenerationError: If description generation fails
        """
        try:
            # Format timestamp as timecode for readability
            minutes = int(timestamp // 60)
            seconds = int(timestamp % 60)
            timecode = f"{minutes:02d}:{seconds:02d}"

            # Create context by extracting relevant portion of transcript
            # Find text near the timestamp (approximate)
            context = self._extract_context_around_timestamp(transcript, timestamp)

            # Create prompt for thumbnail description
            prompt = (
                f"Based on this portion of the transcript around timestamp {timecode}, "
                f"describe what is likely happening visually in the video at this moment. "
                f"Focus on concrete visual elements that would make a compelling thumbnail. "
                f"Keep your description under 30 words and make it visually descriptive. "
                f"Do not include the timestamp in your description.\n\n"
                f"Transcript context: {context}"
            )

            response = self._model.generate_content(
                prompt,
                generation_config={"temperature": 0.7, "max_output_tokens": 100},
            )

            return response.text.strip()

        except Exception as e:
            error_msg = f"Failed to generate thumbnail description: {str(e)}"
            logging.error(error_msg)
            raise MetadataGenerationError(error_msg) from e

    def summarize_content(self, transcript: str, max_length: int = 500) -> str:
        """
        Generate a summary of the content from a transcript.

        Args:
            transcript: Transcript text
            max_length: Maximum length of the summary in characters

        Returns:
            A summary of the content

        Raises:
            MetadataGenerationError: If summary generation fails
        """
        try:
            prompt = (
                f"Create detailed show notes for this video transcript. "
                f"Include key takeaways, any mentioned resources, and important points. "
                f"Format with Markdown headings (##) and bullet points. "
                f"Include a brief introduction, main sections with key points, "
                f"and a conclusion or summary section. "
                f"Maximum length: {max_length} characters."
            )

            response = self._model.generate_content(
                [prompt, transcript],
                generation_config={
                    "temperature": 0.7,
                    "max_output_tokens": max(1024, max_length // 2),
                },
            )

            summary = response.text.strip()

            # Truncate if needed while preserving complete sentences
            if len(summary) > max_length:
                truncated = summary[:max_length]
                last_period = truncated.rfind(".")
                if (
                    last_period > max_length * 0.7
                ):  # Only truncate if we're not losing too much
                    summary = truncated[: last_period + 1]

            return summary

        except Exception as e:
            error_msg = f"Failed to generate content summary: {str(e)}"
            logging.error(error_msg)
            raise MetadataGenerationError(error_msg) from e

    def set_model(self, model_name: str) -> None:
        """
        Set the AI model to use for generation.

        Args:
            model_name: Name of the model to use

        Raises:
            ValueError: If the model name is invalid
        """
        valid_models = [
            "gemini-1.5-pro",
            "gemini-1.5-flash",
            "gemini-2.0-pro",
            "gemini-2.0-flash-001",
            "text-bison",
            "text-unicorn",
        ]

        if model_name not in valid_models:
            logging.warning(
                f"Model {model_name} not in known models: {valid_models}. Using anyway."
            )

        self._model_name = model_name

        # Reinitialize the model
        if model_name.startswith("gemini"):
            self._model = GenerativeModel(self._model_name)
        else:
            # For non-Gemini models, use a different initialization approach
            self._model = aiplatform.Model.get_model_from_name(model_name)

        logging.info(f"Switched to model: {self._model_name}")

    def generate_chapters(
        self, transcript: str, num_chapters: Optional[int] = None
    ) -> List[Dict]:
        """
        Generate chapters from a transcript.

        Args:
            transcript: Transcript text
            num_chapters: Optional number of chapters to generate,
                          or None to let the AI determine the optimal number

        Returns:
            A list of chapter dictionaries:
            [
                {"title": str, "start_time": float, "end_time": float},
                ...
            ]

        Raises:
            MetadataGenerationError: If chapter generation fails
        """
        try:
            # Construct the prompt
            chapter_prompt = (
                "Analyze this transcript and divide it into logical chapters. "
            )

            if num_chapters:
                chapter_prompt += f"Create exactly {num_chapters} chapters. "

            chapter_prompt += (
                "For each chapter, provide a concise title and estimate the start and end times. "
                "Return the result ONLY as a valid JSON array of objects, "
                'where each object has the keys "title" (string), '
                '"start_time" (float, seconds from start), and '
                '"end_time" (float, seconds from start). '
                "Example JSON output format:\n"
                "[\n"
                '  {"title": "Introduction to the topic", "start_time": 0.0, "end_time": 120.5},\n'
                '  {"title": "First main point", "start_time": 120.5, "end_time": 300.0}\n'
                "]"
            )

            response = self._model.generate_content(
                [chapter_prompt, transcript],
                generation_config={
                    "temperature": 0.6,
                    "response_mime_type": "application/json",
                },
            )

            # Parse and validate the JSON response
            try:
                chapter_list = json.loads(response.text)

                # Basic validation
                if not isinstance(chapter_list, list):
                    raise ValueError("Response is not a list")

                for chapter in chapter_list:
                    if not isinstance(chapter, dict):
                        raise ValueError("Chapter is not a dictionary")
                    if not all(
                        key in chapter for key in ["title", "start_time", "end_time"]
                    ):
                        raise ValueError("Chapter missing required keys")

                return chapter_list

            except json.JSONDecodeError as e:
                logging.error(
                    f"Failed to parse chapters JSON: {e}. Raw response: {response.text}"
                )
                raise ValueError(f"Invalid JSON response: {e}")

        except Exception as e:
            error_msg = f"Failed to generate chapters: {str(e)}"
            logging.error(error_msg)
            raise MetadataGenerationError(error_msg) from e

    def _extract_context_around_timestamp(
        self, transcript: str, timestamp: float, context_window: int = 200
    ) -> str:
        """
        Extract a portion of the transcript around the specified timestamp.

        This is a simple heuristic that assumes the transcript is roughly chronological.
        A more accurate implementation would require a timestamped transcript.

        Args:
            transcript: The full transcript text
            timestamp: The timestamp in seconds
            context_window: Number of characters to include before and after

        Returns:
            A portion of the transcript around the timestamp
        """
        # Estimate position in transcript based on timestamp
        # This is a rough heuristic assuming average speaking rate
        total_duration = len(transcript) / 10  # Rough estimate: 10 chars per second

        if total_duration == 0:
            return transcript

        position_ratio = timestamp / total_duration
        estimated_position = int(len(transcript) * position_ratio)

        # Get a window around the estimated position
        start = max(0, estimated_position - context_window)
        end = min(len(transcript), estimated_position + context_window)

        # Try to start at the beginning of a sentence
        while start > 0 and transcript[start] != ".":
            start -= 1
        if start > 0:
            start += 1  # Skip the period

        # Try to end at the end of a sentence
        while end < len(transcript) - 1 and transcript[end] != ".":
            end += 1
        if end < len(transcript) - 1:
            end += 1  # Include the period

        return transcript[start:end].strip()

================
File: video_processor/adapters/publishing/__init__.py
================
"""
Publishing adapters for video distribution.

This package contains implementations of the PublishingInterface for
various platforms like YouTube, Vimeo, etc.
"""

from video_processor.adapters.publishing.youtube import YouTubeAdapter

__all__ = ["YouTubeAdapter"]

================
File: video_processor/adapters/publishing/youtube.py
================
"""
YouTube adapter for publishing videos.

This module provides an implementation of the PublishingInterface using
the YouTube Data API for uploading videos and managing metadata.
"""

import http.client
import json
import logging
import os
import time
from typing import Any, Dict, List, Optional

import google.oauth2.credentials
import httplib2
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from googleapiclient.http import MediaFileUpload

from video_processor.application.interfaces.publishing import PublishingInterface
from video_processor.domain.exceptions import PublishingError

# Configure logger
logger = logging.getLogger(__name__)

# YouTube API constants
YOUTUBE_API_SERVICE_NAME = "youtube"
YOUTUBE_API_VERSION = "v3"
RETRIABLE_EXCEPTIONS = (
    httplib2.HttpLib2Error,
    IOError,
    http.client.NotConnected,
    http.client.IncompleteRead,
    http.client.ImproperConnectionState,
    http.client.CannotSendRequest,
    http.client.CannotSendHeader,
    http.client.ResponseNotReady,
    http.client.BadStatusLine,
)
RETRIABLE_STATUS_CODES = [500, 502, 503, 504]
MAX_RETRIES = 10
RETRIABLE_EXCEPTIONS = (httplib2.HttpLib2Error, IOError)


class YouTubeAdapter(PublishingInterface):
    """YouTube adapter for publishing videos and managing metadata."""

    def __init__(
        self,
        client_secrets_file: str,
        oauth_token_file: str,
        scopes: Optional[List[str]] = None,
        api_service_name: str = YOUTUBE_API_SERVICE_NAME,
        api_version: str = YOUTUBE_API_VERSION,
    ):
        """
        Initialize the YouTube adapter.

        Args:
            client_secrets_file: Path to the client secrets file
            oauth_token_file: Path to the OAuth token file
            scopes: List of OAuth scopes (default: upload and read)
            api_service_name: YouTube API service name
            api_version: YouTube API version
        """
        self._client_secrets_file = client_secrets_file
        self._oauth_token_file = oauth_token_file
        self._scopes = scopes or [
            "https://www.googleapis.com/auth/youtube.upload",
            "https://www.googleapis.com/auth/youtube",
            "https://www.googleapis.com/auth/youtube.force-ssl",
        ]
        self._api_service_name = api_service_name
        self._api_version = api_version
        self._youtube = None

        logger.info("Initializing YouTube adapter")
        self._initialize_youtube_client()

    def _initialize_youtube_client(self) -> None:
        """
        Initialize the YouTube API client.

        Raises:
            PublishingError: If client initialization fails
        """
        try:
            # Check if OAuth token file exists
            if not os.path.exists(self._oauth_token_file):
                raise PublishingError(
                    f"OAuth token file not found: {self._oauth_token_file}. "
                    "Please run the authentication flow first."
                )

            # Load credentials from token file
            with open(self._oauth_token_file, "r") as token_file:
                token_data = json.load(token_file)

            # Create credentials object
            credentials = google.oauth2.credentials.Credentials(
                token=token_data.get("token"),
                refresh_token=token_data.get("refresh_token"),
                token_uri=token_data.get(
                    "token_uri", "https://oauth2.googleapis.com/token"
                ),
                client_id=token_data.get("client_id"),
                client_secret=token_data.get("client_secret"),
                scopes=self._scopes,
            )

            # Build YouTube API client
            self._youtube = build(
                self._api_service_name,
                self._api_version,
                credentials=credentials,
                cache_discovery=False,
            )

            logger.info("YouTube API client initialized successfully")

        except Exception as e:
            logger.error(f"Failed to initialize YouTube API client: {str(e)}")
            raise PublishingError(f"Failed to initialize YouTube API client: {str(e)}")

    def upload_video(self, video_file: str, metadata: Dict[str, Any]) -> str:
        """
        Upload a video to YouTube with metadata.

        Args:
            video_file: Path to the video file
            metadata: Dictionary containing video metadata:
                      - title: Video title
                      - description: Video description
                      - tags: List of tags
                      - category_id: YouTube category ID (default: 22 for People & Blogs)
                      - privacy_status: Privacy status (default: private)

        Returns:
            YouTube video ID of the uploaded video

        Raises:
            PublishingError: If the upload fails
        """
        if not self._youtube:
            self._initialize_youtube_client()

        if not os.path.exists(video_file):
            raise PublishingError(f"Video file not found: {video_file}")

        try:
            # Prepare video metadata
            body = {
                "snippet": {
                    "title": metadata.get("title", "Untitled Video"),
                    "description": metadata.get("description", ""),
                    "tags": metadata.get("tags", []),
                    "categoryId": metadata.get(
                        "category_id", "22"
                    ),  # 22 = People & Blogs
                },
                "status": {
                    "privacyStatus": metadata.get("privacy_status", "private"),
                    "selfDeclaredMadeForKids": metadata.get("made_for_kids", False),
                },
            }

            # Configure media file upload
            media = MediaFileUpload(
                video_file,
                mimetype="video/*",
                resumable=True,
                chunksize=1024 * 1024 * 5,  # 5MB chunks
            )

            # Create upload request
            insert_request = self._youtube.videos().insert(
                part=",".join(body.keys()), body=body, media_body=media
            )

            # Execute the upload with progress tracking and retries
            video_id = self._execute_upload_with_retries(insert_request)

            logger.info(f"Video uploaded successfully with ID: {video_id}")
            return video_id

        except HttpError as e:
            error_content = json.loads(e.content.decode("utf-8"))
            error_message = error_content.get("error", {}).get("message", str(e))
            logger.error(f"HTTP error during upload: {error_message}")
            raise PublishingError(f"YouTube upload failed: {error_message}")

        except Exception as e:
            logger.error(f"Failed to upload video: {str(e)}")
            raise PublishingError(f"YouTube upload failed: {str(e)}")

    def _execute_upload_with_retries(self, request):
        """
        Execute an upload request with retries for transient errors.

        Args:
            request: The YouTube API request object

        Returns:
            YouTube video ID

        Raises:
            PublishingError: If the upload fails after all retries
        """
        response = None
        error = None
        retry = 0
        video_id = None

        while response is None:
            try:
                logger.info(f"Uploading video (attempt {retry + 1}/{MAX_RETRIES})")
                status, response = request.next_chunk()

                if response is not None:
                    if "id" in response:
                        video_id = response["id"]
                        return video_id
                    else:
                        raise PublishingError(
                            f"YouTube upload failed, no video ID in response: {response}"
                        )

                # If status is available, log the progress
                if status:
                    progress = int(status.progress() * 100)
                    logger.info(f"Upload progress: {progress}%")

            except HttpError as e:
                if e.resp.status in RETRIABLE_STATUS_CODES:
                    error = (
                        f"A retriable HTTP error {e.resp.status} occurred: {e.content}"
                    )
                else:
                    raise

            except RETRIABLE_EXCEPTIONS as e:
                error = f"A retriable error occurred: {e}"

            if error is not None:
                logger.warning(error)
                retry += 1

                if retry > MAX_RETRIES:
                    logger.error("Maximum retries exceeded")
                    raise PublishingError(
                        f"YouTube upload failed after {MAX_RETRIES} retries: {error}"
                    )

                # Exponential backoff: wait 2^retry seconds before retrying
                max_sleep = 2**retry
                sleep_seconds = min(max_sleep, 60)  # Cap at 60 seconds
                logger.info(f"Sleeping {sleep_seconds} seconds before retrying...")
                time.sleep(sleep_seconds)

    def update_metadata(self, video_id: str, metadata: Dict[str, Any]) -> bool:
        """
        Update metadata for an existing YouTube video.

        Args:
            video_id: YouTube video ID
            metadata: Dictionary containing video metadata to update

        Returns:
            True if successful, False otherwise

        Raises:
            PublishingError: If the update fails
        """
        if not self._youtube:
            self._initialize_youtube_client()

        try:
            # Get current video data to merge with updates
            video_response = (
                self._youtube.videos()
                .list(part="snippet,status", id=video_id)
                .execute()
            )

            if not video_response.get("items"):
                raise PublishingError(f"Video with ID {video_id} not found")

            video_item = video_response["items"][0]
            snippet = video_item["snippet"]
            status = video_item["status"]

            # Update snippet fields if provided
            if "title" in metadata:
                snippet["title"] = metadata["title"]

            if "description" in metadata:
                snippet["description"] = metadata["description"]

            if "tags" in metadata:
                snippet["tags"] = metadata["tags"]

            if "category_id" in metadata:
                snippet["categoryId"] = metadata["category_id"]

            # Update status fields if provided
            if "privacy_status" in metadata:
                status["privacyStatus"] = metadata["privacy_status"]

            if "made_for_kids" in metadata:
                status["selfDeclaredMadeForKids"] = metadata["made_for_kids"]

            # Prepare update request
            update_request = self._youtube.videos().update(
                part="snippet,status",
                body={"id": video_id, "snippet": snippet, "status": status},
            )

            # Execute update request
            update_response = update_request.execute()

            logger.info(f"Video metadata updated successfully for ID: {video_id}")
            return True

        except HttpError as e:
            error_content = json.loads(e.content.decode("utf-8"))
            error_message = error_content.get("error", {}).get("message", str(e))
            logger.error(f"HTTP error during metadata update: {error_message}")
            raise PublishingError(f"YouTube metadata update failed: {error_message}")

        except Exception as e:
            logger.error(f"Failed to update video metadata: {str(e)}")
            raise PublishingError(f"YouTube metadata update failed: {str(e)}")

    def get_upload_status(self, video_id: str) -> str:
        """
        Get the upload/processing status of a YouTube video.

        Args:
            video_id: YouTube video ID

        Returns:
            Status string (e.g., "processing", "ready", "failed")

        Raises:
            PublishingError: If the status check fails
        """
        if not self._youtube:
            self._initialize_youtube_client()

        try:
            # Get video processing details
            processing_response = (
                self._youtube.videos()
                .list(part="processingDetails,status", id=video_id)
                .execute()
            )

            if not processing_response.get("items"):
                raise PublishingError(f"Video with ID {video_id} not found")

            video_item = processing_response["items"][0]
            processing_details = video_item.get("processingDetails", {})
            status = video_item.get("status", {})

            # Check if video is being processed
            if processing_details.get("processingStatus") == "processing":
                return "processing"

            # Check if video processing failed
            if processing_details.get("processingStatus") == "failed":
                return "failed"

            # Check upload status
            upload_status = status.get("uploadStatus")
            if upload_status == "uploaded" or upload_status == "processed":
                return "ready"
            elif upload_status == "failed":
                return "failed"

            # If we can't determine status, return the raw processing status
            return processing_details.get("processingStatus", "unknown")

        except HttpError as e:
            error_content = json.loads(e.content.decode("utf-8"))
            error_message = error_content.get("error", {}).get("message", str(e))
            logger.error(f"HTTP error during status check: {error_message}")
            raise PublishingError(f"YouTube status check failed: {error_message}")

        except Exception as e:
            logger.error(f"Failed to get video status: {str(e)}")
            raise PublishingError(f"YouTube status check failed: {str(e)}")

    def delete_video(self, video_id: str) -> bool:
        """
        Delete a YouTube video.

        Args:
            video_id: YouTube video ID

        Returns:
            True if successful, False otherwise

        Raises:
            PublishingError: If the deletion fails
        """
        if not self._youtube:
            self._initialize_youtube_client()

        try:
            # Delete the video
            self._youtube.videos().delete(id=video_id).execute()

            logger.info(f"Video with ID {video_id} deleted successfully")
            return True

        except HttpError as e:
            # Check if the video doesn't exist (404)
            if e.resp.status == 404:
                logger.warning(
                    f"Video with ID {video_id} not found, considering deletion successful"
                )
                return True

            error_content = json.loads(e.content.decode("utf-8"))
            error_message = error_content.get("error", {}).get("message", str(e))
            logger.error(f"HTTP error during video deletion: {error_message}")
            raise PublishingError(f"YouTube video deletion failed: {error_message}")

        except Exception as e:
            logger.error(f"Failed to delete video: {str(e)}")
            raise PublishingError(f"YouTube video deletion failed: {str(e)}")

================
File: video_processor/adapters/storage/__init__.py
================
"""
Storage adapters package.

Contains implementations of the StorageInterface for different storage providers.
"""

from video_processor.adapters.storage.gcs import GCSStorageAdapter
from video_processor.adapters.storage.local import LocalStorageAdapter

__all__ = [
    "GCSStorageAdapter",
    "LocalStorageAdapter",
]

================
File: video_processor/adapters/storage/gcs.py
================
"""
Google Cloud Storage adapter implementation.

This module provides a concrete implementation of the StorageInterface
for Google Cloud Storage. It handles file operations using GCS.
"""

import os
import time
from typing import List, Optional, Union

from google.cloud import storage
from google.cloud.exceptions import Forbidden, NotFound

from video_processor.application.interfaces.storage import StorageInterface
from video_processor.domain.exceptions import StorageError


class GCSStorageAdapter(StorageInterface):
    """
    Google Cloud Storage implementation of StorageInterface.

    This adapter implements storage operations using the Google Cloud Storage client.
    """

    def __init__(self, client: Optional[storage.Client] = None):
        """
        Initialize the GCS Storage Adapter.

        Args:
            client: Optional pre-configured storage client
        """
        self._client = client or storage.Client()

    def upload_file(self, file_path: str, destination_path: str) -> str:
        """
        Upload a file to GCS storage.

        Args:
            file_path: Local path to the file to upload
            destination_path: Path in storage where the file should be saved

        Returns:
            The GCS path to the uploaded file

        Raises:
            StorageError: If the upload fails
        """
        try:
            if not os.path.exists(file_path):
                raise StorageError(f"Source file not found: {file_path}")

            bucket_name, blob_name = self._parse_gcs_path(destination_path)
            bucket = self._client.bucket(bucket_name)
            blob = bucket.blob(blob_name)

            # Upload with retry logic
            max_retries = 3
            backoff = 1  # Initial backoff in seconds

            for attempt in range(max_retries):
                try:
                    blob.upload_from_filename(file_path)
                    return f"gs://{bucket_name}/{blob_name}"
                except Exception:
                    if attempt == max_retries - 1:
                        # Last attempt failed, raise exception
                        raise
                    # Exponential backoff
                    time.sleep(backoff)
                    backoff *= 2

        except NotFound:
            raise StorageError(f"Bucket not found: {bucket_name}")
        except Forbidden:
            raise StorageError(f"Permission denied uploading to {destination_path}")
        except Exception as e:
            raise StorageError(f"Failed to upload file: {str(e)}")

    def upload_from_string(
        self,
        content: Union[str, bytes],
        destination_path: str,
        content_type: Optional[str] = None,
    ) -> str:
        """
        Upload string content to GCS storage.

        Args:
            content: String or bytes content to upload
            destination_path: Path in storage where the content should be saved
            content_type: Optional MIME type of the content

        Returns:
            The GCS path to the uploaded content

        Raises:
            StorageError: If the upload fails
        """
        try:
            bucket_name, blob_name = self._parse_gcs_path(destination_path)
            bucket = self._client.bucket(bucket_name)
            blob = bucket.blob(blob_name)

            # Upload with retry logic
            max_retries = 3
            backoff = 1  # Initial backoff in seconds

            for attempt in range(max_retries):
                try:
                    if content_type:
                        blob.upload_from_string(content, content_type=content_type)
                    else:
                        blob.upload_from_string(content)
                    return f"gs://{bucket_name}/{blob_name}"
                except Exception:
                    if attempt == max_retries - 1:
                        # Last attempt failed, raise exception
                        raise
                    # Exponential backoff
                    time.sleep(backoff)
                    backoff *= 2

        except NotFound:
            raise StorageError(f"Bucket not found: {bucket_name}")
        except Forbidden:
            raise StorageError(f"Permission denied uploading to {destination_path}")
        except Exception as e:
            raise StorageError(f"Failed to upload content: {str(e)}")

    def download_file(self, source_path: str, destination_path: str) -> str:
        """
        Download a file from GCS storage.

        Args:
            source_path: Path in storage to the file to download
            destination_path: Local path where the file should be saved

        Returns:
            The local path to the downloaded file

        Raises:
            StorageError: If the download fails
        """
        try:
            bucket_name, blob_name = self._parse_gcs_path(source_path)
            bucket = self._client.bucket(bucket_name)
            blob = bucket.blob(blob_name)

            # Ensure the destination directory exists
            os.makedirs(os.path.dirname(destination_path), exist_ok=True)

            if not blob.exists():
                raise StorageError(f"File not found in storage: {source_path}")

            # Download with retry logic
            max_retries = 3
            backoff = 1  # Initial backoff in seconds

            for attempt in range(max_retries):
                try:
                    blob.download_to_filename(destination_path)
                    return destination_path
                except Exception:
                    if attempt == max_retries - 1:
                        # Last attempt failed, raise exception
                        raise
                    # Exponential backoff
                    time.sleep(backoff)
                    backoff *= 2

        except NotFound:
            raise StorageError(f"Bucket not found: {bucket_name}")
        except Forbidden:
            raise StorageError(f"Permission denied downloading from {source_path}")
        except Exception as e:
            raise StorageError(f"Failed to download file: {str(e)}")

    def delete_file(self, path: str) -> bool:
        """
        Delete a file from GCS storage.

        Args:
            path: Path in storage to the file to delete

        Returns:
            True if the file was deleted, False if the file didn't exist

        Raises:
            StorageError: If the deletion fails
        """
        try:
            bucket_name, blob_name = self._parse_gcs_path(path)
            bucket = self._client.bucket(bucket_name)
            blob = bucket.blob(blob_name)

            if not blob.exists():
                return False

            blob.delete()
            return True

        except NotFound:
            raise StorageError(f"Bucket not found: {bucket_name}")
        except Forbidden:
            raise StorageError(f"Permission denied deleting {path}")
        except Exception as e:
            raise StorageError(f"Failed to delete file: {str(e)}")

    def get_public_url(self, path: str) -> str:
        """
        Get a public URL for a file in GCS storage.

        Args:
            path: Path in storage to the file

        Returns:
            A public URL for the file
        """
        bucket_name, blob_name = self._parse_gcs_path(path)
        return f"https://storage.googleapis.com/{bucket_name}/{blob_name}"

    def get_signed_url(self, path: str, expiration_seconds: int = 3600) -> str:
        """
        Get a signed URL for a file in GCS storage.

        Args:
            path: Path in storage to the file
            expiration_seconds: Number of seconds until the URL expires

        Returns:
            A signed URL for the file

        Raises:
            StorageError: If the URL generation fails
        """
        try:
            bucket_name, blob_name = self._parse_gcs_path(path)
            bucket = self._client.bucket(bucket_name)
            blob = bucket.blob(blob_name)

            if not blob.exists():
                raise StorageError(f"File not found in storage: {path}")

            url = blob.generate_signed_url(
                version="v4", expiration=expiration_seconds, method="GET"
            )
            return url

        except Exception as e:
            raise StorageError(f"Failed to generate signed URL: {str(e)}")

    def list_files(
        self, directory_path: str, filter_prefix: Optional[str] = None
    ) -> List[str]:
        """
        List files in a directory in GCS storage.

        Args:
            directory_path: Path in storage to the directory to list
            filter_prefix: Optional prefix to filter the files

        Returns:
            A list of file paths in the directory

        Raises:
            StorageError: If the listing fails
        """
        try:
            bucket_name, prefix = self._parse_gcs_path(directory_path)
            bucket = self._client.bucket(bucket_name)

            # Ensure prefix ends with a slash to represent a directory
            if prefix and not prefix.endswith("/"):
                prefix += "/"

            # Add filter prefix if provided
            if filter_prefix:
                prefix = prefix + filter_prefix

            blobs = bucket.list_blobs(prefix=prefix)
            return [f"gs://{bucket_name}/{blob.name}" for blob in blobs]

        except NotFound:
            raise StorageError(f"Bucket not found: {bucket_name}")
        except Exception as e:
            raise StorageError(f"Failed to list files: {str(e)}")

    def copy_file(self, source_path: str, destination_path: str) -> str:
        """
        Copy a file within GCS storage.

        Args:
            source_path: Path in storage to the file to copy
            destination_path: Path in storage where the file should be copied

        Returns:
            The path to the copied file

        Raises:
            StorageError: If the copy fails
        """
        try:
            src_bucket_name, src_blob_name = self._parse_gcs_path(source_path)
            dst_bucket_name, dst_blob_name = self._parse_gcs_path(destination_path)

            source_bucket = self._client.bucket(src_bucket_name)
            source_blob = source_bucket.blob(src_blob_name)

            if not source_blob.exists():
                raise StorageError(f"Source file not found: {source_path}")

            destination_bucket = self._client.bucket(dst_bucket_name)
            destination_blob = destination_bucket.blob(dst_blob_name)

            # Copy the blob
            source_bucket.copy_blob(source_blob, destination_bucket, dst_blob_name)

            return f"gs://{dst_bucket_name}/{dst_blob_name}"

        except NotFound:
            raise StorageError("Bucket not found")
        except Forbidden:
            raise StorageError("Permission denied copying file")
        except Exception as e:
            raise StorageError(f"Failed to copy file: {str(e)}")

    def move_file(self, source_path: str, destination_path: str) -> str:
        """
        Move a file within GCS storage.

        Args:
            source_path: Path in storage to the file to move
            destination_path: Path in storage where the file should be moved

        Returns:
            The path to the moved file

        Raises:
            StorageError: If the move fails
        """
        try:
            # Copy the file
            self.copy_file(source_path, destination_path)

            # Delete the source file
            self.delete_file(source_path)

            return destination_path

        except Exception as e:
            raise StorageError(f"Failed to move file: {str(e)}")

    def file_exists(self, path: str) -> bool:
        """
        Check if a file exists in GCS storage.

        Args:
            path: Path in storage to the file to check

        Returns:
            True if the file exists, False otherwise
        """
        try:
            bucket_name, blob_name = self._parse_gcs_path(path)
            bucket = self._client.bucket(bucket_name)
            blob = bucket.blob(blob_name)

            return blob.exists()

        except Exception:
            return False

    def _parse_gcs_path(self, path: str) -> tuple[str, str]:
        """
        Parse a GCS path into bucket name and blob name.

        Args:
            path: GCS path in format 'bucket_name/path/to/file'
                or 'gs://bucket_name/path/to/file'

        Returns:
            Tuple of (bucket_name, blob_name)

        Raises:
            ValueError: If path format is invalid
        """
        if path.startswith("gs://"):
            path = path[5:]

        parts = path.split("/", 1)
        if len(parts) < 2:
            raise ValueError(
                f"Invalid GCS path: {path}. Must be in format 'bucket_name/blob_name'"
            )

        bucket_name, blob_name = parts
        return bucket_name, blob_name

================
File: video_processor/adapters/storage/local.py
================
"""
Local filesystem storage adapter implementation.

This module provides a concrete implementation of the StorageInterface
for local filesystem storage. It's used primarily for development and testing.
"""

import os
import shutil
from typing import List, Optional, Union
from urllib.parse import quote

from video_processor.application.interfaces.storage import StorageInterface
from video_processor.domain.exceptions import StorageError


class LocalStorageAdapter(StorageInterface):
    """
    Local filesystem implementation of StorageInterface.

    This adapter implements storage operations using the local filesystem,
    which is useful for development and testing environments.
    """

    def __init__(self, base_dir: str = "storage"):
        """
        Initialize the Local Storage Adapter.

        Args:
            base_dir: Base directory for storage (relative to current directory)
        """
        self.base_dir = os.path.abspath(base_dir)
        # Create the base directory if it doesn't exist
        os.makedirs(self.base_dir, exist_ok=True)

    def upload_file(self, file_path: str, destination_path: str) -> str:
        """
        Upload (copy) a file to local storage.

        Args:
            file_path: Local path to the file to upload
            destination_path: Path in storage where the file should be saved

        Returns:
            The local path to the uploaded file

        Raises:
            StorageError: If the upload fails
        """
        try:
            if not os.path.exists(file_path):
                raise StorageError(f"Source file not found: {file_path}")

            dest_full_path = os.path.join(self.base_dir, destination_path)

            # Create destination directory if it doesn't exist
            os.makedirs(os.path.dirname(dest_full_path), exist_ok=True)

            # Copy the file
            shutil.copy2(file_path, dest_full_path)

            return dest_full_path

        except Exception as e:
            raise StorageError(f"Failed to upload file: {str(e)}")

    def upload_from_string(
        self,
        content: Union[str, bytes],
        destination_path: str,
        content_type: Optional[str] = None,
    ) -> str:
        """
        Upload string content to local storage.

        Args:
            content: String or bytes content to upload
            destination_path: Path in storage where the content should be saved
            content_type: Optional MIME type of the content (ignored in local implementation)

        Returns:
            The local path to the uploaded content

        Raises:
            StorageError: If the upload fails
        """
        try:
            dest_full_path = os.path.join(self.base_dir, destination_path)

            # Create destination directory if it doesn't exist
            os.makedirs(os.path.dirname(dest_full_path), exist_ok=True)

            # Write the content to the file
            mode = "wb" if isinstance(content, bytes) else "w"
            with open(dest_full_path, mode) as f:
                f.write(content)

            return dest_full_path

        except Exception as e:
            raise StorageError(f"Failed to upload content: {str(e)}")

    def download_file(self, source_path: str, destination_path: str) -> str:
        """
        Download (copy) a file from local storage.

        Args:
            source_path: Path in storage to the file to download
            destination_path: Local path where the file should be saved

        Returns:
            The local path to the downloaded file

        Raises:
            StorageError: If the download fails
        """
        try:
            source_full_path = os.path.join(self.base_dir, source_path)

            if not os.path.exists(source_full_path):
                raise StorageError(f"File not found in storage: {source_path}")

            # Create destination directory if it doesn't exist
            os.makedirs(os.path.dirname(destination_path), exist_ok=True)

            # Copy the file
            shutil.copy2(source_full_path, destination_path)

            return destination_path

        except Exception as e:
            raise StorageError(f"Failed to download file: {str(e)}")

    def delete_file(self, path: str) -> bool:
        """
        Delete a file from local storage.

        Args:
            path: Path in storage to the file to delete

        Returns:
            True if the file was deleted, False if the file didn't exist

        Raises:
            StorageError: If the deletion fails
        """
        try:
            full_path = os.path.join(self.base_dir, path)

            if not os.path.exists(full_path):
                return False

            os.remove(full_path)
            return True

        except Exception as e:
            raise StorageError(f"Failed to delete file: {str(e)}")

    def get_public_url(self, path: str) -> str:
        """
        Get a public URL for a file in local storage.

        In local storage, this returns a file:// URL.

        Args:
            path: Path in storage to the file

        Returns:
            A file:// URL for the file
        """
        full_path = os.path.join(self.base_dir, path)
        full_path = os.path.abspath(full_path)
        return f"file://{quote(full_path)}"

    def get_signed_url(self, path: str, expiration_seconds: int = 3600) -> str:
        """
        Get a signed URL for a file in local storage.

        In local storage, this simply returns a file:// URL since
        we can't create an actual signed URL for local files.

        Args:
            path: Path in storage to the file
            expiration_seconds: Number of seconds until the URL expires

        Returns:
            A file:// URL for the file

        Raises:
            StorageError: If the file doesn't exist
        """
        full_path = os.path.join(self.base_dir, path)

        if not os.path.exists(full_path):
            raise StorageError(f"File not found in storage: {path}")

        full_path = os.path.abspath(full_path)
        return f"file://{quote(full_path)}"

    def list_files(
        self, directory_path: str, filter_prefix: Optional[str] = None
    ) -> List[str]:
        """
        List files in a directory in local storage.

        Args:
            directory_path: Path in storage to the directory to list
            filter_prefix: Optional prefix to filter the files

        Returns:
            A list of file paths in the directory

        Raises:
            StorageError: If the listing fails
        """
        try:
            dir_path = os.path.join(self.base_dir, directory_path)

            if not os.path.exists(dir_path):
                return []

            result = []
            for root, _, files in os.walk(dir_path):
                for file in files:
                    if filter_prefix and not file.startswith(filter_prefix):
                        continue

                    # Get the relative path from the base directory
                    rel_path = os.path.relpath(os.path.join(root, file), self.base_dir)
                    result.append(rel_path)

            return result

        except Exception as e:
            raise StorageError(f"Failed to list files: {str(e)}")

    def copy_file(self, source_path: str, destination_path: str) -> str:
        """
        Copy a file within local storage.

        Args:
            source_path: Path in storage to the file to copy
            destination_path: Path in storage where the file should be copied

        Returns:
            The path to the copied file

        Raises:
            StorageError: If the copy fails
        """
        try:
            source_full_path = os.path.join(self.base_dir, source_path)
            dest_full_path = os.path.join(self.base_dir, destination_path)

            if not os.path.exists(source_full_path):
                raise StorageError(f"Source file not found: {source_path}")

            # Create destination directory if it doesn't exist
            os.makedirs(os.path.dirname(dest_full_path), exist_ok=True)

            # Copy the file
            shutil.copy2(source_full_path, dest_full_path)

            return destination_path

        except Exception as e:
            raise StorageError(f"Failed to copy file: {str(e)}")

    def move_file(self, source_path: str, destination_path: str) -> str:
        """
        Move a file within local storage.

        Args:
            source_path: Path in storage to the file to move
            destination_path: Path in storage where the file should be moved

        Returns:
            The path to the moved file

        Raises:
            StorageError: If the move fails
        """
        try:
            source_full_path = os.path.join(self.base_dir, source_path)
            dest_full_path = os.path.join(self.base_dir, destination_path)

            if not os.path.exists(source_full_path):
                raise StorageError(f"Source file not found: {source_path}")

            # Create destination directory if it doesn't exist
            os.makedirs(os.path.dirname(dest_full_path), exist_ok=True)

            # Move the file
            shutil.move(source_full_path, dest_full_path)

            return destination_path

        except Exception as e:
            raise StorageError(f"Failed to move file: {str(e)}")

    def file_exists(self, path: str) -> bool:
        """
        Check if a file exists in local storage.

        Args:
            path: Path in storage to the file to check

        Returns:
            True if the file exists, False otherwise
        """
        full_path = os.path.join(self.base_dir, path)
        return os.path.exists(full_path) and os.path.isfile(full_path)

================
File: video_processor/application/dtos/__init__.py
================
"""
DTOs (Data Transfer Objects) package.

Contains the DTOs used for transferring data between layers of the application,
particularly for API interactions.
"""

from video_processor.application.dtos.job_dto import JobDTO
from video_processor.application.dtos.metadata_dto import MetadataDTO
from video_processor.application.dtos.video_dto import VideoDTO

__all__ = [
    "JobDTO",
    "MetadataDTO",
    "VideoDTO",
]

================
File: video_processor/application/dtos/job_dto.py
================
"""
DTO (Data Transfer Object) for VideoJob entity.
"""

from dataclasses import dataclass, field
from datetime import datetime
from typing import Dict, List, Optional

from video_processor.application.dtos.metadata_dto import MetadataDTO
from video_processor.application.dtos.video_dto import VideoDTO
from video_processor.domain.models.enums import ProcessingStage, ProcessingStatus
from video_processor.domain.models.job import VideoJob


@dataclass
class JobDTO:
    """
    Data Transfer Object for VideoJob entity.

    This DTO is used to transfer job data between layers of the application,
    particularly for API interactions.
    """

    job_id: str
    video: VideoDTO
    status: str  # ProcessingStatus enum name
    current_stage: str  # ProcessingStage enum name
    completed_stages: List[str] = field(
        default_factory=list
    )  # ProcessingStage enum names
    created_at: Optional[str] = None
    updated_at: Optional[str] = None
    metadata: Optional[MetadataDTO] = None
    error_message: Optional[str] = None
    processed_path: Optional[str] = None
    output_files: Dict[str, str] = field(default_factory=dict)
    youtube_video_id: Optional[str] = None

    @classmethod
    def from_entity(cls, entity: VideoJob) -> "JobDTO":
        """
        Create DTO from domain entity.

        Args:
            entity: VideoJob domain entity

        Returns:
            JobDTO object
        """
        video_dto = VideoDTO.from_entity(entity.video)
        metadata_dto = (
            MetadataDTO.from_entity(entity.metadata) if entity.metadata else None
        )

        return cls(
            job_id=entity.job_id,
            video=video_dto,
            status=entity.status.name,
            current_stage=entity.current_stage.name,
            completed_stages=[stage.name for stage in entity.completed_stages],
            created_at=entity.created_at.isoformat() if entity.created_at else None,
            updated_at=entity.updated_at.isoformat() if entity.updated_at else None,
            metadata=metadata_dto,
            error_message=entity.error_message,
            processed_path=entity.processed_path,
            output_files=entity.output_files.copy() if entity.output_files else {},
            youtube_video_id=entity.youtube_video_id,
        )

    def to_entity(self) -> VideoJob:
        """
        Convert DTO to domain entity.

        Returns:
            VideoJob domain entity
        """
        video = self.video.to_entity()
        metadata = self.metadata.to_entity() if self.metadata else None

        created_at = None
        if self.created_at:
            created_at = datetime.fromisoformat(self.created_at)

        updated_at = None
        if self.updated_at:
            updated_at = datetime.fromisoformat(self.updated_at)

        # Convert string enum names to actual enum values
        status = ProcessingStatus[self.status]
        current_stage = ProcessingStage[self.current_stage]
        completed_stages = [ProcessingStage[stage] for stage in self.completed_stages]

        job = VideoJob(
            job_id=self.job_id,
            video=video,
            status=status,
            current_stage=current_stage,
            completed_stages=completed_stages,
            created_at=created_at,
            updated_at=updated_at,
            metadata=metadata,
            error_message=self.error_message,
            processed_path=self.processed_path,
            output_files=self.output_files.copy() if self.output_files else {},
            youtube_video_id=self.youtube_video_id,
        )

        return job

    def to_dict(self) -> Dict:
        """
        Convert to dictionary for JSON serialization.

        Returns:
            Dictionary representation of the DTO
        """
        metadata_dict = self.metadata.to_dict() if self.metadata else None

        return {
            "job_id": self.job_id,
            "video": self.video.to_dict(),
            "status": self.status,
            "current_stage": self.current_stage,
            "completed_stages": self.completed_stages,
            "created_at": self.created_at,
            "updated_at": self.updated_at,
            "metadata": metadata_dict,
            "error_message": self.error_message,
            "processed_path": self.processed_path,
            "output_files": self.output_files,
            "youtube_video_id": self.youtube_video_id,
            # Add computed properties for convenience
            "progress_percent": self._calculate_progress_percent(),
            "duration_seconds": self._calculate_duration_seconds(),
            "has_error": bool(self.error_message),
            "is_complete": self.status == ProcessingStatus.COMPLETED.name,
        }

    @classmethod
    def from_dict(cls, data: Dict) -> "JobDTO":
        """
        Create from dictionary.

        Args:
            data: Dictionary with job data

        Returns:
            JobDTO object
        """
        video_dto = VideoDTO.from_dict(data["video"])
        metadata_dto = (
            MetadataDTO.from_dict(data["metadata"]) if data.get("metadata") else None
        )

        return cls(
            job_id=data["job_id"],
            video=video_dto,
            status=data["status"],
            current_stage=data["current_stage"],
            completed_stages=data["completed_stages"],
            created_at=data.get("created_at"),
            updated_at=data.get("updated_at"),
            metadata=metadata_dto,
            error_message=data.get("error_message"),
            processed_path=data.get("processed_path"),
            output_files=data.get("output_files", {}),
            youtube_video_id=data.get("youtube_video_id"),
        )

    def _calculate_progress_percent(self) -> int:
        """Calculate job progress as a percentage based on completed stages."""
        total_stages = len(ProcessingStage.__members__) - 1  # Exclude COMPLETE
        completed_count = len(self.completed_stages)

        if self.status == ProcessingStatus.COMPLETED.name:
            return 100
        elif self.status == ProcessingStatus.FAILED.name:
            # For failed jobs, calculate how far it got before failing
            return min(int((completed_count / total_stages) * 100), 99)
        else:
            # For in-progress jobs, calculate based on completed stages
            return min(int((completed_count / total_stages) * 100), 99)

    def _calculate_duration_seconds(self) -> Optional[int]:
        """Calculate job duration in seconds."""
        if not self.created_at or not self.updated_at:
            return None

        created = datetime.fromisoformat(self.created_at)
        updated = datetime.fromisoformat(self.updated_at)

        return int((updated - created).total_seconds())

================
File: video_processor/application/dtos/metadata_dto.py
================
"""
DTO (Data Transfer Object) for VideoMetadata entity.
"""

from dataclasses import dataclass, field
from typing import Dict, List, Optional

from video_processor.domain.models.metadata import VideoMetadata


@dataclass
class MetadataDTO:
    """
    Data Transfer Object for VideoMetadata entity.

    This DTO is used to transfer video metadata between layers of the application,
    particularly for API interactions.
    """

    title: str
    description: Optional[str] = None
    keywords: Optional[str] = None
    category_id: str = "22"  # Default to "People & Blogs"
    duration_seconds: Optional[int] = None
    width: Optional[int] = None
    height: Optional[int] = None
    channel: str = "daily"  # 'daily' or 'main'
    tags: List[str] = field(default_factory=list)
    show_notes: Optional[str] = None
    thumbnail_url: Optional[str] = None
    transcript: Optional[str] = None
    chapters: List[Dict[str, str]] = field(default_factory=list)

    @classmethod
    def from_entity(cls, entity: VideoMetadata) -> "MetadataDTO":
        """
        Create DTO from domain entity.

        Args:
            entity: VideoMetadata domain entity

        Returns:
            MetadataDTO object
        """
        return cls(
            title=entity.title,
            description=entity.description,
            keywords=entity.keywords,
            category_id=entity.category_id,
            duration_seconds=entity.duration_seconds,
            width=entity.width,
            height=entity.height,
            channel=entity.channel,
            tags=entity.tags[:] if entity.tags else [],
            show_notes=entity.show_notes,
            thumbnail_url=entity.thumbnail_url,
            transcript=entity.transcript,
            chapters=entity.chapters[:] if entity.chapters else [],
        )

    def to_entity(self) -> VideoMetadata:
        """
        Convert DTO to domain entity.

        Returns:
            VideoMetadata domain entity
        """
        return VideoMetadata(
            title=self.title,
            description=self.description,
            keywords=self.keywords,
            category_id=self.category_id,
            duration_seconds=self.duration_seconds,
            width=self.width,
            height=self.height,
            channel=self.channel,
            tags=self.tags[:] if self.tags else [],
            show_notes=self.show_notes,
            thumbnail_url=self.thumbnail_url,
            transcript=self.transcript,
            chapters=self.chapters[:] if self.chapters else [],
        )

    def to_dict(self) -> Dict:
        """
        Convert to dictionary for JSON serialization.

        Returns:
            Dictionary representation of the DTO
        """
        return {
            "title": self.title,
            "description": self.description,
            "keywords": self.keywords,
            "category_id": self.category_id,
            "duration_seconds": self.duration_seconds,
            "width": self.width,
            "height": self.height,
            "channel": self.channel,
            "tags": self.tags,
            "show_notes": self.show_notes,
            "thumbnail_url": self.thumbnail_url,
            "transcript": self.transcript,
            "chapters": self.chapters,
            # Add computed properties for convenience
            "has_transcript": bool(self.transcript),
            "has_chapters": bool(self.chapters),
            "tag_count": len(self.tags) if self.tags else 0,
        }

    @classmethod
    def from_dict(cls, data: Dict) -> "MetadataDTO":
        """
        Create from dictionary.

        Args:
            data: Dictionary with metadata

        Returns:
            MetadataDTO object
        """
        return cls(
            title=data.get("title", ""),
            description=data.get("description"),
            keywords=data.get("keywords"),
            category_id=data.get("category_id", "22"),
            duration_seconds=data.get("duration_seconds"),
            width=data.get("width"),
            height=data.get("height"),
            channel=data.get("channel", "daily"),
            tags=data.get("tags", []),
            show_notes=data.get("show_notes"),
            thumbnail_url=data.get("thumbnail_url"),
            transcript=data.get("transcript"),
            chapters=data.get("chapters", []),
        )

    def to_youtube_metadata(self) -> Dict:
        """
        Convert to YouTube-specific metadata format.

        Returns:
            Dictionary with YouTube API compatible metadata
        """
        # Convert tags list to comma-separated keywords string if needed
        keywords = self.keywords
        if not keywords and self.tags:
            keywords = ",".join(self.tags)

        return {
            "snippet": {
                "title": self.title,
                "description": self.description or "",
                "tags": self.tags,
                "categoryId": self.category_id,
            },
            "status": {
                "privacyStatus": "private",  # Default to private for safety
                "selfDeclaredMadeForKids": False,
            },
        }

================
File: video_processor/application/dtos/video_dto.py
================
"""
DTO (Data Transfer Object) for Video entity.
"""

from dataclasses import dataclass
from datetime import datetime
from typing import Dict, Optional

from video_processor.domain.models.video import Video
from video_processor.domain.value_objects import VideoFormat, VideoResolution


@dataclass
class VideoDTO:
    """
    Data Transfer Object for Video entity.

    This DTO is used to transfer video data between layers of the application,
    particularly for API interactions.
    """

    id: str
    file_path: str
    file_name: str
    file_size: Optional[int] = None
    file_format: Optional[str] = None
    duration: Optional[float] = None
    width: Optional[int] = None
    height: Optional[int] = None
    created_at: Optional[str] = None
    bucket_name: Optional[str] = None

    @classmethod
    def from_entity(cls, entity: Video) -> "VideoDTO":
        """
        Create DTO from domain entity.

        Args:
            entity: Video domain entity

        Returns:
            VideoDTO object
        """
        return cls(
            id=entity.id,
            file_path=entity.file_path,
            file_name=entity.file_name,
            file_size=entity.file_size,
            file_format=entity.file_format,
            duration=entity.duration,
            width=entity.width,
            height=entity.height,
            created_at=entity.created_at.isoformat() if entity.created_at else None,
            bucket_name=entity.bucket_name,
        )

    def to_entity(self) -> Video:
        """
        Convert DTO to domain entity.

        Returns:
            Video domain entity
        """
        created_at = None
        if self.created_at:
            created_at = datetime.fromisoformat(self.created_at)

        return Video(
            id=self.id,
            file_path=self.file_path,
            file_name=self.file_name,
            file_size=self.file_size,
            file_format=self.file_format,
            duration=self.duration,
            width=self.width,
            height=self.height,
            created_at=created_at,
            bucket_name=self.bucket_name,
        )

    def to_dict(self) -> Dict:
        """
        Convert to dictionary for JSON serialization.

        Returns:
            Dictionary representation of the DTO
        """
        return {
            "id": self.id,
            "file_path": self.file_path,
            "file_name": self.file_name,
            "file_size": self.file_size,
            "file_format": self.file_format,
            "duration": self.duration,
            "width": self.width,
            "height": self.height,
            "created_at": self.created_at,
            "bucket_name": self.bucket_name,
            # Add computed properties for convenience
            "resolution": (
                f"{self.width}x{self.height}" if self.width and self.height else None
            ),
            "extension": (
                self.file_name.rsplit(".", 1)[1].lower()
                if "." in self.file_name
                else None
            ),
        }

    @classmethod
    def from_dict(cls, data: Dict) -> "VideoDTO":
        """
        Create from dictionary.

        Args:
            data: Dictionary with video data

        Returns:
            VideoDTO object
        """
        return cls(
            id=data["id"],
            file_path=data["file_path"],
            file_name=data["file_name"],
            file_size=data.get("file_size"),
            file_format=data.get("file_format"),
            duration=data.get("duration"),
            width=data.get("width"),
            height=data.get("height"),
            created_at=data.get("created_at"),
            bucket_name=data.get("bucket_name"),
        )

    @property
    def resolution(self) -> Optional[VideoResolution]:
        """Get video resolution as a VideoResolution value object."""
        if self.width is not None and self.height is not None:
            return VideoResolution(width=self.width, height=self.height)
        return None

    @property
    def format(self) -> Optional[VideoFormat]:
        """Get video format as a VideoFormat value object."""
        if self.file_format:
            return VideoFormat(name=self.file_format)
        # Try to infer from file extension
        if "." in self.file_name:
            ext = self.file_name.rsplit(".", 1)[1].lower()
            return VideoFormat(name=ext)
        return None

================
File: video_processor/application/interfaces/__init__.py
================
"""
Application interfaces package exports.
"""

from video_processor.application.interfaces.ai import AIServiceInterface
from video_processor.application.interfaces.publishing import PublishingInterface
from video_processor.application.interfaces.storage import StorageInterface

__all__ = [
    "AIServiceInterface",
    "PublishingInterface",
    "StorageInterface",
]

================
File: video_processor/application/interfaces/ai.py
================
"""
AI service interface for the video processing application.

Defines the contract for AI operations independent of any specific
AI service implementation (Gemini, Vertex AI, etc.).
"""

from abc import ABC, abstractmethod
from typing import Dict, List, Optional


class AIServiceInterface(ABC):
    """
    Interface for AI service operations.

    This interface defines the contract for all AI service adapter implementations,
    ensuring they provide the necessary methods for content generation.
    """

    @abstractmethod
    def generate_transcript(self, audio_file: str) -> str:
        """
        Generate a transcript from an audio file.

        Args:
            audio_file: Path to the audio file

        Returns:
            The generated transcript text

        Raises:
            TranscriptionError: If transcript generation fails
        """
        pass

    @abstractmethod
    def generate_metadata(self, transcript: str) -> Dict:
        """
        Generate video metadata from a transcript.

        Args:
            transcript: Transcript text

        Returns:
            A dictionary containing generated metadata:
            {
                "title": str,
                "description": str,
                "tags": List[str],
                "show_notes": str,
                "chapters": List[Dict[str, str]]
            }

        Raises:
            MetadataGenerationError: If metadata generation fails
        """
        pass

    @abstractmethod
    def generate_thumbnail_description(self, transcript: str, timestamp: float) -> str:
        """
        Generate a description for a thumbnail at a specific timestamp.

        Args:
            transcript: Transcript text
            timestamp: Time in seconds for the thumbnail

        Returns:
            A text description for the thumbnail image

        Raises:
            MetadataGenerationError: If description generation fails
        """
        pass

    @abstractmethod
    def summarize_content(self, transcript: str, max_length: int = 500) -> str:
        """
        Generate a summary of the content from a transcript.

        Args:
            transcript: Transcript text
            max_length: Maximum length of the summary in characters

        Returns:
            A summary of the content

        Raises:
            MetadataGenerationError: If summary generation fails
        """
        pass

    @abstractmethod
    def set_model(self, model_name: str) -> None:
        """
        Set the AI model to use for generation.

        Args:
            model_name: Name of the model to use

        Raises:
            ValueError: If the model name is invalid
        """
        pass

    @abstractmethod
    def generate_chapters(
        self, transcript: str, num_chapters: Optional[int] = None
    ) -> List[Dict]:
        """
        Generate chapters from a transcript.

        Args:
            transcript: Transcript text
            num_chapters: Optional number of chapters to generate,
                          or None to let the AI determine the optimal number

        Returns:
            A list of chapter dictionaries:
            [
                {"title": str, "start_time": float, "end_time": float},
                ...
            ]

        Raises:
            MetadataGenerationError: If chapter generation fails
        """
        pass

================
File: video_processor/application/interfaces/messaging.py
================
"""
Interfaces for message handling and event-driven communication.
"""

from abc import ABC, abstractmethod
from typing import Any, Callable, Dict


class MessageHandlerInterface(ABC):
    """
    Interface for handling asynchronous messaging between components.

    This interface defines methods for publishing messages to topics and
    subscribing to topics to receive messages.
    """

    @abstractmethod
    def publish(self, topic: str, message: Dict[str, Any]) -> str:
        """
        Publish a message to a topic.

        Args:
            topic: The topic to publish to
            message: The message payload as a dictionary

        Returns:
            A message ID or reference for the published message

        Raises:
            MessagingError: If publishing fails
        """
        pass

    @abstractmethod
    def subscribe(self, topic: str, callback: Callable[[Dict[str, Any]], None]) -> None:
        """
        Subscribe to a topic with a callback function.

        Args:
            topic: The topic to subscribe to
            callback: A function to call when a message is received

        Raises:
            MessagingError: If subscription fails
        """
        pass

    @abstractmethod
    def unsubscribe(self, topic: str) -> bool:
        """
        Unsubscribe from a topic.

        Args:
            topic: The topic to unsubscribe from

        Returns:
            True if unsubscribed successfully, False otherwise

        Raises:
            MessagingError: If unsubscription fails
        """
        pass

    @abstractmethod
    def pull_messages(self, topic: str, max_messages: int = 10) -> list[Dict[str, Any]]:
        """
        Pull messages from a topic without a subscription.

        Args:
            topic: The topic to pull messages from
            max_messages: The maximum number of messages to pull

        Returns:
            A list of message payloads

        Raises:
            MessagingError: If message pulling fails
        """
        pass

    @abstractmethod
    def ack_message(self, topic: str, message_id: str) -> bool:
        """
        Acknowledge a message as processed.

        Args:
            topic: The topic the message was from
            message_id: The ID of the message to acknowledge

        Returns:
            True if acknowledged successfully, False otherwise

        Raises:
            MessagingError: If acknowledgment fails
        """
        pass

    @abstractmethod
    def create_topic(self, topic: str) -> bool:
        """
        Create a new topic if it doesn't exist.

        Args:
            topic: The topic to create

        Returns:
            True if created or already exists, False otherwise

        Raises:
            MessagingError: If topic creation fails
        """
        pass

    @abstractmethod
    def delete_topic(self, topic: str) -> bool:
        """
        Delete a topic.

        Args:
            topic: The topic to delete

        Returns:
            True if deleted successfully, False otherwise

        Raises:
            MessagingError: If topic deletion fails
        """
        pass

================
File: video_processor/application/interfaces/publishing.py
================
"""
Publishing interface for the video processing application.

Defines the contract for publishing operations independent of any specific
platform implementation (YouTube, Vimeo, etc.).
"""

from abc import ABC, abstractmethod
from typing import Dict, Optional


class PublishingInterface(ABC):
    """
    Interface for video publishing operations.

    This interface defines the contract for all publishing adapter implementations,
    ensuring they provide the necessary methods for publishing videos to platforms.
    """

    @abstractmethod
    def upload_video(self, video_file: str, metadata: Dict) -> str:
        """
        Upload a video to the publishing platform.

        Args:
            video_file: Path to the video file
            metadata: Dictionary containing video metadata:
                      {
                          "title": str,
                          "description": str,
                          "tags": List[str],
                          "category_id": str,
                          ...
                      }

        Returns:
            The video ID on the platform

        Raises:
            PublishingError: If the upload fails
        """
        pass

    @abstractmethod
    def update_metadata(self, video_id: str, metadata: Dict) -> bool:
        """
        Update metadata for a video on the publishing platform.

        Args:
            video_id: ID of the video on the platform
            metadata: Dictionary containing video metadata to update

        Returns:
            True if the update succeeded, False otherwise

        Raises:
            PublishingError: If the update fails
        """
        pass

    @abstractmethod
    def get_upload_status(self, video_id: str) -> str:
        """
        Get the status of a video upload.

        Args:
            video_id: ID of the video on the platform

        Returns:
            Status of the upload (e.g., "uploading", "processing", "ready", "failed")

        Raises:
            PublishingError: If the status check fails
        """
        pass

    @abstractmethod
    def delete_video(self, video_id: str) -> bool:
        """
        Delete a video from the publishing platform.

        Args:
            video_id: ID of the video on the platform

        Returns:
            True if the deletion succeeded, False otherwise

        Raises:
            PublishingError: If the deletion fails
        """
        pass

    @abstractmethod
    def get_video_url(self, video_id: str) -> str:
        """
        Get the public URL for a video on the publishing platform.

        Args:
            video_id: ID of the video on the platform

        Returns:
            Public URL for the video

        Raises:
            PublishingError: If the URL retrieval fails
        """
        pass

    @abstractmethod
    def upload_caption(
        self, video_id: str, caption_file: str, language: str = "en"
    ) -> bool:
        """
        Upload a caption file for a video on the publishing platform.

        Args:
            video_id: ID of the video on the platform
            caption_file: Path to the caption file (VTT, SRT, etc.)
            language: Language code for the captions

        Returns:
            True if the upload succeeded, False otherwise

        Raises:
            PublishingError: If the caption upload fails
        """
        pass

    @abstractmethod
    def set_publishing_time(
        self, video_id: str, publish_at: Optional[str] = None
    ) -> bool:
        """
        Set the publishing time for a video on the platform.

        Args:
            video_id: ID of the video on the platform
            publish_at: ISO 8601 format datetime string when the video should be published,
                        or None to publish immediately

        Returns:
            True if the publishing time was set successfully, False otherwise

        Raises:
            PublishingError: If setting the publishing time fails
        """
        pass

================
File: video_processor/application/interfaces/repositories.py
================
"""
Repository interfaces for data access.

This module defines the interfaces for repositories that handle persistence
of domain entities such as VideoJob and Video.
"""

from abc import ABC, abstractmethod
from typing import Any, List, Optional

from video_processor.domain.models.job import JobStatus, VideoJob
from video_processor.domain.models.video import Video


class RepositoryInterface(ABC):
    """Base repository interface with common CRUD operations."""

    @abstractmethod
    def get_by_id(self, id: str) -> Any:
        """Retrieve an entity by its ID."""
        pass

    @abstractmethod
    def save(self, entity: Any) -> str:
        """Save an entity and return its ID."""
        pass

    @abstractmethod
    def update(self, entity: Any) -> bool:
        """Update an existing entity and return success status."""
        pass

    @abstractmethod
    def delete(self, id: str) -> bool:
        """Delete an entity by its ID and return success status."""
        pass


class JobRepositoryInterface(RepositoryInterface):
    """Interface for VideoJob repository operations."""

    @abstractmethod
    def get_by_id(self, job_id: str) -> Optional[VideoJob]:
        """
        Retrieve a VideoJob by its ID.

        Args:
            job_id: ID of the job to retrieve

        Returns:
            VideoJob if found, None otherwise
        """
        pass

    @abstractmethod
    def save(self, job: VideoJob) -> str:
        """
        Save a new VideoJob.

        Args:
            job: VideoJob to save

        Returns:
            ID of the saved job
        """
        pass

    @abstractmethod
    def update(self, job: VideoJob) -> bool:
        """
        Update an existing VideoJob.

        Args:
            job: VideoJob to update

        Returns:
            True if successful, False otherwise
        """
        pass

    @abstractmethod
    def delete(self, job_id: str) -> bool:
        """
        Delete a VideoJob by its ID.

        Args:
            job_id: ID of the job to delete

        Returns:
            True if successful, False otherwise
        """
        pass

    @abstractmethod
    def get_jobs_by_status(self, status: JobStatus) -> List[VideoJob]:
        """
        Retrieve jobs with a specific status.

        Args:
            status: Status to filter by

        Returns:
            List of VideoJob with the specified status
        """
        pass

    @abstractmethod
    def get_pending_jobs(self) -> List[VideoJob]:
        """
        Retrieve all pending jobs.

        Returns:
            List of VideoJob with status PENDING
        """
        pass

    @abstractmethod
    def update_job_status(
        self, job_id: str, status: JobStatus, error: Optional[str] = None
    ) -> bool:
        """
        Update the status of a job.

        Args:
            job_id: ID of the job to update
            status: New status value
            error: Error message if status is FAILED

        Returns:
            True if successful, False otherwise
        """
        pass


class VideoRepositoryInterface(RepositoryInterface):
    """Interface for Video repository operations."""

    @abstractmethod
    def get_by_id(self, video_id: str) -> Optional[Video]:
        """
        Retrieve a Video by its ID.

        Args:
            video_id: ID of the video to retrieve

        Returns:
            Video if found, None otherwise
        """
        pass

    @abstractmethod
    def save(self, video: Video) -> str:
        """
        Save a new Video.

        Args:
            video: Video to save

        Returns:
            ID of the saved video
        """
        pass

    @abstractmethod
    def update(self, video: Video) -> bool:
        """
        Update an existing Video.

        Args:
            video: Video to update

        Returns:
            True if successful, False otherwise
        """
        pass

    @abstractmethod
    def delete(self, video_id: str) -> bool:
        """
        Delete a Video by its ID.

        Args:
            video_id: ID of the video to delete

        Returns:
            True if successful, False otherwise
        """
        pass

    @abstractmethod
    def get_videos_by_user(self, user_id: str) -> List[Video]:
        """
        Retrieve videos for a specific user.

        Args:
            user_id: ID of the user

        Returns:
            List of Video for the specified user
        """
        pass

================
File: video_processor/application/interfaces/storage.py
================
"""
Storage interface for the video processing application.

Defines the contract for storage operations independent of any specific
storage implementation (GCS, local filesystem, etc.).
"""

from abc import ABC, abstractmethod
from typing import List, Optional, Union


class StorageInterface(ABC):
    """
    Interface for storage operations.

    This interface defines the contract for all storage adapter implementations,
    ensuring they provide the necessary methods for file operations.
    """

    @abstractmethod
    def upload_file(self, file_path: str, destination_path: str) -> str:
        """
        Upload a file to storage.

        Args:
            file_path: Local path to the file to upload
            destination_path: Path in storage where the file should be saved

        Returns:
            The public URL or path to the uploaded file

        Raises:
            StorageError: If the upload fails
        """
        pass

    @abstractmethod
    def upload_from_string(
        self,
        content: Union[str, bytes],
        destination_path: str,
        content_type: Optional[str] = None,
    ) -> str:
        """
        Upload string content to storage.

        Args:
            content: String or bytes content to upload
            destination_path: Path in storage where the content should be saved
            content_type: Optional MIME type of the content

        Returns:
            The path or URL to the uploaded content

        Raises:
            StorageError: If the upload fails
        """
        pass

    @abstractmethod
    def download_file(self, source_path: str, destination_path: str) -> str:
        """
        Download a file from storage.

        Args:
            source_path: Path in storage to the file to download
            destination_path: Local path where the file should be saved

        Returns:
            The local path to the downloaded file

        Raises:
            StorageError: If the download fails
        """
        pass

    @abstractmethod
    def delete_file(self, path: str) -> bool:
        """
        Delete a file from storage.

        Args:
            path: Path in storage to the file to delete

        Returns:
            True if the file was deleted, False otherwise

        Raises:
            StorageError: If the deletion fails
        """
        pass

    @abstractmethod
    def get_public_url(self, path: str) -> str:
        """
        Get a public URL for a file in storage.

        Args:
            path: Path in storage to the file

        Returns:
            A public URL for the file that can be accessed without authentication
        """
        pass

    @abstractmethod
    def get_signed_url(self, path: str, expiration_seconds: int = 3600) -> str:
        """
        Get a signed URL for a file in storage.

        Args:
            path: Path in storage to the file
            expiration_seconds: Number of seconds until the URL expires

        Returns:
            A signed URL for the file that expires after the specified time
        """
        pass

    @abstractmethod
    def list_files(
        self, directory_path: str, filter_prefix: Optional[str] = None
    ) -> List[str]:
        """
        List files in a directory in storage.

        Args:
            directory_path: Path in storage to the directory to list
            filter_prefix: Optional prefix to filter the files

        Returns:
            A list of file paths in the directory
        """
        pass

    @abstractmethod
    def copy_file(self, source_path: str, destination_path: str) -> str:
        """
        Copy a file within storage.

        Args:
            source_path: Path in storage to the file to copy
            destination_path: Path in storage where the file should be copied

        Returns:
            The path to the copied file

        Raises:
            StorageError: If the copy fails
        """
        pass

    @abstractmethod
    def move_file(self, source_path: str, destination_path: str) -> str:
        """
        Move a file within storage.

        Args:
            source_path: Path in storage to the file to move
            destination_path: Path in storage where the file should be moved

        Returns:
            The path to the moved file

        Raises:
            StorageError: If the move fails
        """
        pass

    @abstractmethod
    def file_exists(self, path: str) -> bool:
        """
        Check if a file exists in storage.

        Args:
            path: Path in storage to the file to check

        Returns:
            True if the file exists, False otherwise
        """
        pass

================
File: video_processor/application/services/metadata.py
================
"""
Metadata service for generating and managing video metadata.

This module provides services for generating video metadata components including
titles, descriptions, tags, and thumbnails.
"""

import logging
import os
from typing import Dict, List

from video_processor.application.interfaces.ai import AIServiceInterface
from video_processor.application.interfaces.storage import StorageInterface
from video_processor.domain.exceptions import MetadataGenerationError
from video_processor.domain.models.metadata import VideoMetadata


class MetadataService:
    """
    Service for generating video metadata.

    This service handles the generation of various metadata components for videos
    using AI services, including titles, descriptions, tags, and thumbnails.
    """

    def __init__(
        self,
        ai_adapter: AIServiceInterface,
        storage_adapter: StorageInterface,
        output_dir: str = "metadata",
    ):
        """
        Initialize the MetadataService with required dependencies.

        Args:
            ai_adapter: AI adapter for content generation
            storage_adapter: Storage adapter for file operations
            output_dir: Directory for generated metadata files
        """
        self._ai = ai_adapter
        self._storage = storage_adapter
        self._output_dir = output_dir

        # Ensure output directory exists
        os.makedirs(output_dir, exist_ok=True)

        logging.info(f"Initialized MetadataService with output_dir={output_dir}")

    def generate_metadata(self, transcript: str) -> VideoMetadata:
        """
        Generate complete metadata from a transcript.

        Args:
            transcript: Transcript text

        Returns:
            VideoMetadata object with all metadata components

        Raises:
            MetadataGenerationError: If metadata generation fails
        """
        try:
            logging.info("Generating complete metadata from transcript")

            # Delegate to AI service to generate all metadata components at once
            metadata_dict = self._ai.generate_metadata(transcript)

            # Create VideoMetadata object
            metadata = VideoMetadata(
                title=metadata_dict.get("title", "Untitled Video"),
                description=metadata_dict.get("description", ""),
                tags=metadata_dict.get("tags", []),
                show_notes=metadata_dict.get("show_notes", ""),
                chapters=metadata_dict.get("chapters", []),
            )

            logging.info(
                f"Successfully generated metadata with title: {metadata.title}"
            )
            return metadata

        except Exception as e:
            error_msg = f"Failed to generate metadata: {str(e)}"
            logging.error(error_msg)
            raise MetadataGenerationError(error_msg) from e

    def generate_title(self, transcript: str) -> str:
        """
        Generate a title from a transcript.

        Args:
            transcript: Transcript text

        Returns:
            Generated title

        Raises:
            MetadataGenerationError: If title generation fails
        """
        try:
            logging.info("Generating title from transcript")

            # Use AI to generate title and keywords
            title_tags = self._generate_title_tags(transcript)

            title = title_tags.get("Description", "Untitled Video")
            logging.info(f"Generated title: {title}")
            return title

        except Exception as e:
            error_msg = f"Failed to generate title: {str(e)}"
            logging.error(error_msg)
            raise MetadataGenerationError(error_msg) from e

    def generate_description(self, transcript: str, max_length: int = 500) -> str:
        """
        Generate a description from a transcript.

        Args:
            transcript: Transcript text
            max_length: Maximum length of the description

        Returns:
            Generated description

        Raises:
            MetadataGenerationError: If description generation fails
        """
        try:
            logging.info(
                f"Generating description from transcript (max {max_length} chars)"
            )

            # Use AI to summarize content
            description = self._ai.summarize_content(transcript, max_length)

            logging.info(f"Generated description ({len(description)} chars)")
            return description

        except Exception as e:
            error_msg = f"Failed to generate description: {str(e)}"
            logging.error(error_msg)
            raise MetadataGenerationError(error_msg) from e

    def generate_tags(self, transcript: str, max_tags: int = 10) -> List[str]:
        """
        Generate tags from a transcript.

        Args:
            transcript: Transcript text
            max_tags: Maximum number of tags to generate

        Returns:
            List of generated tags

        Raises:
            MetadataGenerationError: If tag generation fails
        """
        try:
            logging.info(f"Generating tags from transcript (max {max_tags} tags)")

            # Use AI to generate title and keywords
            title_tags = self._generate_title_tags(transcript)

            # Convert comma-separated keywords to list and limit to max_tags
            keywords = title_tags.get("Keywords", "video,content")
            tags = [tag.strip() for tag in keywords.split(",") if tag.strip()][
                :max_tags
            ]

            logging.info(f"Generated {len(tags)} tags")
            return tags

        except Exception as e:
            error_msg = f"Failed to generate tags: {str(e)}"
            logging.error(error_msg)
            raise MetadataGenerationError(error_msg) from e

    def generate_thumbnail_description(
        self, transcript: str, timestamp: float = 30.0
    ) -> str:
        """
        Generate a description for a thumbnail at a specific timestamp.

        Args:
            transcript: Transcript text
            timestamp: Time in seconds for the thumbnail

        Returns:
            Description for the thumbnail

        Raises:
            MetadataGenerationError: If description generation fails
        """
        try:
            logging.info(f"Generating thumbnail description at timestamp {timestamp}s")

            # Delegate to AI adapter
            description = self._ai.generate_thumbnail_description(transcript, timestamp)

            logging.info(f"Generated thumbnail description: {description}")
            return description

        except Exception as e:
            error_msg = f"Failed to generate thumbnail description: {str(e)}"
            logging.error(error_msg)
            raise MetadataGenerationError(error_msg) from e

    def save_metadata_to_json(self, metadata: VideoMetadata, filename: str) -> str:
        """
        Save metadata to a JSON file.

        Args:
            metadata: VideoMetadata object
            filename: Base filename without extension

        Returns:
            Path to the saved JSON file

        Raises:
            MetadataGenerationError: If saving fails
        """
        try:
            # Ensure filename has .json extension
            if not filename.endswith(".json"):
                filename = f"{filename}.json"

            output_path = os.path.join(self._output_dir, filename)

            # Convert metadata to dictionary
            metadata_dict = metadata.to_dict()

            # Save to storage
            json_content = metadata.to_json()
            self._storage.upload_from_string(
                json_content, output_path, content_type="application/json"
            )

            logging.info(f"Saved metadata to {output_path}")
            return output_path

        except Exception as e:
            error_msg = f"Failed to save metadata to JSON: {str(e)}"
            logging.error(error_msg)
            raise MetadataGenerationError(error_msg) from e

    def _generate_title_tags(self, transcript: str) -> Dict[str, str]:
        """
        Generate title and tags as a dictionary from transcript.

        Args:
            transcript: Transcript text

        Returns:
            Dictionary with "Description" (title) and "Keywords" (comma-separated tags)

        Raises:
            MetadataGenerationError: If generation fails
        """
        try:
            # This is a wrapper around the AI adapter method to simplify testing
            # and to provide consistent error handling
            result = self._ai._generate_title_tags(transcript)

            # Ensure required keys are present
            if "Description" not in result:
                result["Description"] = "Untitled Video"

            if "Keywords" not in result:
                result["Keywords"] = "video,content"

            return result

        except Exception as e:
            error_msg = f"Failed to generate title and tags: {str(e)}"
            logging.error(error_msg)
            # Return default values instead of raising to make this more robust
            return {"Description": "Untitled Video", "Keywords": "video,content"}

================
File: video_processor/application/services/subtitle.py
================
"""
Subtitle service for generating subtitles in various formats.

This module provides services for generating subtitles in WebVTT, SRT, and other
formats from a transcript using AI services.
"""

import logging
import os
from typing import Optional

from video_processor.application.interfaces.ai import AIServiceInterface
from video_processor.application.interfaces.storage import StorageInterface
from video_processor.domain.exceptions import MetadataGenerationError


class SubtitleService:
    """
    Service for generating subtitles from transcripts.

    This service handles the generation of subtitles in various formats
    using AI services or direct conversion from transcripts.
    """

    def __init__(
        self,
        ai_adapter: AIServiceInterface,
        storage_adapter: StorageInterface,
        output_dir: str = "subtitles",
    ):
        """
        Initialize the SubtitleService with required dependencies.

        Args:
            ai_adapter: AI adapter for content generation
            storage_adapter: Storage adapter for file operations
            output_dir: Directory for generated subtitle files
        """
        self._ai = ai_adapter
        self._storage = storage_adapter
        self._output_dir = output_dir

        # Ensure output directory exists
        os.makedirs(output_dir, exist_ok=True)

        logging.info(f"Initialized SubtitleService with output_dir={output_dir}")

    def generate_vtt(self, audio_path: str, transcript: Optional[str] = None) -> str:
        """
        Generate WebVTT subtitles from an audio file or transcript.

        Args:
            audio_path: Path to the audio file
            transcript: Optional transcript text (if already generated)

        Returns:
            Path to the generated VTT file

        Raises:
            MetadataGenerationError: If subtitle generation fails
        """
        try:
            logging.info(f"Generating WebVTT subtitles from {audio_path}")

            # If transcript isn't provided, rely on AI to generate directly from audio
            if not transcript:
                # Use AI to generate VTT directly from audio
                logging.info("Generating VTT directly from audio file")

                # Create audio part (this is done inside the AI adapter)
                vtt_content = self._ai_generate_vtt(audio_path)
            else:
                # Convert transcript to VTT format
                logging.info("Converting transcript to VTT format")
                vtt_content = self._transcript_to_vtt(transcript)

            # Ensure content starts with WEBVTT
            if not vtt_content.strip().startswith("WEBVTT"):
                vtt_content = f"WEBVTT\n\n{vtt_content}"

            # Save to file
            base_name = os.path.basename(audio_path)
            file_name = os.path.splitext(base_name)[0] + ".vtt"
            output_path = os.path.join(self._output_dir, file_name)

            with open(output_path, "w", encoding="utf-8") as f:
                f.write(vtt_content)

            logging.info(f"Successfully saved WebVTT subtitles to {output_path}")
            return output_path

        except Exception as e:
            error_msg = f"Failed to generate WebVTT subtitles: {str(e)}"
            logging.error(error_msg)
            raise MetadataGenerationError(error_msg) from e

    def generate_srt(self, audio_path: str, transcript: Optional[str] = None) -> str:
        """
        Generate SRT subtitles from an audio file or transcript.

        Args:
            audio_path: Path to the audio file
            transcript: Optional transcript text (if already generated)

        Returns:
            Path to the generated SRT file

        Raises:
            MetadataGenerationError: If subtitle generation fails
        """
        try:
            logging.info(f"Generating SRT subtitles from {audio_path}")

            # Get VTT content first (either from transcript or directly)
            vtt_path = self.generate_vtt(audio_path, transcript)

            # Read VTT content
            with open(vtt_path, "r", encoding="utf-8") as f:
                vtt_content = f.read()

            # Convert VTT to SRT
            srt_content = self._vtt_to_srt(vtt_content)

            # Save to file
            base_name = os.path.basename(audio_path)
            file_name = os.path.splitext(base_name)[0] + ".srt"
            output_path = os.path.join(self._output_dir, file_name)

            with open(output_path, "w", encoding="utf-8") as f:
                f.write(srt_content)

            logging.info(f"Successfully saved SRT subtitles to {output_path}")
            return output_path

        except Exception as e:
            error_msg = f"Failed to generate SRT subtitles: {str(e)}"
            logging.error(error_msg)
            raise MetadataGenerationError(error_msg) from e

    def _ai_generate_vtt(self, audio_path: str) -> str:
        """
        Use AI to generate VTT subtitles directly from audio.

        Args:
            audio_path: Path to the audio file

        Returns:
            VTT subtitle content as string

        Raises:
            MetadataGenerationError: If subtitle generation fails
        """
        try:
            # Create a prompt for the AI to generate VTT format
            # This might be implementation-specific based on the AI adapter
            prompt = (
                "Generate subtitles in WebVTT format for the following audio. "
                "Ensure accurate timing.\n"
                "Example format:\n"
                "WEBVTT\n\n"
                "00:00:00.000 --> 00:00:05.000\n"
                "Hello everyone and welcome back.\n\n"
                "00:00:05.500 --> 00:00:10.000\n"
                "Today we are discussing..."
            )

            # We're getting this directly from the AI adapter, with custom prompting
            # The actual implementation might vary based on the AI service
            # This is a simplification - the actual AI adapter would handle
            # the audio file and prompt differently

            # Use audio part and a special method, or fall back to transcript + processing
            # The method name is fictional and depends on the AI adapter implementation
            vtt_content = self._generate_vtt_from_audio(audio_path, prompt)

            return vtt_content

        except Exception as e:
            error_msg = f"Failed to generate VTT using AI: {str(e)}"
            logging.error(error_msg)
            raise MetadataGenerationError(error_msg) from e

    def _generate_vtt_from_audio(self, audio_path: str, prompt: str) -> str:
        """
        Wrapper around AI adapter call to generate VTT from audio.

        This method exists primarily to simplify testing and mocking.

        Args:
            audio_path: Path to the audio file
            prompt: Prompt for the AI

        Returns:
            VTT subtitle content as string
        """
        # This is a placeholder for the actual AI-specific implementation
        # In a real implementation, this might use different AI adapter methods
        # or might process the audio differently based on the AI service

        # This is a simplification - in reality, this would be a more complex
        # interaction with the AI adapter based on its capabilities
        vtt_result = self._ai.generate_transcript(audio_path)

        # Simple conversion - in reality, we'd need more processing to add timestamps
        return self._transcript_to_vtt(vtt_result)

    def _transcript_to_vtt(self, transcript: str) -> str:
        """
        Convert a plain transcript to VTT format with estimated timestamps.

        Args:
            transcript: Plain text transcript

        Returns:
            VTT subtitle content as string
        """
        # Very simple conversion for demonstration
        # In a real implementation, we'd use natural breaks in the text,
        # sentence boundaries, etc., and estimate durations based on word count

        lines = []
        lines.append("WEBVTT\n")

        # Split by sentences or paragraphs
        paragraphs = [p.strip() for p in transcript.split(".") if p.strip()]

        # Estimate 5 seconds per sentence
        current_time = 0
        for i, paragraph in enumerate(paragraphs):
            # Estimate duration based on word count (rough approximation)
            words = paragraph.split()
            duration = max(
                2, len(words) * 0.5
            )  # 0.5 seconds per word, minimum 2 seconds

            start_time = self._format_timestamp(current_time)
            end_time = self._format_timestamp(current_time + duration)
            current_time += duration

            lines.append(f"\n{start_time} --> {end_time}")
            lines.append(f"{paragraph}.")

        return "\n".join(lines)

    def _vtt_to_srt(self, vtt_content: str) -> str:
        """
        Convert WebVTT content to SRT format.

        Args:
            vtt_content: WebVTT content as string

        Returns:
            SRT subtitle content as string
        """
        try:
            # Parse VTT content
            vtt_lines = vtt_content.strip().split("\n")

            # Skip WebVTT header
            start_index = 0
            while start_index < len(vtt_lines) and not self._is_timestamp_line(
                vtt_lines[start_index]
            ):
                start_index += 1

            srt_lines = []
            subtitle_index = 1
            i = start_index

            while i < len(vtt_lines):
                # Find timestamp line
                if self._is_timestamp_line(vtt_lines[i]):
                    # Add subtitle index
                    srt_lines.append(str(subtitle_index))
                    subtitle_index += 1

                    # Convert timestamp format from VTT to SRT
                    timestamp_line = vtt_lines[i].replace(".", ",")
                    srt_lines.append(timestamp_line)

                    # Add subtitle text
                    text_lines = []
                    i += 1
                    while (
                        i < len(vtt_lines)
                        and not self._is_timestamp_line(vtt_lines[i])
                        and vtt_lines[i].strip()
                    ):
                        text_lines.append(vtt_lines[i])
                        i += 1

                    srt_lines.append("\n".join(text_lines))
                    srt_lines.append("")  # Empty line between subtitles
                else:
                    i += 1

            return "\n".join(srt_lines)

        except Exception as e:
            error_msg = f"Failed to convert VTT to SRT: {str(e)}"
            logging.error(error_msg)
            raise MetadataGenerationError(error_msg) from e

    def _is_timestamp_line(self, line: str) -> bool:
        """
        Check if a line contains a VTT timestamp.

        Args:
            line: Line of text to check

        Returns:
            True if the line contains a timestamp, False otherwise
        """
        return "-->" in line and ":" in line

    def _format_timestamp(self, seconds: float) -> str:
        """
        Format a timestamp in VTT format (HH:MM:SS.mmm).

        Args:
            seconds: Time in seconds

        Returns:
            Formatted timestamp string
        """
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        seconds = seconds % 60
        return f"{hours:02d}:{minutes:02d}:{seconds:06.3f}"

================
File: video_processor/application/services/transcription.py
================
"""
Transcription service for generating transcripts from audio and video files.

This module provides services for extracting audio from videos and generating
transcripts using AI services.
"""

import logging
import os
import subprocess
from typing import Optional

from video_processor.application.interfaces.ai import AIServiceInterface
from video_processor.application.interfaces.storage import StorageInterface
from video_processor.domain.exceptions import TranscriptionError


class TranscriptionService:
    """
    Service for audio extraction and transcript generation.

    This service handles audio extraction from video files and transcript
    generation using AI services.
    """

    def __init__(
        self,
        ai_adapter: AIServiceInterface,
        storage_adapter: StorageInterface,
        temp_dir: str = "/tmp",
    ):
        """
        Initialize the TranscriptionService with required dependencies.

        Args:
            ai_adapter: AI adapter for transcript generation
            storage_adapter: Storage adapter for file operations
            temp_dir: Directory for temporary files
        """
        self._ai = ai_adapter
        self._storage = storage_adapter
        self._temp_dir = temp_dir

        # Ensure temp directory exists
        os.makedirs(temp_dir, exist_ok=True)

        logging.info(f"Initialized TranscriptionService with temp_dir={temp_dir}")

    def extract_audio(
        self,
        video_path: str,
        output_path: Optional[str] = None,
        sample_rate: int = 16000,
        channels: int = 1,
    ) -> str:
        """
        Extract audio from a video file using FFmpeg.

        Args:
            video_path: Path to the video file
            output_path: Path where the extracted audio should be saved (optional)
            sample_rate: Audio sample rate in Hz (default: 16000)
            channels: Number of audio channels (default: 1)

        Returns:
            Path to the extracted audio file

        Raises:
            TranscriptionError: If audio extraction fails
        """
        try:
            # Generate output path if not provided
            if not output_path:
                video_filename = os.path.basename(video_path)
                audio_filename = os.path.splitext(video_filename)[0] + ".wav"
                output_path = os.path.join(self._temp_dir, audio_filename)

            # Ensure output directory exists
            os.makedirs(os.path.dirname(output_path), exist_ok=True)

            logging.info(f"Extracting audio from {video_path} to {output_path}")

            # Build FFmpeg command for audio extraction
            command = [
                "ffmpeg",
                "-i",
                video_path,  # Input file
                "-vn",  # No video
                "-ar",
                str(sample_rate),  # Sample rate
                "-ac",
                str(channels),  # Number of channels
                "-acodec",
                "pcm_s16le",  # Audio codec
                "-y",  # Overwrite output file if exists
                output_path,  # Output file
            ]

            # Execute FFmpeg command
            result = subprocess.run(
                command,
                capture_output=True,
                text=True,
                check=False,  # Don't raise exception, handle errors manually
            )

            # Check for errors
            if result.returncode != 0:
                error_msg = (
                    f"FFmpeg failed with code {result.returncode}: {result.stderr}"
                )
                logging.error(error_msg)
                raise TranscriptionError(error_msg)

            # Check if output file exists
            if not os.path.exists(output_path):
                raise TranscriptionError(
                    f"FFmpeg did not create output file: {output_path}"
                )

            logging.info(f"Successfully extracted audio to {output_path}")
            return output_path

        except subprocess.SubprocessError as e:
            error_msg = f"FFmpeg subprocess error: {str(e)}"
            logging.error(error_msg)
            raise TranscriptionError(error_msg) from e

        except Exception as e:
            error_msg = f"Failed to extract audio: {str(e)}"
            logging.error(error_msg)
            raise TranscriptionError(error_msg) from e

    def generate_transcript(self, audio_path: str) -> str:
        """
        Generate a transcript from an audio file.

        Args:
            audio_path: Path to the audio file

        Returns:
            Generated transcript text

        Raises:
            TranscriptionError: If transcript generation fails
        """
        try:
            logging.info(f"Generating transcript from {audio_path}")

            # Delegate to AI adapter for transcript generation
            transcript = self._ai.generate_transcript(audio_path)

            if not transcript:
                raise TranscriptionError("Generated transcript is empty")

            logging.info(f"Successfully generated transcript ({len(transcript)} chars)")
            return transcript

        except Exception as e:
            error_msg = f"Failed to generate transcript: {str(e)}"
            logging.error(error_msg)
            raise TranscriptionError(error_msg) from e

    def process_video_to_transcript(self, video_path: str) -> str:
        """
        Process a video file to generate a transcript.

        This is a convenience method that combines audio extraction and
        transcript generation.

        Args:
            video_path: Path to the video file

        Returns:
            Generated transcript text

        Raises:
            TranscriptionError: If processing fails
        """
        try:
            # Extract audio from video
            audio_path = self.extract_audio(video_path)

            # Generate transcript from audio
            transcript = self.generate_transcript(audio_path)

            # Clean up temporary audio file
            try:
                os.remove(audio_path)
                logging.debug(f"Cleaned up temporary audio file: {audio_path}")
            except OSError:
                logging.warning(
                    f"Failed to clean up temporary audio file: {audio_path}"
                )

            return transcript

        except Exception as e:
            error_msg = f"Failed to process video to transcript: {str(e)}"
            logging.error(error_msg)
            raise TranscriptionError(error_msg) from e

================
File: video_processor/application/services/video_processor.py
================
"""
Video processor service for orchestrating the video processing pipeline.

This module provides the main service for coordinating the video processing workflow,
from initial upload to completed processing and publishing.
"""

import logging
import os
import tempfile
from typing import Dict, Optional

from video_processor.application.interfaces.ai import AIServiceInterface
from video_processor.application.interfaces.storage import StorageInterface
from video_processor.application.services.metadata import MetadataService
from video_processor.application.services.subtitle import SubtitleService
from video_processor.application.services.transcription import TranscriptionService
from video_processor.domain.exceptions import (
    InvalidVideoError,
    MetadataGenerationError,
    ProcessingError,
    StorageError,
)
from video_processor.domain.models.job import JobStatus, VideoJob
from video_processor.utils.ffmpeg import (
    extract_audio,
    extract_frame,
    get_video_metadata,
)


class VideoProcessorService:
    """
    Service for coordinating the video processing workflow.

    This service orchestrates the entire video processing pipeline, including:
    - Video validation and preparation
    - Audio extraction
    - Transcript generation
    - Metadata generation
    - Subtitle creation
    - Thumbnail generation
    - Output file management
    """

    def __init__(
        self,
        storage_adapter: StorageInterface,
        ai_adapter: AIServiceInterface,
        output_bucket: Optional[str] = None,
        local_output_dir: Optional[str] = None,
    ):
        """
        Initialize the VideoProcessorService with required dependencies.

        Args:
            storage_adapter: Storage adapter for file operations
            ai_adapter: AI adapter for content generation
            output_bucket: GCS bucket for output files (optional)
            local_output_dir: Local directory for output files (optional, for testing)
        """
        self._storage = storage_adapter
        self._ai = ai_adapter
        self._output_bucket = output_bucket
        self._local_output_dir = local_output_dir or "output"

        # Initialize dependent services
        self._transcription_service = TranscriptionService(ai_adapter)
        self._subtitle_service = SubtitleService()
        self._metadata_service = MetadataService(ai_adapter)

        logging.info(
            f"Initialized VideoProcessorService with output_bucket={output_bucket}, "
            f"local_output_dir={local_output_dir}"
        )

    def process_video(self, job: VideoJob) -> VideoJob:
        """
        Process a video based on the provided job.

        This is the main entry point for the video processing workflow.
        It coordinates all the steps required to process a video and generate metadata.

        Args:
            job: VideoJob object containing video and processing details

        Returns:
            Updated VideoJob with processing results

        Raises:
            VideoProcessingError: If any step in the processing pipeline fails
        """
        try:
            logging.info(f"Starting video processing for job {job.id}")

            # Update job status
            job.status = JobStatus.PROCESSING
            job.update_stage("started")

            # Create a temporary directory for processing files
            with tempfile.TemporaryDirectory() as temp_dir:
                # Download video file if needed
                video_path = self._prepare_video_file(job, temp_dir)
                job.update_stage("video_prepared")

                # Extract video metadata
                self._extract_video_metadata(job, video_path)
                job.update_stage("metadata_extracted")

                # Generate transcript
                transcript_path = self._generate_transcript(job, video_path, temp_dir)
                job.update_stage("transcript_generated")

                # Generate metadata (title, description, tags)
                self._generate_content_metadata(job, transcript_path)
                job.update_stage("metadata_generated")

                # Generate subtitles
                subtitle_paths = self._generate_subtitles(
                    job, transcript_path, temp_dir
                )
                job.update_stage("subtitles_generated")

                # Generate thumbnail
                thumbnail_path = self._generate_thumbnail(job, video_path, temp_dir)
                job.update_stage("thumbnail_generated")

                # Upload and organize output files
                self._upload_output_files(
                    job, video_path, transcript_path, subtitle_paths, thumbnail_path
                )
                job.update_stage("files_uploaded")

            # Mark job as completed
            job.status = JobStatus.COMPLETED
            job.update_stage("completed")

            logging.info(f"Completed video processing for job {job.id}")
            return job

        except InvalidVideoError as e:
            job.status = JobStatus.FAILED
            job.error = f"Invalid video: {str(e)}"
            logging.error(f"Invalid video in job {job.id}: {str(e)}")
            raise

        except MetadataGenerationError as e:
            job.status = JobStatus.FAILED
            job.error = f"Metadata generation failed: {str(e)}"
            logging.error(f"Metadata generation failed for job {job.id}: {str(e)}")
            raise

        except StorageError as e:
            job.status = JobStatus.FAILED
            job.error = f"Storage operation failed: {str(e)}"
            logging.error(f"Storage operation failed for job {job.id}: {str(e)}")
            raise

        except Exception as e:
            job.status = JobStatus.FAILED
            job.error = f"Processing error: {str(e)}"
            logging.error(f"Unexpected error in job {job.id}: {str(e)}")
            raise ProcessingError(f"Video processing failed: {str(e)}") from e

    def _prepare_video_file(self, job: VideoJob, temp_dir: str) -> str:
        """
        Prepare the video file for processing.

        Downloads the file if needed or uses the local path.

        Args:
            job: The video job
            temp_dir: Temporary directory for processing

        Returns:
            Path to the video file
        """
        logging.info(f"Preparing video file for job {job.id}")

        video_path = job.video.file_path

        # If the path is a GCS URL or other remote path, download it
        if video_path.startswith("gs://") or video_path.startswith("http"):
            local_video_path = os.path.join(
                temp_dir, f"input_video{os.path.splitext(video_path)[1]}"
            )
            video_path = self._storage.download_file(video_path, local_video_path)
            logging.info(f"Downloaded video to {video_path}")

        # Validate the video file
        if not os.path.exists(video_path):
            raise InvalidVideoError(f"Video file not found: {video_path}")

        return video_path

    def _extract_video_metadata(self, job: VideoJob, video_path: str) -> None:
        """
        Extract metadata from the video file and update the job.

        Args:
            job: The video job
            video_path: Path to the video file
        """
        logging.info(f"Extracting video metadata for job {job.id}")

        try:
            # Extract metadata using FFmpeg
            video_info = get_video_metadata(video_path)

            # Update video and metadata objects
            job.video.duration = video_info.get("duration", 0)
            job.video.resolution = video_info.get("resolution", (0, 0))
            job.video.format = video_info.get("format", "unknown")

            logging.info(
                f"Video metadata extracted: duration={job.video.duration}, resolution={job.video.resolution}"
            )

        except Exception as e:
            logging.error(f"Failed to extract video metadata: {str(e)}")
            raise InvalidVideoError(f"Failed to extract video metadata: {str(e)}")

    def _generate_transcript(
        self, job: VideoJob, video_path: str, temp_dir: str
    ) -> str:
        """
        Generate transcript for the video.

        Args:
            job: The video job
            video_path: Path to the video file
            temp_dir: Temporary directory for processing

        Returns:
            Path to the transcript file
        """
        logging.info(f"Generating transcript for job {job.id}")

        try:
            # Extract audio
            audio_path = os.path.join(temp_dir, "audio.wav")
            extract_audio(video_path, audio_path)

            # Generate transcript
            transcript_path = os.path.join(temp_dir, "transcript.txt")
            transcript_text = self._transcription_service.generate_transcript(
                audio_path
            )

            # Save transcript to file
            with open(transcript_path, "w") as f:
                f.write(transcript_text)

            # Update job metadata with transcript
            job.metadata.transcript = transcript_text

            logging.info(f"Transcript generated and saved to {transcript_path}")
            return transcript_path

        except Exception as e:
            logging.error(f"Failed to generate transcript: {str(e)}")
            raise MetadataGenerationError(f"Failed to generate transcript: {str(e)}")

    def _generate_content_metadata(self, job: VideoJob, transcript_path: str) -> None:
        """
        Generate content metadata (title, description, tags) from transcript.

        Args:
            job: The video job
            transcript_path: Path to the transcript file
        """
        logging.info(f"Generating content metadata for job {job.id}")

        try:
            with open(transcript_path, "r") as f:
                transcript_text = f.read()

            # Generate title
            job.metadata.title = self._metadata_service.generate_title(transcript_text)

            # Generate description
            job.metadata.description = self._metadata_service.generate_description(
                transcript_text
            )

            # Generate tags
            job.metadata.tags = self._metadata_service.generate_tags(transcript_text)

            # Generate show notes
            job.metadata.show_notes = self._metadata_service.generate_show_notes(
                transcript_text
            )

            logging.info(f"Content metadata generated for job {job.id}")

        except Exception as e:
            logging.error(f"Failed to generate content metadata: {str(e)}")
            raise MetadataGenerationError(
                f"Failed to generate content metadata: {str(e)}"
            )

    def _generate_subtitles(
        self, job: VideoJob, transcript_path: str, temp_dir: str
    ) -> Dict[str, str]:
        """
        Generate subtitles in various formats from the transcript.

        Args:
            job: The video job
            transcript_path: Path to the transcript file
            temp_dir: Temporary directory for processing

        Returns:
            Dictionary mapping format names to file paths
        """
        logging.info(f"Generating subtitles for job {job.id}")

        subtitle_paths = {}

        try:
            with open(transcript_path, "r") as f:
                transcript_text = f.read()

            # Generate VTT subtitles
            vtt_path = os.path.join(temp_dir, "subtitles.vtt")
            self._subtitle_service.generate_vtt(transcript_text, vtt_path)
            subtitle_paths["vtt"] = vtt_path

            # Generate SRT subtitles
            srt_path = os.path.join(temp_dir, "subtitles.srt")
            self._subtitle_service.generate_srt(transcript_text, srt_path)
            subtitle_paths["srt"] = srt_path

            logging.info(f"Subtitles generated for job {job.id}")
            return subtitle_paths

        except Exception as e:
            logging.error(f"Failed to generate subtitles: {str(e)}")
            raise MetadataGenerationError(f"Failed to generate subtitles: {str(e)}")

    def _generate_thumbnail(self, job: VideoJob, video_path: str, temp_dir: str) -> str:
        """
        Generate thumbnail image for the video.

        Args:
            job: The video job
            video_path: Path to the video file
            temp_dir: Temporary directory for processing

        Returns:
            Path to the thumbnail image
        """
        logging.info(f"Generating thumbnail for job {job.id}")

        try:
            # Determine thumbnail timestamp (e.g., 10% into the video)
            timestamp = job.video.get_thumbnail_time()

            # Extract frame
            thumbnail_path = os.path.join(temp_dir, "thumbnail.jpg")
            extract_frame(video_path, timestamp, thumbnail_path)

            logging.info(f"Thumbnail generated at {thumbnail_path}")
            return thumbnail_path

        except Exception as e:
            logging.error(f"Failed to generate thumbnail: {str(e)}")
            # Not raising an error here as this is not critical for processing
            return ""

    def _upload_output_files(
        self,
        job: VideoJob,
        video_path: str,
        transcript_path: str,
        subtitle_paths: Dict[str, str],
        thumbnail_path: str,
    ) -> None:
        """
        Upload and organize all output files.

        Args:
            job: The video job
            video_path: Path to the video file
            transcript_path: Path to the transcript file
            subtitle_paths: Dictionary of subtitle format to file paths
            thumbnail_path: Path to the thumbnail image
        """
        logging.info(f"Uploading output files for job {job.id}")

        try:
            # Create base output path
            base_path = f"processed/{job.id}"
            os.makedirs(os.path.join(self._local_output_dir, base_path), exist_ok=True)

            # Upload transcript
            transcript_dest = f"{base_path}/transcript.txt"
            if self._output_bucket:
                transcript_url = self._storage.upload_file(
                    transcript_path, f"{self._output_bucket}/{transcript_dest}"
                )
            else:
                transcript_url = self._storage.upload_file(
                    transcript_path,
                    os.path.join(self._local_output_dir, transcript_dest),
                )
            job.metadata.transcript_url = transcript_url

            # Upload subtitles
            subtitle_urls = {}
            for format_name, subtitle_path in subtitle_paths.items():
                subtitle_dest = f"{base_path}/subtitles.{format_name}"
                if self._output_bucket:
                    subtitle_url = self._storage.upload_file(
                        subtitle_path, f"{self._output_bucket}/{subtitle_dest}"
                    )
                else:
                    subtitle_url = self._storage.upload_file(
                        subtitle_path,
                        os.path.join(self._local_output_dir, subtitle_dest),
                    )
                subtitle_urls[format_name] = subtitle_url
            job.metadata.subtitle_urls = subtitle_urls

            # Upload thumbnail if available
            if thumbnail_path and os.path.exists(thumbnail_path):
                thumbnail_dest = f"{base_path}/thumbnail.jpg"
                if self._output_bucket:
                    thumbnail_url = self._storage.upload_file(
                        thumbnail_path, f"{self._output_bucket}/{thumbnail_dest}"
                    )
                else:
                    thumbnail_url = self._storage.upload_file(
                        thumbnail_path,
                        os.path.join(self._local_output_dir, thumbnail_dest),
                    )
                job.metadata.thumbnail_url = thumbnail_url

            logging.info(f"All output files uploaded for job {job.id}")

        except Exception as e:
            logging.error(f"Failed to upload output files: {str(e)}")
            raise StorageError(f"Failed to upload output files: {str(e)}")

================
File: video_processor/application/__init__.py
================
"""
Application package for the video processing pipeline.

Contains the application services and use cases that implement the business logic
of the application, independent of specific infrastructure concerns.
"""

from video_processor.application.dtos import JobDTO, MetadataDTO, VideoDTO
from video_processor.application.interfaces import (
    AIServiceInterface,
    PublishingInterface,
    StorageInterface,
)

__all__ = [
    # DTOs
    "JobDTO",
    "MetadataDTO",
    "VideoDTO",
    # Interfaces
    "AIServiceInterface",
    "PublishingInterface",
    "StorageInterface",
]

================
File: video_processor/config/__init__.py
================
"""
Configuration package for video processor.
"""

from .settings import get_settings

__all__ = ["get_settings"]

================
File: video_processor/config/environment.py
================
"""
Environment variable handling for the video processor.
"""

import os
from typing import Any, Optional


def get_env(key: str, default: Any = None, required: bool = False) -> Any:
    """
    Get an environment variable.

    Args:
        key: The environment variable name
        default: Default value if not found
        required: Whether the variable is required (raises error if missing)

    Returns:
        The environment variable value or default

    Raises:
        ValueError: If required is True and the variable is not set
    """
    value = os.environ.get(key)
    if value is None:
        if required:
            raise ValueError(f"Required environment variable {key} is not set")
        return default
    return value


def get_bool_env(key: str, default: bool = False) -> bool:
    """
    Get a boolean environment variable.

    Args:
        key: The environment variable name
        default: Default value if not found

    Returns:
        The boolean value (True for 'true', 'yes', '1', etc.)
    """
    value = get_env(key)
    if value is None:
        return default
    return value.lower() in ("true", "yes", "1", "y", "t")


def get_int_env(key: str, default: Optional[int] = None) -> Optional[int]:
    """
    Get an integer environment variable.

    Args:
        key: The environment variable name
        default: Default value if not found or not convertible

    Returns:
        The integer value or default
    """
    value = get_env(key)
    if value is None:
        return default
    try:
        return int(value)
    except ValueError:
        return default

================
File: video_processor/config/settings.py
================
"""
Settings management for the video processor.
"""

from dataclasses import dataclass
from functools import lru_cache

from .environment import get_bool_env, get_env, get_int_env


@dataclass
class Settings:
    """Application settings loaded from environment variables."""

    # General settings
    project_id: str
    region: str
    testing_mode: bool
    real_api_test: bool
    local_output: bool
    google_application_credentials: str

    # GCS settings
    gcs_upload_bucket: str

    # Supabase settings
    supabase_url: str
    supabase_anon_key: str
    supabase_service_role_key: str

    # AI model settings
    ai_model: str

    # YouTube settings
    default_privacy_status: str

    # API settings
    port: int
    debug: bool


@lru_cache()
def get_settings() -> Settings:
    """
    Get application settings from environment variables with defaults.

    Returns:
        Settings: Application settings
    """
    return Settings(
        # General settings
        project_id=get_env("GOOGLE_CLOUD_PROJECT", "automations-457120"),
        region=get_env("REGION", "us-east1"),
        testing_mode=get_bool_env("TESTING_MODE", False),
        real_api_test=get_bool_env("REAL_API_TEST", False),
        local_output=get_bool_env("LOCAL_OUTPUT", False),
        google_application_credentials=get_env(
            "GOOGLE_APPLICATION_CREDENTIALS", required=True
        ),
        # GCS settings
        gcs_upload_bucket=get_env(
            "GCS_UPLOAD_BUCKET", "automations-youtube-videos-2025"
        ),
        # Supabase settings
        supabase_url=get_env("SUPABASE_URL", required=True),
        supabase_anon_key=get_env("SUPABASE_ANON_KEY", required=True),
        supabase_service_role_key=get_env("SUPABASE_SERVICE_ROLE_KEY", required=True),
        # AI model settings
        ai_model=get_env("MODEL", "gemini-2.0-flash-001"),
        # YouTube settings
        default_privacy_status=get_env("DEFAULT_PRIVACY_STATUS", "unlisted"),
        # API settings
        port=get_int_env("PORT", 8080),
        debug=get_bool_env("DEBUG", False),
    )

================
File: video_processor/domain/models/__init__.py
================
"""
Domain models package exports.
"""

from video_processor.domain.models.enums import ProcessingStage, ProcessingStatus
from video_processor.domain.models.job import VideoJob
from video_processor.domain.models.metadata import VideoMetadata
from video_processor.domain.models.video import Video

__all__ = [
    "ProcessingStage",
    "ProcessingStatus",
    "Video",
    "VideoJob",
    "VideoMetadata",
]

================
File: video_processor/domain/models/enums.py
================
"""
Domain enums for video processing status and stages.
"""

from enum import Enum, auto


class ProcessingStage(Enum):
    """Stages of video processing."""

    DOWNLOAD = auto()
    EXTRACT_AUDIO = auto()
    GENERATE_TRANSCRIPT = auto()
    GENERATE_SUBTITLES = auto()
    GENERATE_SHOWNOTES = auto()
    GENERATE_CHAPTERS = auto()
    GENERATE_TITLES = auto()
    UPLOAD_OUTPUTS = auto()
    UPLOAD_TO_YOUTUBE = auto()
    COMPLETE = auto()


class ProcessingStatus(Enum):
    """Status of job processing."""

    PENDING = auto()
    IN_PROGRESS = auto()
    COMPLETED = auto()
    FAILED = auto()
    PARTIAL = auto()  # Some stages completed but not all

================
File: video_processor/domain/models/job.py
================
"""
Domain model for video processing job.
"""

from dataclasses import dataclass, field
from datetime import datetime
from typing import Dict, List, Optional

from video_processor.domain.models.enums import ProcessingStage, ProcessingStatus
from video_processor.domain.models.metadata import VideoMetadata
from video_processor.domain.models.video import Video


@dataclass
class VideoJob:
    """
    A video processing job.

    Tracks the state of a video being processed, including its
    metadata, processing stage, and output paths.

    The job is the central entity that coordinates the processing
    workflow for a video, maintaining state and tracking progress.
    """

    job_id: str  # Unique identifier for the job
    video: Video  # Reference to the video being processed
    created_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)
    metadata: VideoMetadata = field(default_factory=lambda: VideoMetadata(title=""))
    status: ProcessingStatus = ProcessingStatus.PENDING
    current_stage: ProcessingStage = ProcessingStage.DOWNLOAD
    completed_stages: List[ProcessingStage] = field(default_factory=list)
    error_message: Optional[str] = None
    processed_path: Optional[str] = None
    output_files: Dict[str, str] = field(default_factory=dict)
    youtube_video_id: Optional[str] = None

    def update_status(
        self, status: ProcessingStatus, error: Optional[str] = None
    ) -> None:
        """Update the job status and timestamp."""
        self.status = status
        self.updated_at = datetime.now()
        if error:
            self.error_message = error

    def move_to_stage(self, stage: ProcessingStage) -> None:
        """Move to a new processing stage."""
        if self.current_stage not in self.completed_stages:
            self.completed_stages.append(self.current_stage)
        self.current_stage = stage
        self.updated_at = datetime.now()

    def complete_current_stage(self) -> None:
        """Mark the current stage as completed."""
        if self.current_stage not in self.completed_stages:
            self.completed_stages.append(self.current_stage)
        self.updated_at = datetime.now()

    def is_stage_completed(self, stage: ProcessingStage) -> bool:
        """Check if a stage has been completed."""
        return stage in self.completed_stages

    def add_output_file(self, file_type: str, file_path: str) -> None:
        """Add an output file to the job."""
        self.output_files[file_type] = file_path
        self.updated_at = datetime.now()

    def fail(self, error_message: str) -> None:
        """Mark the job as failed with an error message."""
        self.status = ProcessingStatus.FAILED
        self.error_message = error_message
        self.updated_at = datetime.now()

    def complete(self) -> None:
        """Mark the job as completed."""
        if self.current_stage not in self.completed_stages:
            self.completed_stages.append(self.current_stage)
        self.current_stage = ProcessingStage.COMPLETE
        self.status = ProcessingStatus.COMPLETED
        self.updated_at = datetime.now()

    def is_complete(self) -> bool:
        """Check if the job is complete."""
        return self.status == ProcessingStatus.COMPLETED

    def to_dict(self) -> Dict:
        """Convert to dictionary for storage."""
        return {
            "job_id": self.job_id,
            "video": self.video.to_dict(),
            "created_at": self.created_at.isoformat(),
            "updated_at": self.updated_at.isoformat(),
            "metadata": self.metadata.to_dict(),
            "status": self.status.name,
            "current_stage": self.current_stage.name,
            "completed_stages": [stage.name for stage in self.completed_stages],
            "error_message": self.error_message,
            "processed_path": self.processed_path,
            "output_files": self.output_files,
            "youtube_video_id": self.youtube_video_id,
        }

    @classmethod
    def from_dict(cls, data: Dict) -> "VideoJob":
        """Create from dictionary."""
        # Convert the nested video dictionary to a Video object
        video = Video.from_dict(data["video"])

        # Convert metadata dictionary to VideoMetadata
        metadata = VideoMetadata.from_dict(data["metadata"])

        return cls(
            job_id=data["job_id"],
            video=video,
            created_at=datetime.fromisoformat(data["created_at"]),
            updated_at=datetime.fromisoformat(data["updated_at"]),
            metadata=metadata,
            status=ProcessingStatus[data["status"]],
            current_stage=ProcessingStage[data["current_stage"]],
            completed_stages=[ProcessingStage[s] for s in data["completed_stages"]],
            error_message=data.get("error_message"),
            processed_path=data.get("processed_path"),
            output_files=data.get("output_files", {}),
            youtube_video_id=data.get("youtube_video_id"),
        )

    @classmethod
    def create_new(cls, video: Video) -> "VideoJob":
        """Create a new job for the given video."""
        job_id = f"job-{video.id}-{int(datetime.now().timestamp())}"

        # Create default metadata with the video file name as title
        metadata = VideoMetadata(
            title=video.file_name.rsplit(".", 1)[0].replace("_", " ").title()
        )

        return cls(
            job_id=job_id,
            video=video,
            metadata=metadata,
        )

================
File: video_processor/domain/models/metadata.py
================
"""
Domain model for video metadata.
"""

from dataclasses import dataclass
from typing import Dict, List, Optional


@dataclass
class VideoMetadata:
    """
    Metadata for a video.

    Contains all information about the video content and characteristics
    that can be used for publishing and organization.
    """

    title: str
    description: Optional[str] = None
    keywords: Optional[str] = None
    category_id: str = "22"  # Default to "People & Blogs"
    duration_seconds: Optional[int] = None
    width: Optional[int] = None
    height: Optional[int] = None
    channel: str = "daily"  # 'daily' or 'main'

    # Additional fields as specified in requirements
    tags: List[str] = None
    show_notes: Optional[str] = None
    thumbnail_url: Optional[str] = None
    transcript: Optional[str] = None
    chapters: List[Dict[str, str]] = None

    def __post_init__(self):
        """Initialize default values for collections."""
        if self.tags is None:
            self.tags = []
        if self.chapters is None:
            self.chapters = []

    def to_dict(self) -> Dict:
        """Convert to dictionary for storage."""
        return {
            "title": self.title,
            "description": self.description,
            "keywords": self.keywords,
            "category_id": self.category_id,
            "duration_seconds": self.duration_seconds,
            "width": self.width,
            "height": self.height,
            "channel": self.channel,
            "tags": self.tags,
            "show_notes": self.show_notes,
            "thumbnail_url": self.thumbnail_url,
            "transcript": self.transcript,
            "chapters": self.chapters,
        }

    @classmethod
    def from_dict(cls, data: Dict) -> "VideoMetadata":
        """Create from dictionary."""
        return cls(
            title=data.get("title", ""),
            description=data.get("description"),
            keywords=data.get("keywords"),
            category_id=data.get("category_id", "22"),
            duration_seconds=data.get("duration_seconds"),
            width=data.get("width"),
            height=data.get("height"),
            channel=data.get("channel", "daily"),
            tags=data.get("tags", []),
            show_notes=data.get("show_notes"),
            thumbnail_url=data.get("thumbnail_url"),
            transcript=data.get("transcript"),
            chapters=data.get("chapters", []),
        )

================
File: video_processor/domain/models/video.py
================
"""
Domain model for video entity.
"""

from dataclasses import dataclass
from datetime import datetime
from typing import Dict, Optional, Tuple


@dataclass
class Video:
    """
    Video entity representing a video file.

    Contains information about the video file itself, such as its location,
    format, resolution, and other technical properties.
    """

    id: str
    file_path: str
    file_name: str
    file_size: Optional[int] = None
    file_format: Optional[str] = None
    duration: Optional[float] = None
    width: Optional[int] = None
    height: Optional[int] = None
    created_at: datetime = None
    bucket_name: Optional[str] = None

    def __post_init__(self):
        """Initialize default values."""
        if self.created_at is None:
            self.created_at = datetime.now()

    @property
    def resolution(self) -> Optional[Tuple[int, int]]:
        """Get video resolution as width x height tuple."""
        if self.width is not None and self.height is not None:
            return (self.width, self.height)
        return None

    def get_thumbnail_time(self) -> float:
        """
        Calculate an ideal time for thumbnails.

        By default, this is 20% into the video duration, which often
        captures the subject after intro but before detailed content.
        """
        if self.duration:
            return max(min(self.duration * 0.2, self.duration - 1), 0)
        return 0

    def get_file_extension(self) -> str:
        """Extract file extension from file path."""
        if "." in self.file_name:
            return self.file_name.rsplit(".", 1)[1].lower()
        return ""

    def is_valid_video(self) -> bool:
        """
        Check if this is a valid video file.

        Basic validation based on supported formats and file existence.
        """
        valid_extensions = ["mp4", "mov", "avi", "mkv", "webm"]
        return self.get_file_extension() in valid_extensions

    def to_dict(self) -> Dict:
        """Convert to dictionary for storage."""
        return {
            "id": self.id,
            "file_path": self.file_path,
            "file_name": self.file_name,
            "file_size": self.file_size,
            "file_format": self.file_format,
            "duration": self.duration,
            "width": self.width,
            "height": self.height,
            "created_at": self.created_at.isoformat() if self.created_at else None,
            "bucket_name": self.bucket_name,
        }

    @classmethod
    def from_dict(cls, data: Dict) -> "Video":
        """Create from dictionary."""
        created_at = None
        if data.get("created_at"):
            created_at = datetime.fromisoformat(data["created_at"])

        return cls(
            id=data["id"],
            file_path=data["file_path"],
            file_name=data["file_name"],
            file_size=data.get("file_size"),
            file_format=data.get("file_format"),
            duration=data.get("duration"),
            width=data.get("width"),
            height=data.get("height"),
            created_at=created_at,
            bucket_name=data.get("bucket_name"),
        )

================
File: video_processor/domain/__init__.py
================
"""
Domain package for the video processing application.

Contains the core business logic and entities independent of any
external frameworks or infrastructure concerns.
"""

from video_processor.domain.models import (
    ProcessingStage,
    ProcessingStatus,
    Video,
    VideoJob,
    VideoMetadata,
)
from video_processor.domain.value_objects import (
    Chapter,
    Subtitle,
    SubtitleCollection,
    TimestampedText,
    VideoFormat,
    VideoResolution,
)

__all__ = [
    # Models
    "ProcessingStage",
    "ProcessingStatus",
    "Video",
    "VideoJob",
    "VideoMetadata",
    # Value objects
    "Chapter",
    "Subtitle",
    "SubtitleCollection",
    "TimestampedText",
    "VideoFormat",
    "VideoResolution",
]

================
File: video_processor/domain/exceptions.py
================
"""
Domain-specific exceptions for the video processing application.
"""


class VideoProcessingError(Exception):
    """Base exception for all domain-related errors in the video processor."""

    pass


class InvalidVideoError(VideoProcessingError):
    """Raised when a video file is invalid or corrupted."""

    pass


class MetadataGenerationError(VideoProcessingError):
    """Raised when metadata generation fails."""

    pass


class PublishingError(VideoProcessingError):
    """Raised when video publishing to platforms like YouTube fails."""

    pass


class StorageError(VideoProcessingError):
    """Raised when file storage operations fail."""

    pass


class TranscriptionError(VideoProcessingError):
    """Raised when transcript generation fails."""

    pass


class JobNotFoundError(VideoProcessingError):
    """Raised when a job with the specified ID cannot be found."""

    pass


class MessagingError(VideoProcessingError):
    """Raised when messaging operations fail."""

    pass

================
File: video_processor/domain/value_objects.py
================
"""
Value objects for the video processing domain.

Value objects are immutable objects that represent domain concepts
without identity. They are compared by their attributes rather than
by identity.
"""

from dataclasses import dataclass
from typing import Dict, List, Optional


@dataclass(frozen=True)
class VideoResolution:
    """
    Video resolution as width and height in pixels.

    This is a frozen (immutable) dataclass representing the resolution
    of a video, which can be compared for equality with other resolutions.
    """

    width: int
    height: int

    def __str__(self) -> str:
        """Return string representation as 'WIDTHxHEIGHT'."""
        return f"{self.width}x{self.height}"

    @classmethod
    def from_string(cls, resolution_str: str) -> "VideoResolution":
        """
        Create resolution from string format 'WIDTHxHEIGHT'.

        Args:
            resolution_str: String in format '1920x1080' or similar

        Returns:
            VideoResolution object

        Raises:
            ValueError: If the string cannot be parsed
        """
        try:
            width, height = resolution_str.lower().split("x")
            return cls(width=int(width), height=int(height))
        except (ValueError, AttributeError):
            raise ValueError(f"Invalid resolution format: {resolution_str}")

    @property
    def aspect_ratio(self) -> float:
        """Calculate the aspect ratio (width / height)."""
        if self.height == 0:
            return 0
        return self.width / self.height

    @property
    def is_hd(self) -> bool:
        """Check if resolution is HD (720p or higher)."""
        return self.height >= 720

    @property
    def is_4k(self) -> bool:
        """Check if resolution is 4K (2160p or higher)."""
        return self.height >= 2160


@dataclass(frozen=True)
class VideoFormat:
    """
    Video format with name and codec information.

    This is a frozen (immutable) dataclass representing the format
    of a video, including the container format and codec.
    """

    name: str  # Container format (mp4, mov, avi, etc.)
    codec: Optional[str] = None  # Video codec (h264, vp9, etc.)
    audio_codec: Optional[str] = None  # Audio codec (aac, mp3, etc.)

    def __str__(self) -> str:
        """Return string representation with format and codec if available."""
        if self.codec:
            return f"{self.name} ({self.codec})"
        return self.name

    @property
    def is_web_compatible(self) -> bool:
        """Check if the format is web-compatible (suitable for browsers)."""
        web_formats = {
            "mp4": ["h264", "h265"],
            "webm": ["vp8", "vp9", "av1"],
            "ogg": ["theora"],
        }

        if self.name not in web_formats:
            return False

        if self.codec is None:
            return self.name in web_formats

        return self.codec in web_formats.get(self.name, [])


@dataclass(frozen=True)
class TimestampedText:
    """
    Text with start and end timestamps.

    This is a frozen (immutable) dataclass representing text with
    associated start and end times, useful for subtitles, chapters, etc.
    """

    text: str
    start_time: float  # Timestamp in seconds
    end_time: float  # Timestamp in seconds

    def __post_init__(self):
        """Validate that end_time is not before start_time."""
        if self.end_time < self.start_time:
            raise ValueError(
                f"End time ({self.end_time}) cannot be before start time ({self.start_time})"
            )

    @property
    def duration(self) -> float:
        """Calculate the duration in seconds."""
        return self.end_time - self.start_time

    def format_timestamps(self, format_type: str = "srt") -> str:
        """
        Format timestamps according to the specified format.

        Args:
            format_type: Format type ('srt', 'vtt', or 'plain')

        Returns:
            Formatted timestamp string
        """
        if format_type == "srt":
            return f"{self._format_srt_time(self.start_time)} --> {self._format_srt_time(self.end_time)}"
        elif format_type == "vtt":
            return f"{self._format_vtt_time(self.start_time)} --> {self._format_vtt_time(self.end_time)}"
        elif format_type == "plain":
            return f"{int(self.start_time)}s - {int(self.end_time)}s"
        else:
            raise ValueError(f"Unsupported format type: {format_type}")

    @staticmethod
    def _format_srt_time(seconds: float) -> str:
        """Format time in SRT format (HH:MM:SS,mmm)."""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        seconds = seconds % 60
        return f"{hours:02d}:{minutes:02d}:{int(seconds):02d},{int(seconds % 1 * 1000):03d}"

    @staticmethod
    def _format_vtt_time(seconds: float) -> str:
        """Format time in WebVTT format (HH:MM:SS.mmm)."""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        seconds = seconds % 60
        return f"{hours:02d}:{minutes:02d}:{int(seconds):02d}.{int(seconds % 1 * 1000):03d}"


@dataclass(frozen=True)
class Chapter(TimestampedText):
    """
    Video chapter with title and timestamps.

    This is a frozen (immutable) dataclass representing a chapter in a video,
    inheriting from TimestampedText with the text field used as the chapter title.
    """

    def to_dict(self) -> Dict:
        """Convert to dictionary for storage and serialization."""
        return {
            "title": self.text,
            "start_time": self.start_time,
            "end_time": self.end_time,
        }

    @classmethod
    def from_dict(cls, data: Dict) -> "Chapter":
        """Create from dictionary."""
        return cls(
            text=data["title"],
            start_time=data["start_time"],
            end_time=data["end_time"],
        )


@dataclass(frozen=True)
class Subtitle(TimestampedText):
    """
    Video subtitle with text and timestamps.

    This is a frozen (immutable) dataclass representing a subtitle in a video,
    inheriting from TimestampedText.
    """

    index: int = 0  # Subtitle index for SRT format

    def to_srt(self) -> str:
        """Format subtitle as SRT format entry."""
        return f"{self.index}\n{self.format_timestamps('srt')}\n{self.text}\n"

    def to_vtt(self) -> str:
        """Format subtitle as WebVTT format entry."""
        return f"{self.format_timestamps('vtt')}\n{self.text}\n"

    @classmethod
    def from_dict(cls, data: Dict) -> "Subtitle":
        """Create from dictionary."""
        return cls(
            text=data["text"],
            start_time=data["start_time"],
            end_time=data["end_time"],
            index=data.get("index", 0),
        )


@dataclass(frozen=True)
class SubtitleCollection:
    """
    Collection of subtitles for a video.

    This is a frozen (immutable) dataclass representing all subtitles for a video.
    It provides methods to export to different subtitle formats.
    """

    subtitles: List[Subtitle]
    language: str = "en"

    def to_srt(self) -> str:
        """Convert subtitles to SRT format."""
        # Ensure subtitles are sorted by start time
        sorted_subs = sorted(self.subtitles, key=lambda s: s.start_time)

        # Assign sequential indices
        indexed_subs = [
            Subtitle(s.text, s.start_time, s.end_time, i + 1)
            for i, s in enumerate(sorted_subs)
        ]

        return "\n".join(sub.to_srt() for sub in indexed_subs)

    def to_vtt(self) -> str:
        """Convert subtitles to WebVTT format."""
        # Ensure subtitles are sorted by start time
        sorted_subs = sorted(self.subtitles, key=lambda s: s.start_time)

        # Build VTT file
        header = f"WEBVTT\nKind: subtitles\nLanguage: {self.language}\n"
        body = "\n".join(sub.to_vtt() for sub in sorted_subs)

        return f"{header}\n{body}"

================
File: video_processor/infrastructure/api/routes/__init__.py
================
"""
API route modules for the video processing API.

This package contains FastAPI router modules for different API endpoints.
"""

from video_processor.infrastructure.api.routes import health, videos

__all__ = ["health", "videos"]

================
File: video_processor/infrastructure/api/routes/health.py
================
"""
Health check routes for the API.

This module provides health check endpoints to verify the API is running and
to check the status of various backend services.
"""

import logging
from datetime import datetime
from typing import Any, Dict

from fastapi import APIRouter, Depends, status

from video_processor.application.interfaces.repositories import JobRepositoryInterface
from video_processor.application.interfaces.storage import StorageInterface
from video_processor.infrastructure.api.dependencies import (
    get_job_repository,
    get_storage_adapter,
)

# Configure logger
logger = logging.getLogger(__name__)

# Create router
router = APIRouter(prefix="/health", tags=["health"])


@router.get("", status_code=status.HTTP_200_OK)
async def health_check() -> Dict[str, Any]:
    """
    Basic health check endpoint.

    Returns:
        Dictionary with status information
    """
    return {
        "status": "ok",
        "timestamp": datetime.now().isoformat(),
        "version": "1.0.0",
    }


@router.get("/detailed", status_code=status.HTTP_200_OK)
async def detailed_health_check(
    job_repository: JobRepositoryInterface = Depends(get_job_repository),
    storage_adapter: StorageInterface = Depends(get_storage_adapter),
) -> Dict[str, Any]:
    """
    Detailed health check endpoint that verifies backend services are operational.

    Args:
        job_repository: Job repository dependency
        storage_adapter: Storage adapter dependency

    Returns:
        Dictionary with detailed status information
    """
    services_status = {
        "api": {"status": "ok"},
        "job_repository": {"status": "unknown"},
        "storage": {"status": "unknown"},
    }

    # Check job repository
    try:
        # Try to get a dummy job
        job_repository.get_by_id("health-check")
        services_status["job_repository"]["status"] = "ok"
    except Exception as e:
        logger.warning(f"Job repository health check failed: {str(e)}")
        services_status["job_repository"]["status"] = "error"
        services_status["job_repository"]["message"] = str(e)

    # Check storage
    try:
        # Just get the URL format as a simple test
        test_url = storage_adapter.get_public_url("test-file.txt")
        services_status["storage"]["status"] = "ok"
    except Exception as e:
        logger.warning(f"Storage health check failed: {str(e)}")
        services_status["storage"]["status"] = "error"
        services_status["storage"]["message"] = str(e)

    # Overall status
    overall_status = "ok"
    for service in services_status.values():
        if service["status"] == "error":
            overall_status = "degraded"
            break

    return {
        "status": overall_status,
        "timestamp": datetime.now().isoformat(),
        "version": "1.0.0",
        "services": services_status,
    }

================
File: video_processor/infrastructure/api/routes/videos.py
================
"""
API routes for video processing.

This module provides API endpoints for uploading, processing, and retrieving videos.
"""

import logging
from typing import Any, Dict, List, Optional

from fastapi import APIRouter, BackgroundTasks, Depends, HTTPException, status

from video_processor.application.interfaces.publishing import PublishingInterface
from video_processor.application.interfaces.repositories import JobRepositoryInterface
from video_processor.application.services.video_processor import VideoProcessorService
from video_processor.domain.exceptions import PublishingError
from video_processor.domain.models.job import JobStatus, VideoJob
from video_processor.domain.models.metadata import VideoMetadata
from video_processor.domain.models.video import Video
from video_processor.infrastructure.api.dependencies import (
    get_job_repository,
    get_publishing_adapter,
    get_video_processor,
)
from video_processor.infrastructure.api.schemas.video import (
    JobResponse,
    JobStatusResponse,
    PublishResponse,
    VideoPublishRequest,
    VideoUploadRequest,
)

# Configure logger
logger = logging.getLogger(__name__)

# Create router
router = APIRouter(prefix="/videos", tags=["videos"])


@router.post(
    "",
    response_model=JobResponse,
    status_code=status.HTTP_202_ACCEPTED,
    summary="Upload and process a video",
    description="Submit a video for processing. This will start an asynchronous processing job.",
)
async def upload_video(
    request: VideoUploadRequest,
    background_tasks: BackgroundTasks,
    job_repository: JobRepositoryInterface = Depends(get_job_repository),
    video_processor: VideoProcessorService = Depends(get_video_processor),
) -> JobResponse:
    """
    Upload and process a video.

    Args:
        request: Video upload request
        background_tasks: FastAPI background tasks
        job_repository: Job repository dependency
        video_processor: Video processor service dependency

    Returns:
        Job information
    """
    try:
        # Create video object
        video = Video(
            file_path=request.file_path,
            duration=0,  # Will be populated during processing
            format="unknown",  # Will be populated during processing
            resolution=(0, 0),  # Will be populated during processing
        )

        # Create metadata object
        metadata = VideoMetadata(
            title=request.title or "",
            description=request.description or "",
        )

        # Create job object
        job = VideoJob(
            video=video,
            metadata=metadata,
            status=JobStatus.PENDING,
        )

        # Save job to repository
        job_id = job_repository.save(job)
        job.id = job_id

        # Start processing in background
        background_tasks.add_task(
            process_video_background,
            job_id=job_id,
            video_processor=video_processor,
            job_repository=job_repository,
            publish_to_youtube=request.publish_to_youtube,
        )

        # Get the job with assigned ID
        job = job_repository.get_by_id(job_id)

        # Convert to response model
        return job_to_response(job)

    except Exception as e:
        logger.exception(f"Failed to upload video: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to upload video: {str(e)}",
        )


@router.get(
    "/{job_id}",
    response_model=JobResponse,
    summary="Get job details",
    description="Get details about a processing job, including video metadata and status.",
)
async def get_job(
    job_id: str,
    job_repository: JobRepositoryInterface = Depends(get_job_repository),
) -> JobResponse:
    """
    Get job details.

    Args:
        job_id: Job ID
        job_repository: Job repository dependency

    Returns:
        Job information

    Raises:
        HTTPException: If job not found
    """
    job = job_repository.get_by_id(job_id)

    if not job:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Job with ID {job_id} not found",
        )

    return job_to_response(job)


@router.get(
    "/{job_id}/status",
    response_model=JobStatusResponse,
    summary="Get job status",
    description="Get the status of a processing job.",
)
async def get_job_status(
    job_id: str,
    job_repository: JobRepositoryInterface = Depends(get_job_repository),
) -> JobStatusResponse:
    """
    Get job status.

    Args:
        job_id: Job ID
        job_repository: Job repository dependency

    Returns:
        Job status information

    Raises:
        HTTPException: If job not found
    """
    job = job_repository.get_by_id(job_id)

    if not job:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Job with ID {job_id} not found",
        )

    # Convert to response model
    stages = list(job.processing_stages.keys()) if job.processing_stages else []

    return JobStatusResponse(
        id=job.id,
        status=JobStatus(job.status.value),
        error=job.error,
        stages=stages,
        updated_at=job.updated_at,
    )


@router.post(
    "/{job_id}/publish",
    response_model=PublishResponse,
    status_code=status.HTTP_202_ACCEPTED,
    summary="Publish a processed video",
    description="Publish a processed video to a platform like YouTube.",
)
async def publish_video(
    job_id: str,
    request: VideoPublishRequest,
    background_tasks: BackgroundTasks,
    job_repository: JobRepositoryInterface = Depends(get_job_repository),
    publishing_adapter: PublishingInterface = Depends(get_publishing_adapter),
) -> PublishResponse:
    """
    Publish a processed video.

    Args:
        job_id: Job ID
        request: Publish request
        background_tasks: FastAPI background tasks
        job_repository: Job repository dependency
        publishing_adapter: Publishing adapter dependency

    Returns:
        Publish response

    Raises:
        HTTPException: If job not found or video not processed
    """
    # Validate job ID in path matches request
    if job_id != request.job_id:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Job ID in path does not match job ID in request body",
        )

    # Get job from repository
    job = job_repository.get_by_id(job_id)

    if not job:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Job with ID {job_id} not found",
        )

    # Check job status
    if job.status != JobStatus.COMPLETED:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Video processing not completed. Current status: {job.status.value}",
        )

    # Start publishing in background
    background_tasks.add_task(
        publish_video_background,
        job=job,
        platform=request.platform,
        custom_metadata=request.metadata.dict() if request.metadata else None,
        job_repository=job_repository,
        publishing_adapter=publishing_adapter,
    )

    # Return initial response
    return PublishResponse(
        job_id=job_id,
        platform=request.platform,
        platform_id="pending",
        status="publishing",
    )


@router.get(
    "",
    response_model=List[JobStatusResponse],
    summary="List processing jobs",
    description="List all processing jobs or filter by status.",
)
async def list_jobs(
    status: Optional[str] = None,
    job_repository: JobRepositoryInterface = Depends(get_job_repository),
) -> List[JobStatusResponse]:
    """
    List processing jobs.

    Args:
        status: Filter by status
        job_repository: Job repository dependency

    Returns:
        List of job status information
    """
    if status:
        try:
            job_status = JobStatus(status)
            jobs = job_repository.get_jobs_by_status(job_status)
        except ValueError:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Invalid status: {status}",
            )
    else:
        # TODO: Implement getting all jobs
        # For now, return pending jobs
        jobs = job_repository.get_pending_jobs()

    # Convert to response models
    responses = []
    for job in jobs:
        stages = list(job.processing_stages.keys()) if job.processing_stages else []
        responses.append(
            JobStatusResponse(
                id=job.id,
                status=JobStatus(job.status.value),
                error=job.error,
                stages=stages,
                updated_at=job.updated_at,
            )
        )

    return responses


@router.delete(
    "/{job_id}",
    status_code=status.HTTP_204_NO_CONTENT,
    summary="Delete a job",
    description="Delete a processing job and its associated data.",
)
async def delete_job(
    job_id: str,
    job_repository: JobRepositoryInterface = Depends(get_job_repository),
) -> None:
    """
    Delete a job.

    Args:
        job_id: Job ID
        job_repository: Job repository dependency

    Raises:
        HTTPException: If job not found or deletion fails
    """
    job = job_repository.get_by_id(job_id)

    if not job:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Job with ID {job_id} not found",
        )

    success = job_repository.delete(job_id)

    if not success:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to delete job with ID {job_id}",
        )


# Helper functions for background tasks


async def process_video_background(
    job_id: str,
    video_processor: VideoProcessorService,
    job_repository: JobRepositoryInterface,
    publish_to_youtube: bool = False,
) -> None:
    """
    Process a video in the background.

    Args:
        job_id: Job ID
        video_processor: Video processor service
        job_repository: Job repository
        publish_to_youtube: Whether to publish to YouTube after processing
    """
    try:
        logger.info(f"Starting background processing for job {job_id}")

        # Get job from repository
        job = job_repository.get_by_id(job_id)

        if not job:
            logger.error(f"Job with ID {job_id} not found")
            return

        # Process video
        updated_job = video_processor.process_video(job)

        # Update job in repository
        job_repository.update(updated_job)

        logger.info(f"Completed background processing for job {job_id}")

        # TODO: If publish_to_youtube is True, start publishing process

    except Exception as e:
        logger.exception(f"Error in background processing for job {job_id}: {str(e)}")

        # Update job status to failed
        try:
            job_repository.update_job_status(
                job_id=job_id,
                status=JobStatus.FAILED,
                error=str(e),
            )
        except Exception as update_error:
            logger.error(f"Failed to update job status: {str(update_error)}")


async def publish_video_background(
    job: VideoJob,
    platform: str,
    custom_metadata: Optional[Dict[str, Any]],
    job_repository: JobRepositoryInterface,
    publishing_adapter: PublishingInterface,
) -> None:
    """
    Publish a video in the background.

    Args:
        job: Video job
        platform: Publishing platform
        custom_metadata: Custom metadata for publishing
        job_repository: Job repository
        publishing_adapter: Publishing adapter
    """
    try:
        logger.info(f"Starting background publishing for job {job.id} to {platform}")

        if platform.lower() != "youtube":
            logger.error(f"Unsupported platform: {platform}")
            raise PublishingError(f"Unsupported platform: {platform}")

        # Prepare metadata
        metadata = {
            "title": job.metadata.title,
            "description": job.metadata.description,
            "tags": job.metadata.tags,
            "privacy_status": "private",  # Default to private
        }

        # Override with custom metadata if provided
        if custom_metadata:
            metadata.update(custom_metadata)

        # Upload video to YouTube
        platform_id = publishing_adapter.upload_video(
            video_file=job.video.file_path,
            metadata=metadata,
        )

        logger.info(
            f"Video for job {job.id} published to {platform} with ID {platform_id}"
        )

        # TODO: Store publishing result in database

    except Exception as e:
        logger.exception(f"Error in background publishing for job {job.id}: {str(e)}")
        # TODO: Store publishing error in database


def job_to_response(job: VideoJob) -> JobResponse:
    """
    Convert a VideoJob to a JobResponse.

    Args:
        job: VideoJob to convert

    Returns:
        JobResponse
    """
    return JobResponse(
        id=job.id,
        status=JobStatus(job.status.value),
        video={
            "id": job.video.id,
            "file_path": job.video.file_path,
            "duration": job.video.duration,
            "format": job.video.format,
            "resolution": job.video.resolution,
            "created_at": job.video.created_at,
            "updated_at": job.video.updated_at,
        },
        metadata={
            "title": job.metadata.title,
            "description": job.metadata.description,
            "tags": job.metadata.tags,
            "show_notes": job.metadata.show_notes,
            "thumbnail_url": job.metadata.thumbnail_url,
            "transcript": job.metadata.transcript,
            "transcript_url": job.metadata.transcript_url,
            "subtitle_urls": job.metadata.subtitle_urls,
        },
        error=job.error,
        processing_stages=job.processing_stages,
        created_at=job.created_at,
        updated_at=job.updated_at,
    )

================
File: video_processor/infrastructure/api/schemas/__init__.py
================
"""
API schema models for request and response validation.

This package contains Pydantic models for API request and response validation.
"""

from video_processor.infrastructure.api.schemas.video import (
    JobResponse,
    JobStatus,
    JobStatusResponse,
    MetadataResponse,
    PublishResponse,
    VideoMetadataRequest,
    VideoPublishRequest,
    VideoResponse,
    VideoUploadRequest,
)

__all__ = [
    "JobResponse",
    "JobStatus",
    "JobStatusResponse",
    "VideoUploadRequest",
    "VideoMetadataRequest",
    "VideoPublishRequest",
    "VideoResponse",
    "MetadataResponse",
    "PublishResponse",
]

================
File: video_processor/infrastructure/api/schemas/video.py
================
"""
API schemas for video-related requests and responses.

This module provides Pydantic models for validating and serializing
video-related data in the API.
"""

from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional

from pydantic import BaseModel, Field, HttpUrl, validator


class JobStatus(str, Enum):
    """Job status enum."""

    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"


class VideoUploadRequest(BaseModel):
    """Video upload request model."""

    file_path: str = Field(..., description="Path to the video file (local or GCS)")
    title: Optional[str] = Field(None, description="Video title")
    description: Optional[str] = Field(None, description="Video description")
    generate_metadata: bool = Field(
        True, description="Whether to generate metadata automatically"
    )
    publish_to_youtube: bool = Field(
        False, description="Whether to publish to YouTube after processing"
    )


class VideoMetadataRequest(BaseModel):
    """Video metadata request model."""

    title: Optional[str] = Field(None, description="Video title")
    description: Optional[str] = Field(None, description="Video description")
    tags: Optional[List[str]] = Field(None, description="Video tags")
    category_id: Optional[str] = Field(None, description="YouTube category ID")
    privacy_status: Optional[str] = Field(
        "private", description="YouTube privacy status"
    )
    made_for_kids: Optional[bool] = Field(
        False, description="Whether the video is made for kids"
    )


class VideoPublishRequest(BaseModel):
    """Video publish request model."""

    job_id: str = Field(..., description="Job ID of the processed video")
    metadata: Optional[VideoMetadataRequest] = Field(
        None, description="Video metadata for publishing"
    )
    platform: str = Field(
        "youtube",
        description="Publishing platform (currently only 'youtube' is supported)",
    )


class VideoResponse(BaseModel):
    """Video response model."""

    id: str = Field(..., description="Video ID")
    file_path: str = Field(..., description="Path to the video file")
    duration: float = Field(..., description="Video duration in seconds")
    format: str = Field(..., description="Video format")
    resolution: tuple = Field(..., description="Video resolution (width, height)")
    created_at: Optional[datetime] = Field(None, description="Creation timestamp")
    updated_at: Optional[datetime] = Field(None, description="Last update timestamp")

    @validator("resolution")
    def validate_resolution(cls, v):
        """Validate resolution tuple."""
        if not isinstance(v, tuple) or len(v) != 2:
            raise ValueError("Resolution must be a tuple of (width, height)")
        return v

    class Config:
        """Configuration for the model."""

        json_encoders = {datetime: lambda v: v.isoformat() if v else None}


class MetadataResponse(BaseModel):
    """Metadata response model."""

    title: str = Field(..., description="Video title")
    description: str = Field(..., description="Video description")
    tags: List[str] = Field([], description="Video tags")
    show_notes: str = Field("", description="Generated show notes")
    thumbnail_url: Optional[str] = Field(None, description="URL of thumbnail image")
    transcript: Optional[str] = Field(None, description="Full transcript text")
    transcript_url: Optional[str] = Field(None, description="URL to transcript file")
    subtitle_urls: Dict[str, str] = Field(
        {}, description="Dictionary of subtitle format to URL"
    )


class JobResponse(BaseModel):
    """Job response model."""

    id: str = Field(..., description="Job ID")
    status: JobStatus = Field(..., description="Job status")
    video: VideoResponse = Field(..., description="Video information")
    metadata: MetadataResponse = Field(..., description="Video metadata")
    error: Optional[str] = Field(None, description="Error message if status is FAILED")
    processing_stages: Dict[str, Any] = Field(
        {}, description="Processing stage information"
    )
    created_at: Optional[datetime] = Field(None, description="Creation timestamp")
    updated_at: Optional[datetime] = Field(None, description="Last update timestamp")

    class Config:
        """Configuration for the model."""

        json_encoders = {datetime: lambda v: v.isoformat() if v else None}


class JobStatusResponse(BaseModel):
    """Job status response model."""

    id: str = Field(..., description="Job ID")
    status: JobStatus = Field(..., description="Job status")
    error: Optional[str] = Field(None, description="Error message if status is FAILED")
    stages: List[str] = Field([], description="Completed processing stages")
    updated_at: Optional[datetime] = Field(None, description="Last update timestamp")

    class Config:
        """Configuration for the model."""

        json_encoders = {datetime: lambda v: v.isoformat() if v else None}


class PublishResponse(BaseModel):
    """Publish response model."""

    job_id: str = Field(..., description="Job ID")
    platform: str = Field(..., description="Publishing platform")
    platform_id: str = Field(
        ..., description="ID on the publishing platform (e.g., YouTube video ID)"
    )
    status: str = Field(..., description="Publishing status")
    url: Optional[HttpUrl] = Field(None, description="URL to the published video")
    timestamp: datetime = Field(
        default_factory=datetime.now, description="Publish timestamp"
    )

    class Config:
        """Configuration for the model."""

        json_encoders = {datetime: lambda v: v.isoformat() if v else None}

================
File: video_processor/infrastructure/api/__init__.py
================
"""
API infrastructure for the video processor.

This package provides the FastAPI implementation for the video processing API.
"""

from video_processor.infrastructure.api.server import app, create_app

__all__ = ["app", "create_app"]

================
File: video_processor/infrastructure/api/auth.py
================
"""Authentication and authorization for the API."""

import os
import time
from typing import Any, Callable, Dict, List, Optional

import jwt
from fastapi import Depends, HTTPException, Request, status
from fastapi.security import HTTPAuthorizationCredentials, HTTPBearer
from pydantic import BaseModel

# Security scheme for Swagger UI
security_scheme = HTTPBearer(
    scheme_name="Bearer Authentication",
    description="Enter JWT token",
    auto_error=False,
)


class TokenPayload(BaseModel):
    """JWT token payload."""

    sub: str  # Subject (user ID)
    iat: int  # Issued at
    exp: int  # Expiration time
    role: str  # User role
    scopes: List[str]  # Authorized scopes


class AuthConfig:
    """Authentication configuration."""

    def __init__(
        self,
        secret_key: Optional[str] = None,
        algorithm: str = "HS256",
        token_expire_minutes: int = 60 * 24,  # 24 hours
        api_key_header_name: str = "X-API-Key",
    ):
        """Initialize authentication configuration.

        Args:
            secret_key: JWT secret key (defaults to environment variable)
            algorithm: JWT algorithm
            token_expire_minutes: Token expiration time in minutes
            api_key_header_name: Name of the header for API key
        """
        self.secret_key = secret_key or os.environ.get("JWT_SECRET_KEY", "")
        if not self.secret_key:
            raise ValueError("JWT_SECRET_KEY not set")

        self.algorithm = algorithm
        self.token_expire_minutes = token_expire_minutes
        self.api_key_header_name = api_key_header_name

        # API keys (in production, these should be stored in a database or secret manager)
        self._api_keys = {}


class Auth:
    """Authentication and authorization handler."""

    def __init__(self, config: AuthConfig):
        """Initialize authentication handler.

        Args:
            config: Authentication configuration
        """
        self.config = config

    def create_access_token(
        self, user_id: str, role: str = "user", scopes: Optional[List[str]] = None
    ) -> str:
        """Create a new JWT access token.

        Args:
            user_id: User ID
            role: User role (default: "user")
            scopes: Authorized scopes

        Returns:
            JWT token string
        """
        if scopes is None:
            scopes = []

        expiration = int(time.time() + self.config.token_expire_minutes * 60)

        payload = {
            "sub": user_id,
            "iat": int(time.time()),
            "exp": expiration,
            "role": role,
            "scopes": scopes,
        }

        return jwt.encode(
            payload, self.config.secret_key, algorithm=self.config.algorithm
        )

    def verify_token(self, token: str) -> TokenPayload:
        """Verify JWT token and return payload.

        Args:
            token: JWT token string

        Returns:
            Token payload

        Raises:
            HTTPException: If token is invalid
        """
        try:
            payload = jwt.decode(
                token, self.config.secret_key, algorithms=[self.config.algorithm]
            )

            return TokenPayload(**payload)

        except jwt.ExpiredSignatureError:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Token has expired",
                headers={"WWW-Authenticate": "Bearer"},
            )

        except jwt.InvalidTokenError:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Invalid token",
                headers={"WWW-Authenticate": "Bearer"},
            )

    def register_api_key(self, api_key: str, user_id: str, role: str = "api") -> None:
        """Register a new API key.

        Args:
            api_key: API key
            user_id: User ID
            role: User role (default: "api")
        """
        self.config._api_keys[api_key] = {
            "user_id": user_id,
            "role": role,
        }

    def verify_api_key(self, api_key: str) -> Dict[str, str]:
        """Verify API key and return user info.

        Args:
            api_key: API key

        Returns:
            User info (user_id and role)

        Raises:
            HTTPException: If API key is invalid
        """
        if api_key not in self.config._api_keys:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Invalid API key",
            )

        return self.config._api_keys[api_key]


# Create auth handler with default configuration
auth_config = AuthConfig()
auth = Auth(auth_config)


def get_auth_from_request(request: Request) -> Dict[str, Any]:
    """Get authentication info from request.

    Attempts to get authentication from:
    1. Bearer token in Authorization header
    2. API key in X-API-Key header

    Args:
        request: FastAPI request

    Returns:
        Authentication info

    Raises:
        HTTPException: If authentication is invalid or missing
    """
    # Try to get Bearer token
    auth_header = request.headers.get("Authorization")
    if auth_header and auth_header.startswith("Bearer "):
        token = auth_header.replace("Bearer ", "")
        payload = auth.verify_token(token)
        return {
            "user_id": payload.sub,
            "role": payload.role,
            "scopes": payload.scopes,
            "auth_method": "jwt",
        }

    # Try to get API key
    api_key = request.headers.get(auth_config.api_key_header_name)
    if api_key:
        user_info = auth.verify_api_key(api_key)
        return {
            "user_id": user_info["user_id"],
            "role": user_info["role"],
            "scopes": [],
            "auth_method": "api_key",
        }

    # No authentication provided
    raise HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Authentication required",
        headers={"WWW-Authenticate": "Bearer"},
    )


def get_current_user(
    credentials: HTTPAuthorizationCredentials = Depends(security_scheme),
    request: Request = None,
) -> Dict[str, Any]:
    """Dependency for getting the current user from JWT token.

    Args:
        credentials: HTTP Bearer credentials
        request: FastAPI request (for API key)

    Returns:
        User info

    Raises:
        HTTPException: If authentication is invalid or missing
    """
    if not credentials:
        # If no Bearer token, try API key
        if request:
            return get_auth_from_request(request)
        else:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Authentication required",
                headers={"WWW-Authenticate": "Bearer"},
            )

    # Verify Bearer token
    payload = auth.verify_token(credentials.credentials)
    return {
        "user_id": payload.sub,
        "role": payload.role,
        "scopes": payload.scopes,
        "auth_method": "jwt",
    }


def get_current_active_user(
    current_user: Dict[str, Any] = Depends(get_current_user),
) -> Dict[str, Any]:
    """Dependency for getting the current active user.

    Args:
        current_user: Current user info

    Returns:
        User info

    Raises:
        HTTPException: If user is inactive
    """
    # In a real application, check if user is active in database
    return current_user


def require_role(allowed_roles: List[str]) -> Callable:
    """Dependency factory for role-based access control.

    Args:
        allowed_roles: List of allowed roles

    Returns:
        Dependency function
    """

    def role_checker(
        current_user: Dict[str, Any] = Depends(get_current_active_user),
    ) -> Dict[str, Any]:
        if current_user["role"] not in allowed_roles:
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail=f"Role {current_user['role']} not authorized",
            )
        return current_user

    return role_checker


def require_scope(required_scope: str) -> Callable:
    """Dependency factory for scope-based access control.

    Args:
        required_scope: Required scope

    Returns:
        Dependency function
    """

    def scope_checker(
        current_user: Dict[str, Any] = Depends(get_current_active_user),
    ) -> Dict[str, Any]:
        if "scopes" not in current_user or required_scope not in current_user["scopes"]:
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail=f"Scope {required_scope} required",
            )
        return current_user

    return scope_checker


# Pre-defined roles for common use cases
require_admin = require_role(["admin"])
require_user = require_role(["user", "admin"])
require_api = require_role(["api", "admin"])

================
File: video_processor/infrastructure/api/dependencies.py
================
"""
Dependencies for FastAPI route handlers.

This module provides dependency functions for FastAPI route handlers.
These functions are used with FastAPI's dependency injection system.
"""

import logging
from typing import Optional

from fastapi import Depends, HTTPException, status

from video_processor.application.interfaces.ai import AIServiceInterface
from video_processor.application.interfaces.publishing import PublishingInterface
from video_processor.application.interfaces.repositories import (
    JobRepositoryInterface,
    VideoRepositoryInterface,
)
from video_processor.application.interfaces.storage import StorageInterface
from video_processor.application.services.video_processor import VideoProcessorService
from video_processor.infrastructure.config.container import Container

# Configure logger
logger = logging.getLogger(__name__)

# Global dependency injection container
container = Container()


def get_container() -> Container:
    """
    Get the dependency injection container.

    Returns:
        Dependency injection container
    """
    return container


def get_job_repository(
    container: Container = Depends(get_container),
) -> JobRepositoryInterface:
    """
    Get the job repository.

    Args:
        container: Dependency injection container

    Returns:
        Job repository instance
    """
    return container.job_repository()


def get_video_repository(
    container: Container = Depends(get_container),
) -> VideoRepositoryInterface:
    """
    Get the video repository.

    Args:
        container: Dependency injection container

    Returns:
        Video repository instance
    """
    return container.video_repository()


def get_storage_adapter(
    container: Container = Depends(get_container),
) -> StorageInterface:
    """
    Get the storage adapter.

    Args:
        container: Dependency injection container

    Returns:
        Storage adapter instance
    """
    return container.storage_adapter()


def get_ai_adapter(container: Container = Depends(get_container)) -> AIServiceInterface:
    """
    Get the AI service adapter.

    Args:
        container: Dependency injection container

    Returns:
        AI service adapter instance
    """
    return container.ai_adapter()


def get_publishing_adapter(
    container: Container = Depends(get_container),
) -> PublishingInterface:
    """
    Get the publishing adapter.

    Args:
        container: Dependency injection container

    Returns:
        Publishing adapter instance
    """
    return container.publishing_adapter()


def get_video_processor(
    storage_adapter: StorageInterface = Depends(get_storage_adapter),
    ai_adapter: AIServiceInterface = Depends(get_ai_adapter),
    container: Container = Depends(get_container),
) -> VideoProcessorService:
    """
    Get the video processor service.

    Args:
        storage_adapter: Storage adapter dependency
        ai_adapter: AI service adapter dependency
        container: Dependency injection container

    Returns:
        Video processor service instance
    """
    output_bucket = container.settings.storage.output_bucket
    local_output_dir = container.settings.storage.local_output_dir

    return VideoProcessorService(
        storage_adapter=storage_adapter,
        ai_adapter=ai_adapter,
        output_bucket=output_bucket,
        local_output_dir=local_output_dir,
    )


def get_current_user(token: str = Depends(lambda: None)) -> Optional[dict]:
    """
    Get the current authenticated user.

    This is a placeholder for future authentication implementation.
    Currently, it always returns None, meaning no authentication.

    Args:
        token: Authentication token (placeholder)

    Returns:
        User information or None if not authenticated
    """
    # TODO: Implement authentication
    return None


def require_auth(user: Optional[dict] = Depends(get_current_user)) -> dict:
    """
    Require authentication for a route.

    Args:
        user: User information from get_current_user

    Returns:
        User information

    Raises:
        HTTPException: If not authenticated
    """
    if user is None:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Authentication required",
            headers={"WWW-Authenticate": "Bearer"},
        )
    return user

================
File: video_processor/infrastructure/api/server.py
================
"""
FastAPI server for the video processing API.

This module provides the main FastAPI application for the video processing API.
"""

import logging
import os
from datetime import datetime
from typing import Dict

from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.openapi.docs import get_swagger_ui_html
from fastapi.openapi.utils import get_openapi
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field

from video_processor.domain.exceptions import VideoProcessingError
from video_processor.infrastructure.api.routes import health, videos
from video_processor.infrastructure.config.settings import APISettings

# Configure logger
logger = logging.getLogger(__name__)


class HealthResponse(BaseModel):
    """Response model for health check."""

    status: str = Field(..., description="Service status")
    version: str = Field(..., description="API version")
    timestamp: datetime = Field(
        default_factory=datetime.utcnow, description="Current server time"
    )


def create_app(settings: APISettings = None) -> FastAPI:
    """
    Create and configure the FastAPI application.

    Args:
        settings: API settings

    Returns:
        Configured FastAPI application
    """
    # Create FastAPI app with metadata for Swagger UI
    app = FastAPI(
        title="Video Processor API",
        description="API for processing videos, generating transcripts, and creating metadata",
        version="1.0.0",
        docs_url=None,  # Disable default docs to customize
        redoc_url=None,  # Disable default redoc to customize
    )

    # Configure CORS
    app.add_middleware(
        CORSMiddleware,
        allow_origins=(
            settings.cors_origins
            if settings
            else os.environ.get("CORS_ORIGINS", "*").split(",")
        ),
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    # Add exception handlers
    @app.exception_handler(VideoProcessingError)
    async def video_processing_error_handler(
        request: Request, exc: VideoProcessingError
    ) -> JSONResponse:
        """Handle video processing errors."""
        return JSONResponse(
            status_code=400,
            content={"error": str(exc), "type": exc.__class__.__name__},
        )

    @app.exception_handler(Exception)
    async def general_exception_handler(
        request: Request, exc: Exception
    ) -> JSONResponse:
        """Handle general exceptions."""
        logger.exception(f"Unhandled exception: {str(exc)}")
        return JSONResponse(
            status_code=500,
            content={"error": "Internal server error", "detail": str(exc)},
        )

    # Custom OpenAPI schema
    def custom_openapi():
        if app.openapi_schema:
            return app.openapi_schema

        openapi_schema = get_openapi(
            title=app.title,
            version=app.version,
            description=app.description,
            routes=app.routes,
        )

        # Add JWT authentication to OpenAPI schema
        openapi_schema["components"]["securitySchemes"] = {
            "bearerAuth": {
                "type": "http",
                "scheme": "bearer",
                "bearerFormat": "JWT",
                "description": "Enter JWT token",
            },
            "apiKeyAuth": {
                "type": "apiKey",
                "in": "header",
                "name": "X-API-Key",
                "description": "API key for authentication",
            },
        }

        openapi_schema["security"] = [{"bearerAuth": []}, {"apiKeyAuth": []}]

        app.openapi_schema = openapi_schema
        return app.openapi_schema

    app.openapi = custom_openapi

    # Custom docs endpoints
    @app.get("/docs", include_in_schema=False)
    async def custom_swagger_ui_html():
        """Serve custom Swagger UI."""
        return get_swagger_ui_html(
            openapi_url=app.openapi_url,
            title=f"{app.title} - API Documentation",
            swagger_js_url="https://cdn.jsdelivr.net/npm/swagger-ui-dist@5.9.0/swagger-ui-bundle.js",
            swagger_css_url="https://cdn.jsdelivr.net/npm/swagger-ui-dist@5.9.0/swagger-ui.css",
        )

    # Health check endpoint
    @app.get("/health", response_model=HealthResponse, tags=["Health"])
    async def health_check():
        """Check if the service is healthy.

        Returns:
            Health status
        """
        return {
            "status": "ok",
            "version": app.version,
            "timestamp": datetime.utcnow(),
        }

    # Include routers
    app.include_router(health.router, tags=["health"])
    app.include_router(videos.router, prefix="/api/v1")

    # Add startup and shutdown events
    @app.on_event("startup")
    async def startup_event():
        """Run tasks on application startup."""
        logger.info("Starting Video Processor API")

    @app.on_event("shutdown")
    async def shutdown_event():
        """Run tasks on application shutdown."""
        logger.info("Shutting down Video Processor API")

    return app


# Create the FastAPI app instance
app = create_app()


@app.get("/", tags=["root"])
async def root() -> Dict[str, str]:
    """
    Root endpoint returning API information.

    Returns:
        API information
    """
    return {
        "name": "Video Processor API",
        "version": "1.0.0",
        "status": "operational",
    }

================
File: video_processor/infrastructure/config/container.py
================
"""
Dependency injection container for the application.

This module sets up the dependency injection container that provides
all services needed by the application. It uses Python's built-in
functions and a registry pattern rather than a full DI framework to
keep things simple but maintainable.
"""

from typing import Any, Dict, Optional, Type

from video_processor.application.interfaces.ai import AIServiceInterface
from video_processor.application.interfaces.publishing import PublishingInterface
from video_processor.application.interfaces.storage import StorageInterface
from video_processor.infrastructure.config.settings import settings


class Container:
    """
    Dependency injection container.

    This container manages all service instances and their dependencies,
    providing a clean way to access services throughout the application.
    """

    def __init__(self):
        """Initialize the container."""
        self._services: Dict[str, Any] = {}
        self._factories: Dict[str, callable] = {}
        self._service_registry: Dict[str, Dict[str, Type]] = {
            "storage": {},
            "ai": {},
            "publishing": {},
        }

    def register_factory(self, service_name: str, factory: callable) -> None:
        """
        Register a factory function for a service.

        Args:
            service_name: Name of the service
            factory: Factory function that creates the service
        """
        self._factories[service_name] = factory

    def register_service_implementation(
        self, interface_name: str, implementation_name: str, implementation_type: Type
    ) -> None:
        """
        Register a service implementation for an interface.

        Args:
            interface_name: Name of the interface (e.g., "storage")
            implementation_name: Name of the implementation (e.g., "gcs")
            implementation_type: Type of the implementation class
        """
        if interface_name not in self._service_registry:
            self._service_registry[interface_name] = {}

        self._service_registry[interface_name][
            implementation_name
        ] = implementation_type

    def get(self, service_name: str, implementation: Optional[str] = None) -> Any:
        """
        Get a service instance.

        Args:
            service_name: Name of the service
            implementation: Optional name of the specific implementation to use
                            If not provided, the default implementation will be used

        Returns:
            The service instance

        Raises:
            ValueError: If the service is not registered
        """
        # If specific implementation is requested and it's in a registry
        if implementation and service_name in self._service_registry:
            registry = self._service_registry[service_name]
            if implementation not in registry:
                raise ValueError(
                    f"Implementation '{implementation}' not found for service '{service_name}'"
                )

            implementation_type = registry[implementation]
            service_key = f"{service_name}_{implementation}"

            if service_key not in self._services:
                self._services[service_key] = self._create_service(implementation_type)

            return self._services[service_key]

        # Normal service lookup
        if service_name not in self._services:
            if service_name not in self._factories:
                raise ValueError(f"Service '{service_name}' not registered")

            self._services[service_name] = self._factories[service_name]()

        return self._services[service_name]

    def _create_service(self, service_type: Type) -> Any:
        """
        Create a service instance.

        This method handles dependency injection for service constructors.

        Args:
            service_type: Type of the service to create

        Returns:
            The service instance
        """
        # This is a simple implementation that assumes the constructor
        # doesn't need any dependencies. In a real application, you would
        # inspect the constructor signature and provide the necessary dependencies.
        return service_type()


# Create storage adapter factory
def storage_adapter_factory() -> StorageInterface:
    """
    Factory function for storage adapter.

    Returns:
        An instance of a StorageInterface implementation based on settings
    """
    if settings.storage.use_local_storage:
        # Import here to avoid circular imports
        from video_processor.adapters.storage.local import LocalStorageAdapter

        return LocalStorageAdapter(root_path=settings.storage.local_storage_path)
    else:
        # Import here to avoid circular imports
        from video_processor.adapters.storage.gcs import GCSStorageAdapter

        return GCSStorageAdapter(
            bucket_name=settings.storage.gcs_bucket,
            input_prefix=settings.storage.gcs_input_prefix,
            output_prefix=settings.storage.gcs_output_prefix,
        )


# Create AI service factory
def ai_service_factory() -> AIServiceInterface:
    """
    Factory function for AI service.

    Returns:
        An instance of an AIServiceInterface implementation based on settings
    """
    if settings.ai.use_vertex_ai:
        # Import here to avoid circular imports
        from video_processor.adapters.ai.vertex_ai import VertexAIAdapter

        return VertexAIAdapter(
            project=settings.ai.vertex_ai_project,
            location=settings.ai.vertex_ai_location,
            model=settings.ai.vertex_ai_model,
        )
    else:
        # Import here to avoid circular imports
        from video_processor.adapters.ai.gemini import GeminiAIAdapter

        return GeminiAIAdapter(
            api_key=settings.ai.gemini_api_key, model=settings.ai.gemini_model
        )


# Create YouTube adapter factory
def youtube_adapter_factory() -> PublishingInterface:
    """
    Factory function for YouTube adapter.

    Returns:
        An instance of a PublishingInterface implementation for YouTube
    """
    # Import here to avoid circular imports
    from video_processor.adapters.publishing.youtube import YouTubeAdapter

    return YouTubeAdapter(
        client_secrets_file=settings.youtube.client_secrets_file,
        token_file=settings.youtube.token_file,
        scopes=settings.youtube.scopes,
    )


# Create and configure the container
container = Container()

# Register factories
container.register_factory("storage", storage_adapter_factory)
container.register_factory("ai", ai_service_factory)
container.register_factory("youtube", youtube_adapter_factory)

# When we implement the actual services, we'll also register implementations:
# container.register_service_implementation("storage", "gcs", GCSStorageAdapter)
# container.register_service_implementation("storage", "local", LocalStorageAdapter)
# container.register_service_implementation("ai", "gemini", GeminiAIAdapter)
# container.register_service_implementation("ai", "vertex", VertexAIAdapter)
# container.register_service_implementation("publishing", "youtube", YouTubeAdapter)

================
File: video_processor/infrastructure/config/secrets.py
================
"""Secure credential management using Google Cloud Secret Manager."""

import json
import os
from functools import lru_cache
from typing import Any, Dict, Optional

from google.cloud import secretmanager
from google.cloud.secretmanager_v1 import SecretManagerServiceClient

from video_processor.infrastructure.monitoring import structured_log


class SecretManagerClient:
    """Client for Google Cloud Secret Manager.

    This class provides methods for securely accessing and managing credentials
    stored in Google Cloud Secret Manager.
    """

    def __init__(
        self,
        project_id: Optional[str] = None,
        client: Optional[SecretManagerServiceClient] = None,
    ):
        """Initialize Secret Manager client.

        Args:
            project_id: Google Cloud project ID (defaults to GOOGLE_CLOUD_PROJECT env var)
            client: Secret Manager client instance (created if not provided)
        """
        self.project_id = project_id or os.environ.get("GOOGLE_CLOUD_PROJECT")
        if not self.project_id:
            raise ValueError(
                "Project ID must be provided or set as GOOGLE_CLOUD_PROJECT environment variable"
            )

        self.client = client or secretmanager.SecretManagerServiceClient()

        # Cache for secrets to avoid repeated API calls
        self._secret_cache: Dict[str, Dict[str, Any]] = {}

    def get_secret_version_path(self, secret_id: str, version: str = "latest") -> str:
        """Get the full path to a secret version.

        Args:
            secret_id: Secret ID
            version: Secret version (default: latest)

        Returns:
            Full path to the secret version
        """
        return f"projects/{self.project_id}/secrets/{secret_id}/versions/{version}"

    def get_secret(self, secret_id: str, version: str = "latest") -> str:
        """Get a secret value.

        Args:
            secret_id: Secret ID
            version: Secret version (default: latest)

        Returns:
            Secret value as a string

        Raises:
            Exception: If the secret cannot be accessed
        """
        # Check cache first
        cache_key = f"{secret_id}:{version}"
        if cache_key in self._secret_cache:
            return self._secret_cache[cache_key]["value"]

        try:
            # Build the resource name of the secret version
            name = self.get_secret_version_path(secret_id, version)

            # Access the secret version
            response = self.client.access_secret_version(request={"name": name})

            # Get the secret value
            payload = response.payload.data.decode("UTF-8")

            # Cache the secret
            self._secret_cache[cache_key] = {
                "value": payload,
                "version": response.name.split("/")[-1],
            }

            structured_log(
                "info",
                f"Retrieved secret {secret_id} (version: {version})",
                {"secret_id": secret_id},
            )

            return payload

        except Exception as e:
            structured_log(
                "error",
                f"Failed to retrieve secret {secret_id}: {str(e)}",
                {"secret_id": secret_id, "error": str(e)},
            )
            raise

    def get_json_secret(
        self, secret_id: str, version: str = "latest"
    ) -> Dict[str, Any]:
        """Get a JSON secret value.

        Args:
            secret_id: Secret ID
            version: Secret version (default: latest)

        Returns:
            Secret value as a parsed JSON object

        Raises:
            Exception: If the secret cannot be accessed or is not valid JSON
        """
        secret_value = self.get_secret(secret_id, version)
        try:
            return json.loads(secret_value)
        except json.JSONDecodeError as e:
            structured_log(
                "error",
                f"Failed to parse JSON secret {secret_id}: {str(e)}",
                {"secret_id": secret_id, "error": str(e)},
            )
            raise

    def create_secret(self, secret_id: str, secret_value: str) -> None:
        """Create a new secret.

        Args:
            secret_id: Secret ID
            secret_value: Secret value

        Raises:
            Exception: If the secret cannot be created
        """
        try:
            parent = f"projects/{self.project_id}"

            # Create the secret
            self.client.create_secret(
                request={
                    "parent": parent,
                    "secret_id": secret_id,
                    "secret": {"replication": {"automatic": {}}},
                }
            )

            # Add the secret version
            self.client.add_secret_version(
                request={
                    "parent": f"{parent}/secrets/{secret_id}",
                    "payload": {"data": secret_value.encode("UTF-8")},
                }
            )

            structured_log(
                "info",
                f"Created secret {secret_id}",
                {"secret_id": secret_id},
            )

        except Exception as e:
            structured_log(
                "error",
                f"Failed to create secret {secret_id}: {str(e)}",
                {"secret_id": secret_id, "error": str(e)},
            )
            raise

    def update_secret(self, secret_id: str, secret_value: str) -> None:
        """Update an existing secret with a new version.

        Args:
            secret_id: Secret ID
            secret_value: New secret value

        Raises:
            Exception: If the secret cannot be updated
        """
        try:
            parent = f"projects/{self.project_id}/secrets/{secret_id}"

            # Add a new version
            response = self.client.add_secret_version(
                request={
                    "parent": parent,
                    "payload": {"data": secret_value.encode("UTF-8")},
                }
            )

            # Update cache
            version = response.name.split("/")[-1]
            cache_key = f"{secret_id}:latest"
            self._secret_cache[cache_key] = {
                "value": secret_value,
                "version": version,
            }

            structured_log(
                "info",
                f"Updated secret {secret_id} (version: {version})",
                {"secret_id": secret_id, "version": version},
            )

        except Exception as e:
            structured_log(
                "error",
                f"Failed to update secret {secret_id}: {str(e)}",
                {"secret_id": secret_id, "error": str(e)},
            )
            raise

    def delete_secret(self, secret_id: str) -> None:
        """Delete a secret.

        Args:
            secret_id: Secret ID

        Raises:
            Exception: If the secret cannot be deleted
        """
        try:
            name = f"projects/{self.project_id}/secrets/{secret_id}"

            # Delete the secret
            self.client.delete_secret(request={"name": name})

            # Clear cache
            for key in list(self._secret_cache.keys()):
                if key.startswith(f"{secret_id}:"):
                    del self._secret_cache[key]

            structured_log(
                "info",
                f"Deleted secret {secret_id}",
                {"secret_id": secret_id},
            )

        except Exception as e:
            structured_log(
                "error",
                f"Failed to delete secret {secret_id}: {str(e)}",
                {"secret_id": secret_id, "error": str(e)},
            )
            raise


# Create a singleton instance with caching
@lru_cache(maxsize=1)
def get_secret_manager_client(project_id: Optional[str] = None) -> SecretManagerClient:
    """Get a Secret Manager client instance.

    This function returns a singleton instance of the Secret Manager client,
    which is cached to avoid creating multiple instances.

    Args:
        project_id: Google Cloud project ID (defaults to GOOGLE_CLOUD_PROJECT env var)

    Returns:
        Secret Manager client instance
    """
    return SecretManagerClient(project_id)


def get_secret(secret_id: str, version: str = "latest") -> str:
    """Convenience function to get a secret value.

    Args:
        secret_id: Secret ID
        version: Secret version (default: latest)

    Returns:
        Secret value as a string
    """
    client = get_secret_manager_client()
    return client.get_secret(secret_id, version)


def get_json_secret(secret_id: str, version: str = "latest") -> Dict[str, Any]:
    """Convenience function to get a JSON secret value.

    Args:
        secret_id: Secret ID
        version: Secret version (default: latest)

    Returns:
        Secret value as a parsed JSON object
    """
    client = get_secret_manager_client()
    return client.get_json_secret(secret_id, version)


def get_or_env(secret_id: str, env_var: str, default: Optional[str] = None) -> str:
    """Get a secret or fallback to an environment variable.

    This is useful for development environments where Secret Manager may not be available.

    Args:
        secret_id: Secret ID
        env_var: Environment variable name
        default: Default value if both secret and env var are not available

    Returns:
        Secret value or environment variable value or default
    """
    try:
        return get_secret(secret_id)
    except Exception:
        value = os.environ.get(env_var)
        if value is not None:
            return value
        if default is not None:
            return default
        raise ValueError(
            f"Neither secret {secret_id} nor environment variable {env_var} is set"
        )

================
File: video_processor/infrastructure/config/settings.py
================
"""
Application settings loaded from environment variables.
"""

import os
from dataclasses import dataclass
from typing import List


@dataclass
class StorageSettings:
    """Google Cloud Storage settings."""

    gcs_bucket: str = os.environ.get("GCS_BUCKET", "video-processor")
    gcs_input_prefix: str = os.environ.get("GCS_INPUT_PREFIX", "uploads/")
    gcs_output_prefix: str = os.environ.get("GCS_OUTPUT_PREFIX", "processed/")
    local_storage_path: str = os.environ.get(
        "LOCAL_STORAGE_PATH", "/tmp/video-processor"
    )
    use_local_storage: bool = (
        os.environ.get("USE_LOCAL_STORAGE", "false").lower() == "true"
    )


@dataclass
class AISettings:
    """AI service settings."""

    gemini_api_key: str = os.environ.get("GEMINI_API_KEY", "")
    gemini_model: str = os.environ.get("GEMINI_MODEL", "gemini-pro")
    vertex_ai_project: str = os.environ.get("VERTEX_AI_PROJECT", "")
    vertex_ai_location: str = os.environ.get("VERTEX_AI_LOCATION", "us-central1")
    vertex_ai_model: str = os.environ.get("VERTEX_AI_MODEL", "text-bison")
    use_vertex_ai: bool = os.environ.get("USE_VERTEX_AI", "false").lower() == "true"


@dataclass
class YouTubeSettings:
    """YouTube publishing settings."""

    client_secrets_file: str = os.environ.get("YOUTUBE_CLIENT_SECRETS", "")
    token_file: str = os.environ.get("YOUTUBE_TOKEN_FILE", "")
    redirect_uri: str = os.environ.get(
        "YOUTUBE_REDIRECT_URI", "urn:ietf:wg:oauth:2.0:oob"
    )
    scopes: List[str] = None

    def __post_init__(self):
        """Initialize default values for collections."""
        if self.scopes is None:
            self.scopes = [
                "https://www.googleapis.com/auth/youtube.upload",
                "https://www.googleapis.com/auth/youtube",
                "https://www.googleapis.com/auth/youtube.force-ssl",
            ]


@dataclass
class APISettings:
    """API server settings."""

    host: str = os.environ.get("API_HOST", "0.0.0.0")
    port: int = int(os.environ.get("API_PORT", "8080"))
    debug: bool = os.environ.get("API_DEBUG", "false").lower() == "true"
    allowed_origins: List[str] = None

    def __post_init__(self):
        """Initialize default values for collections."""
        if self.allowed_origins is None:
            origins = os.environ.get("ALLOWED_ORIGINS", "*")
            self.allowed_origins = [o.strip() for o in origins.split(",")]


@dataclass
class AppSettings:
    """Main application settings."""

    environment: str = os.environ.get("ENVIRONMENT", "development")
    log_level: str = os.environ.get("LOG_LEVEL", "INFO")
    storage: StorageSettings = None
    ai: AISettings = None
    youtube: YouTubeSettings = None
    api: APISettings = None

    def __post_init__(self):
        """Initialize nested settings."""
        if self.storage is None:
            self.storage = StorageSettings()
        if self.ai is None:
            self.ai = AISettings()
        if self.youtube is None:
            self.youtube = YouTubeSettings()
        if self.api is None:
            self.api = APISettings()


# Create a singleton instance
settings = AppSettings()

================
File: video_processor/infrastructure/messaging/__init__.py
================
"""
Messaging infrastructure for event-driven communication.
"""

from video_processor.infrastructure.messaging.handlers import (
    handle_processing_complete,
    handle_publishing_complete,
    handle_video_uploaded,
    register_handlers,
)
from video_processor.infrastructure.messaging.pubsub import PubSubAdapter

================
File: video_processor/infrastructure/messaging/handlers.py
================
"""
Event handlers for processing messages in the video processor.
"""

import logging
from typing import Any, Dict

from video_processor.application.services.video_processor import VideoProcessorService
from video_processor.domain.exceptions import VideoProcessingError
from video_processor.domain.models.job import JobStatus
from video_processor.infrastructure.repositories.job_repository import JobRepository

# Configure logger
logger = logging.getLogger(__name__)


def handle_video_uploaded(
    message: Dict[str, Any],
    video_processor: VideoProcessorService,
    job_repository: JobRepository,
) -> None:
    """
    Handle a message indicating a video has been uploaded.

    Args:
        message: Message data containing video information
        video_processor: Instance of VideoProcessorService
        job_repository: Instance of JobRepository

    Raises:
        VideoProcessingError: If processing fails
    """
    try:
        logger.info(
            f"Handling video uploaded event: {message.get('file_name', 'unknown')}"
        )

        # Extract required information from message
        bucket_name = message.get("bucket_name")
        file_name = message.get("file_name")

        if not bucket_name or not file_name:
            raise ValueError(
                "Missing required fields in message: bucket_name, file_name"
            )

        # Create a job for the video
        job_id = job_repository.create(
            video_path=f"gs://{bucket_name}/{file_name}", status=JobStatus.PENDING
        )

        # Start processing asynchronously
        # Note: In a real system, this might be a background task or separate process
        video_processor.process_video(job_id)

        logger.info(f"Started processing job {job_id} for video {file_name}")
    except Exception as e:
        logger.error(f"Error handling video upload event: {str(e)}", exc_info=True)
        raise VideoProcessingError(
            f"Failed to process video upload event: {str(e)}"
        ) from e


def handle_processing_complete(
    message: Dict[str, Any],
    job_repository: JobRepository,
) -> None:
    """
    Handle a message indicating video processing has completed.

    Args:
        message: Message data containing job information
        job_repository: Instance of JobRepository

    Raises:
        VideoProcessingError: If handling fails
    """
    try:
        job_id = message.get("job_id")
        status = message.get("status")

        if not job_id:
            raise ValueError("Missing required field in message: job_id")

        logger.info(f"Handling processing complete event for job {job_id}")

        # Update job status
        if status == "success":
            job_repository.update_status(job_id, JobStatus.COMPLETED)
        elif status == "failure":
            error_message = message.get("error", "Unknown error")
            job_repository.update_status(job_id, JobStatus.FAILED, error=error_message)
        else:
            logger.warning(f"Unknown status in processing complete event: {status}")

        logger.info(f"Updated job {job_id} status to {status}")
    except Exception as e:
        logger.error(
            f"Error handling processing complete event: {str(e)}", exc_info=True
        )
        raise VideoProcessingError(
            f"Failed to process completion event: {str(e)}"
        ) from e


def handle_publishing_complete(
    message: Dict[str, Any],
    job_repository: JobRepository,
) -> None:
    """
    Handle a message indicating video publishing has completed.

    Args:
        message: Message data containing publishing information
        job_repository: Instance of JobRepository

    Raises:
        VideoProcessingError: If handling fails
    """
    try:
        job_id = message.get("job_id")
        platform = message.get("platform", "unknown")
        platform_id = message.get("platform_id")
        status = message.get("status")

        if not job_id or not status:
            raise ValueError("Missing required fields in message: job_id, status")

        logger.info(
            f"Handling publishing complete event for job {job_id} on {platform}"
        )

        # Get the job
        job = job_repository.get_by_id(job_id)
        if not job:
            logger.warning(f"Job {job_id} not found for publishing complete event")
            return

        # Update the job with publishing information
        metadata = {}
        if platform and platform_id:
            metadata[f"{platform}_id"] = platform_id

        if status == "success":
            metadata[f"{platform}_status"] = "published"
        else:
            error_message = message.get("error", "Unknown publishing error")
            metadata[f"{platform}_status"] = "failed"
            metadata[f"{platform}_error"] = error_message

        # Update the job with the publishing metadata
        job_repository.update_metadata(job_id, metadata)

        logger.info(f"Updated job {job_id} with publishing information for {platform}")
    except Exception as e:
        logger.error(
            f"Error handling publishing complete event: {str(e)}", exc_info=True
        )
        raise VideoProcessingError(
            f"Failed to process publishing event: {str(e)}"
        ) from e


def register_handlers(
    message_handler,
    video_processor: VideoProcessorService,
    job_repository: JobRepository,
) -> None:
    """
    Register all event handlers with the message handler.

    Args:
        message_handler: The message handler instance
        video_processor: Instance of VideoProcessorService
        job_repository: Instance of JobRepository
    """

    # Define a wrapper for each handler that provides the required dependencies
    def video_uploaded_wrapper(message_data: Dict[str, Any]) -> None:
        handle_video_uploaded(message_data, video_processor, job_repository)

    def processing_complete_wrapper(message_data: Dict[str, Any]) -> None:
        handle_processing_complete(message_data, job_repository)

    def publishing_complete_wrapper(message_data: Dict[str, Any]) -> None:
        handle_publishing_complete(message_data, job_repository)

    # Register handlers for specific topics
    message_handler.subscribe("video-uploaded", video_uploaded_wrapper)
    message_handler.subscribe("processing-complete", processing_complete_wrapper)
    message_handler.subscribe("publishing-complete", publishing_complete_wrapper)

    logger.info("Registered all event handlers")

================
File: video_processor/infrastructure/messaging/pubsub.py
================
"""
Google Cloud Pub/Sub implementation of the messaging interface.
"""

import json
import logging
import os
import threading
import time
from typing import Any, Callable, Dict, List, Optional

from google.api_core.exceptions import AlreadyExists, NotFound
from google.cloud.pubsub_v1 import PublisherClient, SubscriberClient
from google.cloud.pubsub_v1.subscriber.message import Message

from video_processor.application.interfaces.messaging import MessageHandlerInterface
from video_processor.domain.exceptions import MessagingError


class PubSubAdapter(MessageHandlerInterface):
    """
    Google Cloud Pub/Sub implementation of the messaging interface.

    This adapter provides asynchronous messaging capabilities using
    Google Cloud Pub/Sub.
    """

    def __init__(self, project_id: str, client_id: Optional[str] = None):
        """
        Initialize the Pub/Sub adapter.

        Args:
            project_id: Google Cloud project ID
            client_id: Optional client ID for subscription naming
        """
        self.project_id = project_id
        self.client_id = client_id or f"client-{os.getpid()}"

        # Initialize clients
        self.publisher = PublisherClient()
        self.subscriber = SubscriberClient()

        # Track active subscriptions
        self._subscriptions = {}
        self._subscription_threads = {}
        self._running = True

        # Logger
        self.logger = logging.getLogger(__name__)

    def _get_topic_path(self, topic: str) -> str:
        """Get the full topic path."""
        return self.publisher.topic_path(self.project_id, topic)

    def _get_subscription_path(self, topic: str) -> str:
        """Get a unique subscription path for this client."""
        subscription_id = f"{topic}-{self.client_id}"
        return self.subscriber.subscription_path(self.project_id, subscription_id)

    def publish(self, topic: str, message: Dict[str, Any]) -> str:
        """
        Publish a message to a topic.

        Args:
            topic: The topic to publish to
            message: The message payload as a dictionary

        Returns:
            The published message ID

        Raises:
            MessagingError: If publishing fails
        """
        try:
            # Ensure topic exists
            topic_path = self._get_topic_path(topic)
            self._ensure_topic_exists(topic)

            # Convert message to bytes
            data = json.dumps(message).encode("utf-8")

            # Publish message
            future = self.publisher.publish(topic_path, data)
            message_id = future.result()  # Wait for publishing to complete

            self.logger.debug(f"Published message {message_id} to {topic}")
            return message_id
        except Exception as e:
            error_msg = f"Failed to publish message to {topic}: {str(e)}"
            self.logger.error(error_msg)
            raise MessagingError(error_msg) from e

    def subscribe(self, topic: str, callback: Callable[[Dict[str, Any]], None]) -> None:
        """
        Subscribe to a topic with a callback function.

        Args:
            topic: The topic to subscribe to
            callback: A function to call when a message is received

        Raises:
            MessagingError: If subscription fails
        """
        try:
            # Ensure topic exists
            self._ensure_topic_exists(topic)

            # Get paths
            topic_path = self._get_topic_path(topic)
            subscription_path = self._get_subscription_path(topic)

            # Create subscription if it doesn't exist
            try:
                self.subscriber.create_subscription(
                    request={"name": subscription_path, "topic": topic_path}
                )
                self.logger.info(f"Created subscription {subscription_path}")
            except AlreadyExists:
                self.logger.debug(f"Subscription {subscription_path} already exists")

            # Define callback wrapper
            def callback_wrapper(message: Message) -> None:
                try:
                    # Parse message data
                    data = json.loads(message.data.decode("utf-8"))

                    # Call user callback
                    callback(data)

                    # Acknowledge message
                    message.ack()
                except Exception as e:
                    self.logger.error(f"Error processing message: {str(e)}")
                    message.nack()

            # Subscribe and start listening
            self._subscriptions[topic] = self.subscriber.subscribe(
                subscription_path, callback_wrapper
            )

            # Start subscriber in a separate thread
            thread = threading.Thread(
                target=self._run_subscriber, args=(topic,), daemon=True
            )
            thread.start()
            self._subscription_threads[topic] = thread

            self.logger.info(f"Subscribed to topic {topic}")
        except Exception as e:
            error_msg = f"Failed to subscribe to {topic}: {str(e)}"
            self.logger.error(error_msg)
            raise MessagingError(error_msg) from e

    def _run_subscriber(self, topic: str) -> None:
        """Run the subscriber in a loop with error handling."""
        subscription = self._subscriptions.get(topic)
        if not subscription:
            return

        while self._running and topic in self._subscriptions:
            try:
                # Blocking call that processes messages
                subscription.result()
            except Exception as e:
                if self._running:  # Only log if we're still supposed to be running
                    self.logger.error(f"Subscription error for {topic}: {str(e)}")
                    time.sleep(1)  # Avoid busy-waiting on repeated errors

    def unsubscribe(self, topic: str) -> bool:
        """
        Unsubscribe from a topic.

        Args:
            topic: The topic to unsubscribe from

        Returns:
            True if unsubscribed successfully, False otherwise

        Raises:
            MessagingError: If unsubscription fails
        """
        try:
            # Check if we have an active subscription
            if topic not in self._subscriptions:
                return False

            # Stop the subscription
            self._subscriptions[topic].cancel()
            del self._subscriptions[topic]

            # Clean up the subscription resource
            subscription_path = self._get_subscription_path(topic)
            self.subscriber.delete_subscription(
                request={"subscription": subscription_path}
            )

            self.logger.info(f"Unsubscribed from topic {topic}")
            return True
        except Exception as e:
            error_msg = f"Failed to unsubscribe from {topic}: {str(e)}"
            self.logger.error(error_msg)
            raise MessagingError(error_msg) from e

    def pull_messages(self, topic: str, max_messages: int = 10) -> List[Dict[str, Any]]:
        """
        Pull messages from a topic without a subscription.

        Args:
            topic: The topic to pull messages from
            max_messages: The maximum number of messages to pull

        Returns:
            A list of message payloads

        Raises:
            MessagingError: If message pulling fails
        """
        try:
            # Ensure topic exists
            self._ensure_topic_exists(topic)

            # Create a temporary subscription for pulling
            temp_subscription_id = f"temp-pull-{self.client_id}-{int(time.time())}"
            temp_subscription_path = self.subscriber.subscription_path(
                self.project_id, temp_subscription_id
            )

            topic_path = self._get_topic_path(topic)

            # Create temporary subscription
            try:
                self.subscriber.create_subscription(
                    request={"name": temp_subscription_path, "topic": topic_path}
                )
            except AlreadyExists:
                # If it somehow already exists, continue using it
                pass

            # Pull messages
            try:
                response = self.subscriber.pull(
                    request={
                        "subscription": temp_subscription_path,
                        "max_messages": max_messages,
                    }
                )

                # Process messages
                messages = []
                ack_ids = []

                for received_message in response.received_messages:
                    # Parse message
                    data = json.loads(received_message.message.data.decode("utf-8"))
                    messages.append(data)
                    ack_ids.append(received_message.ack_id)

                # Acknowledge all messages
                if ack_ids:
                    self.subscriber.acknowledge(
                        request={
                            "subscription": temp_subscription_path,
                            "ack_ids": ack_ids,
                        }
                    )

                return messages
            finally:
                # Clean up temporary subscription
                try:
                    self.subscriber.delete_subscription(
                        request={"subscription": temp_subscription_path}
                    )
                except Exception as e:
                    self.logger.warning(
                        f"Failed to delete temporary subscription: {str(e)}"
                    )
        except Exception as e:
            error_msg = f"Failed to pull messages from {topic}: {str(e)}"
            self.logger.error(error_msg)
            raise MessagingError(error_msg) from e

    def ack_message(self, topic: str, message_id: str) -> bool:
        """
        Acknowledge a message as processed.

        Note: This is a no-op for the Pub/Sub adapter as messages are
        automatically acknowledged in the callback.

        Args:
            topic: The topic the message was from
            message_id: The ID of the message to acknowledge

        Returns:
            True (always succeeds as a no-op)
        """
        # This is a no-op since we auto-ack in the callback
        self.logger.debug(
            f"ack_message is a no-op for Pub/Sub adapter (topic: {topic}, message: {message_id})"
        )
        return True

    def create_topic(self, topic: str) -> bool:
        """
        Create a new topic if it doesn't exist.

        Args:
            topic: The topic to create

        Returns:
            True if created or already exists, False otherwise

        Raises:
            MessagingError: If topic creation fails
        """
        return self._ensure_topic_exists(topic)

    def _ensure_topic_exists(self, topic: str) -> bool:
        """Ensure that a topic exists, creating it if necessary."""
        try:
            topic_path = self._get_topic_path(topic)

            try:
                self.publisher.get_topic(request={"topic": topic_path})
                return True  # Topic already exists
            except NotFound:
                # Topic doesn't exist, create it
                self.publisher.create_topic(request={"name": topic_path})
                self.logger.info(f"Created topic {topic_path}")
                return True
        except Exception as e:
            error_msg = f"Failed to ensure topic {topic} exists: {str(e)}"
            self.logger.error(error_msg)
            raise MessagingError(error_msg) from e

    def delete_topic(self, topic: str) -> bool:
        """
        Delete a topic.

        Args:
            topic: The topic to delete

        Returns:
            True if deleted successfully, False otherwise

        Raises:
            MessagingError: If topic deletion fails
        """
        try:
            topic_path = self._get_topic_path(topic)

            try:
                self.publisher.delete_topic(request={"topic": topic_path})
                self.logger.info(f"Deleted topic {topic_path}")
                return True
            except NotFound:
                # Topic doesn't exist
                return False
        except Exception as e:
            error_msg = f"Failed to delete topic {topic}: {str(e)}"
            self.logger.error(error_msg)
            raise MessagingError(error_msg) from e

    def close(self) -> None:
        """
        Close all connections and clean up resources.
        """
        self._running = False

        # Cancel all subscriptions
        for topic in list(self._subscriptions.keys()):
            try:
                self.unsubscribe(topic)
            except Exception as e:
                self.logger.warning(f"Error while unsubscribing from {topic}: {str(e)}")

        # Close clients
        self.subscriber.close()

        self.logger.info("Pub/Sub adapter closed")

================
File: video_processor/infrastructure/repositories/__init__.py
================
"""
Repository implementations for data persistence.

This package contains concrete implementations of the repository interfaces
defined in the application layer, allowing for data persistence in various
storage systems.
"""

from video_processor.infrastructure.repositories.job_repository import (
    FirestoreJobRepository,
)
from video_processor.infrastructure.repositories.video_repository import (
    FirestoreVideoRepository,
)

__all__ = ["FirestoreJobRepository", "FirestoreVideoRepository"]

================
File: video_processor/infrastructure/repositories/job_repository.py
================
"""
Firestore implementation of the JobRepositoryInterface.

This module provides a Firestore-backed implementation of the job repository
for storing and retrieving VideoJob entities.
"""

import datetime
import logging
from typing import Any, Dict, List, Optional

from google.cloud import firestore
from google.cloud.firestore_v1.base_query import FieldFilter

from video_processor.application.interfaces.repositories import JobRepositoryInterface
from video_processor.domain.models.job import JobStatus, VideoJob
from video_processor.domain.models.metadata import VideoMetadata
from video_processor.domain.models.video import Video

# Configure logger
logger = logging.getLogger(__name__)


class FirestoreJobRepository(JobRepositoryInterface):
    """Firestore implementation of the job repository."""

    def __init__(self, project_id: str, collection_name: str = "video_jobs"):
        """
        Initialize the Firestore job repository.

        Args:
            project_id: Google Cloud project ID
            collection_name: Firestore collection name for jobs
        """
        self._db = firestore.Client(project=project_id)
        self._collection = self._db.collection(collection_name)
        logger.info(
            f"Initialized FirestoreJobRepository with collection '{collection_name}'"
        )

    def get_by_id(self, job_id: str) -> Optional[VideoJob]:
        """
        Retrieve a VideoJob by its ID.

        Args:
            job_id: ID of the job to retrieve

        Returns:
            VideoJob if found, None otherwise
        """
        doc_ref = self._collection.document(job_id)
        doc = doc_ref.get()

        if not doc.exists:
            logger.warning(f"Job with ID {job_id} not found")
            return None

        job_data = doc.to_dict()
        return self._deserialize_job(job_id, job_data)

    def save(self, job: VideoJob) -> str:
        """
        Save a new VideoJob.

        Args:
            job: VideoJob to save

        Returns:
            ID of the saved job
        """
        # If job has no ID, generate one
        if not job.id:
            doc_ref = self._collection.document()
            job.id = doc_ref.id
        else:
            doc_ref = self._collection.document(job.id)

        # Serialize job to Firestore document
        job_data = self._serialize_job(job)

        # Add timestamps
        now = datetime.datetime.now()
        job_data["created_at"] = now
        job_data["updated_at"] = now

        # Save to Firestore
        doc_ref.set(job_data)
        logger.info(f"Saved job with ID {job.id}")

        return job.id

    def update(self, job: VideoJob) -> bool:
        """
        Update an existing VideoJob.

        Args:
            job: VideoJob to update

        Returns:
            True if successful, False otherwise
        """
        if not job.id:
            logger.error("Cannot update job without ID")
            return False

        doc_ref = self._collection.document(job.id)

        # Check if document exists
        if not doc_ref.get().exists:
            logger.error(f"Job with ID {job.id} does not exist")
            return False

        # Serialize job to Firestore document
        job_data = self._serialize_job(job)

        # Update timestamp
        job_data["updated_at"] = datetime.datetime.now()

        # Update in Firestore
        doc_ref.update(job_data)
        logger.info(f"Updated job with ID {job.id}")

        return True

    def delete(self, job_id: str) -> bool:
        """
        Delete a VideoJob by its ID.

        Args:
            job_id: ID of the job to delete

        Returns:
            True if successful, False otherwise
        """
        doc_ref = self._collection.document(job_id)

        # Check if document exists
        if not doc_ref.get().exists:
            logger.error(f"Job with ID {job_id} does not exist")
            return False

        # Delete from Firestore
        doc_ref.delete()
        logger.info(f"Deleted job with ID {job_id}")

        return True

    def get_jobs_by_status(self, status: JobStatus) -> List[VideoJob]:
        """
        Retrieve jobs with a specific status.

        Args:
            status: Status to filter by

        Returns:
            List of VideoJob with the specified status
        """
        query = self._collection.where(filter=FieldFilter("status", "==", status.value))
        docs = query.stream()

        jobs = []
        for doc in docs:
            job_id = doc.id
            job_data = doc.to_dict()
            job = self._deserialize_job(job_id, job_data)
            jobs.append(job)

        logger.info(f"Retrieved {len(jobs)} jobs with status {status}")
        return jobs

    def get_pending_jobs(self) -> List[VideoJob]:
        """
        Retrieve all pending jobs.

        Returns:
            List of VideoJob with status PENDING
        """
        return self.get_jobs_by_status(JobStatus.PENDING)

    def update_job_status(
        self, job_id: str, status: JobStatus, error: Optional[str] = None
    ) -> bool:
        """
        Update the status of a job.

        Args:
            job_id: ID of the job to update
            status: New status value
            error: Error message if status is FAILED

        Returns:
            True if successful, False otherwise
        """
        doc_ref = self._collection.document(job_id)

        # Check if document exists
        if not doc_ref.get().exists:
            logger.error(f"Job with ID {job_id} does not exist")
            return False

        # Prepare update data
        update_data = {"status": status.value, "updated_at": datetime.datetime.now()}

        # Add error message if provided
        if error and status == JobStatus.FAILED:
            update_data["error"] = error

        # Update in Firestore
        doc_ref.update(update_data)
        logger.info(f"Updated status of job {job_id} to {status}")

        return True

    def _serialize_job(self, job: VideoJob) -> Dict[str, Any]:
        """
        Serialize a VideoJob to a Firestore document.

        Args:
            job: VideoJob to serialize

        Returns:
            Dictionary representation of the job
        """
        video_data = {
            "id": job.video.id,
            "file_path": job.video.file_path,
            "duration": job.video.duration,
            "format": job.video.format,
            "resolution": job.video.resolution,
        }

        metadata_data = {
            "title": job.metadata.title,
            "description": job.metadata.description,
            "tags": job.metadata.tags,
            "show_notes": job.metadata.show_notes,
            "thumbnail_url": job.metadata.thumbnail_url,
            "transcript": job.metadata.transcript,
            "transcript_url": job.metadata.transcript_url,
            "subtitle_urls": job.metadata.subtitle_urls,
        }

        job_data = {
            "status": job.status.value,
            "error": job.error,
            "video": video_data,
            "metadata": metadata_data,
            "processing_stages": job.processing_stages,
            "created_at": job.created_at or datetime.datetime.now(),
            "updated_at": job.updated_at or datetime.datetime.now(),
        }

        return job_data

    def _deserialize_job(self, job_id: str, job_data: Dict[str, Any]) -> VideoJob:
        """
        Deserialize a Firestore document to a VideoJob.

        Args:
            job_id: ID of the job
            job_data: Dictionary representation of the job

        Returns:
            Deserialized VideoJob
        """
        # Create Video object
        video_data = job_data.get("video", {})
        video = Video(
            id=video_data.get("id", ""),
            file_path=video_data.get("file_path", ""),
            duration=video_data.get("duration", 0),
            format=video_data.get("format", "unknown"),
            resolution=video_data.get("resolution", (0, 0)),
        )

        # Create VideoMetadata object
        metadata_data = job_data.get("metadata", {})
        metadata = VideoMetadata(
            title=metadata_data.get("title", ""),
            description=metadata_data.get("description", ""),
            tags=metadata_data.get("tags", []),
            show_notes=metadata_data.get("show_notes", ""),
            thumbnail_url=metadata_data.get("thumbnail_url", ""),
            transcript=metadata_data.get("transcript", ""),
            transcript_url=metadata_data.get("transcript_url", ""),
            subtitle_urls=metadata_data.get("subtitle_urls", {}),
        )

        # Create VideoJob object
        job = VideoJob(
            id=job_id,
            video=video,
            metadata=metadata,
            status=JobStatus(job_data.get("status", JobStatus.PENDING.value)),
            error=job_data.get("error", ""),
            processing_stages=job_data.get("processing_stages", {}),
            created_at=job_data.get("created_at"),
            updated_at=job_data.get("updated_at"),
        )

        return job

================
File: video_processor/infrastructure/repositories/video_repository.py
================
"""
Firestore implementation of the VideoRepositoryInterface.

This module provides a Firestore-backed implementation of the video repository
for storing and retrieving Video entities.
"""

import datetime
import logging
from typing import Any, Dict, List, Optional

from google.cloud import firestore
from google.cloud.firestore_v1.base_query import FieldFilter

from video_processor.application.interfaces.repositories import VideoRepositoryInterface
from video_processor.domain.models.video import Video

# Configure logger
logger = logging.getLogger(__name__)


class FirestoreVideoRepository(VideoRepositoryInterface):
    """Firestore implementation of the video repository."""

    def __init__(self, project_id: str, collection_name: str = "videos"):
        """
        Initialize the Firestore video repository.

        Args:
            project_id: Google Cloud project ID
            collection_name: Firestore collection name for videos
        """
        self._db = firestore.Client(project=project_id)
        self._collection = self._db.collection(collection_name)
        logger.info(
            f"Initialized FirestoreVideoRepository with collection '{collection_name}'"
        )

    def get_by_id(self, video_id: str) -> Optional[Video]:
        """
        Retrieve a Video by its ID.

        Args:
            video_id: ID of the video to retrieve

        Returns:
            Video if found, None otherwise
        """
        doc_ref = self._collection.document(video_id)
        doc = doc_ref.get()

        if not doc.exists:
            logger.warning(f"Video with ID {video_id} not found")
            return None

        video_data = doc.to_dict()
        return self._deserialize_video(video_id, video_data)

    def save(self, video: Video) -> str:
        """
        Save a new Video.

        Args:
            video: Video to save

        Returns:
            ID of the saved video
        """
        # If video has no ID, generate one
        if not video.id:
            doc_ref = self._collection.document()
            video.id = doc_ref.id
        else:
            doc_ref = self._collection.document(video.id)

        # Serialize video to Firestore document
        video_data = self._serialize_video(video)

        # Add timestamps
        now = datetime.datetime.now()
        if not video.created_at:
            video_data["created_at"] = now
        video_data["updated_at"] = now

        # Save to Firestore
        doc_ref.set(video_data)
        logger.info(f"Saved video with ID {video.id}")

        return video.id

    def update(self, video: Video) -> bool:
        """
        Update an existing Video.

        Args:
            video: Video to update

        Returns:
            True if successful, False otherwise
        """
        if not video.id:
            logger.error("Cannot update video without ID")
            return False

        doc_ref = self._collection.document(video.id)

        # Check if document exists
        if not doc_ref.get().exists:
            logger.error(f"Video with ID {video.id} does not exist")
            return False

        # Serialize video to Firestore document
        video_data = self._serialize_video(video)

        # Update timestamp
        video_data["updated_at"] = datetime.datetime.now()

        # Update in Firestore
        doc_ref.update(video_data)
        logger.info(f"Updated video with ID {video.id}")

        return True

    def delete(self, video_id: str) -> bool:
        """
        Delete a Video by its ID.

        Args:
            video_id: ID of the video to delete

        Returns:
            True if successful, False otherwise
        """
        doc_ref = self._collection.document(video_id)

        # Check if document exists
        if not doc_ref.get().exists:
            logger.error(f"Video with ID {video_id} does not exist")
            return False

        # Delete from Firestore
        doc_ref.delete()
        logger.info(f"Deleted video with ID {video_id}")

        return True

    def get_videos_by_user(self, user_id: str) -> List[Video]:
        """
        Retrieve videos for a specific user.

        Args:
            user_id: ID of the user

        Returns:
            List of Video for the specified user
        """
        query = self._collection.where(filter=FieldFilter("user_id", "==", user_id))
        docs = query.stream()

        videos = []
        for doc in docs:
            video_id = doc.id
            video_data = doc.to_dict()
            video = self._deserialize_video(video_id, video_data)
            videos.append(video)

        logger.info(f"Retrieved {len(videos)} videos for user {user_id}")
        return videos

    def _serialize_video(self, video: Video) -> Dict[str, Any]:
        """
        Serialize a Video to a Firestore document.

        Args:
            video: Video to serialize

        Returns:
            Dictionary representation of the video
        """
        video_data = {
            "file_path": video.file_path,
            "duration": video.duration,
            "format": video.format,
            "resolution": video.resolution,
            "created_at": video.created_at or datetime.datetime.now(),
            "updated_at": datetime.datetime.now(),
            "user_id": getattr(video, "user_id", None),  # Optional user ID
        }

        return video_data

    def _deserialize_video(self, video_id: str, video_data: Dict[str, Any]) -> Video:
        """
        Deserialize a Firestore document to a Video.

        Args:
            video_id: ID of the video
            video_data: Dictionary representation of the video

        Returns:
            Deserialized Video
        """
        video = Video(
            id=video_id,
            file_path=video_data.get("file_path", ""),
            duration=video_data.get("duration", 0),
            format=video_data.get("format", "unknown"),
            resolution=video_data.get("resolution", (0, 0)),
        )

        # Set timestamps if available
        video.created_at = video_data.get("created_at")
        video.updated_at = video_data.get("updated_at")

        # Set user_id if available (not in the base model but might be used)
        if "user_id" in video_data:
            video.user_id = video_data["user_id"]

        return video

================
File: video_processor/infrastructure/monitoring.py
================
"""Monitoring and observability utilities for the video processor service."""
import json
import logging
import os
import time
import uuid
from datetime import datetime
from functools import wraps
from typing import Any, Callable, Dict, List, Optional, TypeVar, Union, cast

# Type variables for decorator functions
F = TypeVar('F', bound=Callable[..., Any])

# Configure base logger
logging.basicConfig(
    level=os.environ.get("LOG_LEVEL", "INFO"),
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)

# Create structured logger
logger = logging.getLogger("video_processor")

# Metrics registry
_metrics: Dict[str, Dict[str, float]] = {
    "counters": {},  # Simple counters
    "gauges": {},    # Point-in-time values
    "histograms": {}, # Distribution of values
}

# Request context for tracking request-specific data
_request_context: Dict[str, Any] = {}


class StructuredLogRecord(logging.LogRecord):
    """Extended LogRecord class that supports structured logging."""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.structured_data = {}


class StructuredLogger(logging.Logger):
    """Logger that supports structured logging."""
    
    def makeRecord(self, name, level, fn, lno, msg, args, exc_info, 
                  func=None, extra=None, sinfo=None):
        """Create a LogRecord with structured data support."""
        record = StructuredLogRecord(name, level, fn, lno, msg, args, exc_info, func, sinfo)
        if extra is not None:
            for key in extra:
                if key in ["message", "asctime", "levelname", "levelno"]:
                    raise KeyError(f"Attempt to overwrite {key} in StructuredLogRecord")
                record.__dict__[key] = extra[key]
        
        # Add request context data
        if _request_context:
            record.structured_data = {**_request_context}
            
        # Add structured data if provided in extra
        if extra and "structured_data" in extra:
            if isinstance(extra["structured_data"], dict):
                if hasattr(record, "structured_data"):
                    record.structured_data.update(extra["structured_data"])
                else:
                    record.structured_data = extra["structured_data"]
        
        return record


class StructuredJSONFormatter(logging.Formatter):
    """Formatter that outputs JSON strings for structured log records."""
    
    def format(self, record: logging.LogRecord) -> str:
        """Format a log record as a JSON string."""
        log_data = {
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "level": record.levelname,
            "name": record.name,
            "message": super().format(record),
            "location": {
                "file": record.pathname,
                "line": record.lineno,
                "function": record.funcName,
            },
        }
        
        # Add structured data if available
        if hasattr(record, "structured_data") and record.structured_data:
            log_data.update(record.structured_data)
            
        # Add exception info if available
        if record.exc_info:
            log_data["exception"] = {
                "type": record.exc_info[0].__name__,
                "message": str(record.exc_info[1]),
            }
            
        return json.dumps(log_data)


def setup_structured_logging(
    log_level: str = "INFO", 
    json_output: bool = False,
    log_file: Optional[str] = None,
) -> None:
    """Set up structured logging.
    
    Args:
        log_level: Logging level (default: INFO)
        json_output: Whether to output logs as JSON (default: False)
        log_file: Path to log file (default: None, logs to stdout)
    """
    # Register the StructuredLogger class
    logging.setLoggerClass(StructuredLogger)
    
    # Get the root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(getattr(logging, log_level))
    
    # Remove existing handlers
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)
    
    # Create formatter
    if json_output:
        formatter = StructuredJSONFormatter()
    else:
        formatter = logging.Formatter(
            "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
        )
    
    # Create handlers
    handlers = []
    
    # Console handler
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    handlers.append(console_handler)
    
    # File handler if specified
    if log_file:
        file_handler = logging.FileHandler(log_file)
        file_handler.setFormatter(formatter)
        handlers.append(file_handler)
    
    # Add handlers to root logger
    for handler in handlers:
        root_logger.addHandler(handler)
    
    # Set application logger
    app_logger = logging.getLogger("video_processor")
    app_logger.setLevel(getattr(logging, log_level))


def structured_log(level: str, message: str, structured_data: Dict[str, Any] = None) -> None:
    """Log a structured message.
    
    Args:
        level: Log level (debug, info, warning, error, critical)
        message: Log message
        structured_data: Additional structured data to include
    """
    log_method = getattr(logger, level.lower())
    if structured_data:
        log_method(message, extra={"structured_data": structured_data})
    else:
        log_method(message)


def start_request_context(request_id: Optional[str] = None) -> str:
    """Start a new request context for request-specific logging.
    
    Args:
        request_id: Optional request ID (UUID generated if not provided)
        
    Returns:
        Request ID
    """
    global _request_context
    request_id = request_id or str(uuid.uuid4())
    _request_context = {
        "request_id": request_id,
        "start_time": time.time(),
    }
    return request_id


def add_request_context_data(data: Dict[str, Any]) -> None:
    """Add data to the current request context.
    
    Args:
        data: Data to add to the request context
    """
    global _request_context
    _request_context.update(data)


def end_request_context() -> None:
    """End the current request context."""
    global _request_context
    _request_context = {}


def log_request(log_request_body: bool = False, log_response_body: bool = False) -> Callable:
    """Decorator for logging requests and responses.
    
    Args:
        log_request_body: Whether to log request body (default: False)
        log_response_body: Whether to log response body (default: False)
        
    Returns:
        Decorator function
    """
    def decorator(func: F) -> F:
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # Get request object
            request = None
            for arg in args:
                if hasattr(arg, "method") and hasattr(arg, "url"):
                    request = arg
                    break
            
            # Start request context
            request_id = start_request_context()
            
            if request:
                add_request_context_data({
                    "http_method": request.method,
                    "url_path": str(request.url),
                    "client_ip": request.client.host if hasattr(request, "client") else None,
                })
                
                # Log request
                log_data = {
                    "request_id": request_id,
                    "http_method": request.method,
                    "url_path": str(request.url),
                }
                
                if log_request_body and hasattr(request, "json"):
                    try:
                        log_data["request_body"] = await request.json()
                    except Exception:
                        pass
                        
                structured_log("info", f"Request received: {request.method} {request.url}", log_data)
                
                # Increment request counter
                increment_counter("http_requests_total")
                increment_counter(f"http_requests_{request.method.lower()}")
            
            # Process request
            start_time = time.time()
            try:
                response = await func(*args, **kwargs)
                
                # Log success response
                duration = time.time() - start_time
                status_code = response.status_code if hasattr(response, "status_code") else 200
                
                log_data = {
                    "request_id": request_id,
                    "status_code": status_code,
                    "duration_ms": round(duration * 1000, 2),
                }
                
                if log_response_body and hasattr(response, "body"):
                    try:
                        log_data["response_body"] = response.body.decode("utf-8")
                    except Exception:
                        pass
                
                structured_log(
                    "info", 
                    f"Request completed: {status_code} in {duration:.2f}s", 
                    log_data
                )
                
                # Record metrics
                observe_histogram("http_request_duration_seconds", duration)
                increment_counter(f"http_responses_{status_code}")
                
                return response
                
            except Exception as e:
                # Log error response
                duration = time.time() - start_time
                
                log_data = {
                    "request_id": request_id,
                    "exception": str(e),
                    "exception_type": e.__class__.__name__,
                    "duration_ms": round(duration * 1000, 2),
                }
                
                structured_log("error", f"Request failed: {e}", log_data)
                
                # Record metrics
                observe_histogram("http_request_duration_seconds", duration)
                increment_counter("http_responses_exception")
                
                # Re-raise exception
                raise
            finally:
                # End request context
                end_request_context()
        
        return cast(F, wrapper)
    
    return decorator


# Metrics functions
def increment_counter(name: str, value: float = 1.0) -> None:
    """Increment a counter metric.
    
    Args:
        name: Metric name
        value: Value to increment by (default: 1.0)
    """
    if name not in _metrics["counters"]:
        _metrics["counters"][name] = 0.0
    
    _metrics["counters"][name] += value


def set_gauge(name: str, value: float) -> None:
    """Set a gauge metric.
    
    Args:
        name: Metric name
        value: Value to set
    """
    _metrics["gauges"][name] = value


def observe_histogram(name: str, value: float) -> None:
    """Observe a value for a histogram metric.
    
    Args:
        name: Metric name
        value: Value to observe
    """
    if name not in _metrics["histograms"]:
        _metrics["histograms"][name] = []
    
    _metrics["histograms"][name].append(value)


def get_metrics() -> Dict[str, Dict[str, Any]]:
    """Get all metrics.
    
    Returns:
        Dictionary of metrics
    """
    result = {
        "counters": _metrics["counters"].copy(),
        "gauges": _metrics["gauges"].copy(),
        "histograms": {},
    }
    
    # Calculate histogram statistics
    for name, values in _metrics["histograms"].items():
        if not values:
            continue
            
        # Sort values for percentile calculation
        sorted_values = sorted(values)
        count = len(sorted_values)
        
        result["histograms"][name] = {
            "count": count,
            "sum": sum(sorted_values),
            "min": min(sorted_values),
            "max": max(sorted_values),
            "mean": sum(sorted_values) / count,
            "p50": sorted_values[int(count * 0.5)] if count > 0 else 0,
            "p90": sorted_values[int(count * 0.9)] if count > 0 else 0,
            "p95": sorted_values[int(count * 0.95)] if count > 0 else 0,
            "p99": sorted_values[int(count * 0.99)] if count > 0 else 0,
        }
    
    return result


def reset_metrics() -> None:
    """Reset all metrics."""
    global _metrics
    _metrics = {
        "counters": {},
        "gauges": {},
        "histograms": {},
    }

================
File: video_processor/services/storage/__init__.py
================
"""
Storage service package for interacting with different storage backends.
"""

from .base import StorageService
from .factory import get_storage_service
from .gcs import GCSStorageService
from .local import LocalStorageService

__all__ = [
    "StorageService",
    "GCSStorageService",
    "LocalStorageService",
    "get_storage_service",
]

================
File: video_processor/services/storage/base.py
================
"""
Base storage service interface.
"""

from abc import ABC, abstractmethod
from typing import Any, BinaryIO, Dict, List, Optional, Union

# Type alias for file-like objects
FileContent = Union[str, bytes, BinaryIO]


class StorageService(ABC):
    """
    Abstract base class for storage services.

    This interface defines the methods that all storage services must implement,
    providing a consistent API regardless of the underlying storage system.
    """

    @abstractmethod
    def download_file(
        self, bucket: str, source_path: str, destination_path: str
    ) -> str:
        """
        Download a file from storage.

        Args:
            bucket: Storage bucket name
            source_path: Path to the file in storage
            destination_path: Local path to save the file

        Returns:
            Local path to the downloaded file

        Raises:
            StorageError: If the download fails
        """
        pass

    @abstractmethod
    def upload_file(self, bucket: str, source_path: str, destination_path: str) -> str:
        """
        Upload a file to storage.

        Args:
            bucket: Storage bucket name
            source_path: Local path to the file
            destination_path: Path to save the file in storage

        Returns:
            Storage path to the uploaded file

        Raises:
            StorageError: If the upload fails
        """
        pass

    @abstractmethod
    def upload_from_string(
        self, bucket: str, content: FileContent, destination_path: str
    ) -> str:
        """
        Upload content directly to storage.

        Args:
            bucket: Storage bucket name
            content: Content to upload (string, bytes, or file-like object)
            destination_path: Path to save the file in storage

        Returns:
            Storage path to the uploaded file

        Raises:
            StorageError: If the upload fails
        """
        pass

    @abstractmethod
    def read_file(self, bucket: str, path: str) -> bytes:
        """
        Read a file from storage as bytes.

        Args:
            bucket: Storage bucket name
            path: Path to the file in storage

        Returns:
            File content as bytes

        Raises:
            StorageError: If the read fails
        """
        pass

    @abstractmethod
    def read_text(self, bucket: str, path: str, encoding: str = "utf-8") -> str:
        """
        Read a file from storage as text.

        Args:
            bucket: Storage bucket name
            path: Path to the file in storage
            encoding: Text encoding to use

        Returns:
            File content as text

        Raises:
            StorageError: If the read fails
        """
        pass

    @abstractmethod
    def list_files(self, bucket: str, prefix: Optional[str] = None) -> List[str]:
        """
        List files in a bucket with optional prefix.

        Args:
            bucket: Storage bucket name
            prefix: Optional prefix to filter files

        Returns:
            List of file paths

        Raises:
            StorageError: If the listing fails
        """
        pass

    @abstractmethod
    def file_exists(self, bucket: str, path: str) -> bool:
        """
        Check if a file exists.

        Args:
            bucket: Storage bucket name
            path: Path to the file in storage

        Returns:
            True if the file exists, False otherwise
        """
        pass

    @abstractmethod
    def delete_file(self, bucket: str, path: str) -> bool:
        """
        Delete a file from storage.

        Args:
            bucket: Storage bucket name
            path: Path to the file in storage

        Returns:
            True if the file was deleted, False otherwise
        """
        pass

    @abstractmethod
    def move_file(self, bucket: str, source_path: str, destination_path: str) -> bool:
        """
        Move a file within a bucket.

        Args:
            bucket: Storage bucket name
            source_path: Original path to the file
            destination_path: New path for the file

        Returns:
            True if the file was moved, False otherwise
        """
        pass

    @abstractmethod
    def get_signed_url(
        self,
        bucket: str,
        path: str,
        expiration_minutes: int = 15,
        http_method: str = "GET",
        content_type: Optional[str] = None,
    ) -> str:
        """
        Generate a signed URL for a file.

        Args:
            bucket: Storage bucket name
            path: Path to the file in storage
            expiration_minutes: URL expiration time in minutes
            http_method: HTTP method for the URL (GET, PUT, etc.)
            content_type: Content type for uploads

        Returns:
            Signed URL

        Raises:
            StorageError: If URL generation fails
        """
        pass

    @abstractmethod
    def get_metadata(self, bucket: str, path: str) -> Dict[str, Any]:
        """
        Get metadata for a file.

        Args:
            bucket: Storage bucket name
            path: Path to the file in storage

        Returns:
            File metadata as a dictionary

        Raises:
            StorageError: If metadata retrieval fails
        """
        pass

================
File: video_processor/services/storage/factory.py
================
"""
Factory for creating storage service instances.
"""

from typing import Optional

from video_processor.config import get_settings
from video_processor.utils.logging import get_logger

from .base import StorageService
from .gcs import GCSStorageService
from .local import LocalStorageService

logger = get_logger(__name__)


def get_storage_service(
    testing_mode: Optional[bool] = None,
    local_output: Optional[bool] = None,
) -> StorageService:
    """
    Get an appropriate storage service based on configuration.

    Args:
        testing_mode: Override testing mode setting
        local_output: Override local output setting

    Returns:
        An instance of StorageService
    """
    settings = get_settings()

    # Use provided values or fall back to settings
    testing = testing_mode if testing_mode is not None else settings.testing_mode
    local = local_output if local_output is not None else settings.local_output

    # In testing or local output mode, use local storage
    if testing or local:
        logger.info("Using LocalStorageService for testing/local output")
        return LocalStorageService()

    # Otherwise, use GCS
    logger.info("Using GCSStorageService for production")
    return GCSStorageService()

================
File: video_processor/services/storage/gcs.py
================
"""
Google Cloud Storage implementation of the storage service.
"""

import os
from datetime import timedelta
from typing import Any, Dict, List, Optional

from google.cloud import storage

from video_processor.utils.error_handling import StorageError, retry
from video_processor.utils.logging import get_logger

from .base import FileContent, StorageService

logger = get_logger(__name__)


class GCSStorageService(StorageService):
    """
    Google Cloud Storage implementation of the storage service.
    """

    def __init__(self, client: Optional[storage.Client] = None):
        """
        Initialize the GCS storage service.

        Args:
            client: Optional storage client (if None, creates a new client)
        """
        self.client = client if client is not None else storage.Client()

    @retry(max_attempts=3)
    def download_file(
        self, bucket: str, source_path: str, destination_path: str
    ) -> str:
        """
        Download a file from GCS.

        Args:
            bucket: GCS bucket name
            source_path: Path to the file in GCS
            destination_path: Local path to save the file

        Returns:
            Local path to the downloaded file

        Raises:
            StorageError: If the download fails
        """
        try:
            logger.info(
                f"Downloading gs://{bucket}/{source_path} to {destination_path}"
            )
            bucket_obj = self.client.bucket(bucket)
            blob = bucket_obj.blob(source_path)

            # Check if the blob exists
            if not blob.exists():
                raise StorageError(f"File gs://{bucket}/{source_path} does not exist")

            # Ensure directory exists
            os.makedirs(os.path.dirname(destination_path), exist_ok=True)

            # Download the file
            blob.download_to_filename(destination_path)
            logger.info(f"Download complete: gs://{bucket}/{source_path}")
            return destination_path
        except Exception as e:
            logger.error(f"Failed to download gs://{bucket}/{source_path}: {e}")
            raise StorageError(f"Failed to download file: {e}") from e

    @retry(max_attempts=3)
    def upload_file(self, bucket: str, source_path: str, destination_path: str) -> str:
        """
        Upload a file to GCS.

        Args:
            bucket: GCS bucket name
            source_path: Local path to the file
            destination_path: Path to save the file in GCS

        Returns:
            GCS path to the uploaded file

        Raises:
            StorageError: If the upload fails
        """
        try:
            logger.info(f"Uploading {source_path} to gs://{bucket}/{destination_path}")

            # Check if source file exists
            if not os.path.exists(source_path):
                raise StorageError(f"Source file {source_path} does not exist")

            # Upload to GCS
            bucket_obj = self.client.bucket(bucket)
            blob = bucket_obj.blob(destination_path)
            blob.upload_from_filename(source_path)

            logger.info(f"Upload complete: gs://{bucket}/{destination_path}")
            return destination_path
        except Exception as e:
            logger.error(
                f"Failed to upload {source_path} to "
                f"gs://{bucket}/{destination_path}: {e}"
            )
            raise StorageError(f"Failed to upload file: {e}") from e

    @retry(max_attempts=3)
    def upload_from_string(
        self, bucket: str, content: FileContent, destination_path: str
    ) -> str:
        """
        Upload content directly to GCS.

        Args:
            bucket: GCS bucket name
            content: Content to upload (string, bytes, or file-like object)
            destination_path: Path to save the file in GCS

        Returns:
            GCS path to the uploaded file

        Raises:
            StorageError: If the upload fails
        """
        try:
            logger.info(f"Uploading content to gs://{bucket}/{destination_path}")

            bucket_obj = self.client.bucket(bucket)
            blob = bucket_obj.blob(destination_path)

            # Handle different content types
            if isinstance(content, str):
                blob.upload_from_string(content)
            elif isinstance(content, bytes):
                blob.upload_from_string(content)
            elif hasattr(content, "read"):  # File-like object
                blob.upload_from_file(content)
            else:
                raise StorageError(f"Unsupported content type: {type(content)}")

            logger.info(f"Upload complete: gs://{bucket}/{destination_path}")
            return destination_path
        except Exception as e:
            logger.error(
                f"Failed to upload content to gs://{bucket}/{destination_path}: {e}"
            )
            raise StorageError(f"Failed to upload content: {e}") from e

    @retry(max_attempts=3)
    def read_file(self, bucket: str, path: str) -> bytes:
        """
        Read a file from GCS as bytes.

        Args:
            bucket: GCS bucket name
            path: Path to the file in GCS

        Returns:
            File content as bytes

        Raises:
            StorageError: If the read fails
        """
        try:
            logger.info(f"Reading gs://{bucket}/{path}")

            bucket_obj = self.client.bucket(bucket)
            blob = bucket_obj.blob(path)

            # Check if the blob exists
            if not blob.exists():
                raise StorageError(f"File gs://{bucket}/{path} does not exist")

            # Download as bytes
            content = blob.download_as_bytes()
            logger.info(f"Read {len(content)} bytes from gs://{bucket}/{path}")
            return content
        except Exception as e:
            logger.error(f"Failed to read gs://{bucket}/{path}: {e}")
            raise StorageError(f"Failed to read file: {e}") from e

    @retry(max_attempts=3)
    def read_text(self, bucket: str, path: str, encoding: str = "utf-8") -> str:
        """
        Read a file from GCS as text.

        Args:
            bucket: GCS bucket name
            path: Path to the file in GCS
            encoding: Text encoding to use

        Returns:
            File content as text

        Raises:
            StorageError: If the read fails
        """
        try:
            logger.info(f"Reading text from gs://{bucket}/{path}")

            bucket_obj = self.client.bucket(bucket)
            blob = bucket_obj.blob(path)

            # Check if the blob exists
            if not blob.exists():
                raise StorageError(f"File gs://{bucket}/{path} does not exist")

            # Download as text
            content = blob.download_as_text(encoding=encoding)
            logger.info(f"Read {len(content)} characters from gs://{bucket}/{path}")
            return content
        except Exception as e:
            logger.error(f"Failed to read text from gs://{bucket}/{path}: {e}")
            raise StorageError(f"Failed to read file as text: {e}") from e

    def list_files(self, bucket: str, prefix: Optional[str] = None) -> List[str]:
        """
        List files in a GCS bucket with optional prefix.

        Args:
            bucket: GCS bucket name
            prefix: Optional prefix to filter files

        Returns:
            List of file paths

        Raises:
            StorageError: If the listing fails
        """
        try:
            logger.info(f"Listing files in gs://{bucket}/{prefix or ''}")

            bucket_obj = self.client.bucket(bucket)
            blobs = bucket_obj.list_blobs(prefix=prefix)

            # Extract paths
            paths = [blob.name for blob in blobs]
            logger.info(f"Found {len(paths)} files in gs://{bucket}/{prefix or ''}")
            return paths
        except Exception as e:
            logger.error(f"Failed to list files in gs://{bucket}/{prefix or ''}: {e}")
            raise StorageError(f"Failed to list files: {e}") from e

    def file_exists(self, bucket: str, path: str) -> bool:
        """
        Check if a file exists in GCS.

        Args:
            bucket: GCS bucket name
            path: Path to the file in GCS

        Returns:
            True if the file exists, False otherwise
        """
        try:
            bucket_obj = self.client.bucket(bucket)
            blob = bucket_obj.blob(path)
            return blob.exists()
        except Exception as e:
            logger.error(f"Error checking if gs://{bucket}/{path} exists: {e}")
            return False

    @retry(max_attempts=3)
    def delete_file(self, bucket: str, path: str) -> bool:
        """
        Delete a file from GCS.

        Args:
            bucket: GCS bucket name
            path: Path to the file in GCS

        Returns:
            True if the file was deleted, False otherwise
        """
        try:
            logger.info(f"Deleting gs://{bucket}/{path}")

            bucket_obj = self.client.bucket(bucket)
            blob = bucket_obj.blob(path)

            # Check if the blob exists
            if not blob.exists():
                logger.warning(
                    f"File gs://{bucket}/{path} does not exist, nothing to delete"
                )
                return False

            # Delete the blob
            blob.delete()
            logger.info(f"Deleted gs://{bucket}/{path}")
            return True
        except Exception as e:
            logger.error(f"Failed to delete gs://{bucket}/{path}: {e}")
            return False

    @retry(max_attempts=3)
    def move_file(self, bucket: str, source_path: str, destination_path: str) -> bool:
        """
        Move a file within a GCS bucket.

        Args:
            bucket: GCS bucket name
            source_path: Original path to the file
            destination_path: New path for the file

        Returns:
            True if the file was moved, False otherwise
        """
        try:
            logger.info(
                f"Moving gs://{bucket}/{source_path} to gs://{bucket}/{destination_path}"
            )

            bucket_obj = self.client.bucket(bucket)
            source_blob = bucket_obj.blob(source_path)

            # Check if the source blob exists
            if not source_blob.exists():
                logger.warning(
                    f"Source file gs://{bucket}/{source_path} does not exist"
                )
                return False

            # Copy to destination
            bucket_obj.copy_blob(source_blob, bucket_obj, destination_path)

            # Delete the source blob
            source_blob.delete()

            logger.info(
                f"Moved gs://{bucket}/{source_path} to gs://{bucket}/{destination_path}"
            )
            return True
        except Exception as e:
            logger.error(f"Failed to move gs://{bucket}/{source_path}: {e}")
            return False

    def get_signed_url(
        self,
        bucket: str,
        path: str,
        expiration_minutes: int = 15,
        http_method: str = "GET",
        content_type: Optional[str] = None,
    ) -> str:
        """
        Generate a signed URL for a file in GCS.

        Args:
            bucket: GCS bucket name
            path: Path to the file in GCS
            expiration_minutes: URL expiration time in minutes
            http_method: HTTP method for the URL (GET, PUT, etc.)
            content_type: Content type for uploads

        Returns:
            Signed URL

        Raises:
            StorageError: If URL generation fails
        """
        try:
            logger.info(f"Generating signed {http_method} URL for gs://{bucket}/{path}")

            bucket_obj = self.client.bucket(bucket)
            blob = bucket_obj.blob(path)

            # Set up arguments for signed URL
            expiration = timedelta(minutes=expiration_minutes)
            kwargs = {
                "version": "v4",
                "expiration": expiration,
                "method": http_method,
            }

            # Add content type for PUT requests
            if http_method.upper() == "PUT" and content_type:
                kwargs["content_type"] = content_type

            # Generate the URL
            url = blob.generate_signed_url(**kwargs)

            logger.info(f"Generated signed URL for gs://{bucket}/{path}")
            return url
        except Exception as e:
            logger.error(f"Failed to generate signed URL for gs://{bucket}/{path}: {e}")
            raise StorageError(f"Failed to generate signed URL: {e}") from e

    def get_metadata(self, bucket: str, path: str) -> Dict[str, Any]:
        """
        Get metadata for a file in GCS.

        Args:
            bucket: GCS bucket name
            path: Path to the file in GCS

        Returns:
            File metadata as a dictionary

        Raises:
            StorageError: If metadata retrieval fails
        """
        try:
            logger.info(f"Getting metadata for gs://{bucket}/{path}")

            bucket_obj = self.client.bucket(bucket)
            blob = bucket_obj.blob(path)

            # Check if the blob exists
            if not blob.exists():
                raise StorageError(f"File gs://{bucket}/{path} does not exist")

            # Get metadata
            blob.reload()  # Ensure we have the latest metadata

            # Create a dictionary of relevant metadata
            metadata = {
                "name": blob.name,
                "bucket": blob.bucket.name,
                "size": blob.size,
                "updated": blob.updated,
                "content_type": blob.content_type,
                "etag": blob.etag,
                "generation": blob.generation,
                "metageneration": blob.metageneration,
                "md5_hash": blob.md5_hash,
                "storage_class": blob.storage_class,
                "time_created": blob.time_created,
                # Include any custom metadata
                "metadata": blob.metadata or {},
            }

            logger.info(f"Got metadata for gs://{bucket}/{path}")
            return metadata
        except Exception as e:
            logger.error(f"Failed to get metadata for gs://{bucket}/{path}: {e}")
            raise StorageError(f"Failed to get file metadata: {e}") from e

================
File: video_processor/services/storage/local.py
================
"""
Local filesystem implementation of the storage service.
"""

import os
import shutil
from datetime import datetime
from typing import Any, Dict, List, Optional

from video_processor.utils.error_handling import StorageError
from video_processor.utils.file_handling import ensure_directory_exists
from video_processor.utils.logging import get_logger

from .base import FileContent, StorageService

logger = get_logger(__name__)


class LocalStorageService(StorageService):
    """
    Local filesystem implementation of the storage service.

    This is useful for local development and testing without connecting to
    cloud storage.
    """

    def __init__(self, base_path: str = "test_data"):
        """
        Initialize the local storage service.

        Args:
            base_path: Base directory to use for storage
        """
        self.base_path = base_path
        ensure_directory_exists(base_path)

    def _get_bucket_path(self, bucket: str) -> str:
        """Get the full path for a bucket."""
        return os.path.join(self.base_path, bucket)

    def _get_file_path(self, bucket: str, path: str) -> str:
        """Get the full local path for a file."""
        return os.path.join(self._get_bucket_path(bucket), path)

    def download_file(
        self, bucket: str, source_path: str, destination_path: str
    ) -> str:
        """
        Copy a file from the local storage.

        Args:
            bucket: Directory name within base_path
            source_path: Path to the file in local storage
            destination_path: Local path to save the file

        Returns:
            Local path to the downloaded file

        Raises:
            StorageError: If the copy fails
        """
        try:
            source_file = self._get_file_path(bucket, source_path)
            logger.info(f"Copying {source_file} to {destination_path}")

            if not os.path.exists(source_file):
                raise StorageError(f"Source file {source_file} does not exist")

            # Ensure destination directory exists
            os.makedirs(os.path.dirname(destination_path), exist_ok=True)

            # Copy the file
            shutil.copy2(source_file, destination_path)

            logger.info(f"Copy complete: {destination_path}")
            return destination_path
        except Exception as e:
            logger.error(f"Failed to copy {source_path}: {e}")
            raise StorageError(f"Failed to copy file: {e}") from e

    def upload_file(self, bucket: str, source_path: str, destination_path: str) -> str:
        """
        Copy a file to the local storage.

        Args:
            bucket: Directory name within base_path
            source_path: Local path to the file
            destination_path: Path to save the file in local storage

        Returns:
            Storage path to the uploaded file

        Raises:
            StorageError: If the copy fails
        """
        try:
            if not os.path.exists(source_path):
                raise StorageError(f"Source file {source_path} does not exist")

            destination_file = self._get_file_path(bucket, destination_path)
            logger.info(f"Copying {source_path} to {destination_file}")

            # Ensure destination directory exists
            os.makedirs(os.path.dirname(destination_file), exist_ok=True)

            # Copy the file
            shutil.copy2(source_path, destination_file)

            logger.info(f"Copy complete: {destination_file}")
            return destination_path
        except Exception as e:
            logger.error(f"Failed to copy {source_path}: {e}")
            raise StorageError(f"Failed to copy file: {e}") from e

    def upload_from_string(
        self, bucket: str, content: FileContent, destination_path: str
    ) -> str:
        """
        Write content directly to local storage.

        Args:
            bucket: Directory name within base_path
            content: Content to write (string, bytes, or file-like object)
            destination_path: Path to save the file in local storage

        Returns:
            Storage path to the written file

        Raises:
            StorageError: If the write fails
        """
        try:
            destination_file = self._get_file_path(bucket, destination_path)
            logger.info(f"Writing content to {destination_file}")

            # Ensure destination directory exists
            os.makedirs(os.path.dirname(destination_file), exist_ok=True)

            # Write the content based on its type
            if isinstance(content, str):
                with open(destination_file, "w") as f:
                    f.write(content)
            elif isinstance(content, bytes):
                with open(destination_file, "wb") as f:
                    f.write(content)
            elif hasattr(content, "read"):  # File-like object
                with open(destination_file, "wb") as f:
                    shutil.copyfileobj(content, f)
            else:
                raise StorageError(f"Unsupported content type: {type(content)}")

            logger.info(f"Write complete: {destination_file}")
            return destination_path
        except Exception as e:
            logger.error(f"Failed to write content to {destination_path}: {e}")
            raise StorageError(f"Failed to write content: {e}") from e

    def read_file(self, bucket: str, path: str) -> bytes:
        """
        Read a file from local storage as bytes.

        Args:
            bucket: Directory name within base_path
            path: Path to the file in local storage

        Returns:
            File content as bytes

        Raises:
            StorageError: If the read fails
        """
        try:
            file_path = self._get_file_path(bucket, path)
            logger.info(f"Reading {file_path}")

            if not os.path.exists(file_path):
                raise StorageError(f"File {file_path} does not exist")

            # Read the file
            with open(file_path, "rb") as f:
                content = f.read()

            logger.info(f"Read {len(content)} bytes from {file_path}")
            return content
        except Exception as e:
            logger.error(f"Failed to read {path}: {e}")
            raise StorageError(f"Failed to read file: {e}") from e

    def read_text(self, bucket: str, path: str, encoding: str = "utf-8") -> str:
        """
        Read a file from local storage as text.

        Args:
            bucket: Directory name within base_path
            path: Path to the file in local storage
            encoding: Text encoding to use

        Returns:
            File content as text

        Raises:
            StorageError: If the read fails
        """
        try:
            file_path = self._get_file_path(bucket, path)
            logger.info(f"Reading text from {file_path}")

            if not os.path.exists(file_path):
                raise StorageError(f"File {file_path} does not exist")

            # Read the file
            with open(file_path, "r", encoding=encoding) as f:
                content = f.read()

            logger.info(f"Read {len(content)} characters from {file_path}")
            return content
        except Exception as e:
            logger.error(f"Failed to read text from {path}: {e}")
            raise StorageError(f"Failed to read file as text: {e}") from e

    def list_files(self, bucket: str, prefix: Optional[str] = None) -> List[str]:
        """
        List files in local storage with optional prefix.

        Args:
            bucket: Directory name within base_path
            prefix: Optional prefix to filter files

        Returns:
            List of file paths

        Raises:
            StorageError: If the listing fails
        """
        try:
            bucket_path = self._get_bucket_path(bucket)
            logger.info(f"Listing files in {bucket_path}/{prefix or ''}")

            if not os.path.exists(bucket_path):
                logger.warning(f"Bucket {bucket} does not exist, returning empty list")
                return []

            # Get list of all files
            all_files = []
            prefix_path = os.path.join(bucket_path, prefix or "")
            prefix_dir = os.path.dirname(prefix_path) if prefix else bucket_path

            for root, _, files in os.walk(prefix_dir):
                for file in files:
                    full_path = os.path.join(root, file)
                    # Convert to bucket-relative path
                    rel_path = os.path.relpath(full_path, bucket_path)

                    # Filter by prefix if provided
                    if prefix and not rel_path.startswith(prefix):
                        continue

                    all_files.append(rel_path)

            logger.info(f"Found {len(all_files)} files in {bucket_path}/{prefix or ''}")
            return all_files
        except Exception as e:
            logger.error(f"Failed to list files in {bucket}/{prefix or ''}: {e}")
            raise StorageError(f"Failed to list files: {e}") from e

    def file_exists(self, bucket: str, path: str) -> bool:
        """
        Check if a file exists in local storage.

        Args:
            bucket: Directory name within base_path
            path: Path to the file in local storage

        Returns:
            True if the file exists, False otherwise
        """
        file_path = self._get_file_path(bucket, path)
        return os.path.exists(file_path) and os.path.isfile(file_path)

    def delete_file(self, bucket: str, path: str) -> bool:
        """
        Delete a file from local storage.

        Args:
            bucket: Directory name within base_path
            path: Path to the file in local storage

        Returns:
            True if the file was deleted, False otherwise
        """
        try:
            file_path = self._get_file_path(bucket, path)
            logger.info(f"Deleting {file_path}")

            if not os.path.exists(file_path):
                logger.warning(f"File {file_path} does not exist, nothing to delete")
                return False

            # Delete the file
            os.remove(file_path)

            logger.info(f"Deleted {file_path}")
            return True
        except Exception as e:
            logger.error(f"Failed to delete {path}: {e}")
            return False

    def move_file(self, bucket: str, source_path: str, destination_path: str) -> bool:
        """
        Move a file within local storage.

        Args:
            bucket: Directory name within base_path
            source_path: Original path to the file
            destination_path: New path for the file

        Returns:
            True if the file was moved, False otherwise
        """
        try:
            source_file = self._get_file_path(bucket, source_path)
            destination_file = self._get_file_path(bucket, destination_path)

            logger.info(f"Moving {source_file} to {destination_file}")

            if not os.path.exists(source_file):
                logger.warning(f"Source file {source_file} does not exist")
                return False

            # Ensure destination directory exists
            os.makedirs(os.path.dirname(destination_file), exist_ok=True)

            # Move the file
            shutil.move(source_file, destination_file)

            logger.info(f"Moved {source_file} to {destination_file}")
            return True
        except Exception as e:
            logger.error(f"Failed to move {source_path}: {e}")
            return False

    def get_signed_url(
        self,
        bucket: str,
        path: str,
        expiration_minutes: int = 15,
        http_method: str = "GET",
        content_type: Optional[str] = None,
    ) -> str:
        """
        Generate a fake signed URL for local development.

        Args:
            bucket: Directory name within base_path
            path: Path to the file in local storage
            expiration_minutes: URL expiration time in minutes
            http_method: HTTP method for the URL (GET, PUT, etc.)
            content_type: Content type for uploads

        Returns:
            Local file URL

        Raises:
            StorageError: If URL generation fails
        """
        # For local development, just return a direct file path
        file_path = self._get_file_path(bucket, path)
        return f"file://{os.path.abspath(file_path)}"

    def get_metadata(self, bucket: str, path: str) -> Dict[str, Any]:
        """
        Get metadata for a file in local storage.

        Args:
            bucket: Directory name within base_path
            path: Path to the file in local storage

        Returns:
            File metadata as a dictionary

        Raises:
            StorageError: If metadata retrieval fails
        """
        try:
            file_path = self._get_file_path(bucket, path)
            logger.info(f"Getting metadata for {file_path}")

            if not os.path.exists(file_path):
                raise StorageError(f"File {file_path} does not exist")

            # Get file stats
            stats = os.stat(file_path)

            # Create metadata dictionary
            metadata = {
                "name": path,
                "bucket": bucket,
                "size": stats.st_size,
                "updated": datetime.fromtimestamp(stats.st_mtime),
                "time_created": datetime.fromtimestamp(stats.st_ctime),
                "content_type": self._guess_content_type(file_path),
                # Add other metadata similar to GCS for compatibility
                "etag": None,
                "generation": None,
                "metageneration": None,
                "md5_hash": None,
                "storage_class": "STANDARD",
                "metadata": {},
            }

            logger.info(f"Got metadata for {file_path}")
            return metadata
        except Exception as e:
            logger.error(f"Failed to get metadata for {path}: {e}")
            raise StorageError(f"Failed to get file metadata: {e}") from e

    def _guess_content_type(self, file_path: str) -> str:
        """Guess the content type of a file based on its extension."""
        _, ext = os.path.splitext(file_path)
        ext = ext.lower()

        content_types = {
            ".txt": "text/plain",
            ".html": "text/html",
            ".htm": "text/html",
            ".css": "text/css",
            ".js": "application/javascript",
            ".json": "application/json",
            ".xml": "application/xml",
            ".jpg": "image/jpeg",
            ".jpeg": "image/jpeg",
            ".png": "image/png",
            ".gif": "image/gif",
            ".svg": "image/svg+xml",
            ".pdf": "application/pdf",
            ".doc": "application/msword",
            ".docx": (
                "application/vnd.openxmlformats-officedocument."
                "wordprocessingml.document"
            ),
            ".xls": "application/vnd.ms-excel",
            ".xlsx": (
                "application/vnd.openxmlformats-officedocument." "spreadsheetml.sheet"
            ),
            ".ppt": "application/vnd.ms-powerpoint",
            ".pptx": (
                "application/vnd.openxmlformats-officedocument."
                "presentationml.presentation"
            ),
            ".mp3": "audio/mpeg",
            ".wav": "audio/wav",
            ".mp4": "video/mp4",
            ".avi": "video/x-msvideo",
            ".mov": "video/quicktime",
            ".zip": "application/zip",
            ".tar": "application/x-tar",
            ".gz": "application/gzip",
        }

        return content_types.get(ext, "application/octet-stream")

================
File: video_processor/services/__init__.py
================
"""
Services package for external service integrations.
"""

================
File: video_processor/tests/unit/test_video_processor.py
================


================
File: video_processor/tests/__init__.py
================
# This file is intentionally left empty to make the directory a Python package

================
File: video_processor/tests/conftest.py
================
"""
Pytest configuration file with common fixtures for testing.

import sys
import os

# Add the parent directory to the path so we can import the module
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
"""

import os
import subprocess
import tempfile
from unittest.mock import MagicMock

import pytest


@pytest.fixture
def mock_storage_client():
    """Mock for Google Cloud Storage client."""
    mock_client = MagicMock()
    mock_bucket = MagicMock()
    mock_blob = MagicMock()

    # Set up the chain of mocks
    mock_client.bucket.return_value = mock_bucket
    mock_bucket.blob.return_value = mock_blob

    return mock_client, mock_bucket, mock_blob


@pytest.fixture
def mock_generative_model():
    """Mock for Vertex AI GenerativeModel."""
    mock_model = MagicMock()
    mock_response = MagicMock()
    mock_response.text = "This is a mock response from Gemini API"
    mock_model.generate_content.return_value = mock_response

    return mock_model


@pytest.fixture
def sample_audio_file():
    """Create a temporary WAV file for testing."""
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_file:
        temp_path = temp_file.name

    try:
        # Generate a simple test tone using ffmpeg
        subprocess.run(
            [
                "ffmpeg",
                "-y",  # Overwrite output files without asking
                "-f",
                "lavfi",  # Use libavfilter
                "-i",
                "sine=frequency=440:duration=1",  # Generate a 1-second 440Hz tone
                "-ar",
                "16000",  # Audio sample rate
                "-ac",
                "1",  # Mono audio
                temp_path,
            ],
            check=True,
            capture_output=True,
            text=True,
        )

        yield temp_path
    finally:
        # Clean up the temporary file
        if os.path.exists(temp_path):
            os.unlink(temp_path)


@pytest.fixture
def sample_video_file():
    """Create a temporary MP4 file for testing."""
    with tempfile.NamedTemporaryFile(suffix=".mp4", delete=False) as temp_file:
        temp_path = temp_file.name

    try:
        # Generate a simple test video using ffmpeg
        subprocess.run(
            [
                "ffmpeg",
                "-y",  # Overwrite output files without asking
                "-f",
                "lavfi",  # Use libavfilter
                "-i",
                "sine=frequency=440:duration=1",  # Generate a 1-second 440Hz tone
                "-f",
                "lavfi",  # Use libavfilter for video
                "-i",
                "color=c=blue:s=320x240:d=1",  # Generate a 1-second blue screen
                "-c:a",
                "aac",  # Audio codec
                "-c:v",
                "h264",  # Video codec
                temp_path,
            ],
            check=True,
            capture_output=True,
            text=True,
        )

        yield temp_path
    finally:
        # Clean up the temporary file
        if os.path.exists(temp_path):
            os.unlink(temp_path)


@pytest.fixture
def mock_part():
    """Mock for Vertex AI Part object."""
    mock = MagicMock()
    mock.from_data.return_value = MagicMock()
    return mock


# Flask app fixture removed - replaced with FastAPI if needed


@pytest.fixture
def mock_cloud_event():
    """Create a mock Cloud Event for testing."""
    event = MagicMock()
    event.data = {
        "bucket": "test-bucket",
        "name": "daily-raw/test_video.mp4",
        "contentType": "video/mp4",
        "size": "1000000",
    }
    return event


@pytest.fixture
def mock_youtube_credentials():
    """Mock for YouTube API credentials."""
    mock_creds = MagicMock()
    mock_creds.refresh.return_value = None
    return mock_creds


@pytest.fixture
def mock_youtube_service():
    """Mock for YouTube API service."""
    mock_service = MagicMock()
    mock_videos = MagicMock()
    mock_captions = MagicMock()

    # Set up the chain of mocks
    mock_service.videos.return_value = mock_videos
    mock_service.captions.return_value = mock_captions

    # Mock the insert methods
    mock_insert_request = MagicMock()
    mock_insert_request.next_chunk.return_value = (None, {"id": "test_video_id"})
    mock_videos.insert.return_value = mock_insert_request

    mock_caption_request = MagicMock()
    mock_caption_request.execute.return_value = {"id": "test_caption_id"}
    mock_captions.insert.return_value = mock_caption_request

    return mock_service


@pytest.fixture
def mock_secretmanager_client():
    """Mock for Secret Manager client."""
    mock_client = MagicMock()
    mock_response = MagicMock()
    mock_response.payload.data.decode.return_value = "test_secret_value"
    mock_client.access_secret_version.return_value = mock_response
    return mock_client

================
File: video_processor/tests/test_chapters_generation.py
================
"""
Tests for the chapters generation functionality.
"""

import json
import os

# Import the functions to test
import sys
from unittest.mock import MagicMock, patch

# Add the parent directory to the path so we can import the module
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from process_uploaded_video import generate_chapters


def test_generate_chapters_valid_json(mock_generative_model, mock_part):
    """Test the generate_chapters function with valid JSON response."""
    # Create a valid JSON response
    valid_chapters = [
        {"timecode": "00:00", "chapterSummary": "Introduction"},
        {"timecode": "02:30", "chapterSummary": "Main topic"},
        {"timecode": "05:45", "chapterSummary": "Conclusion"},
    ]

    # Set up the mock response
    mock_response = MagicMock()
    mock_response.text = json.dumps(valid_chapters)
    mock_generative_model.generate_content.return_value = mock_response

    # Create a mock audio part
    mock_audio_part = MagicMock()

    # Patch the GenerativeModel class to return our mock
    with patch(
        "video_processor.process_uploaded_video.GenerativeModel",
        return_value=mock_generative_model,
    ):
        # Call the function
        result = generate_chapters(mock_audio_part)

        # Verify the result
        assert len(result) == 3
        assert result[0]["timecode"] == "00:00"
        assert result[0]["chapterSummary"] == "Introduction"
        assert result[1]["timecode"] == "02:30"
        assert result[2]["chapterSummary"] == "Conclusion"

        # Verify the model was called with the correct parameters
        mock_generative_model.generate_content.assert_called_once()
        args, kwargs = mock_generative_model.generate_content.call_args

        # Check that the prompt and audio part were passed correctly
        assert len(args[0]) == 2
        assert "Chapterize the video content" in args[0][0]  # Check part of the prompt
        assert args[0][1] == mock_audio_part  # Check the audio part

        # Check the generation config
        assert "temperature" in kwargs.get("generation_config", {})
        assert kwargs["generation_config"]["temperature"] == 0.6
        assert kwargs["generation_config"]["response_mime_type"] == "application/json"


def test_generate_chapters_invalid_json(mock_generative_model, mock_part):
    """Test the generate_chapters function with invalid JSON response."""
    # Set up the mock response with invalid JSON
    mock_response = MagicMock()
    mock_response.text = "This is not valid JSON"
    mock_generative_model.generate_content.return_value = mock_response

    # Create a mock audio part
    mock_audio_part = MagicMock()

    # Patch the GenerativeModel class to return our mock
    with patch(
        "video_processor.process_uploaded_video.GenerativeModel",
        return_value=mock_generative_model,
    ):
        # Call the function
        result = generate_chapters(mock_audio_part)

        # Verify the result is an empty list for invalid JSON
        assert result == []


def test_generate_chapters_missing_keys(mock_generative_model, mock_part):
    """Test the generate_chapters function with JSON missing required keys."""
    # Create JSON with missing keys
    invalid_chapters = [
        {"timecode": "00:00", "summary": "Introduction"},  # Missing chapterSummary
        {"time": "02:30", "chapterSummary": "Main topic"},  # Missing timecode
        {"other": "field"},  # Missing both required keys
    ]

    # Set up the mock response
    mock_response = MagicMock()
    mock_response.text = json.dumps(invalid_chapters)
    mock_generative_model.generate_content.return_value = mock_response

    # Create a mock audio part
    mock_audio_part = MagicMock()

    # Patch the GenerativeModel class to return our mock
    with patch(
        "video_processor.process_uploaded_video.GenerativeModel",
        return_value=mock_generative_model,
    ):
        # Call the function
        result = generate_chapters(mock_audio_part)

        # Verify the result is an empty list for invalid structure
        assert result == []


def test_generate_chapters_error_handling(mock_generative_model, mock_part):
    """Test error handling in the generate_chapters function."""
    # Set up the mock to raise an exception
    mock_generative_model.generate_content.side_effect = Exception("API error")

    # Create a mock audio part
    mock_audio_part = MagicMock()

    # Patch the GenerativeModel class to return our mock
    with patch(
        "video_processor.process_uploaded_video.GenerativeModel",
        return_value=mock_generative_model,
    ):
        # Call the function - it should handle the exception and return an empty list
        result = generate_chapters(mock_audio_part)

        # Verify the result is an empty list when an exception occurs
        assert result == []

================
File: video_processor/tests/test_firestore_trigger_listener.py
================
import os
from unittest.mock import patch

import firestore_trigger_listener as listener
import pytest
from google.cloud import firestore

# Mark all tests in this module as "integration" so they can be
# selectively run via `pytest -m integration`
pytestmark = pytest.mark.integration

TEST_COLLECTION = "videos_test"


@pytest.fixture(scope="module")
def firestore_client():
    # Use the same credentials as the main app, but a test collection
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = listener.SERVICE_ACCOUNT_PATH
    client = firestore.Client()
    yield client
    # Cleanup: delete all docs in the test collection after tests
    docs = client.collection(TEST_COLLECTION).stream()
    for doc in docs:
        doc.reference.delete()


def create_test_doc(client, doc_id, data):
    doc_ref = client.collection(TEST_COLLECTION).document(doc_id)
    doc_ref.set(data)
    return doc_ref


def update_test_doc(client, doc_id, updates):
    doc_ref = client.collection(TEST_COLLECTION).document(doc_id)
    doc_ref.update(updates)


def get_doc_data(client, doc_id):
    doc_ref = client.collection(TEST_COLLECTION).document(doc_id)
    return doc_ref.get().to_dict()


def run_listener_once(client, last_snapshots):
    # Simulate one iteration of the listener's polling logic
    docs = client.collection(TEST_COLLECTION).stream()
    for doc in docs:
        doc_id = doc.id
        data = doc.to_dict()
        prev = last_snapshots.get(doc_id)

        if prev is not None:
            for key, value in data.items():
                if key == "thumbnails" and "thumbnails" in prev:
                    for idx, thumb in enumerate(value):
                        prev_thumb = (
                            prev["thumbnails"][idx]
                            if idx < len(prev["thumbnails"])
                            else {}
                        )
                        if thumb.get("prompt") != prev_thumb.get("prompt"):
                            listener.regenerate_thumbnail(
                                doc_id, idx, thumb.get("prompt")
                            )
                elif key in ["title", "tags", "description", "scheduledTime"]:
                    if value != prev.get(key):
                        listener.update_metadata(doc_id, {key: value})
        last_snapshots[doc_id] = data


def test_firestore_trigger_listener_metadata_update(firestore_client):
    doc_id = "test_video_1"
    initial_data = {
        "title": "Original Title",
        "tags": ["tag1"],
        "description": "Original description",
        "scheduledTime": "2025-05-01T10:00:00Z",
        "thumbnails": [{"prompt": "original prompt"}],
    }
    create_test_doc(firestore_client, doc_id, initial_data)
    last_snapshots = {doc_id: initial_data.copy()}

    # Simulate a metadata update
    updates = {"title": "New Title"}
    update_test_doc(firestore_client, doc_id, updates)

    with patch.object(
        listener, "update_metadata"
    ) as mock_update_metadata, patch.object(
        listener, "regenerate_thumbnail"
    ) as mock_regen_thumb:
        run_listener_once(firestore_client, last_snapshots)
        mock_update_metadata.assert_called_once_with(doc_id, {"title": "New Title"})
        mock_regen_thumb.assert_not_called()


def test_firestore_trigger_listener_thumbnail_prompt_change(firestore_client):
    doc_id = "test_video_2"
    initial_data = {
        "title": "Title",
        "tags": ["tag1"],
        "description": "desc",
        "scheduledTime": "2025-05-01T10:00:00Z",
        "thumbnails": [{"prompt": "original prompt"}],
    }
    create_test_doc(firestore_client, doc_id, initial_data)
    last_snapshots = {doc_id: initial_data.copy()}

    # Simulate a thumbnail prompt change
    updates = {"thumbnails": [{"prompt": "new prompt"}]}
    update_test_doc(firestore_client, doc_id, updates)

    with patch.object(
        listener, "update_metadata"
    ) as mock_update_metadata, patch.object(
        listener, "regenerate_thumbnail"
    ) as mock_regen_thumb:
        run_listener_once(firestore_client, last_snapshots)
        mock_regen_thumb.assert_called_once_with(doc_id, 0, "new prompt")
        mock_update_metadata.assert_not_called()

================
File: video_processor/tests/test_generate_youtube_token.py
================
"""
Tests for the generate_youtube_token.py module.
"""

import os
import sys
from unittest.mock import MagicMock, patch

# Add the root directory to the path so we can import the module
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")))

from video_processor import generate_youtube_token


def test_save_refresh_token_to_secret(mock_secretmanager_client):
    """Test saving a refresh token to Secret Manager."""
    with patch(
        "video_processor.generate_youtube_token.secretmanager.SecretManagerServiceClient",
        return_value=mock_secretmanager_client,
    ):
        # Call the function
        generate_youtube_token.save_refresh_token_to_secret(
            "test-project", "test-secret", "test-token"
        )

        # Verify the correct API call was made
        mock_secretmanager_client.add_secret_version.assert_called_once_with(
            request={
                "parent": "projects/test-project/secrets/test-secret",
                "payload": {"data": b"test-token"},
            }
        )


def test_main_client_secrets_not_found():
    """Test main function when client secrets file is not found."""
    # Mock the argument parser
    mock_args = MagicMock()
    mock_args.channel = "daily"
    mock_args.save = False

    with patch("argparse.ArgumentParser.parse_args", return_value=mock_args):
        with patch("os.path.exists", return_value=False):
            with patch("builtins.print") as mock_print:
                # Call the function
                generate_youtube_token.main()

                # Verify error message was printed
                mock_print.assert_any_call(
                    f"\nError: Client secrets file not found at "
                    f"'{generate_youtube_token.CLIENT_SECRETS_FILE}'"
                )


def test_main_successful_flow():
    """Test successful flow of the main function."""
    # Mock the argument parser
    mock_args = MagicMock()
    mock_args.channel = "daily"
    mock_args.save = False

    mock_flow = MagicMock()
    mock_credentials = MagicMock()
    mock_credentials.refresh_token = "test-refresh-token"
    mock_flow.credentials = mock_credentials

    # Mock the authorization URL
    mock_flow.authorization_url.return_value = ("https://example.com/auth", None)

    # Mock the fetch_token method
    mock_flow.fetch_token.return_value = None

    with patch("argparse.ArgumentParser.parse_args", return_value=mock_args):
        with patch("os.path.exists", return_value=True):
            with patch(
                "google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file",
                return_value=mock_flow,
            ):
                with patch("builtins.input", return_value="test-code"):
                    with patch("builtins.print") as mock_print:
                        # Disable auto-saving to Secret Manager
                        with patch.object(
                            generate_youtube_token, "SAVE_TO_SECRET_MANAGER", False
                        ):
                            # Call the function
                            generate_youtube_token.main()

                            # Verify the flow was used correctly
                            mock_flow.authorization_url.assert_called_once_with(
                                prompt="consent"
                            )
                            mock_flow.fetch_token.assert_called_once_with(
                                code="test-code"
                            )

                            # Verify the refresh token was printed
                            mock_print.assert_any_call(
                                "Refresh Token: test-refresh-token"
                            )


def test_main_with_secret_manager_saving():
    """Test main function with saving to Secret Manager enabled."""
    # Mock the argument parser
    mock_args = MagicMock()
    mock_args.channel = "daily"
    mock_args.save = True

    # Create a mock secret ID
    mock_secret_id = "youtube-daily-refresh-token"

    mock_flow = MagicMock()
    mock_credentials = MagicMock()
    mock_credentials.refresh_token = "test-refresh-token"
    mock_flow.credentials = mock_credentials

    # Mock the authorization URL
    mock_flow.authorization_url.return_value = ("https://example.com/auth", None)

    # Mock the fetch_token method
    mock_flow.fetch_token.return_value = None

    with patch("argparse.ArgumentParser.parse_args", return_value=mock_args):
        with patch.dict(generate_youtube_token.SECRET_IDS, {"daily": mock_secret_id}):
            with patch("os.path.exists", return_value=True):
                with patch(
                    "google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file",
                    return_value=mock_flow,
                ):
                    with patch("builtins.input", return_value="test-code"):
                        with patch("builtins.print"):
                            with patch(
                                "video_processor.generate_youtube_token.save_refresh_token_to_secret"
                            ) as mock_save:
                                # Call the function
                                generate_youtube_token.main()

                                # Verify save_refresh_token_to_secret was called
                                mock_save.assert_called_once_with(
                                    generate_youtube_token.PROJECT_ID,
                                    mock_secret_id,
                                    "test-refresh-token",
                                )

================
File: video_processor/tests/test_titles_generation.py
================
"""
Tests for the titles and keywords generation functionality.
"""

import json
import os

# Import the functions to test
import sys
from unittest.mock import MagicMock, patch

# Add the parent directory to the path so we can import the module
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from process_uploaded_video import generate_titles


def test_generate_titles_valid_json(mock_generative_model, mock_part):
    """Test the generate_titles function with valid JSON response."""
    # Create a valid JSON response
    valid_title_dict = {
        "Description": "Exciting Video Title",
        "Keywords": "keyword1,keyword2,keyword3,keyword4",
    }

    # Set up the mock response
    mock_response = MagicMock()
    mock_response.text = json.dumps(valid_title_dict)
    mock_generative_model.generate_content.return_value = mock_response

    # Create a mock audio part
    mock_audio_part = MagicMock()

    # Patch the GenerativeModel class to return our mock
    with patch(
        "video_processor.process_uploaded_video.GenerativeModel",
        return_value=mock_generative_model,
    ):
        # Call the function
        result = generate_titles(mock_audio_part)

        # Verify the result
        assert result["Description"] == "Exciting Video Title"
        assert result["Keywords"] == "keyword1,keyword2,keyword3,keyword4"

        # Verify the model was called with the correct parameters
        mock_generative_model.generate_content.assert_called_once()
        args, kwargs = mock_generative_model.generate_content.call_args

        # Check that the prompt and audio part were passed correctly
        assert len(args[0]) == 2
        assert (
            "Please write a 40-character long intriguing title" in args[0][0]
        )  # Check part of the prompt
        assert args[0][1] == mock_audio_part  # Check the audio part

        # Check the generation config
        assert "temperature" in kwargs.get("generation_config", {})
        assert kwargs["generation_config"]["temperature"] == 0.8
        assert kwargs["generation_config"]["response_mime_type"] == "application/json"


def test_generate_titles_invalid_json(mock_generative_model, mock_part):
    """Test the generate_titles function with invalid JSON response."""
    # Set up the mock response with invalid JSON
    mock_response = MagicMock()
    mock_response.text = "This is not valid JSON"
    mock_generative_model.generate_content.return_value = mock_response

    # Create a mock audio part
    mock_audio_part = MagicMock()

    # Patch the GenerativeModel class to return our mock
    with patch(
        "video_processor.process_uploaded_video.GenerativeModel",
        return_value=mock_generative_model,
    ):
        # Call the function
        result = generate_titles(mock_audio_part)

        # Verify the result is the default dictionary for invalid JSON
        assert result["Description"] == "Default Title"
        assert result["Keywords"] == "default,keywords"


def test_generate_titles_missing_keys(mock_generative_model, mock_part):
    """Test the generate_titles function with JSON missing required keys."""
    # Create JSON with missing keys
    invalid_title_dict = {
        "Title": "Wrong Key Name",  # Missing Description
        "Tags": "tag1,tag2,tag3",  # Missing Keywords
    }

    # Set up the mock response
    mock_response = MagicMock()
    mock_response.text = json.dumps(invalid_title_dict)
    mock_generative_model.generate_content.return_value = mock_response

    # Create a mock audio part
    mock_audio_part = MagicMock()

    # Patch the GenerativeModel class to return our mock
    with patch(
        "video_processor.process_uploaded_video.GenerativeModel",
        return_value=mock_generative_model,
    ):
        # Call the function
        result = generate_titles(mock_audio_part)

        # Verify the result is the default dictionary for invalid structure
        assert result["Description"] == "Default Title"
        assert result["Keywords"] == "default,keywords"


def test_generate_titles_error_handling(mock_generative_model, mock_part):
    """Test error handling in the generate_titles function."""
    # Set up the mock to raise an exception
    mock_generative_model.generate_content.side_effect = Exception("API error")

    # Create a mock audio part
    mock_audio_part = MagicMock()

    # Patch the GenerativeModel class to return our mock
    with patch(
        "video_processor.process_uploaded_video.GenerativeModel",
        return_value=mock_generative_model,
    ):
        # Call the function - it should handle the exception and return
        # the default dictionary
        result = generate_titles(mock_audio_part)

        # Verify the result is the default dictionary when an exception occurs
        assert result["Description"] == "Default Title"
        assert result["Keywords"] == "default,keywords"

================
File: video_processor/tests/test_vtt_generation.py
================
"""
Tests for the VTT subtitles generation functionality.
"""

import os

# Import the functions to test
import sys
from unittest.mock import MagicMock, patch

import pytest

# Add the parent directory to the path so we can import the module
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from process_uploaded_video import generate_vtt

# Import Part for mocking
from vertexai.preview.generative_models import Part


@pytest.mark.parametrize(
    "mock_response_text,expected_result",
    [
        (
            "WEBVTT\n\n00:00:00.000 --> 00:00:05.000\nHello world",
            "WEBVTT\n\n00:00:00.000 --> 00:00:05.000\nHello world",
        ),
        (
            "00:00:00.000 --> 00:00:05.000\nMissing WEBVTT header",
            "WEBVTT\n\n00:00:00.000 --> 00:00:05.000\nMissing WEBVTT header".replace(
                "\\n", "\n"
            ),
        ),
        ("  WEBVTT  ", "WEBVTT"),
    ],
)
def test_generate_vtt(
    mock_generative_model, mock_part, mock_response_text, expected_result
):
    """Test the generate_vtt function with various inputs."""
    # Set up the mock response
    mock_response = MagicMock()
    mock_response.text = mock_response_text
    mock_generative_model.generate_content.return_value = mock_response

    # Create a mock audio part that is a proper Part instance
    mock_audio_part = MagicMock(spec=Part)

    # Patch the necessary classes and functions
    with patch(
        "process_uploaded_video.GenerativeModel",
        return_value=mock_generative_model,
    ):
        # Call the function
        result = generate_vtt(mock_audio_part)

        # Verify the result - normalize newlines for comparison
        assert result.replace("\\n", "\n") == expected_result

        # Verify the model was called
        mock_generative_model.generate_content.assert_called_once()


def test_generate_vtt_webvtt_correction():
    """Test that the VTT generator adds WEBVTT header if missing."""
    # Create a mock response without WEBVTT header
    mock_response = MagicMock()
    mock_response.text = "00:00:00.000 --> 00:00:05.000\nThis is a test subtitle"

    mock_model = MagicMock()
    mock_model.generate_content.return_value = mock_response

    # Create a mock audio part that is a proper Part instance
    mock_audio_part = MagicMock(spec=Part)

    # Patch the necessary classes and functions
    with patch(
        "process_uploaded_video.GenerativeModel",
        return_value=mock_model,
    ):
        # Call the function
        result = generate_vtt(mock_audio_part)

        # Verify the result has WEBVTT header added
        assert result.replace("\\n", "\n").startswith("WEBVTT\n\n")
        assert "00:00:00.000 --> 00:00:05.000" in result
        assert "This is a test subtitle" in result


def test_generate_vtt_error_handling(mock_generative_model):
    """Test error handling in the generate_vtt function."""
    # Set up the mock to raise an exception
    mock_generative_model.generate_content.side_effect = Exception("API error")

    # Create a mock audio part that is a proper Part instance
    mock_audio_part = MagicMock(spec=Part)

    # Patch the necessary classes and functions
    with patch(
        "process_uploaded_video.GenerativeModel",
        return_value=mock_generative_model,
    ):
        # Call the function and expect an exception
        with pytest.raises(Exception) as excinfo:
            generate_vtt(mock_audio_part)

        # Verify the exception message
        assert "API error" in str(excinfo.value)

================
File: video_processor/tests/test_youtube_uploader.py
================
"""
Tests for the youtube_uploader.py module.
"""

import os
import sys
import tempfile
from unittest.mock import MagicMock, patch

# Add the root directory to the path so we can import the module
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")))

from video_processor import youtube_uploader


def test_get_secret(mock_secretmanager_client):
    """Test retrieving a secret from Secret Manager."""
    with patch(
        "video_processor.youtube_uploader.secretmanager.SecretManagerServiceClient",
        return_value=mock_secretmanager_client,
    ):
        # Call the function
        result = youtube_uploader.get_secret("test-secret", "test-project")

        # Verify the result
        assert result == "test_secret_value"

        # Verify the correct API call was made
        mock_secretmanager_client.access_secret_version.assert_called_once_with(
            request={
                "name": "projects/test-project/secrets/test-secret/versions/latest"
            }
        )


def test_get_youtube_credentials(mock_secretmanager_client, mock_youtube_credentials):
    """Test building YouTube credentials from stored secrets."""
    with patch(
        "video_processor.youtube_uploader.secretmanager.SecretManagerServiceClient",
        return_value=mock_secretmanager_client,
    ):
        with patch(
            "video_processor.youtube_uploader.Credentials",
            return_value=mock_youtube_credentials,
        ):
            with patch("video_processor.youtube_uploader.Request"):
                # Call the function
                result = youtube_uploader.get_youtube_credentials(
                    {
                        "client_id": "test-client-id",
                        "client_secret": "test-client-secret",
                        "refresh_token": "test-refresh-token",
                    }
                )

                # Verify the result
                assert result == mock_youtube_credentials

                # Verify the refresh method was called
                mock_youtube_credentials.refresh.assert_called_once()


def test_download_blob(mock_storage_client):
    """Test downloading a blob from GCS."""
    mock_client, mock_bucket, mock_blob = mock_storage_client

    with patch(
        "video_processor.youtube_uploader.storage.Client", return_value=mock_client
    ):
        # Call the function
        result = youtube_uploader.download_blob(
            "test-bucket", "test-blob", "/tmp/test-file"
        )

        # Verify the result
        assert result == "/tmp/test-file"

        # Verify the correct API calls were made
        mock_client.bucket.assert_called_once_with("test-bucket")
        mock_bucket.blob.assert_called_once_with("test-blob")
        mock_blob.download_to_filename.assert_called_once_with("/tmp/test-file")


def test_read_blob_content(mock_storage_client):
    """Test reading content from a blob in GCS."""
    mock_client, mock_bucket, mock_blob = mock_storage_client
    mock_blob.download_as_text.return_value = "test content"

    with patch(
        "video_processor.youtube_uploader.storage.Client", return_value=mock_client
    ):
        # Call the function
        result = youtube_uploader.read_blob_content("test-bucket", "test-blob")

        # Verify the result
        assert result == "test content"

        # Verify the correct API calls were made
        mock_client.bucket.assert_called_once_with("test-bucket")
        mock_bucket.blob.assert_called_once_with("test-blob")
        mock_blob.download_as_text.assert_called_once()


def test_upload_video(mock_youtube_service):
    """Test uploading a video to YouTube."""
    # Create a temporary file for testing
    with tempfile.NamedTemporaryFile(suffix=".mp4") as temp_file:
        with patch(
            "video_processor.youtube_uploader.MediaFileUpload"
        ) as mock_media_upload:
            # Call the function
            result = youtube_uploader.upload_video(
                mock_youtube_service, temp_file.name, "Test Title", "Test Description"
            )

            # Verify the result
            assert result == {"id": "test_video_id"}

            # Verify the correct API calls were made
            mock_youtube_service.videos.assert_called_once()
            mock_youtube_service.videos().insert.assert_called_once()
            mock_media_upload.assert_called_once_with(
                temp_file.name, chunksize=-1, resumable=True
            )


def test_upload_captions(mock_youtube_service):
    """Test uploading captions to YouTube."""
    # Create a temporary file for testing
    with tempfile.NamedTemporaryFile(suffix=".vtt") as temp_file:
        with patch(
            "video_processor.youtube_uploader.MediaFileUpload"
        ) as mock_media_upload:
            # Call the function
            result = youtube_uploader.upload_captions(
                mock_youtube_service, "test_video_id", temp_file.name
            )

            # Verify the result
            assert result == {"id": "test_caption_id"}

            # Verify the correct API calls were made
            mock_youtube_service.captions.assert_called_once()
            mock_youtube_service.captions().insert.assert_called_once()
            mock_media_upload.assert_called_once_with(temp_file.name)


def test_upload_to_youtube_daily(
    mock_cloud_event,
    mock_storage_client,
    mock_youtube_credentials,
    mock_youtube_service,
):
    """Test the Cloud Function for uploading to YouTube Daily channel."""
    mock_client, mock_bucket, mock_blob = mock_storage_client

    # Mock the list_blobs method to return a list of blobs
    mock_blobs = [
        MagicMock(name="processed-daily/test_video/video.mp4"),
        MagicMock(name="processed-daily/test_video/description.txt"),
        MagicMock(name="processed-daily/test_video/subtitles.vtt"),
    ]
    mock_blobs[0].name = "processed-daily/test_video/video.mp4"
    mock_blobs[1].name = "processed-daily/test_video/description.txt"
    mock_blobs[2].name = "processed-daily/test_video/subtitles.vtt"

    mock_client.list_blobs.return_value = mock_blobs

    # Set up the cloud event data
    mock_cloud_event.data = {
        "bucket": "test-bucket",
        "name": "processed-daily/test_video/video.mp4",
    }

    with patch(
        "video_processor.youtube_uploader.storage.Client", return_value=mock_client
    ):
        with patch(
            "video_processor.youtube_uploader.get_youtube_credentials",
            return_value=mock_youtube_credentials,
        ):
            with patch(
                "video_processor.youtube_uploader.build",
                return_value=mock_youtube_service,
            ):
                with patch("video_processor.youtube_uploader.download_blob"):
                    with patch(
                        "video_processor.youtube_uploader.upload_video",
                        return_value={"id": "test_video_id"},
                    ):
                        with patch("video_processor.youtube_uploader.upload_captions"):
                            with patch(
                                "video_processor.youtube_uploader.os.path.exists",
                                return_value=True,
                            ):
                                with patch(
                                    "video_processor.youtube_uploader.os.remove"
                                ):
                                    # Call the function
                                    youtube_uploader.upload_to_youtube_daily(
                                        mock_cloud_event
                                    )

                                    # Verify the correct API calls were made
                                    mock_client.list_blobs.assert_called_once_with(
                                        "test-bucket",
                                        prefix="processed-daily/test_video/",
                                    )


def test_upload_to_youtube_main(
    mock_cloud_event,
    mock_storage_client,
    mock_youtube_credentials,
    mock_youtube_service,
):
    """Test the Cloud Function for uploading to YouTube Main channel."""
    mock_client, mock_bucket, mock_blob = mock_storage_client

    # Mock the list_blobs method to return a list of blobs
    mock_blobs = [
        MagicMock(name="processed-main/test_video/video.mp4"),
        MagicMock(name="processed-main/test_video/description.txt"),
        MagicMock(name="processed-main/test_video/subtitles.vtt"),
    ]
    mock_blobs[0].name = "processed-main/test_video/video.mp4"
    mock_blobs[1].name = "processed-main/test_video/description.txt"
    mock_blobs[2].name = "processed-main/test_video/subtitles.vtt"

    mock_client.list_blobs.return_value = mock_blobs

    # Set up the cloud event data
    mock_cloud_event.data = {
        "bucket": "test-bucket",
        "name": "processed-main/test_video/video.mp4",
    }

    with patch(
        "video_processor.youtube_uploader.storage.Client", return_value=mock_client
    ):
        with patch(
            "video_processor.youtube_uploader.get_youtube_credentials",
            return_value=mock_youtube_credentials,
        ):
            with patch(
                "video_processor.youtube_uploader.build",
                return_value=mock_youtube_service,
            ):
                with patch("video_processor.youtube_uploader.download_blob"):
                    with patch(
                        "video_processor.youtube_uploader.upload_video",
                        return_value={"id": "test_video_id"},
                    ):
                        with patch("video_processor.youtube_uploader.upload_captions"):
                            with patch(
                                "video_processor.youtube_uploader.os.path.exists",
                                return_value=True,
                            ):
                                with patch(
                                    "video_processor.youtube_uploader.os.remove"
                                ):
                                    # Call the function
                                    youtube_uploader.upload_to_youtube_main(
                                        mock_cloud_event
                                    )

                                    # Verify the correct API calls were made
                                    mock_client.list_blobs.assert_called_once_with(
                                        "test-bucket",
                                        prefix="processed-main/test_video/",
                                    )

================
File: video_processor/utils/__init__.py
================
"""
Utilities module for shared functionality across the application.
"""

================
File: video_processor/utils/error_handling.py
================
"""
Error handling utilities.
"""

import functools
import traceback
from typing import Any, Callable, Optional, Type, TypeVar, Union, cast

from .logging import get_logger

logger = get_logger(__name__)

# Type variables for better type hinting with decorators
F = TypeVar("F", bound=Callable[..., Any])
R = TypeVar("R")


class ProcessingError(Exception):
    """Base exception for video processing errors."""

    pass


class StorageError(ProcessingError):
    """Exception for storage-related errors."""

    pass


class TranscriptionError(ProcessingError):
    """Exception for transcription-related errors."""

    pass


class VideoProcessingError(ProcessingError):
    """Exception for video processing errors."""

    pass


class ConfigurationError(Exception):
    """Exception for configuration errors."""

    pass


def handle_exceptions(
    fallback_return: Optional[Any] = None,
    exception_types: Optional[
        Union[Type[Exception], tuple[Type[Exception], ...]]
    ] = None,
    log_level: str = "error",
) -> Callable[[F], F]:
    """
    Decorator to handle exceptions in a function.

    Args:
        fallback_return: Value to return if an exception occurs
        exception_types: Exception types to catch (defaults to Exception)
        log_level: Logging level to use when an exception occurs

    Returns:
        Decorated function
    """
    if exception_types is None:
        exception_types = Exception

    def decorator(func: F) -> F:
        @functools.wraps(func)
        def wrapper(*args: Any, **kwargs: Any) -> Any:
            try:
                return func(*args, **kwargs)
            except exception_types as e:
                log_func = getattr(logger, log_level)
                log_func(
                    f"Error in {func.__name__}: {str(e)}\n"
                    f"{''.join(traceback.format_tb(e.__traceback__))}"
                )
                return fallback_return

        return cast(F, wrapper)

    return decorator


def retry(
    max_attempts: int = 3,
    backoff_factor: float = 0.5,
    exception_types: Optional[
        Union[Type[Exception], tuple[Type[Exception], ...]]
    ] = None,
) -> Callable[[F], F]:
    """
    Retry a function on failure with exponential backoff.

    Args:
        max_attempts: Maximum number of attempts
        backoff_factor: Factor for exponential backoff
        exception_types: Exception types to catch and retry on

    Returns:
        Decorated function
    """
    if exception_types is None:
        exception_types = Exception

    def decorator(func: F) -> F:
        @functools.wraps(func)
        def wrapper(*args: Any, **kwargs: Any) -> Any:
            import random
            import time

            attempt = 0
            last_exception = None

            while attempt < max_attempts:
                try:
                    return func(*args, **kwargs)
                except exception_types as e:
                    attempt += 1
                    last_exception = e

                    if attempt >= max_attempts:
                        break

                    # Calculate sleep time with jitter
                    sleep_time = backoff_factor * (2 ** (attempt - 1))
                    sleep_time = sleep_time + (random.random() * sleep_time * 0.5)

                    logger.warning(
                        f"Attempt {attempt}/{max_attempts} failed for "
                        f"{func.__name__}: {e}. "
                        f"Retrying in {sleep_time:.2f}s..."
                    )
                    time.sleep(sleep_time)

            # If we reach here, all attempts failed
            logger.error(f"All {max_attempts} attempts failed for {func.__name__}")
            raise last_exception

        return cast(F, wrapper)

    return decorator

================
File: video_processor/utils/ffmpeg.py
================
"""
FFmpeg utility functions for media file operations.

This module provides wrapper functions for common FFmpeg operations used in the
video processing pipeline, such as audio extraction, frame extraction, and
metadata retrieval.
"""

import json
import logging
import os
import subprocess
from typing import Any, Dict, Optional, Tuple

from video_processor.domain.exceptions import InvalidVideoError

# Configure logger
logger = logging.getLogger(__name__)


def run_ffmpeg_command(command: list, check: bool = True) -> Tuple[str, str]:
    """
    Run an FFmpeg command and return stdout and stderr.

    Args:
        command: List of command arguments (including the ffmpeg executable)
        check: Whether to check the return code and raise an exception on failure

    Returns:
        Tuple of (stdout, stderr) as strings

    Raises:
        RuntimeError: If check=True and the command fails
    """
    logger.debug(f"Running FFmpeg command: {' '.join(command)}")

    try:
        result = subprocess.run(
            command,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            check=check,
        )
        return result.stdout, result.stderr
    except subprocess.CalledProcessError as e:
        logger.error(f"FFmpeg command failed: {e.stderr}")
        if check:
            raise RuntimeError(f"FFmpeg command failed: {e.stderr}") from e
        return e.stdout, e.stderr


def extract_audio(video_path: str, output_path: str, format: str = "wav") -> str:
    """
    Extract audio from a video file.

    Args:
        video_path: Path to the input video file
        output_path: Path where the audio file should be saved
        format: Audio format (default: wav)

    Returns:
        Path to the extracted audio file

    Raises:
        InvalidVideoError: If the video file is invalid or FFmpeg fails
    """
    if not os.path.exists(video_path):
        raise InvalidVideoError(f"Video file not found: {video_path}")

    # Ensure output directory exists
    os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)

    logger.info(f"Extracting audio from {video_path} to {output_path}")

    try:
        # Construct FFmpeg command for audio extraction
        command = [
            "ffmpeg",
            "-i",
            video_path,
            "-vn",  # Disable video
            "-acodec",
            "pcm_s16le",  # Audio codec for WAV
            "-ar",
            "44100",  # Audio sample rate
            "-ac",
            "2",  # Audio channels (stereo)
            "-y",  # Overwrite output file if it exists
            output_path,
        ]

        # Run the command
        stdout, stderr = run_ffmpeg_command(command)

        if not os.path.exists(output_path):
            raise InvalidVideoError("Failed to extract audio: Output file not created")

        logger.info(f"Audio extracted successfully to {output_path}")
        return output_path

    except Exception as e:
        logger.error(f"Error extracting audio: {str(e)}")
        raise InvalidVideoError(f"Failed to extract audio: {str(e)}")


def extract_frame(video_path: str, time: float, output_path: str) -> str:
    """
    Extract a single frame from a video at a specific timestamp.

    Args:
        video_path: Path to the input video file
        time: Timestamp in seconds for the frame to extract
        output_path: Path where the frame image should be saved

    Returns:
        Path to the extracted frame image

    Raises:
        InvalidVideoError: If the video file is invalid or FFmpeg fails
    """
    if not os.path.exists(video_path):
        raise InvalidVideoError(f"Video file not found: {video_path}")

    # Ensure output directory exists
    os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)

    logger.info(f"Extracting frame from {video_path} at {time}s to {output_path}")

    try:
        # Format time as HH:MM:SS.mmm
        time_str = str(time)

        # Construct FFmpeg command for frame extraction
        command = [
            "ffmpeg",
            "-ss",
            time_str,  # Seek to time position
            "-i",
            video_path,
            "-vframes",
            "1",  # Extract one frame
            "-q:v",
            "2",  # Quality level (lower is better, 2-31)
            "-y",  # Overwrite output file if it exists
            output_path,
        ]

        # Run the command
        stdout, stderr = run_ffmpeg_command(command)

        if not os.path.exists(output_path):
            raise InvalidVideoError("Failed to extract frame: Output file not created")

        logger.info(f"Frame extracted successfully to {output_path}")
        return output_path

    except Exception as e:
        logger.error(f"Error extracting frame: {str(e)}")
        raise InvalidVideoError(f"Failed to extract frame: {str(e)}")


def get_video_metadata(video_path: str) -> Dict[str, Any]:
    """
    Extract metadata from a video file using FFprobe.

    Args:
        video_path: Path to the video file

    Returns:
        Dictionary containing video metadata (duration, resolution, format, etc.)

    Raises:
        InvalidVideoError: If the video file is invalid or FFprobe fails
    """
    if not os.path.exists(video_path):
        raise InvalidVideoError(f"Video file not found: {video_path}")

    logger.info(f"Extracting metadata from {video_path}")

    try:
        # Construct FFprobe command to get video metadata in JSON format
        command = [
            "ffprobe",
            "-v",
            "quiet",
            "-print_format",
            "json",
            "-show_format",
            "-show_streams",
            video_path,
        ]

        # Run the command
        stdout, stderr = run_ffmpeg_command(command)

        if not stdout:
            raise InvalidVideoError(
                "Failed to extract metadata: No output from FFprobe"
            )

        # Parse JSON output
        probe_data = json.loads(stdout)

        # Extract relevant metadata
        metadata = {}

        # Extract format information
        if "format" in probe_data:
            format_data = probe_data["format"]
            metadata["format"] = format_data.get("format_name", "unknown")

            # Duration in seconds
            if "duration" in format_data:
                metadata["duration"] = float(format_data["duration"])

            # File size in bytes
            if "size" in format_data:
                metadata["size"] = int(format_data["size"])

        # Extract video stream information
        video_stream = None
        audio_stream = None

        if "streams" in probe_data:
            for stream in probe_data["streams"]:
                if stream.get("codec_type") == "video" and not video_stream:
                    video_stream = stream
                elif stream.get("codec_type") == "audio" and not audio_stream:
                    audio_stream = stream

        if video_stream:
            # Resolution
            width = int(video_stream.get("width", 0))
            height = int(video_stream.get("height", 0))
            metadata["resolution"] = (width, height)

            # Frame rate
            if "r_frame_rate" in video_stream:
                fps_parts = video_stream["r_frame_rate"].split("/")
                if len(fps_parts) == 2 and int(fps_parts[1]) != 0:
                    metadata["fps"] = float(int(fps_parts[0]) / int(fps_parts[1]))

            # Video codec
            metadata["video_codec"] = video_stream.get("codec_name", "unknown")

        if audio_stream:
            # Audio codec
            metadata["audio_codec"] = audio_stream.get("codec_name", "unknown")

            # Audio sample rate
            if "sample_rate" in audio_stream:
                metadata["audio_sample_rate"] = int(audio_stream["sample_rate"])

            # Audio channels
            if "channels" in audio_stream:
                metadata["audio_channels"] = int(audio_stream["channels"])

        logger.info(f"Metadata extracted successfully: {metadata}")
        return metadata

    except Exception as e:
        logger.error(f"Error extracting metadata: {str(e)}")
        raise InvalidVideoError(f"Failed to extract metadata: {str(e)}")


def compress_video(
    input_path: str, output_path: str, target_size_mb: Optional[int] = None
) -> str:
    """
    Compress a video file to a smaller size.

    Args:
        input_path: Path to the input video file
        output_path: Path where the compressed video should be saved
        target_size_mb: Target file size in MB (if None, uses default compression)

    Returns:
        Path to the compressed video

    Raises:
        InvalidVideoError: If the video file is invalid or FFmpeg fails
    """
    if not os.path.exists(input_path):
        raise InvalidVideoError(f"Video file not found: {input_path}")

    # Ensure output directory exists
    os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)

    logger.info(f"Compressing video {input_path} to {output_path}")

    try:
        # Get input video metadata
        metadata = get_video_metadata(input_path)

        # Calculate bitrate if target size is specified
        bitrate_args = []
        if target_size_mb:
            # Calculate target bitrate in kbps
            # Formula: (target_size_bytes * 8) / (duration_seconds * 1000)
            duration = metadata.get("duration", 0)
            if duration > 0:
                target_size_bits = target_size_mb * 8 * 1024 * 1024
                target_bitrate = int(target_size_bits / (duration * 1000))
                bitrate_args = ["-b:v", f"{target_bitrate}k"]
                logger.info(
                    f"Targeting bitrate of {target_bitrate} kbps for {target_size_mb}MB output"
                )

        # Construct FFmpeg command for video compression
        command = [
            "ffmpeg",
            "-i",
            input_path,
            "-c:v",
            "libx264",  # H.264 codec
            "-preset",
            "medium",  # Compression preset (slower = better compression)
            "-c:a",
            "aac",  # AAC audio codec
            "-b:a",
            "128k",  # Audio bitrate
        ]

        # Add bitrate arguments if target size specified
        if bitrate_args:
            command.extend(bitrate_args)
        else:
            # Default to CRF compression (constant quality)
            command.extend(["-crf", "23"])  # 0-51, lower is better quality

        # Add output file
        command.extend(["-y", output_path])

        # Run the command
        stdout, stderr = run_ffmpeg_command(command)

        if not os.path.exists(output_path):
            raise InvalidVideoError("Failed to compress video: Output file not created")

        logger.info(f"Video compressed successfully to {output_path}")
        return output_path

    except Exception as e:
        logger.error(f"Error compressing video: {str(e)}")
        raise InvalidVideoError(f"Failed to compress video: {str(e)}")

================
File: video_processor/utils/file_handling.py
================
"""
File handling utilities.
"""

import os
import tempfile

from .logging import get_logger

logger = get_logger(__name__)


def ensure_directory_exists(directory_path: str) -> bool:
    """
    Ensure a directory exists, creating it if necessary.

    Args:
        directory_path: Path to the directory

    Returns:
        True if successful, False otherwise
    """
    try:
        os.makedirs(directory_path, exist_ok=True)
        return True
    except Exception as e:
        logger.error(f"Error creating directory {directory_path}: {e}")
        return False


def get_temp_directory() -> str:
    """
    Create and return a temporary directory path.

    Returns:
        Path to a new temporary directory
    """
    return tempfile.mkdtemp()


def get_file_extension(file_path: str) -> str:
    """
    Get the extension of a file.

    Args:
        file_path: Path to the file

    Returns:
        File extension without the dot
    """
    return os.path.splitext(file_path)[1].lstrip(".")


def normalize_filename(filename: str) -> str:
    """
    Normalize a filename by replacing spaces with hyphens.

    Args:
        filename: Original filename

    Returns:
        Normalized filename
    """
    return filename.replace(" ", "-")


def get_safe_path(base_directory: str, *paths: str) -> str:
    """
    Create a safe path by joining path components and normalizing.

    Args:
        base_directory: Base directory
        *paths: Path components to join

    Returns:
        Normalized, safe path
    """
    path = os.path.join(base_directory, *paths)
    return os.path.normpath(path)

================
File: video_processor/utils/logging.py
================
"""
Logging configuration for the video processing application.

This module provides utilities for structured logging throughout
the application, with support for different environments and
log levels.
"""

import json
import logging
import logging.config
import os
import sys
from datetime import datetime
from typing import Any, Optional


class StructuredLogFormatter(logging.Formatter):
    """
    Custom formatter that outputs logs in a structured JSON format.
    Includes standard fields like timestamp, level, and message,
    plus any additional fields passed as extra args.
    """

    def format(self, record: logging.LogRecord) -> str:
        """
        Format the log record as a structured JSON object.

        Args:
            record: The log record to format

        Returns:
            A JSON string representation of the log record
        """
        # Basic log information
        log_data = {
            "timestamp": datetime.utcfromtimestamp(record.created).isoformat() + "Z",
            "level": record.levelname,
            "logger": record.name,
            "message": record.getMessage(),
            "module": record.module,
            "function": record.funcName,
            "line": record.lineno,
        }

        # If there's an exception, add traceback info
        if record.exc_info:
            log_data["exception"] = self.formatException(record.exc_info)

        # Add any additional attributes passed as extra
        for key, value in record.__dict__.items():
            if key not in {
                "args",
                "asctime",
                "created",
                "exc_info",
                "exc_text",
                "filename",
                "funcName",
                "id",
                "levelname",
                "levelno",
                "lineno",
                "module",
                "msecs",
                "message",
                "msg",
                "name",
                "pathname",
                "process",
                "processName",
                "relativeCreated",
                "stack_info",
                "thread",
                "threadName",
            } and not key.startswith("_"):
                log_data[key] = value

        return json.dumps(log_data)


class DevelopmentFormatter(logging.Formatter):
    """
    Formatter optimized for local development with colors and readability.
    """

    # ANSI color codes
    COLORS = {
        "DEBUG": "\033[36m",  # Cyan
        "INFO": "\033[32m",  # Green
        "WARNING": "\033[33m",  # Yellow
        "ERROR": "\033[31m",  # Red
        "CRITICAL": "\033[41m\033[37m",  # White on red background
        "RESET": "\033[0m",  # Reset
    }

    def format(self, record: logging.LogRecord) -> str:
        """
        Format the log record with colors for development.

        Args:
            record: The log record to format

        Returns:
            A formatted log string
        """
        level_color = self.COLORS.get(record.levelname, self.COLORS["RESET"])
        reset = self.COLORS["RESET"]

        # Format the timestamp
        timestamp = datetime.utcfromtimestamp(record.created).strftime("%H:%M:%S")

        # Basic log format with colors
        log_format = (
            f"{level_color}[{timestamp}] {record.levelname:<8}{reset} "
            f"{record.name:<20} | {record.getMessage()}"
        )

        # Add exception info if present
        if record.exc_info:
            log_format += f"\n{self.formatException(record.exc_info)}"

        # Add any additional context as a dictionary at the end
        extra_attrs = {}
        for key, value in record.__dict__.items():
            if key not in {
                "args",
                "asctime",
                "created",
                "exc_info",
                "exc_text",
                "filename",
                "funcName",
                "id",
                "levelname",
                "levelno",
                "lineno",
                "module",
                "msecs",
                "message",
                "msg",
                "name",
                "pathname",
                "process",
                "processName",
                "relativeCreated",
                "stack_info",
                "thread",
                "threadName",
            } and not key.startswith("_"):
                extra_attrs[key] = value

        if extra_attrs:
            log_format += f"\n{json.dumps(extra_attrs, indent=2)}"

        return log_format


def configure_logging(
    level: str = "INFO",
    environment: str = "development",
    log_file: Optional[str] = None,
) -> None:
    """
    Configure logging for the application.

    Args:
        level: Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
        environment: Environment type (development, production, test)
        log_file: Optional path to log file
    """
    # Default handlers write to stdout and file (if specified)
    handlers = ["console"]
    if log_file:
        handlers.append("file")

    # Basic logging config
    config = {
        "version": 1,
        "disable_existing_loggers": False,
        "formatters": {
            "structured": {
                "()": StructuredLogFormatter,
            },
            "development": {
                "()": DevelopmentFormatter,
            },
        },
        "handlers": {
            "console": {
                "class": "logging.StreamHandler",
                "formatter": (
                    "development" if environment == "development" else "structured"
                ),
                "stream": sys.stdout,
            },
        },
        "loggers": {
            "": {  # Root logger
                "handlers": handlers,
                "level": level,
                "propagate": True,
            },
            "video_processor": {
                "handlers": handlers,
                "level": level,
                "propagate": False,
            },
            # Third-party loggers
            "google": {
                "level": "WARNING",
            },
            "requests": {
                "level": "WARNING",
            },
        },
    }

    # Add file handler if log_file is specified
    if log_file:
        # Ensure the log directory exists
        os.makedirs(os.path.dirname(os.path.abspath(log_file)), exist_ok=True)

        config["handlers"]["file"] = {
            "class": "logging.handlers.RotatingFileHandler",
            "formatter": "structured",  # Always use structured for files
            "filename": log_file,
            "maxBytes": 10485760,  # 10 MB
            "backupCount": 5,
            "encoding": "utf8",
        }

    # Configure logging with the defined config
    logging.config.dictConfig(config)

    # Log the configuration to confirm settings
    logger = logging.getLogger(__name__)
    logger.info(
        "Logging configured",
        extra={
            "level": level,
            "environment": environment,
            "log_file": log_file,
        },
    )


def get_logger(name: str) -> logging.Logger:
    """
    Get a logger with the specified name.

    Args:
        name: Logger name, typically __name__ of the module

    Returns:
        Configured logger instance
    """
    return logging.getLogger(name)


# Helper functions for structured logging


def log_event(logger: logging.Logger, event_name: str, **kwargs: Any) -> None:
    """
    Log a structured event with additional context.

    Args:
        logger: Logger instance
        event_name: Name of the event
        **kwargs: Additional context to include in the log
    """
    logger.info(f"Event: {event_name}", extra={"event": event_name, **kwargs})


def log_api_request(
    logger: logging.Logger,
    method: str,
    path: str,
    status_code: int,
    duration_ms: float,
    **kwargs: Any,
) -> None:
    """
    Log an API request with standardized fields.

    Args:
        logger: Logger instance
        method: HTTP method
        path: Request path
        status_code: Response status code
        duration_ms: Request duration in milliseconds
        **kwargs: Additional context to include in the log
    """
    logger.info(
        f"API {method} {path} {status_code} ({duration_ms:.2f}ms)",
        extra={
            "http_method": method,
            "path": path,
            "status_code": status_code,
            "duration_ms": duration_ms,
            **kwargs,
        },
    )

================
File: video_processor/utils/profiling.py
================
"""Profiling utilities for performance monitoring and optimization."""
import functools
import time
import logging
from typing import Any, Callable, Dict, Optional, TypeVar, cast

# Type variable for decorator
F = TypeVar('F', bound=Callable[..., Any])

# Setup logger
logger = logging.getLogger(__name__)

# Global registry for timing statistics
_timing_stats: Dict[str, Dict[str, float]] = {
    "count": {},     # Number of calls
    "total_time": {},  # Total execution time
    "min_time": {},    # Minimum execution time
    "max_time": {},    # Maximum execution time
}


def timed(func: F) -> F:
    """Decorator to time function execution.
    
    Logs the execution time and maintains statistics in the global registry.
    
    Args:
        func: The function to time
        
    Returns:
        The wrapped function
    """
    @functools.wraps(func)
    def wrapper(*args: Any, **kwargs: Any) -> Any:
        func_name = f"{func.__module__}.{func.__qualname__}"
        start_time = time.time()
        
        try:
            result = func(*args, **kwargs)
            return result
        finally:
            end_time = time.time()
            execution_time = end_time - start_time
            
            # Log the execution time
            logger.info(f"{func_name} executed in {execution_time:.4f} seconds")
            
            # Update statistics
            if func_name not in _timing_stats["count"]:
                _timing_stats["count"][func_name] = 0
                _timing_stats["total_time"][func_name] = 0.0
                _timing_stats["min_time"][func_name] = float('inf')
                _timing_stats["max_time"][func_name] = 0.0
                
            _timing_stats["count"][func_name] += 1
            _timing_stats["total_time"][func_name] += execution_time
            _timing_stats["min_time"][func_name] = min(
                _timing_stats["min_time"][func_name], execution_time
            )
            _timing_stats["max_time"][func_name] = max(
                _timing_stats["max_time"][func_name], execution_time
            )
            
    return cast(F, wrapper)


class Timer:
    """Context manager for timing code blocks.
    
    Example:
        with Timer("processing_video"):
            # Code to time
            process_video(video_path)
    """
    
    def __init__(self, name: str):
        """Initialize timer.
        
        Args:
            name: Name of the operation being timed
        """
        self.name = name
        self.start_time: Optional[float] = None
        self.end_time: Optional[float] = None
        
    def __enter__(self) -> 'Timer':
        """Start timing when entering context.
        
        Returns:
            self: This Timer instance
        """
        self.start_time = time.time()
        return self
        
    def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None:
        """End timing when exiting context.
        
        Args:
            exc_type: Exception type if raised
            exc_val: Exception value if raised
            exc_tb: Exception traceback if raised
        """
        self.end_time = time.time()
        execution_time = self.end_time - self.start_time
        
        # Log the execution time
        logger.info(f"{self.name} executed in {execution_time:.4f} seconds")
        
        # Update statistics
        if self.name not in _timing_stats["count"]:
            _timing_stats["count"][self.name] = 0
            _timing_stats["total_time"][self.name] = 0.0
            _timing_stats["min_time"][self.name] = float('inf')
            _timing_stats["max_time"][self.name] = 0.0
            
        _timing_stats["count"][self.name] += 1
        _timing_stats["total_time"][self.name] += execution_time
        _timing_stats["min_time"][self.name] = min(
            _timing_stats["min_time"][self.name], execution_time
        )
        _timing_stats["max_time"][self.name] = max(
            _timing_stats["max_time"][self.name], execution_time
        )


def get_timing_stats() -> Dict[str, Dict[str, Any]]:
    """Get the current timing statistics.
    
    Returns:
        Dict containing timing statistics with calculated averages
    """
    result = {
        "count": _timing_stats["count"].copy(),
        "total_time": _timing_stats["total_time"].copy(),
        "min_time": _timing_stats["min_time"].copy(),
        "max_time": _timing_stats["max_time"].copy(),
        "avg_time": {},
    }
    
    # Calculate average times
    for func_name in result["count"]:
        if result["count"][func_name] > 0:
            result["avg_time"][func_name] = (
                result["total_time"][func_name] / result["count"][func_name]
            )
        else:
            result["avg_time"][func_name] = 0.0
            
    return result


def reset_timing_stats() -> None:
    """Reset all timing statistics."""
    global _timing_stats
    _timing_stats = {
        "count": {},
        "total_time": {},
        "min_time": {},
        "max_time": {},
    }


def print_timing_report() -> None:
    """Print a report of all timing statistics."""
    stats = get_timing_stats()
    
    print("\n===== TIMING REPORT =====")
    print(f"{'Function/Operation':<50} {'Count':<8} {'Avg(s)':<10} {'Min(s)':<10} {'Max(s)':<10} {'Total(s)':<10}")
    print("-" * 100)
    
    # Sort by total time (descending)
    sorted_funcs = sorted(
        stats["count"].keys(),
        key=lambda x: stats["total_time"].get(x, 0),
        reverse=True
    )
    
    for func_name in sorted_funcs:
        count = stats["count"][func_name]
        avg_time = stats["avg_time"][func_name]
        min_time = stats["min_time"][func_name]
        max_time = stats["max_time"][func_name]
        total_time = stats["total_time"][func_name]
        
        print(f"{func_name:<50} {count:<8} {avg_time:<10.4f} {min_time:<10.4f} {max_time:<10.4f} {total_time:<10.4f}")
        
    print("=" * 100)

================
File: video_processor/utils/youtube_auth.py
================
"""
YouTube authentication utilities.

This module provides utilities for YouTube OAuth authentication and token management.
"""

import json
import logging
import os
from typing import Any, Dict, List, Optional

import google.oauth2.credentials
import google_auth_oauthlib.flow
from google.auth.exceptions import RefreshError
from google.auth.transport.requests import Request
from googleapiclient.discovery import build

from video_processor.domain.exceptions import PublishingError

# Configure logger
logger = logging.getLogger(__name__)

# YouTube API constants
YOUTUBE_API_SERVICE_NAME = "youtube"
YOUTUBE_API_VERSION = "v3"
DEFAULT_SCOPES = [
    "https://www.googleapis.com/auth/youtube.upload",
    "https://www.googleapis.com/auth/youtube",
    "https://www.googleapis.com/auth/youtube.force-ssl",
]


def get_youtube_auth_url(
    client_secrets_file: str,
    scopes: Optional[List[str]] = None,
    redirect_uri: str = "http://localhost:8080",
) -> str:
    """
    Generate a YouTube authentication URL.

    Args:
        client_secrets_file: Path to the client secrets file
        scopes: List of OAuth scopes (default: upload and read)
        redirect_uri: OAuth redirect URI

    Returns:
        Authentication URL for the user to visit

    Raises:
        PublishingError: If the URL generation fails
    """
    try:
        scopes = scopes or DEFAULT_SCOPES

        # Create flow instance to manage OAuth 2.0 authorization flow
        flow = google_auth_oauthlib.flow.Flow.from_client_secrets_file(
            client_secrets_file, scopes=scopes
        )

        # Set redirect URI
        flow.redirect_uri = redirect_uri

        # Generate authorization URL
        auth_url, _ = flow.authorization_url(
            access_type="offline", prompt="consent", include_granted_scopes="true"
        )

        return auth_url

    except Exception as e:
        logger.error(f"Failed to generate YouTube auth URL: {str(e)}")
        raise PublishingError(f"Failed to generate YouTube auth URL: {str(e)}")


def exchange_code_for_token(
    client_secrets_file: str,
    code: str,
    redirect_uri: str = "http://localhost:8080",
    scopes: Optional[List[str]] = None,
    token_file: Optional[str] = None,
) -> Dict[str, Any]:
    """
    Exchange an authorization code for OAuth tokens.

    Args:
        client_secrets_file: Path to the client secrets file
        code: Authorization code from callback
        redirect_uri: OAuth redirect URI
        scopes: List of OAuth scopes (default: upload and read)
        token_file: Path to save the token (optional)

    Returns:
        Dictionary containing token information

    Raises:
        PublishingError: If the token exchange fails
    """
    try:
        scopes = scopes or DEFAULT_SCOPES

        # Create flow instance to manage OAuth 2.0 authorization flow
        flow = google_auth_oauthlib.flow.Flow.from_client_secrets_file(
            client_secrets_file, scopes=scopes
        )

        # Set redirect URI
        flow.redirect_uri = redirect_uri

        # Exchange authorization code for access and refresh tokens
        flow.fetch_token(code=code)

        # Get credentials from flow
        credentials = flow.credentials

        # Convert credentials to a dictionary
        token_data = {
            "token": credentials.token,
            "refresh_token": credentials.refresh_token,
            "token_uri": credentials.token_uri,
            "client_id": credentials.client_id,
            "client_secret": credentials.client_secret,
            "scopes": credentials.scopes,
        }

        # Save token to file if specified
        if token_file:
            save_token(token_data, token_file)

        return token_data

    except Exception as e:
        logger.error(f"Failed to exchange code for token: {str(e)}")
        raise PublishingError(f"Failed to exchange code for token: {str(e)}")


def save_token(token_data: Dict[str, Any], token_file: str) -> None:
    """
    Save token data to a file.

    Args:
        token_data: Dictionary containing token information
        token_file: Path to save the token

    Raises:
        PublishingError: If saving the token fails
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(os.path.abspath(token_file)), exist_ok=True)

        # Write token data to file
        with open(token_file, "w") as f:
            json.dump(token_data, f, indent=2)

        logger.info(f"Token saved to {token_file}")

    except Exception as e:
        logger.error(f"Failed to save token to {token_file}: {str(e)}")
        raise PublishingError(f"Failed to save token: {str(e)}")


def load_token(token_file: str) -> Dict[str, Any]:
    """
    Load token data from a file.

    Args:
        token_file: Path to the token file

    Returns:
        Dictionary containing token information

    Raises:
        PublishingError: If loading the token fails
    """
    try:
        if not os.path.exists(token_file):
            raise PublishingError(f"Token file not found: {token_file}")

        # Read token data from file
        with open(token_file, "r") as f:
            token_data = json.load(f)

        logger.info(f"Token loaded from {token_file}")
        return token_data

    except Exception as e:
        logger.error(f"Failed to load token from {token_file}: {str(e)}")
        raise PublishingError(f"Failed to load token: {str(e)}")


def refresh_token(token_file: str) -> Dict[str, Any]:
    """
    Refresh an OAuth token and update the token file.

    Args:
        token_file: Path to the token file

    Returns:
        Updated dictionary containing token information

    Raises:
        PublishingError: If refreshing the token fails
    """
    try:
        # Load current token data
        token_data = load_token(token_file)

        # Create credentials object
        credentials = google.oauth2.credentials.Credentials(
            token=token_data.get("token"),
            refresh_token=token_data.get("refresh_token"),
            token_uri=token_data.get(
                "token_uri", "https://oauth2.googleapis.com/token"
            ),
            client_id=token_data.get("client_id"),
            client_secret=token_data.get("client_secret"),
            scopes=token_data.get("scopes", DEFAULT_SCOPES),
        )

        # Refresh token
        credentials.refresh(Request())

        # Update token data
        token_data["token"] = credentials.token

        # Save updated token
        save_token(token_data, token_file)

        logger.info("Token refreshed successfully")
        return token_data

    except RefreshError as e:
        logger.error(f"Failed to refresh token, authentication required: {str(e)}")
        raise PublishingError(
            f"Failed to refresh token, re-authentication required: {str(e)}"
        )
    except Exception as e:
        logger.error(f"Failed to refresh token: {str(e)}")
        raise PublishingError(f"Failed to refresh token: {str(e)}")


def get_authenticated_service(
    token_file: str,
    auto_refresh: bool = True,
    api_service_name: str = YOUTUBE_API_SERVICE_NAME,
    api_version: str = YOUTUBE_API_VERSION,
) -> Any:
    """
    Build an authenticated YouTube API service.

    Args:
        token_file: Path to the token file
        auto_refresh: Whether to automatically refresh the token if expired
        api_service_name: YouTube API service name
        api_version: YouTube API version

    Returns:
        Authenticated YouTube API service

    Raises:
        PublishingError: If authentication fails
    """
    try:
        # Load token data
        token_data = load_token(token_file)

        # Create credentials object
        credentials = google.oauth2.credentials.Credentials(
            token=token_data.get("token"),
            refresh_token=token_data.get("refresh_token"),
            token_uri=token_data.get(
                "token_uri", "https://oauth2.googleapis.com/token"
            ),
            client_id=token_data.get("client_id"),
            client_secret=token_data.get("client_secret"),
            scopes=token_data.get("scopes", DEFAULT_SCOPES),
        )

        # Check if token is expired and refresh if needed
        if auto_refresh and not credentials.valid:
            try:
                credentials.refresh(Request())

                # Update token file with refreshed token
                token_data["token"] = credentials.token
                save_token(token_data, token_file)

                logger.info("Token refreshed automatically")

            except RefreshError as e:
                logger.error(
                    f"Failed to refresh token, authentication required: {str(e)}"
                )
                raise PublishingError(
                    f"Token expired and cannot be refreshed. Please re-authenticate: {str(e)}"
                )

        # Build and return authenticated service
        service = build(
            api_service_name,
            api_version,
            credentials=credentials,
            cache_discovery=False,
        )

        logger.info(f"Authenticated {api_service_name} service created")
        return service

    except Exception as e:
        logger.error(f"Failed to create authenticated service: {str(e)}")
        raise PublishingError(f"Failed to create authenticated service: {str(e)}")

================
File: video_processor/__init__.py
================
# This file makes the video_processor directory a Python package

================
File: video_processor/.gcloudignore
================
# Ignore local configs and noise
__pycache__/
*.pyc
.venv/
.env
.gcloudignore
.gitinore
client_secret.json

================
File: video_processor/.gitignore
================
# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
dev-debug.log

# Dependency directories
node_modules/

# Environment variables
.env

# Editor directories and files
.idea
.vscode
*.suo
*.ntvs*
*.njsproj
*.sln
*.sw?

# OS specific
.DS_Store

# Task files
tasks.json
tasks/

================
File: video_processor/conftest.py
================
"""
Global pytest fixtures and configuration.
"""

import os
from unittest.mock import MagicMock, patch

import pytest

# Set environment variables for testing
os.environ["GOOGLE_CLOUD_PROJECT"] = "automations-457120"

# For CI environment, we'll use the credentials file created by the workflow
# For local testing, we'll use the credentials file in the credentials directory
credentials_path = os.path.abspath(
    os.path.join(os.path.dirname(__file__), "../credentials/service_account.json")
)
if os.path.exists(credentials_path):
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = credentials_path
else:
    # In CI, the credentials file is created in the root directory
    ci_credentials_path = os.path.abspath(
        os.path.join(
            os.path.dirname(__file__), "../../credentials/service_account.json"
        )
    )
    if os.path.exists(ci_credentials_path):
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = ci_credentials_path
    else:
        # If no credentials file is found, we'll use mock authentication
        from unittest.mock import patch

        import pytest

        @pytest.fixture(autouse=True, scope="session")
        def mock_google_auth():
            """Mock Google Cloud authentication for all tests if no credentials
            file is found."""
            with patch(
                "google.auth.default", return_value=(None, "automations-457120")
            ):
                yield


# For tests that need mock storage client
@pytest.fixture
def mock_storage_client():
    """Mock for Google Cloud Storage client."""
    mock_client = MagicMock()
    mock_bucket = MagicMock()
    mock_blob = MagicMock()

    # Set up the mock chain
    mock_client.bucket.return_value = mock_bucket
    mock_bucket.blob.return_value = mock_blob

    # Return the mock objects for use in tests
    return mock_client, mock_bucket, mock_blob

================
File: video_processor/pytest.ini
================
[pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts = --verbose --cov=video_processor --cov=.. --cov-report=term-missing

# Custom markers
markers =
    integration: integration tests that rely on external services or emulators

================
File: video_processor/README.md
================
# Video Processor Service

A modular service for processing videos, extracting audio, generating transcripts, subtitles, and more with AI-powered features.

## Project Structure

The project follows a modular architecture design to improve maintainability, testability, and developer experience:

```
backend/video_processor/
├── api/                  # API endpoints and controllers
├── config/               # Configuration management
├── core/                 # Core domain logic
│   ├── models/           # Domain models
│   └── processors/       # Processing components
├── services/             # External service integrations
│   ├── ai/               # AI model integrations
│   ├── storage/          # Storage service (GCS, local)
│   └── youtube/          # YouTube API integration
├── utils/                # Shared utilities
└── tests/                # Tests organized by type
    ├── unit/             # Unit tests
    ├── integration/      # Integration tests
    └── e2e/              # End-to-end tests
```

## Architecture Overview

```mermaid
flowchart TD
    A[Upload MP4 to GCS Bucket] --> B[Eventarc Trigger]
    B --> C[Cloud Run Service]
    C --> D[Download Video]
    D --> E[Extract Audio with ffmpeg]
    E --> F[Create Part Object for Audio]
    F --> G[Process with Gemini API]
    G --> H1[Generate Transcript]
    G --> H2[Generate VTT Subtitles]
    G --> H3[Generate Shownotes]
    G --> H4[Generate Chapters]
    G --> H5[Generate Title & Keywords]
    H1 & H2 & H3 & H4 & H5 --> I[Upload Results to GCS]
    I --> J[Move Original Video to Processed Folder]
    I --> K[Trigger YouTube Upload Function]
```

## Key Design Features

1. **Dependency Injection**
   - Services and components receive dependencies rather than creating them
   - Improves testability and flexibility

2. **Interface-Based Design**
   - Components work with interfaces instead of concrete implementations
   - Enables easy swapping of implementations (e.g., GCS vs. local storage)

3. **Centralized Configuration**
   - Environment variables managed through dedicated system
   - Default values and validation in one place

4. **Error Handling**
   - Consistent error handling with custom exceptions
   - Retry mechanisms for transient failures

5. **Logging**
   - Structured logging throughout the application
   - Consistent format and levels

## Development Workflow

### Local Development

1. Set up your environment:

```bash
# Create a virtual environment
python -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Set environment variables for local testing
export TESTING_MODE=true
export LOCAL_OUTPUT=true
```

2. Run the service locally:

```bash
python -m video_processor.app
```

3. Test a video upload:
   - Use the `scripts/simulate_firestore_update.py` script, or
   - Make a POST request to the service

### Running Tests

Tests are organized by type:

```bash
# Run unit tests
pytest tests/unit

# Run integration tests
pytest tests/integration

# Run end-to-end tests
pytest tests/e2e

# Run comprehensive test
python scripts/run_comprehensive_test.py
```

## Deployment

The service can be deployed to Google Cloud Run:

```bash
# Deploy to Cloud Run
./deploy.sh

# Deploy in dry-run mode (no actual deployment)
./deploy.sh --dry-run

# Skip tests during deployment
./deploy.sh --skip-tests
```

## Service Components

### Storage Service

The `StorageService` interface abstracts storage operations, with implementations for:

- **GCSStorageService**: Google Cloud Storage
- **LocalStorageService**: Local filesystem (for testing and development)

### Video Processing

The core processing pipeline:

1. **Download Video**: Get video from storage
2. **Extract Audio**: Convert video to audio
3. **Generate Transcript**: Create full transcript
4. **Generate Subtitles**: Create subtitles in VTT format
5. **Generate Shownotes**: Create detailed notes
6. **Generate Chapters**: Create timestamped chapters
7. **Generate Title/Keywords**: Create optimized metadata
8. **Upload Results**: Store all outputs

### YouTube Integration

Handles automatic uploading to YouTube channels:

- Authentication using OAuth2
- Separate channel configurations
- Caption and metadata management

## API Integration

### Gemini API

The module uses Google's Gemini API through Vertex AI to process audio and generate various metadata:

1. **Transcript**: Full text transcript of the audio
2. **VTT Subtitles**: WebVTT format subtitles with timestamps
3. **Shownotes**: Detailed notes about the content
4. **Chapters**: Timestamped chapters with summaries
5. **Title & Keywords**: Engaging title and relevant keywords

### Audio Format Requirements

When sending audio to Gemini API:
- Use the `Part.from_data()` method to create a properly formatted Part object
- Specify the correct MIME type (e.g., "audio/wav")
- The audio should be in a supported format (WAV, MP3, etc.)

## Future Improvements

1. Add more robust error handling for different types of audio files and formats
2. Implement retry logic for API calls
3. Increase test coverage for edge cases and error conditions
4. Consider adding support for additional audio formats
5. Optimize audio extraction parameters for better quality
6. Add integration tests with actual GCS and Gemini API (using test credentials)
7. Implement CI/CD pipeline for automated testing
8. Add performance tests to measure processing time for different file sizes
9. Create a test data generator for creating test audio/video files with known content

================
File: video_processor/test_audio_processing.py
================
#!/usr/bin/env python3
"""
Test script to verify the audio processing with Gemini API.
"""

import logging
import os
import subprocess
import tempfile

import vertexai
from vertexai.preview.generative_models import GenerativeModel, Part

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)

# Set project ID directly for testing
PROJECT_ID = "automations-457120"  # Use the same project ID as in the main code
REGION = "us-central1"


def test_audio_processing():
    """Test audio processing with a sample WAV file."""
    # Initialize Vertex AI
    vertexai.init(project=PROJECT_ID, location=REGION)
    # Create a temporary directory for our test
    with tempfile.TemporaryDirectory() as tmpdir:
        # Create a simple test audio file using ffmpeg
        test_audio_path = os.path.join(tmpdir, "test_audio.wav")

        # Generate a simple test tone using ffmpeg
        logging.info(f"Generating test audio file at {test_audio_path}...")
        try:
            subprocess.run(
                [
                    "ffmpeg",
                    "-y",  # Overwrite output files without asking
                    "-f",
                    "lavfi",  # Use libavfilter
                    "-i",
                    "sine=frequency=440:duration=5",  # Generate a 5-second 440Hz tone
                    "-ar",
                    "16000",  # Audio sample rate
                    "-ac",
                    "1",  # Mono audio
                    test_audio_path,
                ],
                check=True,
                capture_output=True,
                text=True,
            )
            logging.info("Test audio generation complete.")
        except subprocess.CalledProcessError as e:
            logging.error(f"ffmpeg failed: {e}\nStderr: {e.stderr}")
            return

        # Read the audio file and create a proper Part object
        logging.info("Reading audio file and creating Part object...")
        try:
            with open(test_audio_path, "rb") as f:
                audio_bytes = f.read()

            # Create a Part object with the correct MIME type
            audio_part = Part.from_data(mime_type="audio/wav", data=audio_bytes)

            # Test the transcript generation with our own implementation
            logging.info("Testing transcript generation...")

            # Define a simple transcript function for testing
            def generate_test_transcript(audio_part):
                """Test function to generate a transcript from audio data."""
                transcription_model = GenerativeModel("gemini-2.0-flash-001")
                prompt = (
                    "Generate a transcription of the audio, only extract speech "
                    "and ignore background audio."
                )
                response = transcription_model.generate_content(
                    [prompt, audio_part],
                    generation_config={"temperature": 0.2},
                )
                return response.text.strip()

            transcript = generate_test_transcript(audio_part)
            logging.info(f"Generated transcript: {transcript}")

            logging.info("Test completed successfully!")
        except Exception as e:
            logging.error(f"Error during testing: {e}")


if __name__ == "__main__":
    test_audio_processing()

================
File: video_processor/test_process_video.py
================
#!/usr/bin/env python3
"""
Test script to verify the video processing with the fixed code.
"""

import logging
import os
import subprocess
import tempfile

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)


def test_process_video():
    """Test the process_video_event function with a sample MP4 file."""
    # Create a temporary directory for our test
    with tempfile.TemporaryDirectory() as tmpdir:
        # Create a simple test video file using ffmpeg
        test_video_path = os.path.join(tmpdir, "test_video.mp4")

        # Generate a simple test video using ffmpeg
        logging.info(f"Generating test video file at {test_video_path}...")
        try:
            subprocess.run(
                [
                    "ffmpeg",
                    "-y",  # Overwrite output files without asking
                    "-f",
                    "lavfi",  # Use libavfilter
                    "-i",
                    "sine=frequency=440:duration=5",  # Generate a 5-second 440Hz tone
                    "-f",
                    "lavfi",  # Use libavfilter for video
                    "-i",
                    "color=c=blue:s=640x480:d=5",  # Generate a 5-second blue screen
                    "-c:a",
                    "aac",  # Audio codec
                    "-c:v",
                    "h264",  # Video codec
                    test_video_path,
                ],
                check=True,
                capture_output=True,
                text=True,
            )
            logging.info("Test video generation complete.")
        except subprocess.CalledProcessError as e:
            logging.error(f"ffmpeg failed: {e}\nStderr: {e.stderr}")
            return

        # Create a mock GCS bucket and upload the test video
        bucket_name = "test-bucket"
        file_name = "daily-raw/test_video.mp4"

        # Mock the GCS operations for testing
        logging.info("Testing process_video_event function...")
        try:
            # In a real test, we would upload the file to GCS and then call
            # process_video_event
            # For this test, we'll just log that we would call the function
            logging.info(f"Would call process_video_event({bucket_name}, {file_name})")
            logging.info("Test completed successfully!")
        except Exception as e:
            logging.error(f"Error during testing: {e}")


if __name__ == "__main__":
    test_process_video()

================
File: video_processor/youtube_uploader_README.md
================
# YouTube Uploader

This component handles uploading processed videos to YouTube channels. It supports uploading to both a "Daily" channel and a "Main" channel.

## Overview

The YouTube uploader is implemented as a Cloud Function that is triggered by Cloud Storage events when a processed video is ready. It handles:

1. Detecting when a processed video is available
2. Downloading the video and associated metadata (description, captions, etc.)
3. Authenticating with the YouTube API
4. Uploading the video to the appropriate YouTube channel
5. Adding captions if available
6. Creating a marker file to prevent duplicate uploads

## Setup Instructions

### 1. Create OAuth Credentials

1. Go to the [Google Cloud Console](https://console.cloud.google.com/apis/credentials)
2. Create an OAuth 2.0 Client ID (Web application type)
3. Add `http://localhost:8080` as an authorized redirect URI
4. Download the JSON file and save it as `credentials/client_secret.json`

### 2. Generate OAuth Tokens

Run the token generator script for each channel:

```bash
# For the Daily channel
python -m video_processor.generate_youtube_token --channel daily

# For the Main channel
python -m video_processor.generate_youtube_token --channel main
```

Follow the prompts to authorize the application and obtain refresh tokens.

### 3. Set Up Secret Manager

Store the OAuth credentials in Secret Manager:

```bash
# Using the setup script
python -m video_processor.setup_youtube_secrets --client-secrets-file credentials/client_secret.json --refresh-token YOUR_REFRESH_TOKEN

# Or manually via the Google Cloud Console
# Go to: https://console.cloud.google.com/security/secret-manager
```

The following secrets need to be created:
- `youtube-daily-client-id`
- `youtube-daily-client-secret`
- `youtube-daily-refresh-token`
- `youtube-main-client-id`
- `youtube-main-client-secret`
- `youtube-refresh-token`

### 4. Deploy the Cloud Functions

The YouTube uploader is deployed as two separate Cloud Functions:

1. `upload-to-youtube-daily`: Triggered by files in the `processed-daily/` folder
2. `upload-to-youtube-main`: Triggered by files in the `processed-main/` folder

## Usage

The YouTube uploader is automatically triggered when a processed video is available. The workflow is:

1. Upload a raw video to the `daily-raw/` or `main-raw/` folder in the GCS bucket
2. The video processor processes the video and generates metadata
3. The processed files are stored in `processed-daily/` or `processed-main/`
4. The YouTube uploader is triggered and uploads the video to the appropriate channel

### Configuration Options

The YouTube uploader can be configured using environment variables:

- `DEFAULT_PRIVACY_STATUS`: Sets the default privacy status for uploaded videos (default: "unlisted")
  - Options: "private", "unlisted", "public"
  - Example: `DEFAULT_PRIVACY_STATUS=unlisted`

## File Structure

- `video_processor/youtube_uploader.py`: Main implementation of the YouTube uploader
- `video_processor/generate_youtube_token.py`: Script to generate OAuth tokens
- `video_processor/setup_youtube_secrets.py`: Script to set up Secret Manager secrets

## Troubleshooting

### Common Issues

1. **Authentication Errors**: Check that the OAuth credentials are correctly stored in Secret Manager.
2. **Missing Files**: Ensure that the video file and metadata files are correctly named and stored in the GCS bucket.
3. **Duplicate Uploads**: The uploader creates a marker file to prevent duplicate uploads. If you need to force a re-upload, delete the `uploaded.marker` file in the processed folder.

### Logs

Check the Cloud Functions logs for detailed error messages:

```bash
gcloud functions logs read upload-to-youtube-daily
gcloud functions logs read upload-to-youtube-main
```

## Testing

Run the unit tests:

```bash
cd video_processor
python -m pytest tests/test_youtube_uploader.py -v
```

================
File: video_processor.egg-info/dependency_links.txt
================


================
File: video_processor.egg-info/PKG-INFO
================
Metadata-Version: 2.4
Name: video_processor
Version: 0.1.0
Summary: Video processing pipeline for Echo project
Author: Echo Team
License: Proprietary
Project-URL: Homepage, https://github.com/yourusername/echo
Project-URL: Bug Tracker, https://github.com/yourusername/echo/issues
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: requests>=2.20.0
Requires-Dist: google-cloud-secret-manager>=2.16.0
Requires-Dist: google-cloud-storage==2.10.0
Requires-Dist: vertexai
Requires-Dist: ffmpeg-python
Requires-Dist: google-cloud-aiplatform
Requires-Dist: gunicorn
Requires-Dist: google-api-python-client>=2.80.0
Requires-Dist: google-auth-oauthlib>=1.0.0
Requires-Dist: google-auth-httplib2>=0.1.0
Requires-Dist: python-dotenv>=1.0.0
Requires-Dist: functions-framework
Requires-Dist: pydantic>=2.0.0
Requires-Dist: supabase>=2.0.0
Requires-Dist: fastapi>=0.100.0
Requires-Dist: uvicorn[standard]>=0.22.0
Provides-Extra: dev
Requires-Dist: pytest==7.4.0; extra == "dev"
Requires-Dist: pytest-mock==3.11.1; extra == "dev"
Requires-Dist: pytest-cov==4.1.0; extra == "dev"
Requires-Dist: black==24.1.1; extra == "dev"
Requires-Dist: ruff==0.2.1; extra == "dev"
Requires-Dist: mypy>=1.0.0; extra == "dev"
Requires-Dist: pre-commit>=3.0.0; extra == "dev"

# Video Processor API

The Video Processor API is a service for processing videos, generating transcripts, and creating metadata using AI capabilities.

This service implements a clean architecture approach, separating concerns into distinct layers to improve modularity, testability, and maintainability.

## Architecture

The project follows a clean architecture with the following layers:

### Domain Layer

The core business logic and entities, independent of any external frameworks or technologies:

- `video_processor/domain/models/` - Core business objects
- `video_processor/domain/exceptions.py` - Domain-specific exceptions

### Application Layer

Application services that orchestrate the use cases of the system:

- `video_processor/application/services/` - Implement business use cases
- `video_processor/application/interfaces/` - Define abstractions for external services
- `video_processor/application/dtos/` - Data transfer objects for service boundaries

### Adapters Layer

Implements interfaces defined in the application layer, adapting external services to the application:

- `video_processor/adapters/ai/` - AI service adapters (e.g., Gemini, Vertex AI)
- `video_processor/adapters/storage/` - Storage adapters (e.g., GCS, local)
- `video_processor/adapters/publishing/` - Publishing adapters (e.g., YouTube)

### Infrastructure Layer

Framework-specific code, configuration, and external dependencies:

- `video_processor/infrastructure/api/` - FastAPI implementation
- `video_processor/infrastructure/config/` - Configuration management
- `video_processor/infrastructure/repositories/` - Data storage implementations
- `video_processor/infrastructure/messaging/` - Messaging systems
- `video_processor/infrastructure/monitoring.py` - Logging and metrics

## Setup and Installation

### Prerequisites

- Python 3.10+
- FFmpeg (for video processing)
- Google Cloud credentials (for GCS, Secret Manager, etc.)

### Local Development Setup

1. Clone the repository:

```bash
git clone https://github.com/yourusername/echo.git
cd echo/api
```

2. Set up a virtual environment:

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:

```bash
pip install -e ".[dev]"
```

4. Set up environment variables:

```bash
cp .env.example .env
# Edit .env with your configuration
```

5. Run pre-commit hooks installation:

```bash
pre-commit install
```

### Running the API

#### Development Mode

```bash
uvicorn video_processor.infrastructure.api.main:app --reload
```

#### Production Mode

```bash
gunicorn video_processor.infrastructure.api.main:app -k uvicorn.workers.UvicornWorker -b 0.0.0.0:8080
```

### Docker

Build and run with Docker:

```bash
docker build -t video-processor-api .
docker run -p 8080:8080 video-processor-api
```

## Testing

### Running Tests

```bash
# Run all tests
pytest

# Run unit tests only
pytest tests/unit

# Run integration tests only
pytest tests/integration

# Run with coverage
pytest --cov=video_processor
```

### Test Configuration

Integration tests require some external services. You can set up test-specific credentials in `.env.test`.

## API Documentation

API documentation is available at `/docs` when the service is running. This includes detailed information about endpoints, request/response formats, and authentication requirements.

## Performance Optimization

The service includes several performance optimizations:

1. **AI Response Caching**: Cacheable AI responses are stored to reduce API calls and latency
2. **Parallel Processing**: Where applicable, operations are performed in parallel 
3. **Profiling Tools**: The service includes utilities to monitor performance and identify bottlenecks

## Authentication and Security

The service supports two authentication methods:

1. **JWT Tokens**: For user authentication
2. **API Keys**: For service-to-service communication

Role-based access control is implemented to restrict access to resources based on user roles.

## Deployment

The service is deployed to Google Cloud Run via the included Cloud Build configuration. The CI/CD pipeline includes:

1. Running tests
2. Building and pushing Docker images
3. Deployment to staging environment
4. Deployment verification
5. (For main branch) Deployment to production environment

## Architecture Diagrams

### High-Level Architecture

```
┌─────────────────┐      ┌─────────────────┐      ┌─────────────────┐
│                 │      │                 │      │                 │
│  Client Apps    │─────▶│  API Gateway    │─────▶│  Video Processor│
│                 │      │                 │      │                 │
└─────────────────┘      └─────────────────┘      └────────┬────────┘
                                                           │
                                                           │
                         ┌─────────────────┐      ┌────────▼────────┐
                         │                 │      │                 │
                         │  Storage (GCS)  │◀─────│  AI Services    │
                         │                 │      │                 │
                         └─────────────────┘      └─────────────────┘
```

### Clean Architecture Layers

```
┌─────────────────────────────────────────────────────────────┐
│                                                             │
│  Infrastructure Layer                                       │
│  (FastAPI, GCP Services, Database)                          │
│                                                             │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  Adapters Layer                                             │
│  (AI Adapters, Storage Adapters, Publishing Adapters)       │
│                                                             │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  Application Layer                                          │
│  (Services, Use Cases, Interfaces)                          │
│                                                             │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  Domain Layer                                               │
│  (Models, Business Logic, Exceptions)                       │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

## Contributing

1. Create a feature branch from `develop`
2. Make changes and add tests
3. Run tests and linters
4. Create a pull request

## License

Proprietary - All rights reserved.

================
File: video_processor.egg-info/requires.txt
================
requests>=2.20.0
google-cloud-secret-manager>=2.16.0
google-cloud-storage==2.10.0
vertexai
ffmpeg-python
google-cloud-aiplatform
gunicorn
google-api-python-client>=2.80.0
google-auth-oauthlib>=1.0.0
google-auth-httplib2>=0.1.0
python-dotenv>=1.0.0
functions-framework
pydantic>=2.0.0
supabase>=2.0.0
fastapi>=0.100.0
uvicorn[standard]>=0.22.0

[dev]
pytest==7.4.0
pytest-mock==3.11.1
pytest-cov==4.1.0
black==24.1.1
ruff==0.2.1
mypy>=1.0.0
pre-commit>=3.0.0

================
File: video_processor.egg-info/SOURCES.txt
================
README.md
pyproject.toml
video_processor/__init__.py
video_processor/conftest.py
video_processor/test_audio_processing.py
video_processor/test_process_video.py
video_processor.egg-info/PKG-INFO
video_processor.egg-info/SOURCES.txt
video_processor.egg-info/dependency_links.txt
video_processor.egg-info/requires.txt
video_processor.egg-info/top_level.txt

================
File: video_processor.egg-info/top_level.txt
================
video_processor

================
File: .pre-commit-config.yaml
================
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-toml
      - id: check-json
      - id: check-added-large-files
      - id: debug-statements
        language_version: python3

  - repo: https://github.com/psf/black
    rev: 24.1.1
    hooks:
      - id: black
        language_version: python3

  - repo: https://github.com/charliermarsh/ruff-pre-commit
    rev: v0.2.1
    hooks:
      - id: ruff
        args: [--fix, --exit-non-zero-on-fix]

  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.9.0
    hooks:
      - id: mypy
        additional_dependencies: [types-requests, pydantic]
        exclude: ^(tests/|docs/|setup.py$)

  - repo: local
    hooks:
      - id: pytest-check
        name: pytest-check
        entry: pytest tests/unit
        language: system
        pass_filenames: false
        always_run: true

================
File: .python-version
================
3.11

================
File: cloudbuild.yaml
================
steps:
  # Install dependencies and run tests
  - name: 'python:3.11-slim'
    id: 'install-and-test'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        pip install --upgrade pip
        pip install -e ".[dev]"
        pytest tests/unit

  # Run lint checks
  - name: 'python:3.11-slim'
    id: 'lint-checks'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        pip install --upgrade pip
        pip install -e ".[dev]"
        black --check video_processor tests
        ruff check video_processor tests
        mypy video_processor

  # Build the container image
  - name: 'gcr.io/cloud-builders/docker'
    id: 'build-image'
    args:
      - 'build'
      - '-t'
      - '${_REGION}-docker.pkg.dev/${PROJECT_ID}/${_REPOSITORY}/video-processor:$COMMIT_SHA'
      - '.'

  # Push the container image to Artifact Registry
  - name: 'gcr.io/cloud-builders/docker'
    id: 'push-image'
    args:
      - 'push'
      - '${_REGION}-docker.pkg.dev/${PROJECT_ID}/${_REPOSITORY}/video-processor:$COMMIT_SHA'

  # Deploy to Cloud Run
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: 'deploy-staging'
    entrypoint: 'gcloud'
    args:
      - 'run'
      - 'deploy'
      - 'video-processor-staging'
      - '--image'
      - '${_REGION}-docker.pkg.dev/${PROJECT_ID}/${_REPOSITORY}/video-processor:$COMMIT_SHA'
      - '--region'
      - '${_REGION}'
      - '--platform'
      - 'managed'
      - '--allow-unauthenticated'
      - '--memory'
      - '512Mi'
      - '--cpu'
      - '1'
      - '--min-instances'
      - '0'
      - '--max-instances'
      - '10'
      - '--set-env-vars'
      - 'ENVIRONMENT=staging,GOOGLE_CLOUD_PROJECT=${PROJECT_ID}'
      - '--service-account'
      - '${_SERVICE_ACCOUNT}'

  # Run deployment verification test
  - name: 'python:3.11-slim'
    id: 'test-staging-deployment'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        pip install --upgrade pip
        pip install -e ".[dev]"
        export API_BASE_URL="https://video-processor-staging-${_SERVICE_HASH}.a.run.app"
        export API_KEY="${_TEST_API_KEY}"
        pytest tests/e2e/test_deployment.py -v

  # Deploy to production (only on main branch)
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: 'deploy-production'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        if [[ "$BRANCH_NAME" == "main" ]]; then
          gcloud run deploy video-processor-production \
            --image ${_REGION}-docker.pkg.dev/${PROJECT_ID}/${_REPOSITORY}/video-processor:$COMMIT_SHA \
            --region ${_REGION} \
            --platform managed \
            --allow-unauthenticated \
            --memory 1Gi \
            --cpu 2 \
            --min-instances 1 \
            --max-instances 20 \
            --set-env-vars ENVIRONMENT=production,GOOGLE_CLOUD_PROJECT=${PROJECT_ID} \
            --service-account ${_SERVICE_ACCOUNT}
        else
          echo "Skipping production deployment, not on main branch"
        fi

# Tag the images with 'latest' for main branch
  - name: 'gcr.io/cloud-builders/docker'
    id: 'tag-latest'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        if [[ "$BRANCH_NAME" == "main" ]]; then
          docker tag ${_REGION}-docker.pkg.dev/${PROJECT_ID}/${_REPOSITORY}/video-processor:$COMMIT_SHA \
            ${_REGION}-docker.pkg.dev/${PROJECT_ID}/${_REPOSITORY}/video-processor:latest
          docker push ${_REGION}-docker.pkg.dev/${PROJECT_ID}/${_REPOSITORY}/video-processor:latest
        else
          echo "Skipping latest tag, not on main branch"
        fi

# Define variables that can be replaced by parameters in the build trigger or 
# default values if not specified
substitutions:
  _REGION: 'us-central1'
  _REPOSITORY: 'video-processor'
  _SERVICE_ACCOUNT: 'video-processor@${PROJECT_ID}.iam.gserviceaccount.com'
  _SERVICE_HASH: '${SHORT_SHA}'
  _TEST_API_KEY: 'test-api-key'

# Define image artifacts
images:
  - '${_REGION}-docker.pkg.dev/${PROJECT_ID}/${_REPOSITORY}/video-processor:$COMMIT_SHA'
  - '${_REGION}-docker.pkg.dev/${PROJECT_ID}/${_REPOSITORY}/video-processor:latest'

# Define timeout for build
timeout: '1800s'  # 30 minutes

# Enable parallel steps where possible
options:
  machineType: 'E2_HIGHCPU_8'
  diskSizeGb: '100'
  env:
    - 'PYTHONUNBUFFERED=1'

================
File: deploy.sh
================
#!/bin/bash
# Deployment script for the Video Processor application

# Exit on any error
set -e

# Configuration
PROJECT_ID="automations-457120"
REGION="us-east1"
SERVICE_NAME="video-processor"
ARTIFACT_REGISTRY="us-east1-docker.pkg.dev/${PROJECT_ID}/cloud-run-source-deploy"
IMAGE_NAME="${ARTIFACT_REGISTRY}/${SERVICE_NAME}"
IMAGE_TAG=$(date +%Y%m%d-%H%M%S)

# Colors for output
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Print with color
print_green() { echo -e "${GREEN}$1${NC}"; }
print_yellow() { echo -e "${YELLOW}$1${NC}"; }
print_red() { echo -e "${RED}$1${NC}"; }
print_blue() { echo -e "${BLUE}$1${NC}"; }

# Log file for deployment
LOG_DIR="logs"
LOG_FILE="${LOG_DIR}/deploy-$(date +%Y%m%d-%H%M%S).log"

# Create logs directory if it doesn't exist
mkdir -p ${LOG_DIR}

# Function to log messages to both console and log file
log() {
    echo "$(date +"%Y-%m-%d %H:%M:%S") - $1" | tee -a "${LOG_FILE}"
}

# Function to handle errors
handle_error() {
    print_red "\n====== ERROR ======"
    print_red "An error occurred during deployment at step: $1"
    print_red "Check the log file for details: ${LOG_FILE}"
    print_red "==================\n"
    exit 1
}

# Trap errors
trap 'handle_error "${BASH_COMMAND}"' ERR

# Parse command line arguments
DRY_RUN=false
SKIP_TESTS=false
VERBOSE=false

while [[ $# -gt 0 ]]; do
    case $1 in
        --dry-run)
            DRY_RUN=true
            shift
            ;;
        --skip-tests)
            SKIP_TESTS=true
            shift
            ;;
        --verbose)
            VERBOSE=true
            shift
            ;;
        *)
            print_red "Unknown option: $1"
            echo "Usage: $0 [--dry-run] [--skip-tests] [--verbose]"
            exit 1
            ;;
    esac
done

if [ "$DRY_RUN" = true ]; then
    print_blue "=== DRY RUN MODE - No actual deployment will occur ==="
fi

# Check if gcloud is installed
log "Checking if gcloud CLI is installed..."
if ! command -v gcloud &> /dev/null; then
    print_red "gcloud CLI is not installed. Please install it first."
    exit 1
fi
log "✓ gcloud CLI is installed"

# Check if docker is installed
log "Checking if Docker is installed..."
if ! command -v docker &> /dev/null; then
    print_red "Docker is not installed. Please install it first."
    exit 1
fi
log "✓ Docker is installed"

# Ensure we're authenticated to GCP
log "Checking GCP authentication..."
GCP_ACCOUNT=$(gcloud auth list --filter=status:ACTIVE --format="value(account)")
if [ -z "$GCP_ACCOUNT" ]; then
    print_red "Not authenticated to GCP. Please run 'gcloud auth login' first."
    exit 1
fi
log "✓ Authenticated to GCP as $GCP_ACCOUNT"

# Set the GCP project
log "Setting GCP project to ${PROJECT_ID}..."
gcloud config set project ${PROJECT_ID}
log "✓ GCP project set to ${PROJECT_ID}"

# Validate project configuration
log "Validating project configuration..."
PROJECT_NUMBER=$(gcloud projects describe ${PROJECT_ID} --format="value(projectNumber)")
if [ -z "$PROJECT_NUMBER" ]; then
    print_red "Failed to get project number. Check if the project ID is correct."
    exit 1
fi
log "✓ Project number: ${PROJECT_NUMBER}"

# Check if the service account exists
SA_EMAIL="${SERVICE_NAME}-sa@${PROJECT_ID}.iam.gserviceaccount.com"
log "Checking if service account ${SA_EMAIL} exists..."
if ! gcloud iam service-accounts describe ${SA_EMAIL} &> /dev/null; then
    log "Service account does not exist. Creating it..."
    if [ "$DRY_RUN" = false ]; then
        gcloud iam service-accounts create "${SERVICE_NAME}-sa" \
            --display-name="${SERVICE_NAME} Service Account"
        log "✓ Service account created"
    else
        log "[DRY RUN] Would create service account ${SA_EMAIL}"
    fi
else
    log "✓ Service account exists"
fi

# Run tests before deployment
if [ "$SKIP_TESTS" = false ]; then
    log "Running tests..."
    cd video_processor
    python -m pytest tests/test_youtube_uploader.py tests/test_generate_youtube_token.py tests/test_main.py -v | tee -a "../${LOG_FILE}" || {
        print_red "Tests failed. Aborting deployment."
        cd ..
        exit 1
    }
    cd ..
    log "✓ All tests passed"
else
    log "Skipping tests as requested"
fi

# Build and test Docker image locally before deployment
log "Building Docker image locally for testing..."
LOCAL_IMAGE_NAME="${SERVICE_NAME}-local:${IMAGE_TAG}"
if [ "$DRY_RUN" = false ]; then
    docker build -t ${LOCAL_IMAGE_NAME} . | tee -a "${LOG_FILE}"
    log "✓ Docker image built successfully"

    # Run the Docker image locally for a quick test
    log "Running Docker image locally for a quick test..."
    CONTAINER_ID=$(docker run -d -p 8080:8080 ${LOCAL_IMAGE_NAME})
    sleep 5  # Give the container time to start

    # Check if the container is running
    if docker ps -q --filter "id=${CONTAINER_ID}" | grep -q .; then
        log "✓ Container is running"

        # Send a simple request to check if the app is responding
        log "Sending a test request to the container..."
        if curl -s -o /dev/null -w "%{http_code}" http://localhost:8080/ | grep -q "200\|204\|404"; then
            log "✓ Container is responding to requests"
        else
            print_yellow "Warning: Container is not responding to requests. This might be expected if the app only accepts POST requests."
        fi

        # Stop and remove the container
        docker stop ${CONTAINER_ID} > /dev/null
        docker rm ${CONTAINER_ID} > /dev/null
        log "Stopped and removed test container"
    else
        print_red "Container failed to start. Check the Docker logs."
        docker logs ${CONTAINER_ID} | tee -a "${LOG_FILE}"
        exit 1
    fi
else
    log "[DRY RUN] Would build and test Docker image ${LOCAL_IMAGE_NAME}"
fi

# Deploy to Cloud Run
log "Deploying to Cloud Run from source..."
if [ "$DRY_RUN" = false ]; then
    # Use a deployment ID to track this deployment
    DEPLOY_ID=$(date +%Y%m%d-%H%M%S)
    log "Deployment ID: ${DEPLOY_ID}"

    # Deploy to Cloud Run
    gcloud run deploy ${SERVICE_NAME} \
        --source . \
        --platform managed \
        --region ${REGION} \
        --allow-unauthenticated \
        --memory 2Gi \
        --cpu 2 \
        --timeout 3600 \
        --concurrency 10 \
        --labels="deploy-id=${DEPLOY_ID}" | tee -a "${LOG_FILE}"

    log "✓ Deployment to Cloud Run completed"

    # Get the service URL
    SERVICE_URL=$(gcloud run services describe ${SERVICE_NAME} --region=${REGION} --format='value(status.url)')
    log "✓ Service URL: ${SERVICE_URL}"

    # Check if the service is responding
    log "Checking if the service is responding..."
    if curl -s -o /dev/null -w "%{http_code}" ${SERVICE_URL} | grep -q "200\|204\|404"; then
        log "✓ Service is responding to requests"
    else
        print_yellow "Warning: Service is not responding to requests. This might be expected if the app only accepts POST requests."
    fi
else
    log "[DRY RUN] Would deploy to Cloud Run with the following command:"
    log "gcloud run deploy ${SERVICE_NAME} --source . --platform managed --region ${REGION} ..."
fi

# Set up Eventarc trigger (if it doesn't exist)
log "Checking if Eventarc trigger exists..."
if ! gcloud eventarc triggers describe video-processor-trigger --location=${REGION} &> /dev/null; then
    log "Creating Eventarc trigger..."
    if [ "$DRY_RUN" = false ]; then
        gcloud eventarc triggers create video-processor-trigger \
            --location=${REGION} \
            --destination-run-service=${SERVICE_NAME} \
            --destination-run-region=${REGION} \
            --event-filters="type=google.cloud.storage.object.v1.finalized" \
            --event-filters="bucket=automations-youtube-videos-2025" \
            --service-account="${SA_EMAIL}" | tee -a "${LOG_FILE}"
        log "✓ Eventarc trigger created"
    else
        log "[DRY RUN] Would create Eventarc trigger"
    fi
else
    log "✓ Eventarc trigger already exists"
fi

# Check deployment logs
log "Fetching recent logs from Cloud Run service..."
if [ "$DRY_RUN" = false ]; then
    gcloud logging read "resource.type=cloud_run_revision AND resource.labels.service_name=${SERVICE_NAME}" \
        --limit=10 \
        --format="table(timestamp, severity, textPayload)" | tee -a "${LOG_FILE}"
else
    log "[DRY RUN] Would fetch logs from Cloud Run service"
fi

print_green "\n====== DEPLOYMENT SUMMARY ======"
print_green "✓ Deployment completed successfully!"
if [ "$DRY_RUN" = false ]; then
    print_green "✓ Service URL: ${SERVICE_URL}"
    print_green "✓ Log file: ${LOG_FILE}"
    print_green "To view logs in real-time, run:"
    print_blue "  gcloud logging read 'resource.type=cloud_run_revision AND resource.labels.service_name=${SERVICE_NAME}' --limit=50 --format='table(timestamp, severity, textPayload)' --follow"
else
    print_blue "This was a dry run. No actual deployment was performed."
fi
print_green "==============================\n"

================
File: Dockerfile
================
# ---- Build Stage ----
FROM python:3.11-slim AS builder

# Install build dependencies
RUN apt-get update && \
    apt-get install -y --no-install-recommends gcc python3-dev && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /build

# Copy pyproject.toml
COPY pyproject.toml .

# Install build tools
RUN pip install --no-cache-dir build wheel

# Copy source code
COPY video_processor/ ./video_processor/

# Build wheel package
RUN pip wheel --no-cache-dir --wheel-dir /wheels -e .

# ---- Runtime Stage ----
FROM python:3.11-slim

# Install ffmpeg system package
RUN apt-get update && \
    apt-get install -y --no-install-recommends ffmpeg && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /app

# Create non-root user
RUN groupadd -r appuser && useradd -r -g appuser appuser

# Copy wheels from builder
COPY --from=builder /wheels /wheels

# Install application and dependencies
RUN pip install --no-cache-dir /wheels/* && \
    rm -rf /wheels

# Copy necessary files for running
COPY tests/ /app/tests/
COPY scripts/ /app/scripts/
COPY video_processor/infrastructure/api/server.py /app/server.py
COPY video_processor/main.py /app/main.py

# Set environment variables
ENV PORT=8080
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1

# Expose port
EXPOSE ${PORT}

# Switch to non-root user
USER appuser

# Create runtime configuration - FastAPI server by default
CMD exec gunicorn --bind :${PORT} --workers 1 --threads 8 --timeout 0 \
    --worker-class uvicorn.workers.UvicornWorker \
    "server:create_app()"

================
File: Dockerfile.mock
================
# Use an official Python runtime as the base image
FROM python:3.11-slim

# Set working directory
WORKDIR /app

# Install Flask and other dependencies
RUN pip install --no-cache-dir flask requests

# Copy the mock service script
COPY scripts/mock_gcs_service.py /app/

# Expose port
EXPOSE 8081
ENV PORT=8081

# Command to run the mock service
CMD ["python", "mock_gcs_service.py"]

================
File: Makefile
================
lint:
	ruff check .

format:
	black .
	ruff check --fix .

test:
	pytest

check: format lint test

install-dev:
	pip install -e ".[dev]"

run-local:
	uvicorn video_processor.infrastructure.api.server:app --reload --port 8080

.PHONY: lint format test check install-dev run-local

================
File: pyproject.toml
================
[build-system]
requires = ["setuptools>=61.0.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "video_processor"
version = "0.1.0"
description = "Video processing pipeline for Echo project"
readme = "README.md"
requires-python = ">=3.10"
license = {text = "Proprietary"}
authors = [
    {name = "Echo Team"}
]
classifiers = [
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
]

dependencies = [
    # Core dependencies
    "requests>=2.20.0",
    "google-cloud-secret-manager>=2.16.0",
    "google-cloud-storage==2.10.0",
    "vertexai",
    "ffmpeg-python",
    "google-cloud-aiplatform",
    "gunicorn",
    
    # YouTube API dependencies
    "google-api-python-client>=2.80.0",
    "google-auth-oauthlib>=1.0.0",
    "google-auth-httplib2>=0.1.0",
    "python-dotenv>=1.0.0",
    
    # Cloud Functions dependencies
    "functions-framework",
    
    # Data validation
    "pydantic>=2.0.0",
    
    # Authentication
    "supabase>=2.0.0",
    
    # FastAPI for new API implementation
    "fastapi>=0.100.0",
    "uvicorn[standard]>=0.22.0",
]

[project.optional-dependencies]
dev = [
    # Testing dependencies
    "pytest==7.4.0",
    "pytest-mock==3.11.1",
    "pytest-cov==4.1.0",
    
    # Development tools
    "black==24.1.1",
    "ruff==0.2.1",
    "mypy>=1.0.0",
    "pre-commit>=3.0.0",
]

[project.urls]
"Homepage" = "https://github.com/parkerrex/echo"
"Bug Tracker" = "https://github.com/parkerrex/echo/issues"
    
[tool.setuptools]
packages = ["video_processor"]

[tool.pytest]
testpaths = ["tests"]
python_files = "test_*.py"
python_classes = "Test*"
python_functions = "test_*"
addopts = "--verbose --cov=video_processor --cov-report=term-missing"
markers = [
    "integration: integration tests that rely on external services or emulators",
]

[tool.black]
line-length = 88
target-version = ["py310", "py311", "py312"]
include = '\.pyi?$'

[tool.ruff]
line-length = 88
target-version = "py310"
select = ["E", "F", "I", "W", "N", "B", "A", "C4", "SIM", "ERA"]
ignore = ["E203"]

[tool.ruff.isort]
known-first-party = ["video_processor"]

[tool.mypy]
python_version = "3.10"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = false
no_implicit_optional = true
strict_optional = true

[[tool.mypy.overrides]]
module = "tests.*"
disallow_untyped_defs = false
disallow_incomplete_defs = false

================
File: python_settings.txt
================
{
    // ... (keep all existing non-Python settings)

    "[python]": {
        "editor.defaultFormatter": "ms-python.black-formatter",
        "editor.formatOnSave": true,
        "editor.codeActionsOnSave": {
            "source.fixAll.ruff": "explicit",
            "source.organizeImports.ruff": "explicit"
        }
    },
    "python.linting.enabled": true,
    "python.linting.ruffEnabled": true,
    "python.linting.pylintEnabled": false,
    "python.linting.flake8Enabled": false,
    "python.formatting.provider": "none",
    
    // Additional Python settings
    "files.associations": {
        "*.py": "python",
        "pyproject.toml": "toml",
        "requirements.txt": "pip-requirements"
    },
    
    // Match Black/Ruff configuration
    "black-formatter.args": ["--line-length", "88"],
    "ruff.lint.args": ["--line-length", "88"],
    
    // Python-specific exclusions (already in files.watcherExclude)
    "python.analysis.exclude": [
        "**/venv/**",
        "**/.venv/**",
        "**/node_modules/**"
    ]
}

================
File: README.md
================
# Video Processor API

The Video Processor API is a service for processing videos, generating transcripts, and creating metadata using AI capabilities.

This service implements a clean architecture approach, separating concerns into distinct layers to improve modularity, testability, and maintainability.

## Architecture

The project follows a clean architecture with the following layers:

### Domain Layer

The core business logic and entities, independent of any external frameworks or technologies:

- `video_processor/domain/models/` - Core business objects
- `video_processor/domain/exceptions.py` - Domain-specific exceptions

### Application Layer

Application services that orchestrate the use cases of the system:

- `video_processor/application/services/` - Implement business use cases
- `video_processor/application/interfaces/` - Define abstractions for external services
- `video_processor/application/dtos/` - Data transfer objects for service boundaries

### Adapters Layer

Implements interfaces defined in the application layer, adapting external services to the application:

- `video_processor/adapters/ai/` - AI service adapters (e.g., Gemini, Vertex AI)
- `video_processor/adapters/storage/` - Storage adapters (e.g., GCS, local)
- `video_processor/adapters/publishing/` - Publishing adapters (e.g., YouTube)

### Infrastructure Layer

Framework-specific code, configuration, and external dependencies:

- `video_processor/infrastructure/api/` - FastAPI implementation
- `video_processor/infrastructure/config/` - Configuration management
- `video_processor/infrastructure/repositories/` - Data storage implementations
- `video_processor/infrastructure/messaging/` - Messaging systems
- `video_processor/infrastructure/monitoring.py` - Logging and metrics

## Setup and Installation

### Prerequisites

- Python 3.10+
- FFmpeg (for video processing)
- Google Cloud credentials (for GCS, Secret Manager, etc.)

### Local Development Setup

1. Clone the repository:

```bash
git clone https://github.com/yourusername/echo.git
cd echo/api
```

2. Set up a virtual environment:

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:

```bash
pip install -e ".[dev]"
```

4. Set up environment variables:

```bash
cp .env.example .env
# Edit .env with your configuration
```

5. Run pre-commit hooks installation:

```bash
pre-commit install
```

### Running the API

#### Development Mode

```bash
uvicorn video_processor.infrastructure.api.main:app --reload
```

#### Production Mode

```bash
gunicorn video_processor.infrastructure.api.main:app -k uvicorn.workers.UvicornWorker -b 0.0.0.0:8080
```

### Docker

Build and run with Docker:

```bash
docker build -t video-processor-api .
docker run -p 8080:8080 video-processor-api
```

## Testing

### Running Tests

```bash
# Run all tests
pytest

# Run unit tests only
pytest tests/unit

# Run integration tests only
pytest tests/integration

# Run with coverage
pytest --cov=video_processor
```

### Test Configuration

Integration tests require some external services. You can set up test-specific credentials in `.env.test`.

## API Documentation

API documentation is available at `/docs` when the service is running. This includes detailed information about endpoints, request/response formats, and authentication requirements.

## Performance Optimization

The service includes several performance optimizations:

1. **AI Response Caching**: Cacheable AI responses are stored to reduce API calls and latency
2. **Parallel Processing**: Where applicable, operations are performed in parallel 
3. **Profiling Tools**: The service includes utilities to monitor performance and identify bottlenecks

## Authentication and Security

The service supports two authentication methods:

1. **JWT Tokens**: For user authentication
2. **API Keys**: For service-to-service communication

Role-based access control is implemented to restrict access to resources based on user roles.

## Deployment

The service is deployed to Google Cloud Run via the included Cloud Build configuration. The CI/CD pipeline includes:

1. Running tests
2. Building and pushing Docker images
3. Deployment to staging environment
4. Deployment verification
5. (For main branch) Deployment to production environment

## Architecture Diagrams

### High-Level Architecture

```
┌─────────────────┐      ┌─────────────────┐      ┌─────────────────┐
│                 │      │                 │      │                 │
│  Client Apps    │─────▶│  API Gateway    │─────▶│  Video Processor│
│                 │      │                 │      │                 │
└─────────────────┘      └─────────────────┘      └────────┬────────┘
                                                           │
                                                           │
                         ┌─────────────────┐      ┌────────▼────────┐
                         │                 │      │                 │
                         │  Storage (GCS)  │◀─────│  AI Services    │
                         │                 │      │                 │
                         └─────────────────┘      └─────────────────┘
```

### Clean Architecture Layers

```
┌─────────────────────────────────────────────────────────────┐
│                                                             │
│  Infrastructure Layer                                       │
│  (FastAPI, GCP Services, Database)                          │
│                                                             │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  Adapters Layer                                             │
│  (AI Adapters, Storage Adapters, Publishing Adapters)       │
│                                                             │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  Application Layer                                          │
│  (Services, Use Cases, Interfaces)                          │
│                                                             │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  Domain Layer                                               │
│  (Models, Business Logic, Exceptions)                       │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

## Contributing

1. Create a feature branch from `develop`
2. Make changes and add tests
3. Run tests and linters
4. Create a pull request

## License

Proprietary - All rights reserved.

# Quick setup command for developers:
source ./setup.sh

================
File: setup.sh
================
# Add this to your Makefile for easy setup
echo "Setting up development environment..."
python -m venv venv
source venv/bin/activate
pip install --upgrade pip
pip install -e ".[dev]"
pre-commit install
echo "Setup complete! Use \"source venv/bin/activate\" to activate the environment."
