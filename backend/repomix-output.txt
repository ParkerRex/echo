This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-05-03T02:27:44.721Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

================================================================
Directory Structure
================================================================
scripts/
  create_sample_videos_firestore.py
  deploy_youtube_uploader.sh
  example_prd.txt
  local_test.sh
  mock_gcs_service.py
  prd.txt
  real_api_test.py
  run_comprehensive_test.py
  simulate_firestore_update.py
  test_locally.py
tests/
  e2e/
    __init__.py
  integration/
    __init__.py
  outdated/
    test_main.py
    test_process_video_event.py
    test_transcript_generation.py
  unit/
    __init__.py
    test_api.py
    test_audio_processor.py
    test_storage_service.py
    test_video_processor.py
  conftest.py
  README.md
video_processor/
  api/
    __init__.py
    controllers.py
    routes.py
    schemas.py
  config/
    __init__.py
    environment.py
    settings.py
  core/
    models/
      __init__.py
      video_job.py
    processors/
      __init__.py
      audio.py
      chapters.py
      transcript.py
      video.py
    __init__.py
  services/
    storage/
      __init__.py
      base.py
      factory.py
      gcs.py
      local.py
    __init__.py
  tests/
    outdated/
      test_process_video_event.py
    unit/
      test_video_processor.py
    __init__.py
    conftest.py
    test_chapters_generation.py
    test_firestore_trigger_listener.py
    test_generate_youtube_token.py
    test_titles_generation.py
    test_vtt_generation.py
    test_youtube_uploader.py
  utils/
    __init__.py
    error_handling.py
    file_handling.py
    logging.py
  __init__.py
  .gcloudignore
  .gitignore
  app.py
  conftest.py
  firestore_trigger_listener.py
  generate_youtube_token.py
  main.py
  process_uploaded_video.py
  pytest.ini
  README.md
  setup_youtube_secrets.py
  setup.py
  test_audio_processing.py
  test_process_video.py
  youtube_uploader_README.md
  youtube_uploader.py
deploy.sh
Dockerfile
Dockerfile.mock
Makefile
pyproject.toml
python_settings.txt
README.md
requirements.txt

================================================================
Files
================================================================

================
File: scripts/create_sample_videos_firestore.py
================
import os

from google.cloud import firestore

# Path to service account key
SERVICE_ACCOUNT_PATH = os.path.abspath(
    os.path.join(os.path.dirname(__file__), "../../credentials/service_account.json")
)


def main():
    # Set the environment variable for authentication
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = SERVICE_ACCOUNT_PATH

    # Initialize Firestore client
    db = firestore.Client()

    # Sample video documents
    videos = [
        {
            "title": "My First Video",
            "status": "Thumbnail Generation",
            "channel": "Main",
            "scheduledTime": "2025-05-01T10:00:00Z",
            "metadata": {"scheduled_time": "2025-05-01T10:00:00Z"},
        },
        {
            "title": "AI News Daily",
            "status": "Transcription",
            "channel": "Daily",
            "scheduledTime": "2025-05-02T09:00:00Z",
            "metadata": {"scheduled_time": "2025-05-02T09:00:00Z"},
        },
    ]

    # Add each video as a document in the "videos" collection
    for video in videos:
        doc_ref = db.collection("videos").document()
        doc_ref.set(video)
        print(f"Added video: {video['title']} (ID: {doc_ref.id})")


if __name__ == "__main__":
    main()

================
File: scripts/deploy_youtube_uploader.sh
================
#!/bin/bash
# Script to deploy the YouTube uploader Cloud Functions

set -e  # Exit on error

# Configuration
PROJECT_ID="automations-457120"
REGION="us-east1"
BUCKET_NAME="automations-videos"
SERVICE_ACCOUNT="vps-automations@automations-457120.iam.gserviceaccount.com"
DEFAULT_PRIVACY_STATUS="unlisted"  # Options: private, unlisted, public

# Colors for output
GREEN='\033[0;32m'
RED='\033[0;31m'
YELLOW='\033[0;33m'
NC='\033[0m' # No Color

echo -e "${YELLOW}Deploying YouTube Uploader Cloud Functions...${NC}"

# Check if gcloud is installed
if ! command -v gcloud &> /dev/null; then
    echo -e "${RED}Error: gcloud CLI is not installed. Please install it first.${NC}"
    exit 1
fi

# Check if user is authenticated
if ! gcloud auth list --filter=status:ACTIVE --format="value(account)" &> /dev/null; then
    echo -e "${RED}Error: Not authenticated with gcloud. Please run 'gcloud auth login' first.${NC}"
    exit 1
fi

# Check if the project is set
CURRENT_PROJECT=$(gcloud config get-value project)
if [ "$CURRENT_PROJECT" != "$PROJECT_ID" ]; then
    echo -e "${YELLOW}Setting project to $PROJECT_ID...${NC}"
    gcloud config set project $PROJECT_ID
fi

# Deploy the Daily channel uploader
echo -e "${YELLOW}Deploying upload-to-youtube-daily function...${NC}"
gcloud functions deploy upload-to-youtube-daily \
    --gen2 \
    --runtime=python311 \
    --region=$REGION \
    --source=. \
    --entry-point=upload_to_youtube_daily \
    --trigger-event-filters="type=google.cloud.storage.object.v1.finalized" \
    --trigger-event-filters="bucket=$BUCKET_NAME" \
    --service-account=$SERVICE_ACCOUNT \
    --memory=512MB \
    --timeout=540s \
    --set-env-vars="DEFAULT_PRIVACY_STATUS=$DEFAULT_PRIVACY_STATUS"

# Deploy the Main channel uploader
echo -e "${YELLOW}Deploying upload-to-youtube-main function...${NC}"
gcloud functions deploy upload-to-youtube-main \
    --gen2 \
    --runtime=python311 \
    --region=$REGION \
    --source=. \
    --entry-point=upload_to_youtube_main \
    --trigger-event-filters="type=google.cloud.storage.object.v1.finalized" \
    --trigger-event-filters="bucket=$BUCKET_NAME" \
    --service-account=$SERVICE_ACCOUNT \
    --memory=512MB \
    --timeout=540s \
    --set-env-vars="DEFAULT_PRIVACY_STATUS=$DEFAULT_PRIVACY_STATUS"

echo -e "${GREEN}Deployment complete!${NC}"
echo -e "${YELLOW}Note: Make sure the service account has the necessary permissions:${NC}"
echo "  - Secret Manager Secret Accessor"
echo "  - Storage Object Admin"
echo "  - YouTube Data API access"

echo -e "${GREEN}YouTube uploader functions are now deployed and will be triggered automatically when videos are processed.${NC}"

================
File: scripts/example_prd.txt
================
<context>
# Overview  
[Provide a high-level overview of your product here. Explain what problem it solves, who it's for, and why it's valuable.]

# Core Features  
[List and describe the main features of your product. For each feature, include:
- What it does
- Why it's important
- How it works at a high level]

# User Experience  
[Describe the user journey and experience. Include:
- User personas
- Key user flows
- UI/UX considerations]
</context>
<PRD>
# Technical Architecture  
[Outline the technical implementation details:
- System components
- Data models
- APIs and integrations
- Infrastructure requirements]

# Development Roadmap  
[Break down the development process into phases:
- MVP requirements
- Future enhancements
- Do not think about timelines whatsoever -- all that matters is scope and detailing exactly what needs to be build in each phase so it can later be cut up into tasks]

# Logical Dependency Chain
[Define the logical order of development:
- Which features need to be built first (foundation)
- Getting as quickly as possible to something usable/visible front end that works
- Properly pacing and scoping each feature so it is atomic but can also be built upon and improved as development approaches]

# Risks and Mitigations  
[Identify potential risks and how they'll be addressed:
- Technical challenges
- Figuring out the MVP that we can build upon
- Resource constraints]

# Appendix  
[Include any additional information:
- Research findings
- Technical specifications]
</PRD>

================
File: scripts/local_test.sh
================
#!/bin/bash
# Script to run local tests for the Video Processor application

# Colors for output
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Print with color
print_green() { echo -e "${GREEN}$1${NC}"; }
print_yellow() { echo -e "${YELLOW}$1${NC}"; }
print_red() { echo -e "${RED}$1${NC}"; }
print_blue() { echo -e "${BLUE}$1${NC}"; }

# Create test data directory if it doesn't exist
mkdir -p test_data

# Check if Docker and Docker Compose are installed
if ! command -v docker &> /dev/null; then
    print_red "Docker is not installed. Please install it first."
    exit 1
fi

if ! command -v docker-compose &> /dev/null; then
    print_red "Docker Compose is not installed. Please install it first."
    exit 1
fi

# Parse command line arguments
REBUILD=false
CLEAN=false

while [[ $# -gt 0 ]]; do
    case $1 in
        --rebuild)
            REBUILD=true
            shift
            ;;
        --clean)
            CLEAN=true
            shift
            ;;
        *)
            print_red "Unknown option: $1"
            echo "Usage: $0 [--rebuild] [--clean]"
            exit 1
            ;;
    esac
done

# Clean up if requested
if [ "$CLEAN" = true ]; then
    print_yellow "Cleaning up Docker containers and volumes..."
    docker-compose down -v
    exit 0
fi

# Build and start the services
if [ "$REBUILD" = true ]; then
    print_yellow "Rebuilding and starting services..."
    docker-compose up --build -d
else
    print_yellow "Starting services..."
    docker-compose up -d
fi

# Wait for services to start
print_yellow "Waiting for services to start..."
sleep 5

# Check if services are running
if ! docker-compose ps | grep -q "Up"; then
    print_red "Services failed to start. Check the logs with 'docker-compose logs'."
    exit 1
fi

print_green "Services are running!"
print_green "Video Processor: http://localhost:8080"
print_green "Mock GCS Service: http://localhost:8081"

# Show how to trigger a test event
print_blue "\nTo trigger a test event, run:"
print_blue "curl -X POST http://localhost:8081/trigger -H \"Content-Type: application/json\" -d '{\"bucket\":\"automations-videos\",\"name\":\"test-video.mp4\"}'"

# Show how to view logs
print_blue "\nTo view logs, run:"
print_blue "docker-compose logs -f"

print_green "\nPress Ctrl+C to exit..."
# Keep the script running to make it easy to stop with Ctrl+C
docker-compose logs -f

================
File: scripts/mock_gcs_service.py
================
#!/usr/bin/env python
"""
Mock GCS service for testing.
This service simulates GCS events and sends them to the video processor.
"""

import argparse
import json
import logging
import os
import sys
from unittest.mock import MagicMock

import requests
from flask import Flask, jsonify, request

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

app = Flask(__name__)

# Default configuration
VIDEO_PROCESSOR_URL = os.environ.get(
    "VIDEO_PROCESSOR_URL", "http://video-processor:8080"
)
TEST_DATA_DIR = os.environ.get("TEST_DATA_DIR", "/app/test_data")

# Add environment variables for testing
os.environ["TESTING_MODE"] = "true"
os.environ["GOOGLE_CLOUD_PROJECT"] = "automations-457120"

# Mock Google Cloud libraries
sys.modules["google.cloud.storage"] = MagicMock()
sys.modules["google.cloud.aiplatform"] = MagicMock()
sys.modules["vertexai"] = MagicMock()


def create_gcs_event(bucket_name, file_name):
    """
    Create a GCS event payload similar to what Cloud Run would receive.

    Args:
        bucket_name: The name of the GCS bucket
        file_name: The name of the file in the bucket

    Returns:
        dict: A dictionary representing the GCS event
    """
    return {
        "bucket": bucket_name,
        "name": file_name,
        "metageneration": "1",
        "timeCreated": "2023-04-21T10:00:00.000Z",
        "updated": "2023-04-21T10:00:00.000Z",
    }


def send_event_to_processor(event_data):
    """
    Send a GCS event to the video processor.

    Args:
        event_data: The GCS event data to send

    Returns:
        requests.Response: The response from the video processor
    """
    headers = {
        "Content-Type": "application/json",
        "Ce-Id": "test-event-id",
        "Ce-Type": "google.cloud.storage.object.v1.finalized",
        "Ce-Source": (
            f"//storage.googleapis.com/projects/_/buckets/{event_data['bucket']}"
        ),
        "Ce-Subject": f"objects/{event_data['name']}",
    }

    logger.info(f"Sending event to {VIDEO_PROCESSOR_URL}")
    logger.info(f"Headers: {json.dumps(headers, indent=2)}")
    logger.info(f"Payload: {json.dumps(event_data, indent=2)}")

    try:
        response = requests.post(VIDEO_PROCESSOR_URL, headers=headers, json=event_data)
        logger.info(f"Response status code: {response.status_code}")
        logger.info(f"Response body: {response.text}")
        return response
    except requests.exceptions.ConnectionError as e:
        logger.error(f"Failed to connect to {VIDEO_PROCESSOR_URL}: {e}")
        return None


@app.route("/", methods=["GET"])
def index():
    """Root endpoint that returns information about the mock service."""
    return jsonify(
        {
            "service": "Mock GCS Service",
            "description": "Simulates GCS events for local testing",
            "endpoints": {
                "/trigger": "POST - Trigger a GCS event",
                "/list-test-files": "GET - List available test files",
            },
        }
    )


@app.route("/trigger", methods=["POST"])
def trigger_event():
    """Endpoint to trigger a GCS event."""
    data = request.get_json()

    if not data or "bucket" not in data or "name" not in data:
        return jsonify({"error": "Invalid request. Required fields: bucket, name"}), 400

    bucket = data["bucket"]
    name = data["name"]

    event_data = create_gcs_event(bucket, name)
    response = send_event_to_processor(event_data)

    if response:
        return jsonify(
            {
                "success": True,
                "message": f"Event sent for gs://{bucket}/{name}",
                "processor_status_code": response.status_code,
                "processor_response": response.text,
            }
        )
    else:
        return (
            jsonify(
                {
                    "success": False,
                    "message": f"Failed to send event for gs://{bucket}/{name}",
                }
            ),
            500,
        )


@app.route("/list-test-files", methods=["GET"])
def list_test_files():
    """Endpoint to list available test files."""
    try:
        files = os.listdir(TEST_DATA_DIR)
        return jsonify({"test_files": files})
    except Exception as e:
        return jsonify({"error": f"Failed to list test files: {str(e)}"}), 500


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Mock GCS service for local testing")
    parser.add_argument(
        "--port", type=int, default=8081, help="Port to run the service on"
    )
    parser.add_argument("--host", default="0.0.0.0", help="Host to run the service on")

    args = parser.parse_args()

    logger.info(f"Starting mock GCS service on {args.host}:{args.port}")
    logger.info(f"Video processor URL: {VIDEO_PROCESSOR_URL}")
    logger.info(f"Test data directory: {TEST_DATA_DIR}")

    app.run(host=args.host, port=args.port, debug=True)

================
File: scripts/prd.txt
================
# Overview
This document outlines the requirements for a containerized service to automate YouTube thumbnail creation. It solves the problem of time-consuming manual thumbnail design by orchestrating AI tools (Google Imagen3 for image generation/editing, Google Gemini 2.5 Flash for text generation) and image processing (Pillow) to produce consistent, branded thumbnails. The primary user is a content creator or developer needing an automated way to generate 1280x720px thumbnails via an HTTP API, deployable on Google Cloud Run. The value lies in significantly reducing manual effort, ensuring brand consistency, and potentially improving click-through rates via AI-optimized titles and backgrounds.

# Core Features
-   **AI Background Generation:**
    -   *What:* Generates a 1280x720px background image using Imagen3 based on a user-provided video title combined with a configurable base prompt.
    -   *Why:* Automates the creation of relevant and visually appealing backgrounds tailored to the video's topic.
    -   *How:* Accepts a `title` via API, calls the Imagen3 API, and returns a reference to the generated image.
-   **AI Title Rewriting:**
    -   *What:* Generates multiple alternative YouTube titles based on an original video title using Gemini 2.5 Flash.
    -   *Why:* Provides optimized, click-worthy title options to improve video discovery and engagement.
    -   *How:* Accepts a `title` via API, calls the Gemini API with a specific prompt, and returns an array of suggested titles.
-   **Automated Face Cutout & Processing:**
    -   *What:* Takes a path to a face photograph, automatically removes the background, optionally refines the cutout using Imagen3 for style consistency, and prepares it for composition.
    -   *Why:* Allows easy inclusion of a consistent personal brand element (the user's face) without manual editing.
    -   *How:* Accepts a `facePhotoPath` via API, uses image processing libraries (and potentially Imagen3 edit features) to isolate the face/shoulders with a transparent background, returns a reference to the processed cutout.
-   **Template-Based Composition:**
    -   *What:* Assembles the generated background, chosen title text, and processed face cutout into a final 1280x720px PNG image based on fixed positional templates.
    -   *Why:* Ensures brand consistency across all thumbnails with predefined placements for key elements.
    -   *How:* Uses the Pillow library to layer the background, render text (scaled to fit) in a specific top-left box, and paste the face cutout (centered) into a specific bottom-right box. Saves the result as a PNG.
-   **HTTP API Interface:**
    -   *What:* Exposes the core features and the end-to-end workflow via distinct HTTP endpoints.
    -   *Why:* Enables programmatic integration with other scripts or automation pipelines.
    -   *How:* Provides endpoints like `/generate` (full workflow), `/rewrite` (titles), `/face-process`, `/compose`, and `/preview`. Requires Bearer token authentication.
-   **Configurable Output:**
    -   *What:* Allows configuration of the output directory and filename pattern for the final thumbnail.
    -   *Why:* Provides flexibility for integrating the generated thumbnail into different file management workflows.
    -   *How:* Uses environment variables to define the output path and filename structure (e.g., `<video-slug>_thumbnail.png`).

# User Experience
-   **Primary User:** A developer or technically-inclined content creator interacting with the service programmatically via its HTTP API.
-   **Key User Flows:**
    1.  **Full Generation:** POST to `/generate` with `title` and `facePhotoPath` -> Receive reference to final PNG thumbnail.
    2.  **Step-by-Step Generation:**
        a. POST to `/rewrite` with `title` -> Get title options.
        b. POST to `/generate` (or similar) with `title` -> Get background reference.
        c. POST to `/face-process` with `facePhotoPath` -> Get face cutout reference.
        d. POST to `/compose` with background ref, chosen title, face ref -> Get final PNG reference.
    3.  **Preview:** GET `/preview` with `slug` -> Receive small JPEG/GIF preview.
-   **UI/UX Considerations:** The service is primarily API-driven, so the "UI" is the API contract (OpenAPI spec). Error handling should return clear JSON messages and appropriate HTTP status codes (e.g., 401 for auth errors, 502 for external API failures). The `/preview` endpoint provides a minimal visual feedback mechanism.

# Technical Architecture
-   **System Components:**
    -   **Web Service:** Python application using Flask/Gunicorn (or similar framework) handling HTTP requests.
    -   **Image Processor:** Python module using Pillow for image manipulation, text rendering, and composition.
    -   **AI Clients:** Python modules interacting with Google Cloud Client Libraries for Imagen3 and Gemini APIs.
    -   **Authentication Middleware:** Component to validate incoming Bearer tokens (potentially using Google Auth libraries).
-   **Data Models:** Primarily transient data related to requests (titles, file paths). No persistent database is required for core functionality. Configuration managed via environment variables. State implicitly managed by file references (background, cutout, final image).
-   **APIs and Integrations:**
    -   **Inbound:** RESTful HTTP API defined by OpenAPI spec (endpoints: `/generate`, `/rewrite`, `/face-process`, `/compose`, `/preview`).
    -   **Outbound:** Google Cloud Imagen3 API, Google Cloud Gemini 2.5 Flash API, Google Cloud Secret Manager (optional, for API key retrieval).
-   **Infrastructure Requirements:**
    -   **Compute:** Google Cloud Run service (configured for ~512MiB RAM, 1 vCPU, auto-scaling).
    -   **Containerization:** Docker image containing the Python application, libraries, font file, and potentially the base face photo.
    -   **Networking:** Standard Cloud Run ingress/egress.
    -   **Authentication:** Mechanism for validating Bearer tokens (e.g., IAM integration).
    -   **Storage:** Local container filesystem for transient processing; potentially GCS for persistent storage of generated assets if needed beyond local scope. Requires access to a font file and the base face photo (via volume mount or baked into image).

# Development Roadmap (Phased Scope)
-   **Phase 1: Core Image Generation & Composition (MVP Foundation)**
    -   Implement basic HTTP server (Flask/Gunicorn).
    -   Integrate Pillow for image loading, saving, and basic composition.
    -   Implement composition logic with hardcoded text/face positioning based on template coordinates (TBD).
    -   Integrate Imagen3 API client for background generation (using base prompt + title).
    -   Implement `/compose` endpoint logic (taking existing background ref, text string, face ref).
    -   Setup Dockerfile and basic Cloud Run deployment.
    -   *Goal: Prove core image creation and layering works.*
-   **Phase 2: AI Text & Face Processing Integration**
    -   Integrate Gemini API client for title rewriting (`/rewrite` endpoint).
    -   Implement text rendering logic within the composition step, including dynamic font scaling to fit the box.
    -   Implement face processing: background removal (using a library like `rembg` or similar) and optional Imagen3 refinement (`/face-process` endpoint).
    -   Update composition logic to use the processed face cutout.
    -   *Goal: Integrate all AI components and automated processing steps.*
-   **Phase 3: End-to-End Workflow & API Polish**
    -   Implement the main `/generate` endpoint orchestrating the full workflow (background -> titles -> face -> compose -> save).
    -   Implement the `/preview` endpoint.
    -   Add Bearer token authentication middleware to all endpoints.
    -   Implement robust error handling (retries for external APIs, proper HTTP status codes, JSON error responses).
    -   Refine configuration via environment variables (API keys, paths, template coordinates).
    -   Add basic logging (Cloud Logging integration).
    -   *Goal: Deliver a fully functional, secured, and robust service.*
-   **Future Enhancements (Post-MVP):**
    -   Batch processing endpoint.
    -   More sophisticated template options (multiple layouts).
    -   User-configurable base prompts for Imagen3/Gemini.
    -   Integration with GCS for input/output instead of local paths.
    -   More advanced error reporting/monitoring (Stackdriver alerts).
    -   Caching mechanisms for API calls or generated assets.

# Logical Dependency Chain
1.  **Basic Server & Image Handling:** Setup Flask/Gunicorn, Docker, Cloud Run deployment. Implement basic Pillow functions (load, save, create canvas). *Provides the execution environment.*
2.  **Composition Logic:** Implement the core `compose` function using Pillow to layer a background, text (initially hardcoded), and a face image (initially a placeholder/pre-cutout) based on template coordinates. *Establishes the core visual assembly.*
3.  **Imagen3 Background:** Integrate the Imagen3 API call to generate the background image based on a title. Wire this into a basic endpoint. *Provides the first dynamic visual element.*
4.  **Text Rendering:** Implement dynamic text rendering (font loading, scaling to fit the text box) within the composition logic. *Makes the text dynamic.*
5.  **Face Processing:** Implement the automated background removal for the face photo. Integrate this into a `/face-process` endpoint or directly into the main flow. *Automates a key input preparation step.*
6.  **Gemini Titles:** Integrate the Gemini API call for title rewriting (`/rewrite` endpoint). *Adds the text generation feature.*
7.  **End-to-End Orchestration (`/generate`):** Combine all previous steps into the main workflow endpoint. *Connects all pieces for the primary use case.*
8.  **Authentication & Preview:** Add Bearer token validation and the `/preview` endpoint. *Secures the API and adds utility.*
9.  **Error Handling & Configuration:** Implement retries, detailed error responses, and make parameters configurable via environment variables. *Increases robustness and flexibility.*

*Prioritization aims to get a visible, composed image output (even with placeholders) quickly, then progressively replace placeholders with dynamic/AI-generated content and finally add production-readiness features.*

# Risks and Mitigations
-   **Technical Challenge: Face Background Removal:** Reliably removing backgrounds from diverse face photos can be difficult.
    -   *Mitigation:* Start with a robust library (e.g., `rembg`). If insufficient, explore using Imagen3's editing/masking features or accept lower reliability for complex backgrounds. Clearly document limitations.
-   **Technical Challenge: Text Fitting:** Dynamically scaling text to perfectly fit a bounding box can be tricky with varying text lengths and fonts.
    -   *Mitigation:* Implement an iterative scaling approach (start large, shrink until fits) or calculate based on font metrics. Allow for minor imperfections or define strict character limits for titles if necessary.
-   **Dependency Risk: AI API Reliability/Cost:** Imagen3 and Gemini APIs might have downtime, rate limits, or unexpected costs.
    -   *Mitigation:* Implement robust retry logic with exponential backoff. Add circuit breakers for persistent failures. Monitor usage and costs closely via Google Cloud Billing/Monitoring. Have fallback logic (e.g., return error, skip step) if APIs are critical.
-   **Scope Creep: Defining MVP:** Difficulty in deciding the minimal feature set to launch quickly.
    -   *Mitigation:* Strictly follow the phased roadmap (Phase 1 -> 2 -> 3). Prioritize the core composition and AI integrations that deliver the main value proposition first. Defer non-essential features (e.g., batch processing, GCS integration) to 'Future Enhancements'.
-   **Resource Constraints (Solo Developer):** Balancing development speed with quality and testing.
    -   *Mitigation:* Focus on automated testing for core logic (Pillow composition, API client interactions). Leverage Cloud Run's managed nature to reduce infrastructure overhead. Prioritize features ruthlessly based on the MVP definition. Accept that certain non-functional requirements (e.g., >80% test coverage) might be achieved progressively.

# Appendix
-   **Template Coordinates (Preliminary - Needs Finalization):**
    -   Canvas: 1280x720 px
    -   Text Box: (x1: 50, y1: 50) -> (x2: 650, y2: 350)
    -   Face Box: (x1: 1070, y1: 520) -> (x2: 1280, y2: 720)
-   **API Authentication:** OAuth 2.0 Bearer Token required for all endpoints.
-   **Key Libraries:** Pillow, google-cloud-aiplatform, Flask/Gunicorn, google-auth.
-   **Configuration:** Via environment variables (e.g., `GOOGLE_API_KEY`, `PROJECT_ID`, `FONT_PATH`, `FACE_PHOTO_PATH`, `OUTPUT_DIR`, `TEXT_BOX_COORDS`, `FACE_BOX_COORDS`, `IMAGEN_BASE_PROMPT`, `GEMINI_TITLE_PROMPT`).

================
File: scripts/real_api_test.py
================
#!/usr/bin/env python3
"""
Real API Test Script for the Video Processor application.
This script runs the video processor with real API calls to see actual outputs.
"""

import argparse
import logging
import os
import shutil
import sys
from pathlib import Path

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# Add the parent directory to the path so we can import the video_processor module
sys.path.append(str(Path(__file__).parent.parent))


def setup_test_environment(video_file, clean=False):
    """
    Set up the test environment by creating the necessary directories and
    copying the test video file.

    Args:
        video_file: The name of the video file to use for testing
        clean: Whether to clean the test environment before setting it up
    """
    # Define paths
    test_data_dir = Path("test_data")
    daily_raw_dir = test_data_dir / "daily-raw"
    processed_daily_dir = test_data_dir / "processed-daily"

    # Create directories if they don't exist
    daily_raw_dir.mkdir(exist_ok=True, parents=True)
    processed_daily_dir.mkdir(exist_ok=True, parents=True)

    # Clean the test environment if requested
    if clean:
        logger.info("Cleaning test environment...")
        # Remove all files in daily-raw
        for file in daily_raw_dir.glob("*"):
            if file.is_file():
                file.unlink()

        # Remove all directories in processed-daily
        for dir in processed_daily_dir.glob("*"):
            if dir.is_dir():
                shutil.rmtree(dir)

    # Copy the test video file to daily-raw
    source_file = test_data_dir / video_file
    if not source_file.exists():
        logger.error(f"Test video file {source_file} does not exist!")
        return False

    target_file = daily_raw_dir / video_file
    shutil.copy2(source_file, target_file)
    logger.info(f"Copied test video file to {target_file}")

    return True


def process_video_with_real_apis(video_file):
    """
    Process the video file using real API calls.

    Args:
        video_file: The name of the video file to process
    """
    import shutil
    import subprocess
    import tempfile

    import vertexai
    from google.oauth2 import service_account
    from vertexai.preview.generative_models import GenerativeModel, Part

    # Set environment variables for real API calls
    os.environ["TESTING_MODE"] = "false"
    os.environ["REAL_API_TEST"] = "true"
    os.environ["LOCAL_OUTPUT"] = "true"  # Write outputs to local filesystem

    # Set the GOOGLE_APPLICATION_CREDENTIALS environment variable to point to
    # the service account file
    service_account_path = Path("credentials/service_account.json").absolute()
    if not service_account_path.exists():
        logger.error(f"Service account credentials not found at {service_account_path}")
        logger.error(
            "Please place your service account credentials at "
            "credentials/service_account.json"
        )
        return False

    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = str(service_account_path)

    logger.info(f"Using service account credentials from {service_account_path}")

    # Authenticate with Google Cloud using the service account
    try:
        credentials = service_account.Credentials.from_service_account_file(
            str(service_account_path),
            scopes=["https://www.googleapis.com/auth/cloud-platform"],
        )
        logger.info(
            f"Successfully authenticated with service account: "
            f"{credentials.service_account_email}"
        )
    except Exception as e:
        logger.error(f"Failed to authenticate with service account: {e}")
        return False

    # Initialize Vertex AI with the project ID from the service account
    project_id = credentials.project_id
    logger.info(f"Initializing Vertex AI with project ID: {project_id}")
    vertexai.init(project=project_id, location="us-central1")

    # Process the video locally instead of using GCS
    with tempfile.TemporaryDirectory() as tmpdir:
        # Define paths
        test_data_dir = Path("test_data")
        source_video_path = test_data_dir / video_file
        video_path = Path(tmpdir) / video_file
        audio_path = Path(tmpdir) / f"{Path(video_file).stem}.wav"

        # Copy the video file to the temp directory
        shutil.copy2(source_video_path, video_path)
        logger.info(f"Copied video file to {video_path}")

        # Extract audio using ffmpeg
        logger.info(f"Extracting audio from {video_path} to {audio_path}...")
        try:
            subprocess.run(
                [
                    "ffmpeg",
                    "-y",  # Overwrite output files without asking
                    "-i",
                    str(video_path),
                    "-vn",  # No video output
                    "-acodec",
                    "pcm_s16le",  # Standard WAV format
                    "-ar",
                    "16000",  # Audio sample rate
                    "-ac",
                    "1",  # Mono audio
                    str(audio_path),
                ],
                check=True,
            )
            logger.info("Audio extraction complete.")
        except subprocess.CalledProcessError as e:
            logger.error(f"Failed to extract audio: {e}")
            return False

        # Call Gemini API
        logger.info(f"Calling Gemini API with audio from {audio_path}...")
        try:
            # Read the audio file
            with open(audio_path, "rb") as f:
                audio_bytes = f.read()

            # Create a Part object with the audio data
            audio_part = Part.from_data(mime_type="audio/wav", data=audio_bytes)

            # Initialize the Gemini model
            model = GenerativeModel("gemini-2.0-flash-001")
            logger.info("Using gemini-2.0-flash-001 model")

            # Generate transcript
            logger.info("Generating transcript...")
            transcript_response = model.generate_content(
                [
                    "Generate a transcript of this audio. Format it as plain text with "
                    "no timestamps or speaker labels.",
                    audio_part,
                ]
            )
            transcript = transcript_response.text

            # Generate subtitles
            logger.info("Generating subtitles...")
            subtitles_response = model.generate_content(
                [
                    "Generate WebVTT subtitles for this audio. "
                    "Include proper timestamps.",
                    audio_part,
                ]
            )
            subtitles_vtt = subtitles_response.text

            # Generate shownotes
            logger.info("Generating shownotes...")
            shownotes_response = model.generate_content(
                [
                    "Generate detailed shownotes for this audio in Markdown format. "
                    "Include key points and timestamps.",
                    audio_part,
                ]
            )
            shownotes = shownotes_response.text

            # Generate chapters
            logger.info("Generating chapters...")
            chapters_response = model.generate_content(
                [
                    "Generate chapters for this audio. Format as "
                    "'MM:SS - Chapter Title' "
                    "with one chapter per line.",
                    audio_part,
                ]
            )
            chapters = chapters_response.text

            # Generate title
            logger.info("Generating title...")
            title_response = model.generate_content(
                [
                    "Generate a concise, engaging title for this audio content.",
                    audio_part,
                ]
            )
            title = title_response.text

            logger.info("Gemini API calls complete.")
        except Exception as e:
            logger.error(f"Error calling Gemini API: {e}")
            return False

        # Save the outputs to the processed directory
        base_name = Path(video_file).stem
        processed_dir_name = base_name.replace(" ", "-")
        processed_dir = test_data_dir / "processed-daily" / processed_dir_name
        processed_dir.mkdir(exist_ok=True, parents=True)

        # Write the outputs to files
        logger.info(f"Saving outputs to {processed_dir}...")
        try:
            with open(processed_dir / "transcript.txt", "w") as f:
                f.write(transcript)

            with open(processed_dir / "subtitles.vtt", "w") as f:
                f.write(subtitles_vtt)

            with open(processed_dir / "shownotes.txt", "w") as f:
                f.write(shownotes)

            with open(processed_dir / "chapters.txt", "w") as f:
                f.write(chapters)

            with open(processed_dir / "title.txt", "w") as f:
                f.write(title)

            # Copy the video file to the processed directory
            shutil.copy2(source_video_path, processed_dir / video_file)

            logger.info("All outputs saved successfully.")
            return True
        except Exception as e:
            logger.error(f"Error saving outputs: {e}")
            return False


def display_outputs(video_file):
    """
    Display the outputs generated by the video processor.

    Args:
        video_file: The name of the video file that was processed
    """
    # Define paths
    test_data_dir = Path("test_data")
    video_name = Path(video_file).stem

    # Handle spaces in filenames - the processor replaces spaces with hyphens
    processed_dir_name = video_name.replace(" ", "-")
    processed_dir = test_data_dir / "processed-daily" / processed_dir_name

    # Also try with the original name if the hyphenated version doesn't exist
    if not processed_dir.exists():
        processed_dir = test_data_dir / "processed-daily" / video_name

    # Also check if any directory in processed-daily contains the output files
    if not processed_dir.exists():
        for dir_path in (test_data_dir / "processed-daily").glob("*"):
            if dir_path.is_dir():
                logger.info(f"Found directory: {dir_path}")
                # Check if this directory contains any of the expected output files
                if any(
                    (dir_path / f).exists() for f in ["transcript.txt", "subtitles.vtt"]
                ):
                    processed_dir = dir_path
                    logger.info(f"Found output files in: {processed_dir}")
                    break

    if not processed_dir.exists():
        logger.error(f"No processed directory found for {video_file}")
        return False

    logger.info(f"Displaying outputs from {processed_dir}:")

    # List of output files to display
    output_files = [
        "transcript.txt",
        "subtitles.vtt",
        "shownotes.txt",
        "chapters.txt",
        "title.txt",
    ]

    for file_name in output_files:
        file_path = processed_dir / file_name
        if file_path.exists():
            logger.info(f"\n{'=' * 40}\n{file_name.upper()}\n{'=' * 40}")
            try:
                with open(file_path, "r", errors="ignore") as f:
                    content = f.read()
                    print(content)
            except Exception as e:
                logger.error(f"Error reading {file_path}: {e}")
        else:
            logger.warning(f"Output file {file_name} not found")

    return True


def main():
    parser = argparse.ArgumentParser(
        description="Test the Video Processor with real API calls"
    )
    parser.add_argument(
        "--video",
        default="Satya Nadella on Vibe Coding.mp4",
        help="The name of the video file to use for testing",
    )
    parser.add_argument(
        "--clean",
        action="store_true",
        help="Clean the test environment before running the test",
    )
    parser.add_argument(
        "--display-only",
        action="store_true",
        help="Only display outputs from a previous run",
    )

    args = parser.parse_args()

    if args.display_only:
        display_outputs(args.video)
        return

    # Set up the test environment
    if not setup_test_environment(args.video, args.clean):
        logger.error("Failed to set up test environment!")
        sys.exit(1)

    # Process the video with real API calls
    if process_video_with_real_apis(args.video):
        logger.info("Video processing completed successfully!")

        # Display the outputs
        display_outputs(args.video)
    else:
        logger.error("Video processing failed!")
        sys.exit(1)


if __name__ == "__main__":
    main()

================
File: scripts/run_comprehensive_test.py
================
#!/usr/bin/env python3
"""
Comprehensive test script for the Video Processor application.
This script:
1. Sets up the test environment
2. Runs the test with a real video file
3. Verifies the outputs
"""

import argparse
import logging
import os
import shutil
import subprocess
import sys
import time
from pathlib import Path

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# Add the parent directory to the path so we can import the video_processor module
sys.path.append(str(Path(__file__).parent.parent))

# Set environment variables for testing
os.environ["TESTING_MODE"] = "true"
os.environ["GOOGLE_CLOUD_PROJECT"] = "automations-457120"


def setup_test_environment(video_file, clean=False):
    """
    Set up the test environment by creating the necessary directories and
    copying the test video file.

    Args:
        video_file: The name of the video file to use for testing
        clean: Whether to clean the test environment before setting it up
    """
    # Define paths
    test_data_dir = Path("test_data")
    daily_raw_dir = test_data_dir / "daily-raw"
    processed_daily_dir = test_data_dir / "processed-daily"

    # Create directories if they don't exist
    daily_raw_dir.mkdir(exist_ok=True, parents=True)
    processed_daily_dir.mkdir(exist_ok=True, parents=True)

    # Clean the test environment if requested
    if clean:
        logger.info("Cleaning test environment...")
        # Remove all files in daily-raw
        for file in daily_raw_dir.glob("*"):
            if file.is_file():
                file.unlink()

        # Remove all directories in processed-daily
        for dir in processed_daily_dir.glob("*"):
            if dir.is_dir():
                shutil.rmtree(dir)

    # Copy the test video file to daily-raw
    source_file = test_data_dir / video_file
    if not source_file.exists():
        logger.error(f"Test video file {source_file} does not exist!")
        return False

    target_file = daily_raw_dir / video_file
    shutil.copy2(source_file, target_file)
    logger.info(f"Copied test video file to {target_file}")

    return True


def run_flask_app(port=8082):
    """
    Run the Flask application in a separate process.

    Args:
        port: The port to run the Flask app on

    Returns:
        subprocess.Popen: The process object
    """

    # Set the PORT environment variable
    os.environ["PORT"] = str(port)

    # Start the Flask app in a separate process
    logger.info(f"Starting Flask application on port {port}...")

    # Use subprocess to run the Flask app
    process = subprocess.Popen(
        [
            sys.executable,
            "-c",
            'import sys; sys.path.append("."); '
            "from video_processor.main import run_app; "
            "run_app(debug=True)",
        ],
        env=dict(
            os.environ,
            PORT=str(port),
            TESTING_MODE="true",
            GOOGLE_CLOUD_PROJECT="automations-457120",
        ),
    )

    # Give the Flask app time to start
    time.sleep(2)

    return process


def send_test_event(video_file, port=8082):
    """
    Send a test event to the Flask application.

    Args:
        video_file: The name of the video file to use for testing
        port: The port the Flask app is running on

    Returns:
        bool: Whether the test event was sent successfully
    """
    import requests

    # Create the event data
    event_data = {
        "bucket": "automations-videos",
        "name": f"daily-raw/{video_file}",
        "metageneration": "1",
        "timeCreated": "2023-04-21T10:00:00.000Z",
        "updated": "2023-04-21T10:00:00.000Z",
    }

    # Create the headers
    headers = {
        "Content-Type": "application/json",
        "Ce-Id": "test-event-id",
        "Ce-Type": "google.cloud.storage.object.v1.finalized",
        "Ce-Source": "//storage.googleapis.com/projects/_/buckets/automations-videos",
        "Ce-Subject": f"objects/daily-raw/{video_file}",
    }

    # Send the request
    url = f"http://localhost:{port}/"
    logger.info(f"Sending test event to {url}")
    logger.info(f"Headers: {headers}")
    logger.info(f"Event data: {event_data}")

    try:
        response = requests.post(url, headers=headers, json=event_data)
        logger.info(f"Response status code: {response.status_code}")
        logger.info(f"Response body: {response.text}")
        return response.status_code == 204
    except requests.exceptions.ConnectionError:
        logger.error(f"Failed to connect to {url}. Is the Flask app running?")
        return False


def check_output_file(file_path, file_type):
    """
    Check if an output file exists and log its status.

    Args:
        file_path: Path to the file to check
        file_type: Description of the file type for logging

    Returns:
        bool: True if file exists and is not empty
    """
    logger.info(f"Checking for {file_type} file: {file_path}")
    if not os.path.exists(file_path):
        logger.warning(f"⚠️ {file_type} file not found")
        return False

    file_size = os.path.getsize(file_path)
    if file_size == 0:
        logger.warning(f"⚠️ {file_type} file is empty")
        return False

    logger.info(f"✅ {file_type} file exists ({file_size} bytes)")
    return True


def preview_file_content(file, logger):
    """
    Try to preview the content of a file and log the first part of it.

    Args:
        file: Path to the file to preview
        logger: Logger instance to use for logging
    """
    try:
        with open(file, "r") as f:
            content = f.read(500)  # Read first 500 characters
            logger.info(f"Content preview: {content[:100]}...")
    except Exception:
        # If that fails, try to read as binary
        try:
            with open(file, "rb") as f:
                content = f.read(100)  # Just read a bit in binary mode
                logger.info(f"Binary content (hex): {content.hex()[:100]}")
        except Exception as e:
            logger.error(f"Could not read file: {e}")


def verify_outputs(video_file):
    """
    Verify that the outputs were generated correctly.

    Args:
        video_file: Path to the input video file

    Returns:
        bool: True if all required outputs were found
    """
    logger.info("Verifying outputs...")

    # Extract base name from the video file
    base_name = os.path.basename(video_file).rsplit(".", 1)[0]

    # Define output paths
    output_dir = os.path.join("outputs", base_name)

    if not os.path.exists(output_dir):
        logger.error(f"❌ Output directory not found: {output_dir}")
        return False

    logger.info(f"Output directory: {output_dir}")

    # List of required output files and their types
    required_outputs = [
        (os.path.join(output_dir, "audio.wav"), "Audio"),
        (os.path.join(output_dir, "transcript.txt"), "Transcript"),
        (os.path.join(output_dir, "subtitles.vtt"), "Subtitles"),
        (os.path.join(output_dir, "shownotes.md"), "Shownotes"),
        (os.path.join(output_dir, "chapters.json"), "Chapters"),
        (os.path.join(output_dir, "metadata.json"), "Metadata"),
    ]

    # Check all required output files
    missing_files = 0
    for output_file, file_type in required_outputs:
        if not check_output_file(output_file, file_type):
            missing_files += 1

    if missing_files > 0:
        logger.warning(f"⚠️ {missing_files} required output files are missing")

    # Check processed video files
    processed_daily_dir = os.path.join(output_dir, "processed-daily", base_name)
    processed_main_dir = os.path.join(output_dir, "processed-main", base_name)

    total_files = 0
    for directory in [processed_daily_dir, processed_main_dir]:
        if os.path.exists(directory):
            files = os.listdir(directory)
            logger.info(f"Files in {directory}: {files}")
            total_files += len(files)

            # Preview content of some key files
            for file in files:
                if (
                    file.endswith(".txt")
                    or file.endswith(".vtt")
                    or file.endswith(".json")
                ):
                    file_path = os.path.join(directory, file)
                    logger.info(f"Previewing {file_path}")
                    preview_file_content(file_path, logger)

    if total_files == 0:
        logger.warning("⚠️ No processed files found")
    else:
        logger.info(f"✅ Found {total_files} processed files")

    # Return success if we found at least some outputs
    success = missing_files == 0 and total_files > 0
    logger.info(f"Verification {'succeeded' if success else 'failed'}")
    return success


def main():
    parser = argparse.ArgumentParser(
        description="Run a comprehensive test of the Video Processor application"
    )
    parser.add_argument(
        "--video",
        default="Satya Nadella on Vibe Coding.mp4",
        help="The name of the video file to use for testing",
    )
    parser.add_argument(
        "--port", type=int, default=8082, help="The port to run the Flask app on"
    )
    parser.add_argument(
        "--clean",
        action="store_true",
        help="Clean the test environment before running the test",
    )

    args = parser.parse_args()

    # Set up the test environment
    if not setup_test_environment(args.video, args.clean):
        logger.error("Failed to set up test environment!")
        sys.exit(1)

    # Run the Flask app
    flask_process = run_flask_app(args.port)

    try:
        # Send the test event
        if not send_test_event(args.video, args.port):
            logger.error("Failed to send test event!")
            sys.exit(1)

        # Give the Flask app time to process the event
        logger.info("Waiting for the Flask app to process the event...")
        time.sleep(5)

        # Check the test_data directory structure
        logger.info("Checking test_data directory structure:")
        test_data_dir = Path("test_data")
        for root, dirs, files in os.walk(test_data_dir):
            logger.info(f"Directory: {root}")
            for d in dirs:
                logger.info(f"  Subdirectory: {d}")
            for f in files:
                logger.info(f"  File: {f}")

        # Verify the outputs
        if verify_outputs(args.video):
            logger.info("✅ Test passed! All outputs were generated correctly.")
        else:
            logger.error("❌ Test failed! Some outputs were not generated correctly.")
            sys.exit(1)

    finally:
        # Terminate the Flask app
        logger.info("Terminating Flask app...")
        flask_process.terminate()
        flask_process.wait()

    logger.info("Test completed successfully!")


if __name__ == "__main__":
    main()

================
File: scripts/simulate_firestore_update.py
================
import os
import random
import string

from google.cloud import firestore

# Path to service account key
SERVICE_ACCOUNT_PATH = os.path.abspath(
    os.path.join(os.path.dirname(__file__), "../../credentials/service_account.json")
)


def random_string(length=6):
    return "".join(random.choices(string.ascii_letters, k=length))


def main():
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = SERVICE_ACCOUNT_PATH
    db = firestore.Client()

    # Get the first video document
    videos_ref = db.collection("videos")
    docs = list(videos_ref.stream())
    if not docs:
        print("No video documents found.")
        return

    doc = docs[0]
    doc_id = doc.id
    data = doc.to_dict()

    # Simulate updating the title
    new_title = f"Simulated Title {random_string()}"
    print(f"Updating title of video {doc_id} to '{new_title}'")
    videos_ref.document(doc_id).update({"title": new_title})

    # Simulate updating a thumbnail prompt if thumbnails exist
    if (
        "thumbnails" in data
        and isinstance(data["thumbnails"], list)
        and data["thumbnails"]
    ):
        new_prompt = f"Simulated prompt {random_string()}"
        print(f"Updating thumbnails.0.prompt of video {doc_id} to '{new_prompt}'")
        videos_ref.document(doc_id).update({"thumbnails.0.prompt": new_prompt})


if __name__ == "__main__":
    main()

================
File: scripts/test_locally.py
================
#!/usr/bin/env python3
"""
Local testing script for the Video Processor application.
This script simulates a Cloud Run environment and GCS events locally.
"""

import argparse
import json
import logging
import os
import sys
from pathlib import Path

import requests

# Add the parent directory to the path so we can import the video_processor module
sys.path.append(str(Path(__file__).parent.parent))

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# Set environment variables for testing
os.environ["TESTING_MODE"] = "true"
os.environ["GOOGLE_CLOUD_PROJECT"] = "automations-457120"


def create_mock_gcs_event(bucket_name, file_name):
    """
    Create a mock GCS event payload similar to what Cloud Run would receive.

    Args:
        bucket_name: The name of the GCS bucket
        file_name: The name of the file in the bucket

    Returns:
        dict: A dictionary representing the GCS event
    """
    return {
        "bucket": bucket_name,
        "name": file_name,
        "metageneration": "1",
        "timeCreated": "2023-04-21T10:00:00.000Z",
        "updated": "2023-04-21T10:00:00.000Z",
    }


def send_local_request(event_data, port=8080):
    """
    Send a request to the locally running Flask application.

    Args:
        event_data: The GCS event data to send
        port: The port the Flask app is running on

    Returns:
        requests.Response: The response from the Flask app
    """
    headers = {
        "Content-Type": "application/json",
        "Ce-Id": "test-event-id",
        "Ce-Type": "google.cloud.storage.object.v1.finalized",
        "Ce-Source": "//storage.googleapis.com/projects/_/buckets/{bucket}".format(
            bucket=event_data["bucket"]
        ),
        "Ce-Subject": f"objects/{event_data['name']}",
    }

    url = f"http://localhost:{port}/"
    logger.info(f"Sending request to {url}")
    logger.info(f"Headers: {json.dumps(headers, indent=2)}")
    logger.info(f"Payload: {json.dumps(event_data, indent=2)}")

    try:
        response = requests.post(url, headers=headers, json=event_data)
        logger.info(f"Response status code: {response.status_code}")
        logger.info(f"Response body: {response.text}")
        return response
    except requests.exceptions.ConnectionError:
        logger.error(f"Failed to connect to {url}. Is the Flask app running?")
        sys.exit(1)


def run_flask_app(port=8080):
    """
    Run the Flask application in a separate process.
    """
    from video_processor.main import run_app

    logger.info(f"Starting Flask application on port {port}...")
    # Set the PORT environment variable
    os.environ["PORT"] = str(port)
    run_app(debug=True)


def main():
    parser = argparse.ArgumentParser(
        description="Test the Video Processor application locally"
    )
    parser.add_argument(
        "--bucket", default="automations-videos", help="The GCS bucket name"
    )
    parser.add_argument("--file", required=True, help="The file name in the GCS bucket")
    parser.add_argument(
        "--port", type=int, default=8080, help="The port to run the Flask app on"
    )
    parser.add_argument(
        "--run-server",
        action="store_true",
        help="Run the Flask server (otherwise assumes it's already running)",
    )

    args = parser.parse_args()

    if args.run_server:
        # Run the Flask app in this process
        run_flask_app(port=args.port)
    else:
        # Send a request to an already running Flask app
        event_data = create_mock_gcs_event(args.bucket, args.file)
        send_local_request(event_data, args.port)


if __name__ == "__main__":
    main()

================
File: tests/e2e/__init__.py
================
"""
End-to-end tests for the complete application flow.
"""

================
File: tests/integration/__init__.py
================
"""
Integration tests for checking the interaction between components.
"""

================
File: tests/outdated/test_main.py
================
"""
Tests for the main.py module.
"""

import os
import sys
from unittest.mock import MagicMock, patch

import pytest

# Add the root directory to the path so we can import the main module
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")))

# Import the main module - authentication is handled by environment variables
# or mocked in conftest.py
try:
    from video_processor import main
except Exception:
    # If there's an authentication error, we'll mock the authentication
    import os
    import sys
    from unittest.mock import patch

    # Mock the authentication
    with patch("google.auth.default", return_value=(None, "automations-457120")):
        from video_processor import main


@pytest.fixture
def mock_process_video_event():
    """Mock for the process_video_event function."""
    return MagicMock()


@pytest.fixture
def test_client(mock_process_video_event):
    """Create a test client for the Flask app with mocked dependencies."""
    # Create a test app with the mock process function
    test_app = main.create_app(process_func=mock_process_video_event)
    test_app.config["TESTING"] = True

    # Create a test client
    with test_app.test_client() as client:
        yield client, mock_process_video_event


@pytest.fixture
def gcs_event_data():
    """Create a sample GCS event data."""
    return {
        "bucket": "test-bucket",
        "name": "daily-raw/test_video.mp4",
        "contentType": "video/mp4",
        "size": "1000000",
    }


def test_handle_gcs_event_success(test_client, gcs_event_data):
    """Test successful handling of a GCS event."""
    client, mock_processor = test_client

    # Create headers that mimic CloudEvent format
    headers = {
        "Ce-Id": "test-event-id",
        "Ce-Type": "google.cloud.storage.object.v1.finalized",
        "Ce-Source": "test-source",
        "Ce-Subject": "test-subject",
    }

    # Send a POST request with the event data
    response = client.post("/", json=gcs_event_data, headers=headers)

    # Check that the response is successful (204 No Content)
    assert response.status_code == 204

    # Verify that process_video_event was called with the correct arguments
    mock_processor.assert_called_once_with(
        gcs_event_data["bucket"], gcs_event_data["name"]
    )


def test_handle_gcs_event_invalid_payload(test_client):
    """Test handling of an invalid event payload."""
    client, _ = test_client

    # Send a POST request with an invalid payload (missing required fields)
    response = client.post(
        "/", json={"invalid": "payload"}, headers={"Ce-Id": "test-event-id"}
    )

    # Check that the response indicates a bad request
    assert response.status_code == 400


def test_handle_gcs_event_processing_error(test_client, gcs_event_data):
    """Test handling of an error during event processing."""
    client, mock_processor = test_client

    # Configure the mock to raise an exception
    mock_processor.side_effect = Exception("Test error")

    # Send a POST request with the event data
    response = client.post("/", json=gcs_event_data, headers={"Ce-Id": "test-event-id"})

    # Check that the response indicates a server error
    assert response.status_code == 500


def test_create_app():
    """Test the create_app function."""
    # Test creating an app with the default processor
    app1 = main.create_app()
    assert app1 is not None

    # Test creating an app with a custom processor
    mock_processor = MagicMock()
    app2 = main.create_app(process_func=mock_processor)
    assert app2 is not None


def test_run_app():
    """Test the run_app function."""
    with patch("video_processor.main.app.run") as mock_run:
        # Call the run_app function
        main.run_app(debug=False)

        # Verify app.run was called with the correct arguments
        mock_run.assert_called_once_with(host="0.0.0.0", port=8080, debug=False)

================
File: tests/outdated/test_process_video_event.py
================
"""
Tests for the main process_video_event function.
"""

import os

# Import the function to test
import sys
from unittest.mock import MagicMock, patch

import pytest

# Add the parent directory to the path so we can import the module
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from process_uploaded_video import process_video_event


def test_process_video_event_skips_non_mp4(mock_storage_client):
    """Test that process_video_event skips non-MP4 files."""
    # Call the function with a non-MP4 file
    process_video_event("test-bucket", "daily-raw/test_file.txt")

    # Verify that the function returns early
    mock_client, mock_bucket, mock_blob = mock_storage_client
    mock_client.bucket.assert_not_called()


def test_process_video_event_skips_wrong_path(mock_storage_client):
    """Test that process_video_event skips files not in the correct paths."""
    # Call the function with a file in the wrong path
    process_video_event("test-bucket", "wrong-path/test_file.mp4")

    # Verify that the function returns early
    mock_client, mock_bucket, mock_blob = mock_storage_client
    mock_client.bucket.assert_not_called()


@patch("video_processor.process_uploaded_video.tempfile.TemporaryDirectory")
@patch("video_processor.process_uploaded_video.subprocess.run")
@patch("video_processor.process_uploaded_video.generate_transcript")
@patch("video_processor.process_uploaded_video.generate_vtt")
@patch("video_processor.process_uploaded_video.generate_shownotes")
@patch("video_processor.process_uploaded_video.generate_chapters")
@patch("video_processor.process_uploaded_video.generate_titles")
@patch("video_processor.process_uploaded_video.write_blob")
@patch("video_processor.process_uploaded_video.Part")
def test_process_video_event_success(
    mock_part,
    mock_write_blob,
    mock_generate_titles,
    mock_generate_chapters,
    mock_generate_shownotes,
    mock_generate_vtt,
    mock_generate_transcript,
    mock_subprocess_run,
    mock_temp_dir,
    mock_storage_client,
):
    """Test the full process_video_event function with successful execution."""
    # Set up mocks
    mock_client, mock_bucket, mock_blob = mock_storage_client

    # Mock the temporary directory
    mock_temp_dir_instance = MagicMock()
    mock_temp_dir_instance.__enter__.return_value = "/tmp/mock_dir"
    mock_temp_dir.return_value = mock_temp_dir_instance

    # Mock the subprocess.run result
    mock_subprocess_run.return_value = MagicMock()

    # Mock the Part.from_data method
    mock_audio_part = MagicMock()
    mock_part.from_data.return_value = mock_audio_part

    # Mock the generate_* functions
    mock_generate_transcript.return_value = "Test transcript"
    mock_generate_vtt.return_value = (
        "WEBVTT\n\n00:00:00.000 --> 00:00:05.000\nTest subtitle"
    )
    mock_generate_shownotes.return_value = "Test shownotes"
    mock_generate_chapters.return_value = [
        {"timecode": "00:00", "chapterSummary": "Introduction"},
        {"timecode": "02:30", "chapterSummary": "Main topic"},
    ]
    mock_generate_titles.return_value = {
        "Description": "Test Title",
        "Keywords": "test,keywords",
    }

    # Call the function
    process_video_event("test-bucket", "daily-raw/test_video.mp4")

    # Verify the bucket and blob were accessed
    mock_client.bucket.assert_called_once_with("test-bucket")
    mock_bucket.blob.assert_called_with("daily-raw/test_video.mp4")

    # Verify the blob was downloaded
    mock_blob.download_to_filename.assert_called_once()

    # Verify ffmpeg was called
    mock_subprocess_run.assert_called_once()

    # Verify the audio part was created
    mock_part.from_data.assert_called_once()
    assert mock_part.from_data.call_args[1]["mime_type"] == "audio/wav"

    # Verify all generate functions were called with the audio part
    mock_generate_transcript.assert_called_once_with(mock_audio_part)
    mock_generate_vtt.assert_called_once_with(mock_audio_part)
    mock_generate_shownotes.assert_called_once_with(mock_audio_part)
    mock_generate_chapters.assert_called_once_with(mock_audio_part)
    mock_generate_titles.assert_called_once_with(mock_audio_part)

    # Verify write_blob was called for each output file
    assert (
        mock_write_blob.call_count == 5
    )  # transcript, subtitles, shownotes, chapters, title

    # Verify the original file was moved
    mock_bucket.copy_blob.assert_called_once_with(
        mock_blob, mock_bucket, "processed-daily/test_video/test_video.mp4"
    )
    mock_blob.delete.assert_called_once()


@patch("video_processor.process_uploaded_video.tempfile.TemporaryDirectory")
def test_process_video_event_download_error(mock_temp_dir, mock_storage_client):
    """Test process_video_event handles download errors."""
    # Set up mocks
    mock_client, mock_bucket, mock_blob = mock_storage_client

    # Mock the temporary directory
    mock_temp_dir_instance = MagicMock()
    mock_temp_dir_instance.__enter__.return_value = "/tmp/mock_dir"
    mock_temp_dir.return_value = mock_temp_dir_instance

    # Make the download fail
    mock_blob.download_to_filename.side_effect = Exception("Download failed")

    # Call the function and expect an exception
    with pytest.raises(Exception) as excinfo:
        process_video_event("test-bucket", "daily-raw/test_video.mp4")

    # Verify the exception message
    assert "Download failed" in str(excinfo.value)

    # Verify the bucket and blob were accessed
    mock_client.bucket.assert_called_once_with("test-bucket")
    mock_bucket.blob.assert_called_with("daily-raw/test_video.mp4")


@patch("video_processor.process_uploaded_video.tempfile.TemporaryDirectory")
@patch("video_processor.process_uploaded_video.subprocess.run")
def test_process_video_event_ffmpeg_error(
    mock_subprocess_run, mock_temp_dir, mock_storage_client
):
    """Test process_video_event handles ffmpeg errors."""
    # Set up mocks
    mock_client, mock_bucket, mock_blob = mock_storage_client

    # Mock the temporary directory
    mock_temp_dir_instance = MagicMock()
    mock_temp_dir_instance.__enter__.return_value = "/tmp/mock_dir"
    mock_temp_dir.return_value = mock_temp_dir_instance

    # Make the subprocess.run fail
    mock_subprocess_run.side_effect = Exception("ffmpeg failed")

    # Call the function and expect an exception
    with pytest.raises(Exception) as excinfo:
        process_video_event("test-bucket", "daily-raw/test_video.mp4")

    # Verify the exception message
    assert "ffmpeg failed" in str(excinfo.value)

    # Verify the bucket and blob were accessed
    mock_client.bucket.assert_called_once_with("test-bucket")
    mock_bucket.blob.assert_called_with("daily-raw/test_video.mp4")

    # Verify the blob was downloaded
    mock_blob.download_to_filename.assert_called_once()

================
File: tests/outdated/test_transcript_generation.py
================
"""
Tests for the transcript generation functionality.
"""

import os

# Import the functions to test
import sys
from unittest.mock import MagicMock, patch

import pytest

# Add the parent directory to the path so we can import the module
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from process_uploaded_video import generate_transcript

# Import Part for mocking
from vertexai.preview.generative_models import Part


@pytest.mark.parametrize(
    "mock_response_text,expected_result",
    [
        ("Test transcript", "Test transcript"),
        ("  Transcript with whitespace  ", "Transcript with whitespace"),
        ("", ""),
    ],
)
def test_generate_transcript(
    mock_generative_model, mock_part, mock_response_text, expected_result
):
    """Test the generate_transcript function with various inputs."""
    # Set up the mock response
    mock_response = MagicMock()
    mock_response.text = mock_response_text
    mock_generative_model.generate_content.return_value = mock_response

    # Create a mock audio part that is a proper Part instance
    mock_audio_part = MagicMock(spec=Part)

    # Patch the necessary classes and functions
    with patch(
        "process_uploaded_video.GenerativeModel",
        return_value=mock_generative_model,
    ):
        # Call the function
        result = generate_transcript(mock_audio_part)

        # Verify the result
        assert result == expected_result

        # Verify the model was called
        mock_generative_model.generate_content.assert_called_once()


def test_generate_transcript_with_real_audio():
    """Test the generate_transcript function with a real audio file."""
    # This test requires actual API credentials, so we'll mock the API call
    mock_response = MagicMock()
    mock_response.text = "This is a test transcript for audio file."

    mock_model = MagicMock()
    mock_model.generate_content.return_value = mock_response

    # Create a mock audio part that is a proper Part instance
    mock_audio_part = MagicMock(spec=Part)

    # Patch the necessary functions
    with patch(
        "process_uploaded_video.GenerativeModel",
        return_value=mock_model,
    ):
        # Call the function directly with the mock audio part
        result = generate_transcript(mock_audio_part)

        # Verify the result
        assert result == "This is a test transcript for audio file."

        # Verify the model was called
        mock_model.generate_content.assert_called_once()


def test_generate_transcript_error_handling(mock_generative_model):
    """Test error handling in the generate_transcript function."""
    # Set up the mock to raise an exception
    mock_generative_model.generate_content.side_effect = Exception("API error")

    # Create a mock audio part that is a proper Part instance
    mock_audio_part = MagicMock(spec=Part)

    # Patch the necessary classes and functions
    with patch(
        "process_uploaded_video.GenerativeModel",
        return_value=mock_generative_model,
    ):
        # Call the function and expect an exception
        with pytest.raises(Exception) as excinfo:
            generate_transcript(mock_audio_part)

        # Verify the exception message
        assert "API error" in str(excinfo.value)

================
File: tests/unit/__init__.py
================
"""
Unit tests for individual components of the video processor.
"""

================
File: tests/unit/test_api.py
================
"""
Unit tests for the API module.
"""

import json
from unittest.mock import MagicMock, patch

from video_processor.api.controllers import (
    get_gcs_upload_url,
    handle_gcs_event,
    health_check,
)
from video_processor.api.routes import create_app


def test_health_check():
    """Test health_check endpoint."""
    response = health_check()
    assert "up and running" in response


def test_handle_gcs_event_success():
    """Test handle_gcs_event with successful processing."""
    # Create event data
    event_data = {"bucket": "test-bucket", "name": "daily-raw/test_video.mp4"}

    # Mock processor function
    mock_processor = MagicMock()

    # Call the function
    result, status_code = handle_gcs_event(event_data, mock_processor)

    # Verify result
    assert status_code == 200
    assert "processed" in result

    # Verify processor was called
    mock_processor.assert_called_once_with("test-bucket", "daily-raw/test_video.mp4")


def test_handle_gcs_event_invalid_data():
    """Test handle_gcs_event with invalid event data."""
    # Create invalid event data (missing required fields)
    event_data = {"some_field": "some_value"}

    # Call the function
    result, status_code = handle_gcs_event(event_data, None)

    # Verify result
    assert status_code == 400
    assert "Invalid" in result


def test_handle_gcs_event_processor_error():
    """Test handle_gcs_event with processor error."""
    # Create event data
    event_data = {"bucket": "test-bucket", "name": "daily-raw/test_video.mp4"}

    # Mock processor function that raises an exception
    mock_processor = MagicMock()
    mock_processor.side_effect = Exception("Test error")

    # Call the function
    result, status_code = handle_gcs_event(event_data, mock_processor)

    # Verify result
    assert status_code == 500
    assert "Failed" in result


@patch("video_processor.api.controllers.get_storage_service")
@patch("video_processor.api.controllers.get_settings")
def test_get_gcs_upload_url_success(mock_get_settings, mock_get_storage_service):
    """Test get_gcs_upload_url with successful URL generation."""
    # Create request data
    request_data = {"filename": "test_video.mp4", "content_type": "video/mp4"}

    # Mock settings
    mock_settings_instance = MagicMock()
    mock_settings_instance.gcs_upload_bucket = "test-bucket"
    mock_get_settings.return_value = mock_settings_instance

    # Mock storage service
    mock_storage_service_instance = MagicMock()
    mock_storage_service_instance.get_signed_url.return_value = (
        "https://test-signed-url.com"
    )
    mock_get_storage_service.return_value = mock_storage_service_instance

    # Call the function
    response = get_gcs_upload_url(request_data)

    # Verify response
    assert response.status_code == 200

    # Parse response JSON
    response_data = json.loads(response.get_data(as_text=True))
    assert response_data["url"] == "https://test-signed-url.com"
    assert response_data["bucket"] == "test-bucket"
    assert "videos/test_video.mp4" in response_data["object_path"]

    # Verify storage service was called
    mock_storage_service_instance.get_signed_url.assert_called_once()
    call_args = mock_storage_service_instance.get_signed_url.call_args[1]
    assert call_args["bucket"] == "test-bucket"
    assert call_args["path"] == "videos/test_video.mp4"
    assert call_args["http_method"] == "PUT"
    assert call_args["content_type"] == "video/mp4"


@patch("video_processor.api.controllers.get_settings")
def test_get_gcs_upload_url_missing_filename(mock_get_settings):
    """Test get_gcs_upload_url with missing filename."""
    # Create request data with missing filename
    request_data = {"content_type": "video/mp4"}

    # Mock settings
    mock_settings_instance = MagicMock()
    mock_settings_instance.gcs_upload_bucket = "test-bucket"
    mock_get_settings.return_value = mock_settings_instance

    # Call the function
    response = get_gcs_upload_url(request_data)

    # Verify response
    assert response.status_code == 400

    # Parse response JSON
    response_data = json.loads(response.get_data(as_text=True))
    assert "error" in response_data
    assert "Missing filename" in response_data["error"]


def test_create_app():
    """Test create_app function."""
    # Create app with default processor
    app = create_app()
    assert app is not None

    # Test app has the expected routes
    rules = [rule.rule for rule in app.url_map.iter_rules()]
    assert "/" in rules
    assert "/api/gcs-upload-url" in rules

    # Create app with custom processor
    mock_processor = MagicMock()
    app = create_app(mock_processor)
    assert app is not None


def test_flask_routes():
    """Test Flask routes integration with controllers."""
    # Create test app with mock processor
    mock_processor = MagicMock()
    app = create_app(mock_processor)
    app.testing = True
    client = app.test_client()

    # Test health check endpoint
    response = client.get("/")
    assert response.status_code == 200
    assert b"up and running" in response.data

    # Test GCS event endpoint with valid data
    gcs_event = {"bucket": "test-bucket", "name": "daily-raw/test_video.mp4"}
    response = client.post("/", json=gcs_event)
    assert response.status_code == 200
    assert b"processed" in response.data

    # Verify processor was called
    mock_processor.assert_called_once_with("test-bucket", "daily-raw/test_video.mp4")

================
File: tests/unit/test_audio_processor.py
================
"""
Unit tests for the AudioProcessor class.
"""

import os
from unittest.mock import MagicMock, patch

import pytest

from video_processor.core.processors.audio import AudioProcessor
from video_processor.utils.error_handling import VideoProcessingError


def test_init():
    """Test AudioProcessor initialization."""
    processor = AudioProcessor()
    assert processor.testing_mode is False

    processor = AudioProcessor(testing_mode=True)
    assert processor.testing_mode is True


def test_extract_audio_testing_mode():
    """Test extract_audio method in testing mode."""
    processor = AudioProcessor(testing_mode=True)
    output_path = os.path.join(os.path.dirname(__file__), "test_output.wav")

    try:
        result = processor.extract_audio("/path/to/nonexistent/video.mp4", output_path)

        # Check result is the output path
        assert result == output_path

        # Check file was created
        assert os.path.exists(output_path)

        # Verify it's a WAV file (at least check the header)
        with open(output_path, "rb") as f:
            header = f.read(12)
            assert header.startswith(b"RIFF") and b"WAVE" in header
    finally:
        # Clean up
        if os.path.exists(output_path):
            os.unlink(output_path)


def test_extract_audio_file_not_found():
    """Test extract_audio method with nonexistent file."""
    processor = AudioProcessor(testing_mode=False)

    with pytest.raises(VideoProcessingError) as excinfo:
        processor.extract_audio("/path/to/nonexistent/video.mp4")

    # Verify the error message
    assert "does not exist" in str(excinfo.value)


@patch("video_processor.core.processors.audio.subprocess.run")
def test_extract_audio_runs_ffmpeg(mock_subprocess_run, sample_video_file):
    """Test extract_audio runs ffmpeg with correct parameters."""
    processor = AudioProcessor(testing_mode=False)
    output_path = os.path.join(os.path.dirname(__file__), "test_output.wav")

    # Mock subprocess.run to return successfully
    mock_subprocess_result = MagicMock()
    mock_subprocess_run.return_value = mock_subprocess_result

    # Also mock the file validity check
    with patch(
        "video_processor.core.processors.audio.AudioProcessor._check_file_validity",
        return_value="MP4 file",
    ):
        try:
            result = processor.extract_audio(sample_video_file, output_path)

            # Check result is the output path
            assert result == output_path

            # Verify ffmpeg was called with correct args
            mock_subprocess_run.assert_called_once()
            args = mock_subprocess_run.call_args[0][0]

            # Basic checks on the ffmpeg command
            assert args[0] == "ffmpeg"
            assert "-y" in args  # Overwrite without asking
            assert "-i" in args and args[args.index("-i") + 1] == sample_video_file
            assert "-vn" in args  # No video output
            assert (
                "-ar" in args and args[args.index("-ar") + 1] == "16000"
            )  # Sample rate
            assert "-ac" in args and args[args.index("-ac") + 1] == "1"  # Mono
            assert args[-1] == output_path  # Output path
        finally:
            # Clean up
            if os.path.exists(output_path):
                os.unlink(output_path)


@patch("video_processor.core.processors.audio.subprocess.run")
def test_extract_audio_fallback_to_dummy(mock_subprocess_run, sample_video_file):
    """Test extract_audio fallback to dummy file on ffmpeg failure."""
    processor = AudioProcessor(testing_mode=False)
    output_path = os.path.join(os.path.dirname(__file__), "test_output.wav")

    # Make ffmpeg fail
    mock_subprocess_run.side_effect = Exception("ffmpeg error")

    # Make the file validity check pass
    with patch(
        "video_processor.core.processors.audio.AudioProcessor._check_file_validity",
        return_value="MP4 file",
    ):
        try:
            result = processor.extract_audio(sample_video_file, output_path)

            # Check result is the output path
            assert result == output_path

            # Check dummy file was created
            assert os.path.exists(output_path)

            # Verify it's a WAV file (at least check the header)
            with open(output_path, "rb") as f:
                header = f.read(12)
                assert header.startswith(b"RIFF") and b"WAVE" in header
        finally:
            # Clean up
            if os.path.exists(output_path):
                os.unlink(output_path)


@patch("video_processor.core.processors.audio.subprocess.run")
def test_check_file_validity(mock_subprocess_run):
    """Test _check_file_validity method."""
    processor = AudioProcessor()

    # Mock subprocess.run to return successfully with MP4 info
    mock_result = MagicMock()
    mock_result.stdout = "test_file: ISO Media, MP4 v2 [ISO 14496-14]"
    mock_subprocess_run.return_value = mock_result

    result = processor._check_file_validity("test_file.mp4")
    assert result == mock_result.stdout

    # Mock subprocess.run to return successfully with non-video info
    mock_result.stdout = "test_file: ASCII text"
    result = processor._check_file_validity("test_file.txt")
    assert result is None

    # Mock subprocess.run to raise exception
    mock_subprocess_run.side_effect = Exception("command failed")
    result = processor._check_file_validity("test_file.mp4")
    assert result is None

================
File: tests/unit/test_storage_service.py
================
"""
Unit tests for storage services.
"""

import os
import tempfile
from unittest.mock import MagicMock, patch

import pytest

from video_processor.services.storage import GCSStorageService, LocalStorageService
from video_processor.services.storage.factory import get_storage_service
from video_processor.utils.error_handling import StorageError


class TestLocalStorageService:
    """Tests for LocalStorageService."""

    def test_init(self):
        """Test LocalStorageService initialization."""
        service = LocalStorageService(base_path="/tmp/test_storage")
        assert service.base_path == "/tmp/test_storage"

        # Clean up
        if os.path.exists("/tmp/test_storage"):
            os.rmdir("/tmp/test_storage")

    def test_get_bucket_path(self):
        """Test _get_bucket_path method."""
        with tempfile.TemporaryDirectory() as tmp_dir:
            service = LocalStorageService(base_path=tmp_dir)
            bucket_path = service._get_bucket_path("test-bucket")
            assert bucket_path == os.path.join(tmp_dir, "test-bucket")

    def test_get_file_path(self):
        """Test _get_file_path method."""
        with tempfile.TemporaryDirectory() as tmp_dir:
            service = LocalStorageService(base_path=tmp_dir)
            file_path = service._get_file_path("test-bucket", "path/to/file.txt")
            assert file_path == os.path.join(tmp_dir, "test-bucket", "path/to/file.txt")

    def test_upload_from_string(self):
        """Test upload_from_string method."""
        with tempfile.TemporaryDirectory() as tmp_dir:
            service = LocalStorageService(base_path=tmp_dir)

            # Test with string content
            result = service.upload_from_string(
                "test-bucket", "Test content", "test_file.txt"
            )
            assert result == "test_file.txt"

            # Verify file was created with correct content
            file_path = os.path.join(tmp_dir, "test-bucket", "test_file.txt")
            assert os.path.exists(file_path)
            with open(file_path, "r") as f:
                assert f.read() == "Test content"

    def test_file_exists(self):
        """Test file_exists method."""
        with tempfile.TemporaryDirectory() as tmp_dir:
            service = LocalStorageService(base_path=tmp_dir)

            # Create a test file
            os.makedirs(os.path.join(tmp_dir, "test-bucket"), exist_ok=True)
            test_file_path = os.path.join(tmp_dir, "test-bucket", "test_file.txt")
            with open(test_file_path, "w") as f:
                f.write("Test content")

            # Test file exists
            assert service.file_exists("test-bucket", "test_file.txt") is True

            # Test file doesn't exist
            assert service.file_exists("test-bucket", "nonexistent.txt") is False

    def test_get_signed_url(self):
        """Test get_signed_url method."""
        with tempfile.TemporaryDirectory() as tmp_dir:
            service = LocalStorageService(base_path=tmp_dir)

            # Create a test file
            os.makedirs(os.path.join(tmp_dir, "test-bucket"), exist_ok=True)
            test_file_path = os.path.join(tmp_dir, "test-bucket", "test_file.txt")
            with open(test_file_path, "w") as f:
                f.write("Test content")

            # Get signed URL
            url = service.get_signed_url("test-bucket", "test_file.txt")

            # Verify it's a file:// URL
            assert url.startswith("file://")
            assert test_file_path in url


class TestGCSStorageService:
    """Tests for GCSStorageService."""

    def test_init(self, mock_storage_client):
        """Test GCSStorageService initialization."""
        mock_client, _, _ = mock_storage_client

        # Test with provided client
        service = GCSStorageService(client=mock_client)
        assert service.client is mock_client

        # Test with default client creation
        with patch(
            "video_processor.services.storage.gcs.storage.Client"
        ) as mock_client_class:
            mock_client_class.return_value = mock_client
            service = GCSStorageService()
            mock_client_class.assert_called_once()
            assert service.client is mock_client

    def test_download_file(self, mock_storage_client):
        """Test download_file method."""
        mock_client, mock_bucket, mock_blob = mock_storage_client

        service = GCSStorageService(client=mock_client)

        # Set up mocks
        mock_client.bucket.return_value = mock_bucket
        mock_bucket.blob.return_value = mock_blob
        mock_blob.exists.return_value = True

        # Test download
        with tempfile.TemporaryDirectory() as tmp_dir:
            dest_path = os.path.join(tmp_dir, "downloaded.txt")
            result = service.download_file("test-bucket", "test_file.txt", dest_path)

            # Verify result
            assert result == dest_path

            # Verify mocks called correctly
            mock_client.bucket.assert_called_once_with("test-bucket")
            mock_bucket.blob.assert_called_once_with("test_file.txt")
            mock_blob.exists.assert_called_once()
            mock_blob.download_to_filename.assert_called_once_with(dest_path)

    def test_download_file_nonexistent(self, mock_storage_client):
        """Test download_file with nonexistent file."""
        mock_client, mock_bucket, mock_blob = mock_storage_client

        service = GCSStorageService(client=mock_client)

        # Set up mocks
        mock_client.bucket.return_value = mock_bucket
        mock_bucket.blob.return_value = mock_blob
        mock_blob.exists.return_value = False

        # Test download
        with tempfile.TemporaryDirectory() as tmp_dir:
            dest_path = os.path.join(tmp_dir, "downloaded.txt")

            # Should raise StorageError
            with pytest.raises(StorageError) as excinfo:
                service.download_file("test-bucket", "nonexistent.txt", dest_path)

            # Verify error message
            assert "does not exist" in str(excinfo.value)

    def test_upload_from_string(self, mock_storage_client):
        """Test upload_from_string method."""
        mock_client, mock_bucket, mock_blob = mock_storage_client

        service = GCSStorageService(client=mock_client)

        # Set up mocks
        mock_client.bucket.return_value = mock_bucket
        mock_bucket.blob.return_value = mock_blob

        # Test with string content
        result = service.upload_from_string(
            "test-bucket", "Test content", "test_file.txt"
        )

        # Verify result
        assert result == "test_file.txt"

        # Verify mocks called correctly
        mock_client.bucket.assert_called_once_with("test-bucket")
        mock_bucket.blob.assert_called_once_with("test_file.txt")
        mock_blob.upload_from_string.assert_called_once_with("Test content")

    def test_move_file(self, mock_storage_client):
        """Test move_file method."""
        mock_client, mock_bucket, mock_blob = mock_storage_client

        service = GCSStorageService(client=mock_client)

        # Set up mocks
        mock_client.bucket.return_value = mock_bucket
        mock_bucket.blob.return_value = mock_blob
        mock_blob.exists.return_value = True

        # Test move
        result = service.move_file("test-bucket", "source.txt", "destination.txt")

        # Verify result
        assert result is True

        # Verify mocks called correctly
        mock_client.bucket.assert_called_once_with("test-bucket")
        mock_bucket.blob.assert_called_once_with("source.txt")
        mock_blob.exists.assert_called_once()
        mock_bucket.copy_blob.assert_called_once_with(
            mock_blob, mock_bucket, "destination.txt"
        )
        mock_blob.delete.assert_called_once()


def test_get_storage_service_testing_mode():
    """Test get_storage_service in testing mode."""
    with patch(
        "video_processor.services.storage.factory.get_settings"
    ) as mock_get_settings:
        # Mock settings to return testing_mode=False, local_output=False
        mock_settings = MagicMock()
        mock_settings.testing_mode = False
        mock_settings.local_output = False
        mock_get_settings.return_value = mock_settings

        # Test with testing_mode=True override
        service = get_storage_service(testing_mode=True)
        assert isinstance(service, LocalStorageService)

        # Test with local_output=True override
        service = get_storage_service(local_output=True)
        assert isinstance(service, LocalStorageService)

        # Test with both False
        with patch(
            "video_processor.services.storage.factory.GCSStorageService",
            return_value=MagicMock(spec=GCSStorageService),
        ) as mock_gcs:
            service = get_storage_service()
            assert mock_gcs.called
            assert not isinstance(service, LocalStorageService)

================
File: tests/unit/test_video_processor.py
================
"""
Unit tests for the video processor core functionality.
"""

from unittest.mock import MagicMock, patch

import pytest

from video_processor.core.models import (
    ProcessingStage,
    ProcessingStatus,
    VideoJob,
    VideoMetadata,
)
from video_processor.core.processors.video import process_video, process_video_event
from video_processor.services.storage.gcs import StorageError
from video_processor.utils.error_handling import VideoProcessingError


def test_process_video_event_skips_non_mp4():
    """Test that process_video_event skips non-MP4 files."""
    # Call with non-MP4 file
    process_video_event("test-bucket", "daily-raw/test_file.txt")

    # No assertions needed - function should return without errors


def test_process_video_event_skips_wrong_path():
    """Test that process_video_event skips files not in the correct paths."""
    # Call with file in wrong path
    process_video_event("test-bucket", "wrong-path/test_file.mp4")

    # No assertions needed - function should return without errors


@patch("video_processor.core.processors.video.get_storage_service")
@patch("video_processor.core.processors.video.AudioProcessor")
@patch("video_processor.core.processors.video.tempfile.TemporaryDirectory")
def test_process_video_event_sets_up_job(
    mock_temp_dir, mock_audio_processor_class, mock_get_storage_service
):
    """Test that process_video_event sets up a job correctly."""
    # Set up mocks
    mock_storage_service = MagicMock()
    mock_get_storage_service.return_value = mock_storage_service

    mock_audio_processor = MagicMock()
    mock_audio_processor_class.return_value = mock_audio_processor

    mock_temp_dir_instance = MagicMock()
    mock_temp_dir_instance.__enter__.return_value = "/tmp/test_dir"
    mock_temp_dir.return_value = mock_temp_dir_instance

    # Patch process_video to isolate the job setup
    with patch(
        "video_processor.core.processors.video.process_video"
    ) as mock_process_video:
        # Call the function
        process_video_event("test-bucket", "daily-raw/test_video.mp4")

        # Verify job setup and passing to process_video
        mock_process_video.assert_called_once()

        # Extract the job argument
        job = mock_process_video.call_args[0][0]

        # Verify job properties
        assert job.bucket_name == "test-bucket"
        assert job.file_name == "daily-raw/test_video.mp4"
        assert job.metadata.title == "test_video"
        assert job.metadata.channel == "daily"
        assert job.processed_path == "processed-daily/test_video/"
        assert job.status == ProcessingStatus.PENDING
        assert job.current_stage == ProcessingStage.DOWNLOAD


@patch("video_processor.core.processors.video.get_storage_service")
@patch("video_processor.core.processors.video.AudioProcessor")
@patch("video_processor.core.processors.video.tempfile.TemporaryDirectory")
def test_process_video_normalizes_filename(
    mock_temp_dir, mock_audio_processor_class, mock_get_storage_service
):
    """Test that process_video_event normalizes filenames with spaces."""
    # Set up mocks
    mock_storage_service = MagicMock()
    mock_get_storage_service.return_value = mock_storage_service

    mock_audio_processor = MagicMock()
    mock_audio_processor_class.return_value = mock_audio_processor

    mock_temp_dir_instance = MagicMock()
    mock_temp_dir_instance.__enter__.return_value = "/tmp/test_dir"
    mock_temp_dir.return_value = mock_temp_dir_instance

    # Patch process_video to isolate the job setup
    with patch(
        "video_processor.core.processors.video.process_video"
    ) as mock_process_video:
        # Call the function with a filename containing spaces
        process_video_event("test-bucket", "daily-raw/Test Video With Spaces.mp4")

        # Verify job setup and passing to process_video
        mock_process_video.assert_called_once()

        # Extract the job argument
        job = mock_process_video.call_args[0][0]

        # Verify normalized paths
        assert job.metadata.title == "Test Video With Spaces"
        assert job.processed_path == "processed-daily/Test-Video-With-Spaces/"


def test_process_video_downloads_file():
    """Test process_video downloads the video file."""
    # Create test job
    job = VideoJob(
        bucket_name="test-bucket",
        file_name="daily-raw/test_video.mp4",
        job_id="test-job",
        metadata=VideoMetadata(title="test_video", channel="daily"),
        processed_path="processed-daily/test_video/",
    )

    # Mock storage service
    mock_storage_service = MagicMock()
    mock_storage_service.download_file.return_value = "/tmp/test_dir/test_video.mp4"

    # Mock audio processor
    mock_audio_processor = MagicMock()
    mock_audio_processor.extract_audio.return_value = "/tmp/test_dir/test_video.wav"

    # Mock temporary directory context
    with patch(
        "video_processor.core.processors.video.tempfile.TemporaryDirectory"
    ) as mock_temp_dir:
        mock_temp_dir_instance = MagicMock()
        mock_temp_dir_instance.__enter__.return_value = "/tmp/test_dir"
        mock_temp_dir.return_value = mock_temp_dir_instance

        # Also need to mock get_settings
        with patch(
            "video_processor.core.processors.video.get_settings"
        ) as mock_get_settings:
            mock_settings = MagicMock()
            mock_settings.testing_mode = False
            mock_get_settings.return_value = mock_settings

            # Call the function
            process_video(job, mock_storage_service, mock_audio_processor)

            # Verify download was called
            mock_storage_service.download_file.assert_called_once_with(
                "test-bucket",
                "daily-raw/test_video.mp4",
                "/tmp/test_dir/test_video.mp4",
            )

            # Verify job status was updated
            assert job.status == ProcessingStatus.COMPLETED
            assert ProcessingStage.DOWNLOAD in job.completed_stages


def test_process_video_handles_download_error():
    """Test process_video handles download errors."""
    # Create test job
    job = VideoJob(
        bucket_name="test-bucket",
        file_name="daily-raw/test_video.mp4",
        job_id="test-job",
        metadata=VideoMetadata(title="test_video", channel="daily"),
        processed_path="processed-daily/test_video/",
    )

    # Mock storage service that fails on download
    mock_storage_service = MagicMock()
    mock_storage_service.download_file.side_effect = StorageError("Download failed")

    # Mock audio processor
    mock_audio_processor = MagicMock()

    # Mock temporary directory context
    with patch(
        "video_processor.core.processors.video.tempfile.TemporaryDirectory"
    ) as mock_temp_dir:
        mock_temp_dir_instance = MagicMock()
        mock_temp_dir_instance.__enter__.return_value = "/tmp/test_dir"
        mock_temp_dir.return_value = mock_temp_dir_instance

        # Also need to mock get_settings
        with patch(
            "video_processor.core.processors.video.get_settings"
        ) as mock_get_settings:
            mock_settings = MagicMock()
            mock_settings.testing_mode = False
            mock_get_settings.return_value = mock_settings

            # Call the function and expect error
            with pytest.raises(VideoProcessingError) as excinfo:
                process_video(job, mock_storage_service, mock_audio_processor)

            # Verify error message
            assert "Download failed" in str(excinfo.value)

            # Verify job status was updated to failed
            assert job.status == ProcessingStatus.FAILED
            assert "Download failed" in job.error_message


def test_add_dummy_outputs():
    """Test the _add_dummy_outputs helper function."""
    from video_processor.core.processors.video import _add_dummy_outputs

    # Create test job
    job = VideoJob(
        bucket_name="test-bucket",
        file_name="daily-raw/test_video.mp4",
        job_id="test-job",
        metadata=VideoMetadata(title="test_video", channel="daily"),
        processed_path="processed-daily/test_video/",
    )

    # Mock storage service
    mock_storage_service = MagicMock()

    # Call the function
    _add_dummy_outputs(job, mock_storage_service)

    # Verify upload_from_string was called for each output file
    assert mock_storage_service.upload_from_string.call_count == 5

    # Verify output files were added to job
    assert len(job.output_files) == 5
    assert "transcript.txt" in job.output_files
    assert "subtitles.vtt" in job.output_files
    assert "shownotes.txt" in job.output_files
    assert "chapters.txt" in job.output_files
    assert "title.txt" in job.output_files

================
File: tests/conftest.py
================
"""
Pytest configuration file with common fixtures for testing.
"""

import os
import subprocess
import tempfile
from unittest.mock import MagicMock

import pytest
from flask import Flask
from google.cloud.storage import Client as StorageClient

from video_processor.config import Settings
from video_processor.core.processors.audio import AudioProcessor
from video_processor.services.storage import (
    LocalStorageService,
    StorageService,
)


@pytest.fixture
def test_settings():
    """Create test settings with testing mode enabled."""
    return Settings(
        project_id="test-project",
        region="test-region",
        testing_mode=True,
        real_api_test=False,
        local_output=True,
        gcs_upload_bucket="test-bucket",
        ai_model="test-model",
        default_privacy_status="unlisted",
        port=8080,
        debug=True,
    )


@pytest.fixture
def mock_storage_client():
    """Mock for Google Cloud Storage client."""
    mock_client = MagicMock(spec=StorageClient)
    mock_bucket = MagicMock()
    mock_blob = MagicMock()

    # Set up the chain of mocks
    mock_client.bucket.return_value = mock_bucket
    mock_bucket.blob.return_value = mock_blob
    mock_bucket.copy_blob.return_value = mock_blob
    mock_blob.exists.return_value = True

    return mock_client, mock_bucket, mock_blob


@pytest.fixture
def mock_storage_service(mock_storage_client):
    """Mock for the StorageService interface."""
    mock_client, _, _ = mock_storage_client
    mock_service = MagicMock(spec=StorageService)

    # Configure methods to return appropriate values
    mock_service.download_file.return_value = "/tmp/test_video.mp4"
    mock_service.upload_file.return_value = "processed-daily/test_video/test_video.mp4"
    mock_service.upload_from_string.return_value = (
        "processed-daily/test_video/test_file.txt"
    )
    mock_service.read_file.return_value = b"test content"
    mock_service.read_text.return_value = "test content"
    mock_service.list_files.return_value = ["file1.txt", "file2.txt"]
    mock_service.file_exists.return_value = True
    mock_service.delete_file.return_value = True
    mock_service.move_file.return_value = True
    mock_service.get_signed_url.return_value = "https://test-signed-url.com"
    mock_service.get_metadata.return_value = {"name": "test_file", "size": 1000}

    return mock_service


@pytest.fixture
def mock_local_storage_service():
    """Create a local storage service for testing."""
    # Create a temporary directory for testing
    with tempfile.TemporaryDirectory() as tmp_dir:
        service = LocalStorageService(base_path=tmp_dir)
        yield service


@pytest.fixture
def mock_generative_model():
    """Mock for Vertex AI GenerativeModel."""
    mock_model = MagicMock()
    mock_response = MagicMock()
    mock_response.text = "This is a mock response from Gemini API"
    mock_model.generate_content.return_value = mock_response

    return mock_model


@pytest.fixture
def sample_audio_file():
    """Create a temporary WAV file for testing."""
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_file:
        temp_path = temp_file.name

    try:
        # Generate a simple test tone using ffmpeg
        subprocess.run(
            [
                "ffmpeg",
                "-y",  # Overwrite output files without asking
                "-f",
                "lavfi",  # Use libavfilter
                "-i",
                "sine=frequency=440:duration=1",  # Generate a 1-second 440Hz tone
                "-ar",
                "16000",  # Audio sample rate
                "-ac",
                "1",  # Mono audio
                temp_path,
            ],
            check=True,
            capture_output=True,
            text=True,
        )

        yield temp_path
    finally:
        # Clean up the temporary file
        if os.path.exists(temp_path):
            os.unlink(temp_path)


@pytest.fixture
def sample_video_file():
    """Create a temporary MP4 file for testing."""
    with tempfile.NamedTemporaryFile(suffix=".mp4", delete=False) as temp_file:
        temp_path = temp_file.name

    try:
        # Generate a simple test video using ffmpeg
        subprocess.run(
            [
                "ffmpeg",
                "-y",  # Overwrite output files without asking
                "-f",
                "lavfi",  # Use libavfilter
                "-i",
                "sine=frequency=440:duration=1",  # Generate a 1-second 440Hz tone
                "-f",
                "lavfi",  # Use libavfilter for video
                "-i",
                "color=c=blue:s=320x240:d=1",  # Generate a 1-second blue screen
                "-c:a",
                "aac",  # Audio codec
                "-c:v",
                "h264",  # Video codec
                temp_path,
            ],
            check=True,
            capture_output=True,
            text=True,
        )

        yield temp_path
    finally:
        # Clean up the temporary file
        if os.path.exists(temp_path):
            os.unlink(temp_path)


@pytest.fixture
def audio_processor():
    """Create an AudioProcessor instance for testing."""
    return AudioProcessor(testing_mode=True)


@pytest.fixture
def mock_part():
    """Mock for Vertex AI Part object."""
    mock = MagicMock()
    mock.from_data.return_value = MagicMock()
    return mock


@pytest.fixture
def mock_flask_app():
    """Create a test Flask app."""
    app = Flask(__name__)
    app.testing = True
    return app


@pytest.fixture
def mock_cloud_event():
    """Create a mock Cloud Event for testing."""
    event = MagicMock()
    event.data = {
        "bucket": "test-bucket",
        "name": "daily-raw/test_video.mp4",
        "contentType": "video/mp4",
        "size": "1000000",
    }
    return event


@pytest.fixture
def mock_youtube_credentials():
    """Mock for YouTube API credentials."""
    mock_creds = MagicMock()
    mock_creds.refresh.return_value = None
    return mock_creds


@pytest.fixture
def mock_youtube_service():
    """Mock for YouTube API service."""
    mock_service = MagicMock()
    mock_videos = MagicMock()
    mock_captions = MagicMock()

    # Set up the chain of mocks
    mock_service.videos.return_value = mock_videos
    mock_service.captions.return_value = mock_captions

    # Mock the insert methods
    mock_insert_request = MagicMock()
    mock_insert_request.next_chunk.return_value = (None, {"id": "test_video_id"})
    mock_videos.insert.return_value = mock_insert_request

    mock_caption_request = MagicMock()
    mock_caption_request.execute.return_value = {"id": "test_caption_id"}
    mock_captions.insert.return_value = mock_caption_request

    return mock_service


@pytest.fixture
def mock_secretmanager_client():
    """Mock for Secret Manager client."""
    mock_client = MagicMock()
    mock_response = MagicMock()
    mock_response.payload.data.decode.return_value = "test_secret_value"
    mock_client.access_secret_version.return_value = mock_response
    return mock_client

================
File: tests/README.md
================
# Testing Guide for Video Processor

This directory contains tests for the video processor service, organized in a modular structure to match the application architecture.

## Test Structure

```
tests/
├── conftest.py                # Common pytest fixtures
├── unit/                      # Unit tests for individual components
│   ├── test_audio_processor.py     # Tests for audio processing
│   ├── test_storage_service.py     # Tests for storage services
│   ├── test_video_processor.py     # Tests for video processing
│   └── test_api.py                 # Tests for API endpoints
├── integration/              # Integration tests between components
├── e2e/                      # End-to-end workflow tests
└── outdated/                 # Legacy tests that need to be migrated
```

## Running Tests

Run all tests:
```bash
cd backend
pytest
```

Run specific test categories:
```bash
# Unit tests only
pytest tests/unit

# Integration tests only
pytest tests/integration

# End-to-end tests only
pytest tests/e2e
```

## Test Coverage

To run tests with coverage report:
```bash
pytest --cov=video_processor
```

## Key Testing Features

1. **Dependency Injection Testing**
   - Mock service implementations are injected for testing
   - Isolated component testing with clear boundaries

2. **Interface-Based Testing**
   - Tests verify that components follow their interface contracts
   - Ensures implementation changes don't break behavior

3. **Consistent Fixture Usage**
   - Common test fixtures in conftest.py
   - Standardized mock objects and setup

## Adding New Tests

When adding new functionality:

1. Add unit tests for the new component in the appropriate subdirectory
2. Update integration tests if the component interacts with other components
3. Ensure all code paths are covered, including error handling
4. Follow the existing test naming and organization patterns

## Legacy Tests

The `outdated/` directory contains tests from the previous architecture that:
1. Need to be migrated to the new structure
2. May be obsolete due to architectural changes
3. Should be referenced when adding test coverage for components

================
File: video_processor/api/__init__.py
================
"""
API package for HTTP endpoints.
"""

from .routes import create_app

__all__ = ["create_app"]

================
File: video_processor/api/controllers.py
================
"""
API endpoint controllers.
"""

from types import SimpleNamespace
from typing import Any, Dict, Optional, Tuple, Union

from flask import Response, jsonify

from video_processor.config import get_settings
from video_processor.services.storage import get_storage_service
from video_processor.utils.error_handling import handle_exceptions
from video_processor.utils.logging import get_logger

logger = get_logger(__name__)


def health_check() -> str:
    """
    Simple health check endpoint.

    Returns:
        Response indicating service is running
    """
    return "✅ Service is up and running."


def handle_gcs_event(
    event: Dict[str, Any], process_video_func: Optional[Any] = None
) -> Tuple[str, int]:
    """
    Handle a GCS event triggered by a file upload.

    Args:
        event: The event data (from request JSON)
        process_video_func: Optional function to process the video event

    Returns:
        Tuple of (response message, status code)
    """
    try:
        # Extract event data based on structure
        if isinstance(event, dict) and "data" in event:
            # Basic check for CloudEvent structure within POST body
            cloud_event_data = event.get("data", event)  # Use inner data if exists
        else:
            # Fallback or handle non-CloudEvent POST
            logger.warning("Received non-standard POST data format.")
            cloud_event_data = event  # Process raw data

        # Ensure data is accessible like an object for process_uploaded_video
        # If process_uploaded_video expects attributes like event.data.bucket
        cloud_event_obj = SimpleNamespace(**cloud_event_data)

        logger.info(
            f"📥 Received event data for: "
            f"{cloud_event_obj.name if hasattr(cloud_event_obj, 'name') else 'Unknown'}"
        )

        # Call the appropriate processor function
        if hasattr(cloud_event_obj, "bucket") and hasattr(cloud_event_obj, "name"):
            # Import here to avoid circular imports
            from video_processor.core.processors.video import process_video_event

            # Use the provided function or the default
            processor = (
                process_video_func
                if process_video_func is not None
                else process_video_event
            )

            # Process the event
            processor(cloud_event_obj.bucket, cloud_event_obj.name)
            return "✅ Event processed.", 200
        else:
            logger.error("cloud_event_obj missing required attributes: bucket, name")
            return "❌ Invalid event payload.", 400
    except Exception as e:
        logger.exception("❌ Error handling GCS event")
        return f"❌ Failed to handle event: {str(e)}", 500


@handle_exceptions(fallback_return=(jsonify({"error": "Internal server error"}), 500))
def get_gcs_upload_url(data: Dict[str, Any]) -> Union[Response, Tuple[Response, int]]:
    """
    Generate a signed URL for uploading a file to GCS.

    Args:
        data: Request data containing filename and content_type

    Returns:
        JSON response with upload URL and file info, or error response
    """
    settings = get_settings()

    # Validate request data
    filename = data.get("filename")
    content_type = data.get("content_type", "video/mp4")

    if not filename:
        return jsonify({"error": "Missing filename"}), 400

    # Get bucket name from settings
    bucket_name = settings.gcs_upload_bucket
    if not bucket_name:
        return jsonify({"error": "GCS bucket not configured"}), 500

    # Generate the upload URL
    storage_service = get_storage_service()
    object_path = f"videos/{filename}"

    try:
        # Generate signed URL for upload (PUT)
        url = storage_service.get_signed_url(
            bucket=bucket_name,
            path=object_path,
            expiration_minutes=15,
            http_method="PUT",
            content_type=content_type,
        )

        # Create GCS URL for later reference
        gcs_url = f"https://storage.googleapis.com/{bucket_name}/{object_path}"

        return (
            jsonify(
                {
                    "url": url,
                    "bucket": bucket_name,
                    "object_path": object_path,
                    "gcs_url": gcs_url,
                }
            ),
            200,
        )
    except Exception as e:
        logger.exception("❌ Error generating GCS signed URL")
        return jsonify({"error": f"Failed to generate signed URL: {str(e)}"}), 500

================
File: video_processor/api/routes.py
================
"""
API routes and endpoint definitions.
"""

from typing import Tuple, Union

from flask import Flask, Response, jsonify, request
from flask_cors import CORS

from video_processor.utils.logging import get_logger

from .controllers import (
    get_gcs_upload_url,
    handle_gcs_event,
    health_check,
)

logger = get_logger(__name__)


def create_app() -> Flask:
    """
    Create and configure a Flask application instance.

    Returns:
        Configured Flask application instance
    """
    # Use settings if needed later
    # settings = get_settings()

    try:
        # Create Flask app
        app = Flask(__name__)
        # Enable CORS for all routes and origins
        CORS(app)
        logger.info("✅ Flask app initialized with CORS enabled")

        # Register routes
        _register_routes(app)

        return app
    except Exception as e:
        logger.error(f"❌ Error initializing Flask: {e}", exc_info=True)
        raise


def _register_routes(app: Flask) -> None:
    """
    Register routes with the Flask application.
    """

    # Health check endpoint
    @app.route("/", methods=["GET"])
    def _health_check() -> str:
        return health_check()

    # GCS event handler endpoint
    @app.route("/", methods=["POST"])
    def _handle_gcs_event() -> Tuple[str, int]:
        try:
            event = request.get_json()

            # Extract CloudEvent headers for structured event handling
            event_id = request.headers.get("Ce-Id")
            event_type = request.headers.get("Ce-Type")
            event_source = request.headers.get("Ce-Source")
            event_subject = request.headers.get("Ce-Subject")

            if event_id:
                logger.info(
                    f"Received CloudEvent: ID={event_id}, Type={event_type}, "
                    f"Source={event_source}, Subject={event_subject}"
                )

            # Process the event
            result, status_code = handle_gcs_event(event)
            return result, status_code
        except Exception:
            logger.exception("❌ Error handling POST request")
            return "❌ Failed to handle event.", 500

    # GCS upload URL endpoint
    @app.route("/api/gcs-upload-url", methods=["POST"])
    def _get_gcs_upload_url() -> Union[Response, Tuple[Response, int]]:
        try:
            data = request.get_json()
            result = get_gcs_upload_url(data)

            if (
                isinstance(result, tuple)
                and len(result) == 2
                and isinstance(result[1], int)
            ):
                return result[0], result[1]
            return result
        except Exception:
            logger.exception("❌ Error generating GCS signed URL")
            return jsonify({"error": "Failed to generate signed URL"}), 500

================
File: video_processor/api/schemas.py
================
"""
Data validation schemas for API requests and responses.
"""

from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Union


@dataclass
class GCSUploadUrlRequest:
    """Request schema for GCS upload URL endpoint."""

    filename: str
    content_type: Optional[str] = None

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "GCSUploadUrlRequest":
        """Create from dictionary."""
        return cls(
            filename=data.get("filename", ""),
            content_type=data.get("content_type"),
        )

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary."""
        result = {"filename": self.filename}
        if self.content_type:
            result["content_type"] = self.content_type
        return result


@dataclass
class GCSUploadUrlResponse:
    """Response schema for GCS upload URL endpoint."""

    url: str
    bucket: str
    object_path: str
    gcs_url: str

    def to_dict(self) -> Dict[str, str]:
        """Convert to dictionary."""
        return {
            "url": self.url,
            "bucket": self.bucket,
            "object_path": self.object_path,
            "gcs_url": self.gcs_url,
        }


@dataclass
class ErrorResponse:
    """Common error response schema."""

    error: str
    details: Optional[Union[str, Dict[str, Any], List[str]]] = None

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary."""
        result = {"error": self.error}
        if self.details:
            result["details"] = self.details
        return result

================
File: video_processor/config/__init__.py
================
"""
Configuration package for video processor.
"""

from .settings import get_settings

__all__ = ["get_settings"]

================
File: video_processor/config/environment.py
================
"""
Environment variable handling for the video processor.
"""

import os
from typing import Any, Optional


def get_env(key: str, default: Any = None, required: bool = False) -> Any:
    """
    Get an environment variable.

    Args:
        key: The environment variable name
        default: Default value if not found
        required: Whether the variable is required (raises error if missing)

    Returns:
        The environment variable value or default

    Raises:
        ValueError: If required is True and the variable is not set
    """
    value = os.environ.get(key)
    if value is None:
        if required:
            raise ValueError(f"Required environment variable {key} is not set")
        return default
    return value


def get_bool_env(key: str, default: bool = False) -> bool:
    """
    Get a boolean environment variable.

    Args:
        key: The environment variable name
        default: Default value if not found

    Returns:
        The boolean value (True for 'true', 'yes', '1', etc.)
    """
    value = get_env(key)
    if value is None:
        return default
    return value.lower() in ("true", "yes", "1", "y", "t")


def get_int_env(key: str, default: Optional[int] = None) -> Optional[int]:
    """
    Get an integer environment variable.

    Args:
        key: The environment variable name
        default: Default value if not found or not convertible

    Returns:
        The integer value or default
    """
    value = get_env(key)
    if value is None:
        return default
    try:
        return int(value)
    except ValueError:
        return default

================
File: video_processor/config/settings.py
================
"""
Settings management for the video processor.
"""

from dataclasses import dataclass
from functools import lru_cache

from .environment import get_bool_env, get_env, get_int_env


@dataclass
class Settings:
    """Application settings loaded from environment variables."""

    # General settings
    project_id: str
    region: str
    testing_mode: bool
    real_api_test: bool
    local_output: bool

    # GCS settings
    gcs_upload_bucket: str

    # AI model settings
    ai_model: str

    # YouTube settings
    default_privacy_status: str

    # Flask settings
    port: int
    debug: bool


@lru_cache()
def get_settings() -> Settings:
    """
    Get application settings from environment variables with defaults.

    Returns:
        Settings: Application settings
    """
    return Settings(
        # General settings
        project_id=get_env("GOOGLE_CLOUD_PROJECT", "automations-457120"),
        region=get_env("REGION", "us-east1"),
        testing_mode=get_bool_env("TESTING_MODE", False),
        real_api_test=get_bool_env("REAL_API_TEST", False),
        local_output=get_bool_env("LOCAL_OUTPUT", False),
        # GCS settings
        gcs_upload_bucket=get_env(
            "GCS_UPLOAD_BUCKET", "automations-youtube-videos-2025"
        ),
        # AI model settings
        ai_model=get_env("MODEL", "gemini-2.0-flash-001"),
        # YouTube settings
        default_privacy_status=get_env("DEFAULT_PRIVACY_STATUS", "unlisted"),
        # Flask settings
        port=get_int_env("PORT", 8080),
        debug=get_bool_env("DEBUG", False),
    )

================
File: video_processor/core/models/__init__.py
================
"""
Domain models for the video processor.
"""

from .video_job import ProcessingStage, ProcessingStatus, VideoJob, VideoMetadata

__all__ = ["VideoJob", "VideoMetadata", "ProcessingStage", "ProcessingStatus"]

================
File: video_processor/core/models/video_job.py
================
"""
Video job models for tracking processing state.
"""

from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum, auto
from typing import Dict, List, Optional


class ProcessingStage(Enum):
    """Stages of video processing."""

    DOWNLOAD = auto()
    EXTRACT_AUDIO = auto()
    GENERATE_TRANSCRIPT = auto()
    GENERATE_SUBTITLES = auto()
    GENERATE_SHOWNOTES = auto()
    GENERATE_CHAPTERS = auto()
    GENERATE_TITLES = auto()
    UPLOAD_OUTPUTS = auto()
    UPLOAD_TO_YOUTUBE = auto()
    COMPLETE = auto()


class ProcessingStatus(Enum):
    """Status of job processing."""

    PENDING = auto()
    IN_PROGRESS = auto()
    COMPLETED = auto()
    FAILED = auto()
    PARTIAL = auto()  # Some stages completed but not all


@dataclass
class VideoMetadata:
    """Metadata for a video."""

    title: str
    description: Optional[str] = None
    keywords: Optional[str] = None
    category_id: str = "22"  # Default to "People & Blogs"
    duration_seconds: Optional[int] = None
    width: Optional[int] = None
    height: Optional[int] = None
    channel: str = "daily"  # 'daily' or 'main'

    def to_dict(self) -> Dict:
        """Convert to dictionary for storage."""
        return {
            "title": self.title,
            "description": self.description,
            "keywords": self.keywords,
            "category_id": self.category_id,
            "duration_seconds": self.duration_seconds,
            "width": self.width,
            "height": self.height,
            "channel": self.channel,
        }

    @classmethod
    def from_dict(cls, data: Dict) -> "VideoMetadata":
        """Create from dictionary."""
        return cls(
            title=data.get("title", ""),
            description=data.get("description"),
            keywords=data.get("keywords"),
            category_id=data.get("category_id", "22"),
            duration_seconds=data.get("duration_seconds"),
            width=data.get("width"),
            height=data.get("height"),
            channel=data.get("channel", "daily"),
        )


@dataclass
class VideoJob:
    """
    A video processing job.

    Tracks the state of a video being processed, including its
    metadata, processing stage, and output paths.
    """

    bucket_name: str
    file_name: str
    job_id: str
    created_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)
    metadata: VideoMetadata = field(default_factory=lambda: VideoMetadata(title=""))
    status: ProcessingStatus = ProcessingStatus.PENDING
    current_stage: ProcessingStage = ProcessingStage.DOWNLOAD
    completed_stages: List[ProcessingStage] = field(default_factory=list)
    error_message: Optional[str] = None
    processed_path: Optional[str] = None
    output_files: Dict[str, str] = field(default_factory=dict)
    youtube_video_id: Optional[str] = None

    def update_status(
        self, status: ProcessingStatus, error: Optional[str] = None
    ) -> None:
        """Update the job status and timestamp."""
        self.status = status
        self.updated_at = datetime.now()
        if error:
            self.error_message = error

    def move_to_stage(self, stage: ProcessingStage) -> None:
        """Move to a new processing stage."""
        if self.current_stage not in self.completed_stages:
            self.completed_stages.append(self.current_stage)
        self.current_stage = stage
        self.updated_at = datetime.now()

    def complete_current_stage(self) -> None:
        """Mark the current stage as completed."""
        if self.current_stage not in self.completed_stages:
            self.completed_stages.append(self.current_stage)
        self.updated_at = datetime.now()

    def is_stage_completed(self, stage: ProcessingStage) -> bool:
        """Check if a stage has been completed."""
        return stage in self.completed_stages

    def add_output_file(self, file_type: str, file_path: str) -> None:
        """Add an output file to the job."""
        self.output_files[file_type] = file_path
        self.updated_at = datetime.now()

    def to_dict(self) -> Dict:
        """Convert to dictionary for storage."""
        return {
            "bucket_name": self.bucket_name,
            "file_name": self.file_name,
            "job_id": self.job_id,
            "created_at": self.created_at.isoformat(),
            "updated_at": self.updated_at.isoformat(),
            "metadata": self.metadata.to_dict(),
            "status": self.status.name,
            "current_stage": self.current_stage.name,
            "completed_stages": [stage.name for stage in self.completed_stages],
            "error_message": self.error_message,
            "processed_path": self.processed_path,
            "output_files": self.output_files,
            "youtube_video_id": self.youtube_video_id,
        }

    @classmethod
    def from_dict(cls, data: Dict) -> "VideoJob":
        """Create from dictionary."""
        return cls(
            bucket_name=data["bucket_name"],
            file_name=data["file_name"],
            job_id=data["job_id"],
            created_at=datetime.fromisoformat(data["created_at"]),
            updated_at=datetime.fromisoformat(data["updated_at"]),
            metadata=VideoMetadata.from_dict(data["metadata"]),
            status=ProcessingStatus[data["status"]],
            current_stage=ProcessingStage[data["current_stage"]],
            completed_stages=[ProcessingStage[s] for s in data["completed_stages"]],
            error_message=data.get("error_message"),
            processed_path=data.get("processed_path"),
            output_files=data.get("output_files", {}),
            youtube_video_id=data.get("youtube_video_id"),
        )

================
File: video_processor/core/processors/__init__.py
================
"""
Processor modules for different aspects of video processing.
"""

from .audio import AudioProcessor
from .chapters import ChaptersProcessor
from .transcript import TranscriptProcessor
from .video import VideoProcessor

__all__ = [
    "VideoProcessor",
    "AudioProcessor",
    "TranscriptProcessor",
    "ChaptersProcessor",
]

================
File: video_processor/core/processors/audio.py
================
"""
Audio processing functionality.
"""

import os
import subprocess
from typing import Optional

from video_processor.utils.error_handling import VideoProcessingError, handle_exceptions
from video_processor.utils.logging import get_logger

logger = get_logger(__name__)


class AudioProcessor:
    """
    Handles extraction and processing of audio from video files.
    """

    def __init__(self, testing_mode: bool = False):
        """
        Initialize the audio processor.

        Args:
            testing_mode: Whether to run in testing mode (skips actual processing)
        """
        self.testing_mode = testing_mode

    def extract_audio(
        self,
        video_path: str,
        output_path: Optional[str] = None,
        sample_rate: int = 16000,
        channels: int = 1,
    ) -> str:
        """
        Extract audio from a video file using ffmpeg.

        Args:
            video_path: Path to the video file
            output_path: Path to save the audio file
                (if None, uses video path with .wav)
            sample_rate: Audio sample rate in Hz
            channels: Number of audio channels (1=mono, 2=stereo)

        Returns:
            Path to the extracted audio file

        Raises:
            VideoProcessingError: If extraction fails
        """
        if not os.path.exists(video_path):
            raise VideoProcessingError(f"Video file does not exist: {video_path}")

        # Generate output path if not provided
        if output_path is None:
            base_path = os.path.splitext(video_path)[0]
            output_path = f"{base_path}.wav"

        logger.info(f"Extracting audio from {video_path} to {output_path}...")

        # For testing mode, just create a dummy WAV file
        if self.testing_mode:
            logger.info(
                "TESTING MODE: Creating dummy WAV file instead of running ffmpeg"
            )
            try:
                with open(output_path, "wb") as f:
                    # Write a minimal WAV header + some data
                    f.write(
                        b"RIFF\x24\x00\x00\x00WAVEfmt \x10\x00\x00\x00\x01\x00\x01\x00"
                        b"\x80>\x00\x00\x00\x7d\x00\x00\x02\x00\x10\x00data\x00\x00\x00\x00"
                    )
                logger.info("Created dummy WAV file for testing.")
                return output_path
            except Exception as e:
                logger.error(f"Failed to create dummy WAV file: {str(e)}")
                raise VideoProcessingError(
                    f"Failed to create dummy WAV file in testing mode: {str(e)}"
                ) from e

        # Check if the file is a valid video
        file_info = self._check_file_validity(video_path)
        if not file_info:
            # Create a dummy WAV file as fallback
            logger.warning(
                f"File {video_path} doesn't appear to be a valid video. "
                f"Creating dummy WAV as fallback."
            )
            with open(output_path, "wb") as f:
                f.write(
                    b"RIFF\x24\x00\x00\x00WAVEfmt \x10\x00\x00\x00\x01\x00\x01\x00"
                    b"\x80>\x00\x00\x00\x7d\x00\x00\x02\x00\x10\x00data\x00\x00\x00\x00"
                )
            return output_path

        # Run ffmpeg with error handling
        try:
            result = self._run_ffmpeg(
                video_path=video_path,
                output_path=output_path,
                sample_rate=sample_rate,
                channels=channels,
            )
            if result:
                logger.info("Audio extraction complete.")
                return output_path
            else:
                # Fallback to dummy file if ffmpeg fails
                self._create_dummy_wav(output_path)
                return output_path
        except Exception as e:
            logger.error(f"Error during audio extraction: {type(e).__name__}: {str(e)}")
            # Create a dummy file as fallback
            self._create_dummy_wav(output_path)
            return output_path

    @handle_exceptions(fallback_return=None)
    def _check_file_validity(self, file_path: str) -> Optional[str]:
        """Check if a file is a valid video using the 'file' command."""
        try:
            file_info = subprocess.run(
                ["file", file_path],
                check=True,
                capture_output=True,
                text=True,
            )
            logger.info(f"File info: {file_info.stdout}")

            # Check if output suggests this is a video file
            if (
                "MP4" in file_info.stdout
                or "video" in file_info.stdout
                or "mov" in file_info.stdout.lower()
                or "avi" in file_info.stdout.lower()
            ):
                return file_info.stdout
            return None
        except Exception as e:
            logger.warning(
                f"Failed to check file type: {type(e).__name__}: {str(e)}. "
                f"Proceeding with ffmpeg anyway."
            )
            return None

    def _run_ffmpeg(
        self, video_path: str, output_path: str, sample_rate: int, channels: int
    ) -> bool:
        """Run ffmpeg to extract audio."""
        try:
            subprocess.run(
                [
                    "ffmpeg",
                    "-y",  # Overwrite output files without asking
                    "-i",
                    video_path,
                    "-vn",  # No video output
                    "-acodec",
                    "pcm_s16le",  # Standard WAV format
                    "-ar",
                    str(sample_rate),  # Audio sample rate
                    "-ac",
                    str(channels),  # Mono audio
                    output_path,
                ],
                check=True,
                capture_output=True,
                text=True,
            )
            return True
        except subprocess.CalledProcessError as e:
            logger.error(
                f"ffmpeg failed with code {e.returncode}: {str(e)}\n"
                f"Command: {e.cmd}\n"
                f"Error output: {e.stderr}"
            )
            return False

    def _create_dummy_wav(self, output_path: str) -> None:
        """Create a dummy WAV file for testing or as fallback."""
        logger.info(f"Creating dummy WAV file at {output_path}")
        try:
            with open(output_path, "wb") as f:
                # Write a minimal WAV header + some data
                f.write(
                    b"RIFF\x24\x00\x00\x00WAVEfmt \x10\x00\x00\x00\x01\x00\x01\x00"
                    b"\x80>\x00\x00\x00\x7d\x00\x00\x02\x00\x10\x00data\x00\x00\x00\x00"
                )
        except Exception as e:
            logger.error(
                f"Failed to create dummy WAV file: {type(e).__name__}: {str(e)}"
            )
            raise VideoProcessingError(
                f"Failed to create fallback WAV file: {str(e)}"
            ) from e

================
File: video_processor/core/processors/chapters.py
================
"""
Chapters generation processor.
"""

import json
from typing import Any, Dict, List

from video_processor.utils.error_handling import retry
from video_processor.utils.logging import get_logger

logger = get_logger(__name__)


class ChaptersProcessor:
    """
    Handles generation of video chapters from audio content.
    """

    def __init__(self, model_service: Any, testing_mode: bool = False):
        """
        Initialize the chapters processor.

        Args:
            model_service: Service for AI model access
            testing_mode: Whether to run in testing mode (returns mock data)
        """
        self.model_service = model_service
        self.testing_mode = testing_mode

    @retry(max_attempts=2)
    def generate_chapters(self, audio_path: str) -> List[Dict[str, str]]:
        """
        Generate timestamped chapters from audio.

        Args:
            audio_path: Path to the audio file

        Returns:
            List of chapter dictionaries with timecode and summary

        Raises:
            Exception: If generation fails
        """
        try:
            logger.info(f"Generating chapters for {audio_path}")

            # For testing mode, return mock chapters
            if self.testing_mode:
                logger.info("TESTING MODE: Returning mock chapters")
                return self._get_mock_chapters()

            # Read the audio file
            with open(audio_path, "rb") as f:
                audio_content = f.read()

            # Prompt for chapter generation
            # Build a structured prompt for chapter generation
            prompt = (
                # Instructions for content
                "Chapterize the video content by grouping the content into chapters "
                "and providing a summary for each chapter. "
                "Please only capture key events and highlights. "
                "If you are not sure about any info, please do not make it up.\n\n"
                # Response format instructions
                "Return the result ONLY as a valid JSON array of objects, "
                "where each object has "
                'the keys "timecode" (string, e.g., "00:00") and '
                '"chapterSummary" (string). '
                "Aim for chapters roughly every 2 minutes.\n\n"
                # Example format
                "Example JSON output format:\n"
                "[\n"
                '  {"timecode": "00:00", "chapterSummary": '
                '"Introduction to the topic..."},\n'
                '  {"timecode": "02:01", "chapterSummary": '
                '"Discussing the first main point..."}\n'
                "]"
            )

            # Generate chapters using the model service
            response = self.model_service.generate_content(
                prompt=prompt,
                content=audio_content,
                content_type="audio/wav",
                config={
                    "temperature": 0.6,
                    "response_mime_type": "application/json",
                },
            )

            # Parse the JSON response
            try:
                if hasattr(response, "text"):
                    chapters = json.loads(response.text)
                else:
                    chapters = json.loads(str(response))

                # Validate the structure
                if not isinstance(chapters, list):
                    raise ValueError("Response is not a list")

                # Validate each chapter
                for chapter in chapters:
                    if not isinstance(chapter, dict):
                        raise ValueError(f"Chapter is not a dictionary: {chapter}")
                    if "timecode" not in chapter or "chapterSummary" not in chapter:
                        raise ValueError(f"Chapter missing required keys: {chapter}")

                logger.info(f"Generated {len(chapters)} chapters")
                return chapters
            except json.JSONDecodeError as e:
                logger.error(
                    f"Failed to parse chapters JSON: {e}. Response: {response}"
                )
                # Fall back to mock chapters if parsing fails
                return self._get_mock_chapters()

        except Exception as e:
            logger.error(f"Failed to generate chapters: {e}")
            # Return empty list on failure
            return []

    def format_chapters_text(self, chapters: List[Dict[str, str]]) -> str:
        """
        Format chapters as text.

        Args:
            chapters: List of chapter dictionaries

        Returns:
            Formatted chapters text
        """
        # Join chapters into a text format
        lines = []
        for chapter in chapters:
            lines.append(f"{chapter['timecode']} - {chapter['chapterSummary']}")

        return "\n".join(lines)

    def _get_mock_chapters(self) -> List[Dict[str, str]]:
        """
        Generate mock chapters for testing.

        Returns:
            List of mock chapters
        """
        return [
            {"timecode": "00:00", "chapterSummary": "Introduction to the topic"},
            {"timecode": "02:00", "chapterSummary": "Background and context"},
            {"timecode": "04:00", "chapterSummary": "Main points and discussion"},
            {"timecode": "06:00", "chapterSummary": "Analysis and implications"},
            {"timecode": "08:00", "chapterSummary": "Conclusion and next steps"},
        ]

================
File: video_processor/core/processors/transcript.py
================
"""
Transcript generation processor.
"""

from typing import Any

from video_processor.utils.error_handling import TranscriptionError, retry
from video_processor.utils.logging import get_logger

logger = get_logger(__name__)


class TranscriptProcessor:
    """
    Handles generation of transcripts from audio files.
    """

    def __init__(self, model_service: Any, testing_mode: bool = False):
        """
        Initialize the transcript processor.

        Args:
            model_service: Service for AI model access
            testing_mode: Whether to run in testing mode (returns mock data)
        """
        self.model_service = model_service
        self.testing_mode = testing_mode

    @retry(max_attempts=3)
    def generate_transcript(self, audio_path: str) -> str:
        """
        Generate a transcript from an audio file.

        Args:
            audio_path: Path to the audio file

        Returns:
            Generated transcript text

        Raises:
            TranscriptionError: If generation fails
        """
        try:
            logger.info(f"Generating transcript for {audio_path}")

            # For testing mode, return a mock transcript
            if self.testing_mode:
                logger.info("TESTING MODE: Returning mock transcript")
                return self._get_mock_transcript()

            # Read the audio file
            with open(audio_path, "rb") as f:
                audio_content = f.read()

            # Generate transcript using the model service
            prompt = (
                "Generate a transcription of the audio, only extract speech "
                "and ignore background audio."
            )
            response = self.model_service.generate_content(
                prompt=prompt,
                content=audio_content,
                content_type="audio/wav",
                config={"temperature": 0.2},
            )

            # Extract and clean the transcript
            transcript = self._clean_transcript(response)
            logger.info(f"Generated transcript ({len(transcript)} chars)")

            return transcript
        except Exception as e:
            logger.error(f"Failed to generate transcript: {e}")
            raise TranscriptionError(f"Failed to generate transcript: {e}") from e

    def _clean_transcript(self, response: Any) -> str:
        """
        Clean and format the transcript response.

        Args:
            response: Raw response from the model

        Returns:
            Cleaned transcript text
        """
        # Extract text from response
        if hasattr(response, "text") and response.text:
            transcript = response.text.strip()
        elif isinstance(response, str):
            transcript = response.strip()
        else:
            # Try to get text from the response or use empty string
            transcript = getattr(response, "text", str(response)).strip()

        # Basic cleaning (can be expanded)
        lines = transcript.split("\n")
        cleaned_lines = []

        for line in lines:
            # Remove common artifacts and unnecessary prefixes
            cleaned = line.strip()

            # Skip empty lines
            if not cleaned:
                continue

            # Remove timestamp prefixes (common in some models)
            if cleaned and cleaned[0].isdigit() and ":" in cleaned[:6]:
                parts = cleaned.split(" ", 1)
                if len(parts) > 1:
                    cleaned = parts[1]

            cleaned_lines.append(cleaned)

        return "\n".join(cleaned_lines)

    def _get_mock_transcript(self) -> str:
        """
        Generate a mock transcript for testing.

        Returns:
            Mock transcript text
        """
        return (
            "This is a mock transcript for testing purposes.\n"
            "It simulates what would be returned by the model in production.\n\n"
            "The transcript would contain all the spoken content from the video,\n"
            "formatted into paragraphs based on natural speech breaks.\n\n"
            "Speaker 1: This demonstrates a speaker change.\n"
            "Speaker 2: And here's another speaker responding.\n"
        )

================
File: video_processor/core/processors/video.py
================
"""
Video processing core functionality.
"""

import os
import tempfile
import uuid
from typing import Any

from video_processor.config import get_settings
from video_processor.core.models import (
    ProcessingStage,
    ProcessingStatus,
    VideoJob,
    VideoMetadata,
)
from video_processor.services.storage import get_storage_service
from video_processor.utils.error_handling import VideoProcessingError
from video_processor.utils.file_handling import normalize_filename
from video_processor.utils.logging import get_logger

from .audio import AudioProcessor

logger = get_logger(__name__)


def process_video_event(bucket_name: str, file_name: str) -> None:
    """
    Process a video upload event.

    Args:
        bucket_name: Name of the GCS bucket
        file_name: Path to the file in the bucket

    Raises:
        VideoProcessingError: If processing fails
    """
    settings = get_settings()

    # Skip non-target files
    if not file_name.endswith(".mp4") or not file_name.startswith(
        ("daily-raw/", "main-raw/")
    ):
        logger.info(f"Skipping non-target file: {file_name}")
        return

    # Create a job ID for tracking
    job_id = str(uuid.uuid4())
    logger.info(
        f"Starting video processing job {job_id} for gs://{bucket_name}/{file_name}"
    )

    # Initialize components with dependency injection
    storage_service = get_storage_service(
        testing_mode=settings.testing_mode, local_output=settings.local_output
    )
    audio_processor = AudioProcessor(testing_mode=settings.testing_mode)

    # Create video job object
    base_name = os.path.splitext(os.path.basename(file_name))[0]
    base_name_normalized = normalize_filename(base_name)
    channel = "daily" if file_name.startswith("daily-raw/") else "main"
    processed_path = f"processed-{channel}/{base_name_normalized}/"

    # Initialize metadata
    metadata = VideoMetadata(title=base_name, channel=channel)

    # Create video job
    job = VideoJob(
        bucket_name=bucket_name,
        file_name=file_name,
        job_id=job_id,
        metadata=metadata,
        processed_path=processed_path,
    )

    # Log job initialization
    logger.info(f"Initialized job {job.job_id}:")
    logger.info(f"  Original file: gs://{bucket_name}/{file_name}")
    logger.info(f"  Output path: gs://{bucket_name}/{processed_path}")

    try:
        # Process the video
        process_video(job, storage_service, audio_processor)

        # Log successful completion
        logger.info(f"✅ Successfully processed job {job.job_id}")
    except Exception as e:
        # Log error and update job status
        logger.error(
            f"❌ Error processing job {job.job_id}: {type(e).__name__}: {str(e)}",
            exc_info=True,
        )
        job.update_status(ProcessingStatus.FAILED, str(e))
        raise VideoProcessingError(f"Failed to process video: {str(e)}") from e


def process_video(
    job: VideoJob, storage_service: Any, audio_processor: AudioProcessor
) -> None:
    """
    Process a video job through all stages.

    Args:
        job: Video job to process
        storage_service: Storage service instance
        audio_processor: Audio processor instance

    Raises:
        VideoProcessingError: If processing fails
    """
    settings = get_settings()

    # Create a temporary directory for processing
    with tempfile.TemporaryDirectory() as tmpdir:
        # Update job status
        job.update_status(ProcessingStatus.IN_PROGRESS)

        # Set file paths
        video_filename = os.path.basename(job.file_name)
        video_path = os.path.join(tmpdir, video_filename)
        base_name = os.path.splitext(video_filename)[0]
        audio_path = os.path.join(tmpdir, f"{base_name}.wav")

        # Execute each processing stage
        _download_video(job, storage_service, video_path, settings)

        # Extract audio from video
        _extract_audio(job, audio_processor, video_path, audio_path)

        # Implement additional processing stages

        # Upload results
        _upload_results(job, storage_service, video_filename, settings)

        # Complete the job
        job.move_to_stage(ProcessingStage.COMPLETE)
        job.update_status(ProcessingStatus.COMPLETED)


def _download_video(
    job: VideoJob, storage_service: Any, video_path: str, settings: Any
) -> None:
    """Download the video file from storage."""
    job.current_stage = ProcessingStage.DOWNLOAD
    try:
        logger.info(
            f"Downloading video from gs://{job.bucket_name}/{job.file_name} "
            f"to local path {video_path}..."
        )

        # If in testing mode, create dummy file
        if getattr(settings, "testing_mode", False):
            logger.info("TESTING MODE: Creating dummy video file")
            with open(video_path, "wb") as f:
                f.write(b"DUMMY MP4 FILE")
        else:
            # Download the video
            storage_service.download_file(job.bucket_name, job.file_name, video_path)

        logger.info("Download complete.")
        job.complete_current_stage()
    except Exception as e:
        logger.error(f"Failed to download video: {type(e).__name__}: {str(e)}")
        job.update_status(ProcessingStatus.FAILED, f"Download failed: {str(e)}")
        raise VideoProcessingError(
            f"Failed to download video from {job.bucket_name}/{job.file_name}: {str(e)}"
        ) from e


def _extract_audio(
    job: VideoJob, audio_processor: AudioProcessor, video_path: str, audio_path: str
) -> None:
    """Extract audio from the video file."""
    job.move_to_stage(ProcessingStage.EXTRACT_AUDIO)
    try:
        logger.info(f"Extracting audio from {video_path} to {audio_path}...")
        audio_processor.extract_audio(video_path, audio_path)
        logger.info("Audio extraction complete.")
        job.complete_current_stage()
    except Exception as e:
        logger.error(f"Failed to extract audio: {type(e).__name__}: {str(e)}")
        job.update_status(ProcessingStatus.FAILED, f"Audio extraction failed: {str(e)}")
        raise VideoProcessingError(
            f"Failed to extract audio from {video_path}: {str(e)}"
        ) from e


def _add_dummy_outputs(job: VideoJob, storage_service: Any) -> None:
    """
    Add dummy output files for testing or initial structure.
    In a production implementation, this would be replaced with real outputs.
    """
    # Add dummy content for each output type
    dummy_outputs = {
        "transcript.txt": "This is a dummy transcript for testing purposes.",
        "subtitles.vtt": (
            "WEBVTT\n\n"
            "00:00:00.000 --> 00:00:05.000\n"
            "This is a dummy subtitle for testing.\n\n"
            "00:00:05.000 --> 00:00:10.000\n"
            "More dummy subtitles."
        ),
        "shownotes.txt": (
            "# Dummy Shownotes\n\n"
            "## Key takeaways\n"
            "- Point 1\n"
            "- Point 2\n\n"
            "## Resources mentioned\n"
            "- [Example link](https://example.com)"
        ),
        "chapters.txt": (
            "00:00 - Introduction\n" "02:00 - Main content\n" "05:00 - Conclusion"
        ),
        "title.txt": job.metadata.title,
    }

    # Upload each dummy output
    for filename, content in dummy_outputs.items():
        output_path = f"{job.processed_path}{filename}"
        storage_service.upload_from_string(
            bucket=job.bucket_name, content=content, destination_path=output_path
        )
        job.add_output_file(filename, output_path)
        logger.info(f"Added dummy output: {output_path}")


def _upload_results(
    job: VideoJob, storage_service: Any, video_filename: str, settings: Any
) -> None:
    """Upload job results to storage."""
    job.move_to_stage(ProcessingStage.UPLOAD_RESULTS)
    try:
        logger.info(f"Uploading results for job {job.job_id}...")

        # In testing mode, generate dummy outputs
        if getattr(settings, "testing_mode", False):
            logger.info("TESTING MODE: Generating dummy outputs")
            _add_dummy_outputs(job, storage_service)
        else:
            # Here we would process and upload real outputs
            # This is a placeholder for implementation
            logger.info("Uploading actual results - implementation needed")
            _add_dummy_outputs(job, storage_service)  # Temporary while implementing

        logger.info("Upload results complete.")
        job.complete_current_stage()
    except Exception as e:
        logger.error(f"Failed to upload outputs: {type(e).__name__}: {str(e)}")
        job.update_status(ProcessingStatus.PARTIAL, f"Output upload failed: {str(e)}")
        # Don't re-raise, try to continue

================
File: video_processor/core/__init__.py
================
"""
Core domain logic for video processing.
"""

================
File: video_processor/services/storage/__init__.py
================
"""
Storage service package for interacting with different storage backends.
"""

from .base import StorageService
from .factory import get_storage_service
from .gcs import GCSStorageService
from .local import LocalStorageService

__all__ = [
    "StorageService",
    "GCSStorageService",
    "LocalStorageService",
    "get_storage_service",
]

================
File: video_processor/services/storage/base.py
================
"""
Base storage service interface.
"""

from abc import ABC, abstractmethod
from typing import Any, BinaryIO, Dict, List, Optional, Union

# Type alias for file-like objects
FileContent = Union[str, bytes, BinaryIO]


class StorageService(ABC):
    """
    Abstract base class for storage services.

    This interface defines the methods that all storage services must implement,
    providing a consistent API regardless of the underlying storage system.
    """

    @abstractmethod
    def download_file(
        self, bucket: str, source_path: str, destination_path: str
    ) -> str:
        """
        Download a file from storage.

        Args:
            bucket: Storage bucket name
            source_path: Path to the file in storage
            destination_path: Local path to save the file

        Returns:
            Local path to the downloaded file

        Raises:
            StorageError: If the download fails
        """
        pass

    @abstractmethod
    def upload_file(self, bucket: str, source_path: str, destination_path: str) -> str:
        """
        Upload a file to storage.

        Args:
            bucket: Storage bucket name
            source_path: Local path to the file
            destination_path: Path to save the file in storage

        Returns:
            Storage path to the uploaded file

        Raises:
            StorageError: If the upload fails
        """
        pass

    @abstractmethod
    def upload_from_string(
        self, bucket: str, content: FileContent, destination_path: str
    ) -> str:
        """
        Upload content directly to storage.

        Args:
            bucket: Storage bucket name
            content: Content to upload (string, bytes, or file-like object)
            destination_path: Path to save the file in storage

        Returns:
            Storage path to the uploaded file

        Raises:
            StorageError: If the upload fails
        """
        pass

    @abstractmethod
    def read_file(self, bucket: str, path: str) -> bytes:
        """
        Read a file from storage as bytes.

        Args:
            bucket: Storage bucket name
            path: Path to the file in storage

        Returns:
            File content as bytes

        Raises:
            StorageError: If the read fails
        """
        pass

    @abstractmethod
    def read_text(self, bucket: str, path: str, encoding: str = "utf-8") -> str:
        """
        Read a file from storage as text.

        Args:
            bucket: Storage bucket name
            path: Path to the file in storage
            encoding: Text encoding to use

        Returns:
            File content as text

        Raises:
            StorageError: If the read fails
        """
        pass

    @abstractmethod
    def list_files(self, bucket: str, prefix: Optional[str] = None) -> List[str]:
        """
        List files in a bucket with optional prefix.

        Args:
            bucket: Storage bucket name
            prefix: Optional prefix to filter files

        Returns:
            List of file paths

        Raises:
            StorageError: If the listing fails
        """
        pass

    @abstractmethod
    def file_exists(self, bucket: str, path: str) -> bool:
        """
        Check if a file exists.

        Args:
            bucket: Storage bucket name
            path: Path to the file in storage

        Returns:
            True if the file exists, False otherwise
        """
        pass

    @abstractmethod
    def delete_file(self, bucket: str, path: str) -> bool:
        """
        Delete a file from storage.

        Args:
            bucket: Storage bucket name
            path: Path to the file in storage

        Returns:
            True if the file was deleted, False otherwise
        """
        pass

    @abstractmethod
    def move_file(self, bucket: str, source_path: str, destination_path: str) -> bool:
        """
        Move a file within a bucket.

        Args:
            bucket: Storage bucket name
            source_path: Original path to the file
            destination_path: New path for the file

        Returns:
            True if the file was moved, False otherwise
        """
        pass

    @abstractmethod
    def get_signed_url(
        self,
        bucket: str,
        path: str,
        expiration_minutes: int = 15,
        http_method: str = "GET",
        content_type: Optional[str] = None,
    ) -> str:
        """
        Generate a signed URL for a file.

        Args:
            bucket: Storage bucket name
            path: Path to the file in storage
            expiration_minutes: URL expiration time in minutes
            http_method: HTTP method for the URL (GET, PUT, etc.)
            content_type: Content type for uploads

        Returns:
            Signed URL

        Raises:
            StorageError: If URL generation fails
        """
        pass

    @abstractmethod
    def get_metadata(self, bucket: str, path: str) -> Dict[str, Any]:
        """
        Get metadata for a file.

        Args:
            bucket: Storage bucket name
            path: Path to the file in storage

        Returns:
            File metadata as a dictionary

        Raises:
            StorageError: If metadata retrieval fails
        """
        pass

================
File: video_processor/services/storage/factory.py
================
"""
Factory for creating storage service instances.
"""

from typing import Optional

from video_processor.config import get_settings
from video_processor.utils.logging import get_logger

from .base import StorageService
from .gcs import GCSStorageService
from .local import LocalStorageService

logger = get_logger(__name__)


def get_storage_service(
    testing_mode: Optional[bool] = None,
    local_output: Optional[bool] = None,
) -> StorageService:
    """
    Get an appropriate storage service based on configuration.

    Args:
        testing_mode: Override testing mode setting
        local_output: Override local output setting

    Returns:
        An instance of StorageService
    """
    settings = get_settings()

    # Use provided values or fall back to settings
    testing = testing_mode if testing_mode is not None else settings.testing_mode
    local = local_output if local_output is not None else settings.local_output

    # In testing or local output mode, use local storage
    if testing or local:
        logger.info("Using LocalStorageService for testing/local output")
        return LocalStorageService()

    # Otherwise, use GCS
    logger.info("Using GCSStorageService for production")
    return GCSStorageService()

================
File: video_processor/services/storage/gcs.py
================
"""
Google Cloud Storage implementation of the storage service.
"""

import os
from datetime import timedelta
from typing import Any, Dict, List, Optional

from google.cloud import storage

from video_processor.utils.error_handling import StorageError, retry
from video_processor.utils.logging import get_logger

from .base import FileContent, StorageService

logger = get_logger(__name__)


class GCSStorageService(StorageService):
    """
    Google Cloud Storage implementation of the storage service.
    """

    def __init__(self, client: Optional[storage.Client] = None):
        """
        Initialize the GCS storage service.

        Args:
            client: Optional storage client (if None, creates a new client)
        """
        self.client = client if client is not None else storage.Client()

    @retry(max_attempts=3)
    def download_file(
        self, bucket: str, source_path: str, destination_path: str
    ) -> str:
        """
        Download a file from GCS.

        Args:
            bucket: GCS bucket name
            source_path: Path to the file in GCS
            destination_path: Local path to save the file

        Returns:
            Local path to the downloaded file

        Raises:
            StorageError: If the download fails
        """
        try:
            logger.info(
                f"Downloading gs://{bucket}/{source_path} to {destination_path}"
            )
            bucket_obj = self.client.bucket(bucket)
            blob = bucket_obj.blob(source_path)

            # Check if the blob exists
            if not blob.exists():
                raise StorageError(f"File gs://{bucket}/{source_path} does not exist")

            # Ensure directory exists
            os.makedirs(os.path.dirname(destination_path), exist_ok=True)

            # Download the file
            blob.download_to_filename(destination_path)
            logger.info(f"Download complete: gs://{bucket}/{source_path}")
            return destination_path
        except Exception as e:
            logger.error(f"Failed to download gs://{bucket}/{source_path}: {e}")
            raise StorageError(f"Failed to download file: {e}") from e

    @retry(max_attempts=3)
    def upload_file(self, bucket: str, source_path: str, destination_path: str) -> str:
        """
        Upload a file to GCS.

        Args:
            bucket: GCS bucket name
            source_path: Local path to the file
            destination_path: Path to save the file in GCS

        Returns:
            GCS path to the uploaded file

        Raises:
            StorageError: If the upload fails
        """
        try:
            logger.info(f"Uploading {source_path} to gs://{bucket}/{destination_path}")

            # Check if source file exists
            if not os.path.exists(source_path):
                raise StorageError(f"Source file {source_path} does not exist")

            # Upload to GCS
            bucket_obj = self.client.bucket(bucket)
            blob = bucket_obj.blob(destination_path)
            blob.upload_from_filename(source_path)

            logger.info(f"Upload complete: gs://{bucket}/{destination_path}")
            return destination_path
        except Exception as e:
            logger.error(
                f"Failed to upload {source_path} to "
                f"gs://{bucket}/{destination_path}: {e}"
            )
            raise StorageError(f"Failed to upload file: {e}") from e

    @retry(max_attempts=3)
    def upload_from_string(
        self, bucket: str, content: FileContent, destination_path: str
    ) -> str:
        """
        Upload content directly to GCS.

        Args:
            bucket: GCS bucket name
            content: Content to upload (string, bytes, or file-like object)
            destination_path: Path to save the file in GCS

        Returns:
            GCS path to the uploaded file

        Raises:
            StorageError: If the upload fails
        """
        try:
            logger.info(f"Uploading content to gs://{bucket}/{destination_path}")

            bucket_obj = self.client.bucket(bucket)
            blob = bucket_obj.blob(destination_path)

            # Handle different content types
            if isinstance(content, str):
                blob.upload_from_string(content)
            elif isinstance(content, bytes):
                blob.upload_from_string(content)
            elif hasattr(content, "read"):  # File-like object
                blob.upload_from_file(content)
            else:
                raise StorageError(f"Unsupported content type: {type(content)}")

            logger.info(f"Upload complete: gs://{bucket}/{destination_path}")
            return destination_path
        except Exception as e:
            logger.error(
                f"Failed to upload content to gs://{bucket}/{destination_path}: {e}"
            )
            raise StorageError(f"Failed to upload content: {e}") from e

    @retry(max_attempts=3)
    def read_file(self, bucket: str, path: str) -> bytes:
        """
        Read a file from GCS as bytes.

        Args:
            bucket: GCS bucket name
            path: Path to the file in GCS

        Returns:
            File content as bytes

        Raises:
            StorageError: If the read fails
        """
        try:
            logger.info(f"Reading gs://{bucket}/{path}")

            bucket_obj = self.client.bucket(bucket)
            blob = bucket_obj.blob(path)

            # Check if the blob exists
            if not blob.exists():
                raise StorageError(f"File gs://{bucket}/{path} does not exist")

            # Download as bytes
            content = blob.download_as_bytes()
            logger.info(f"Read {len(content)} bytes from gs://{bucket}/{path}")
            return content
        except Exception as e:
            logger.error(f"Failed to read gs://{bucket}/{path}: {e}")
            raise StorageError(f"Failed to read file: {e}") from e

    @retry(max_attempts=3)
    def read_text(self, bucket: str, path: str, encoding: str = "utf-8") -> str:
        """
        Read a file from GCS as text.

        Args:
            bucket: GCS bucket name
            path: Path to the file in GCS
            encoding: Text encoding to use

        Returns:
            File content as text

        Raises:
            StorageError: If the read fails
        """
        try:
            logger.info(f"Reading text from gs://{bucket}/{path}")

            bucket_obj = self.client.bucket(bucket)
            blob = bucket_obj.blob(path)

            # Check if the blob exists
            if not blob.exists():
                raise StorageError(f"File gs://{bucket}/{path} does not exist")

            # Download as text
            content = blob.download_as_text(encoding=encoding)
            logger.info(f"Read {len(content)} characters from gs://{bucket}/{path}")
            return content
        except Exception as e:
            logger.error(f"Failed to read text from gs://{bucket}/{path}: {e}")
            raise StorageError(f"Failed to read file as text: {e}") from e

    def list_files(self, bucket: str, prefix: Optional[str] = None) -> List[str]:
        """
        List files in a GCS bucket with optional prefix.

        Args:
            bucket: GCS bucket name
            prefix: Optional prefix to filter files

        Returns:
            List of file paths

        Raises:
            StorageError: If the listing fails
        """
        try:
            logger.info(f"Listing files in gs://{bucket}/{prefix or ''}")

            bucket_obj = self.client.bucket(bucket)
            blobs = bucket_obj.list_blobs(prefix=prefix)

            # Extract paths
            paths = [blob.name for blob in blobs]
            logger.info(f"Found {len(paths)} files in gs://{bucket}/{prefix or ''}")
            return paths
        except Exception as e:
            logger.error(f"Failed to list files in gs://{bucket}/{prefix or ''}: {e}")
            raise StorageError(f"Failed to list files: {e}") from e

    def file_exists(self, bucket: str, path: str) -> bool:
        """
        Check if a file exists in GCS.

        Args:
            bucket: GCS bucket name
            path: Path to the file in GCS

        Returns:
            True if the file exists, False otherwise
        """
        try:
            bucket_obj = self.client.bucket(bucket)
            blob = bucket_obj.blob(path)
            return blob.exists()
        except Exception as e:
            logger.error(f"Error checking if gs://{bucket}/{path} exists: {e}")
            return False

    @retry(max_attempts=3)
    def delete_file(self, bucket: str, path: str) -> bool:
        """
        Delete a file from GCS.

        Args:
            bucket: GCS bucket name
            path: Path to the file in GCS

        Returns:
            True if the file was deleted, False otherwise
        """
        try:
            logger.info(f"Deleting gs://{bucket}/{path}")

            bucket_obj = self.client.bucket(bucket)
            blob = bucket_obj.blob(path)

            # Check if the blob exists
            if not blob.exists():
                logger.warning(
                    f"File gs://{bucket}/{path} does not exist, nothing to delete"
                )
                return False

            # Delete the blob
            blob.delete()
            logger.info(f"Deleted gs://{bucket}/{path}")
            return True
        except Exception as e:
            logger.error(f"Failed to delete gs://{bucket}/{path}: {e}")
            return False

    @retry(max_attempts=3)
    def move_file(self, bucket: str, source_path: str, destination_path: str) -> bool:
        """
        Move a file within a GCS bucket.

        Args:
            bucket: GCS bucket name
            source_path: Original path to the file
            destination_path: New path for the file

        Returns:
            True if the file was moved, False otherwise
        """
        try:
            logger.info(
                f"Moving gs://{bucket}/{source_path} to gs://{bucket}/{destination_path}"
            )

            bucket_obj = self.client.bucket(bucket)
            source_blob = bucket_obj.blob(source_path)

            # Check if the source blob exists
            if not source_blob.exists():
                logger.warning(
                    f"Source file gs://{bucket}/{source_path} does not exist"
                )
                return False

            # Copy to destination
            bucket_obj.copy_blob(source_blob, bucket_obj, destination_path)

            # Delete the source blob
            source_blob.delete()

            logger.info(
                f"Moved gs://{bucket}/{source_path} to gs://{bucket}/{destination_path}"
            )
            return True
        except Exception as e:
            logger.error(f"Failed to move gs://{bucket}/{source_path}: {e}")
            return False

    def get_signed_url(
        self,
        bucket: str,
        path: str,
        expiration_minutes: int = 15,
        http_method: str = "GET",
        content_type: Optional[str] = None,
    ) -> str:
        """
        Generate a signed URL for a file in GCS.

        Args:
            bucket: GCS bucket name
            path: Path to the file in GCS
            expiration_minutes: URL expiration time in minutes
            http_method: HTTP method for the URL (GET, PUT, etc.)
            content_type: Content type for uploads

        Returns:
            Signed URL

        Raises:
            StorageError: If URL generation fails
        """
        try:
            logger.info(f"Generating signed {http_method} URL for gs://{bucket}/{path}")

            bucket_obj = self.client.bucket(bucket)
            blob = bucket_obj.blob(path)

            # Set up arguments for signed URL
            expiration = timedelta(minutes=expiration_minutes)
            kwargs = {
                "version": "v4",
                "expiration": expiration,
                "method": http_method,
            }

            # Add content type for PUT requests
            if http_method.upper() == "PUT" and content_type:
                kwargs["content_type"] = content_type

            # Generate the URL
            url = blob.generate_signed_url(**kwargs)

            logger.info(f"Generated signed URL for gs://{bucket}/{path}")
            return url
        except Exception as e:
            logger.error(f"Failed to generate signed URL for gs://{bucket}/{path}: {e}")
            raise StorageError(f"Failed to generate signed URL: {e}") from e

    def get_metadata(self, bucket: str, path: str) -> Dict[str, Any]:
        """
        Get metadata for a file in GCS.

        Args:
            bucket: GCS bucket name
            path: Path to the file in GCS

        Returns:
            File metadata as a dictionary

        Raises:
            StorageError: If metadata retrieval fails
        """
        try:
            logger.info(f"Getting metadata for gs://{bucket}/{path}")

            bucket_obj = self.client.bucket(bucket)
            blob = bucket_obj.blob(path)

            # Check if the blob exists
            if not blob.exists():
                raise StorageError(f"File gs://{bucket}/{path} does not exist")

            # Get metadata
            blob.reload()  # Ensure we have the latest metadata

            # Create a dictionary of relevant metadata
            metadata = {
                "name": blob.name,
                "bucket": blob.bucket.name,
                "size": blob.size,
                "updated": blob.updated,
                "content_type": blob.content_type,
                "etag": blob.etag,
                "generation": blob.generation,
                "metageneration": blob.metageneration,
                "md5_hash": blob.md5_hash,
                "storage_class": blob.storage_class,
                "time_created": blob.time_created,
                # Include any custom metadata
                "metadata": blob.metadata or {},
            }

            logger.info(f"Got metadata for gs://{bucket}/{path}")
            return metadata
        except Exception as e:
            logger.error(f"Failed to get metadata for gs://{bucket}/{path}: {e}")
            raise StorageError(f"Failed to get file metadata: {e}") from e

================
File: video_processor/services/storage/local.py
================
"""
Local filesystem implementation of the storage service.
"""

import os
import shutil
from datetime import datetime
from typing import Any, Dict, List, Optional

from video_processor.utils.error_handling import StorageError
from video_processor.utils.file_handling import ensure_directory_exists
from video_processor.utils.logging import get_logger

from .base import FileContent, StorageService

logger = get_logger(__name__)


class LocalStorageService(StorageService):
    """
    Local filesystem implementation of the storage service.

    This is useful for local development and testing without connecting to
    cloud storage.
    """

    def __init__(self, base_path: str = "test_data"):
        """
        Initialize the local storage service.

        Args:
            base_path: Base directory to use for storage
        """
        self.base_path = base_path
        ensure_directory_exists(base_path)

    def _get_bucket_path(self, bucket: str) -> str:
        """Get the full path for a bucket."""
        return os.path.join(self.base_path, bucket)

    def _get_file_path(self, bucket: str, path: str) -> str:
        """Get the full local path for a file."""
        return os.path.join(self._get_bucket_path(bucket), path)

    def download_file(
        self, bucket: str, source_path: str, destination_path: str
    ) -> str:
        """
        Copy a file from the local storage.

        Args:
            bucket: Directory name within base_path
            source_path: Path to the file in local storage
            destination_path: Local path to save the file

        Returns:
            Local path to the downloaded file

        Raises:
            StorageError: If the copy fails
        """
        try:
            source_file = self._get_file_path(bucket, source_path)
            logger.info(f"Copying {source_file} to {destination_path}")

            if not os.path.exists(source_file):
                raise StorageError(f"Source file {source_file} does not exist")

            # Ensure destination directory exists
            os.makedirs(os.path.dirname(destination_path), exist_ok=True)

            # Copy the file
            shutil.copy2(source_file, destination_path)

            logger.info(f"Copy complete: {destination_path}")
            return destination_path
        except Exception as e:
            logger.error(f"Failed to copy {source_path}: {e}")
            raise StorageError(f"Failed to copy file: {e}") from e

    def upload_file(self, bucket: str, source_path: str, destination_path: str) -> str:
        """
        Copy a file to the local storage.

        Args:
            bucket: Directory name within base_path
            source_path: Local path to the file
            destination_path: Path to save the file in local storage

        Returns:
            Storage path to the uploaded file

        Raises:
            StorageError: If the copy fails
        """
        try:
            if not os.path.exists(source_path):
                raise StorageError(f"Source file {source_path} does not exist")

            destination_file = self._get_file_path(bucket, destination_path)
            logger.info(f"Copying {source_path} to {destination_file}")

            # Ensure destination directory exists
            os.makedirs(os.path.dirname(destination_file), exist_ok=True)

            # Copy the file
            shutil.copy2(source_path, destination_file)

            logger.info(f"Copy complete: {destination_file}")
            return destination_path
        except Exception as e:
            logger.error(f"Failed to copy {source_path}: {e}")
            raise StorageError(f"Failed to copy file: {e}") from e

    def upload_from_string(
        self, bucket: str, content: FileContent, destination_path: str
    ) -> str:
        """
        Write content directly to local storage.

        Args:
            bucket: Directory name within base_path
            content: Content to write (string, bytes, or file-like object)
            destination_path: Path to save the file in local storage

        Returns:
            Storage path to the written file

        Raises:
            StorageError: If the write fails
        """
        try:
            destination_file = self._get_file_path(bucket, destination_path)
            logger.info(f"Writing content to {destination_file}")

            # Ensure destination directory exists
            os.makedirs(os.path.dirname(destination_file), exist_ok=True)

            # Write the content based on its type
            if isinstance(content, str):
                with open(destination_file, "w") as f:
                    f.write(content)
            elif isinstance(content, bytes):
                with open(destination_file, "wb") as f:
                    f.write(content)
            elif hasattr(content, "read"):  # File-like object
                with open(destination_file, "wb") as f:
                    shutil.copyfileobj(content, f)
            else:
                raise StorageError(f"Unsupported content type: {type(content)}")

            logger.info(f"Write complete: {destination_file}")
            return destination_path
        except Exception as e:
            logger.error(f"Failed to write content to {destination_path}: {e}")
            raise StorageError(f"Failed to write content: {e}") from e

    def read_file(self, bucket: str, path: str) -> bytes:
        """
        Read a file from local storage as bytes.

        Args:
            bucket: Directory name within base_path
            path: Path to the file in local storage

        Returns:
            File content as bytes

        Raises:
            StorageError: If the read fails
        """
        try:
            file_path = self._get_file_path(bucket, path)
            logger.info(f"Reading {file_path}")

            if not os.path.exists(file_path):
                raise StorageError(f"File {file_path} does not exist")

            # Read the file
            with open(file_path, "rb") as f:
                content = f.read()

            logger.info(f"Read {len(content)} bytes from {file_path}")
            return content
        except Exception as e:
            logger.error(f"Failed to read {path}: {e}")
            raise StorageError(f"Failed to read file: {e}") from e

    def read_text(self, bucket: str, path: str, encoding: str = "utf-8") -> str:
        """
        Read a file from local storage as text.

        Args:
            bucket: Directory name within base_path
            path: Path to the file in local storage
            encoding: Text encoding to use

        Returns:
            File content as text

        Raises:
            StorageError: If the read fails
        """
        try:
            file_path = self._get_file_path(bucket, path)
            logger.info(f"Reading text from {file_path}")

            if not os.path.exists(file_path):
                raise StorageError(f"File {file_path} does not exist")

            # Read the file
            with open(file_path, "r", encoding=encoding) as f:
                content = f.read()

            logger.info(f"Read {len(content)} characters from {file_path}")
            return content
        except Exception as e:
            logger.error(f"Failed to read text from {path}: {e}")
            raise StorageError(f"Failed to read file as text: {e}") from e

    def list_files(self, bucket: str, prefix: Optional[str] = None) -> List[str]:
        """
        List files in local storage with optional prefix.

        Args:
            bucket: Directory name within base_path
            prefix: Optional prefix to filter files

        Returns:
            List of file paths

        Raises:
            StorageError: If the listing fails
        """
        try:
            bucket_path = self._get_bucket_path(bucket)
            logger.info(f"Listing files in {bucket_path}/{prefix or ''}")

            if not os.path.exists(bucket_path):
                logger.warning(f"Bucket {bucket} does not exist, returning empty list")
                return []

            # Get list of all files
            all_files = []
            prefix_path = os.path.join(bucket_path, prefix or "")
            prefix_dir = os.path.dirname(prefix_path) if prefix else bucket_path

            for root, _, files in os.walk(prefix_dir):
                for file in files:
                    full_path = os.path.join(root, file)
                    # Convert to bucket-relative path
                    rel_path = os.path.relpath(full_path, bucket_path)

                    # Filter by prefix if provided
                    if prefix and not rel_path.startswith(prefix):
                        continue

                    all_files.append(rel_path)

            logger.info(f"Found {len(all_files)} files in {bucket_path}/{prefix or ''}")
            return all_files
        except Exception as e:
            logger.error(f"Failed to list files in {bucket}/{prefix or ''}: {e}")
            raise StorageError(f"Failed to list files: {e}") from e

    def file_exists(self, bucket: str, path: str) -> bool:
        """
        Check if a file exists in local storage.

        Args:
            bucket: Directory name within base_path
            path: Path to the file in local storage

        Returns:
            True if the file exists, False otherwise
        """
        file_path = self._get_file_path(bucket, path)
        return os.path.exists(file_path) and os.path.isfile(file_path)

    def delete_file(self, bucket: str, path: str) -> bool:
        """
        Delete a file from local storage.

        Args:
            bucket: Directory name within base_path
            path: Path to the file in local storage

        Returns:
            True if the file was deleted, False otherwise
        """
        try:
            file_path = self._get_file_path(bucket, path)
            logger.info(f"Deleting {file_path}")

            if not os.path.exists(file_path):
                logger.warning(f"File {file_path} does not exist, nothing to delete")
                return False

            # Delete the file
            os.remove(file_path)

            logger.info(f"Deleted {file_path}")
            return True
        except Exception as e:
            logger.error(f"Failed to delete {path}: {e}")
            return False

    def move_file(self, bucket: str, source_path: str, destination_path: str) -> bool:
        """
        Move a file within local storage.

        Args:
            bucket: Directory name within base_path
            source_path: Original path to the file
            destination_path: New path for the file

        Returns:
            True if the file was moved, False otherwise
        """
        try:
            source_file = self._get_file_path(bucket, source_path)
            destination_file = self._get_file_path(bucket, destination_path)

            logger.info(f"Moving {source_file} to {destination_file}")

            if not os.path.exists(source_file):
                logger.warning(f"Source file {source_file} does not exist")
                return False

            # Ensure destination directory exists
            os.makedirs(os.path.dirname(destination_file), exist_ok=True)

            # Move the file
            shutil.move(source_file, destination_file)

            logger.info(f"Moved {source_file} to {destination_file}")
            return True
        except Exception as e:
            logger.error(f"Failed to move {source_path}: {e}")
            return False

    def get_signed_url(
        self,
        bucket: str,
        path: str,
        expiration_minutes: int = 15,
        http_method: str = "GET",
        content_type: Optional[str] = None,
    ) -> str:
        """
        Generate a fake signed URL for local development.

        Args:
            bucket: Directory name within base_path
            path: Path to the file in local storage
            expiration_minutes: URL expiration time in minutes
            http_method: HTTP method for the URL (GET, PUT, etc.)
            content_type: Content type for uploads

        Returns:
            Local file URL

        Raises:
            StorageError: If URL generation fails
        """
        # For local development, just return a direct file path
        file_path = self._get_file_path(bucket, path)
        return f"file://{os.path.abspath(file_path)}"

    def get_metadata(self, bucket: str, path: str) -> Dict[str, Any]:
        """
        Get metadata for a file in local storage.

        Args:
            bucket: Directory name within base_path
            path: Path to the file in local storage

        Returns:
            File metadata as a dictionary

        Raises:
            StorageError: If metadata retrieval fails
        """
        try:
            file_path = self._get_file_path(bucket, path)
            logger.info(f"Getting metadata for {file_path}")

            if not os.path.exists(file_path):
                raise StorageError(f"File {file_path} does not exist")

            # Get file stats
            stats = os.stat(file_path)

            # Create metadata dictionary
            metadata = {
                "name": path,
                "bucket": bucket,
                "size": stats.st_size,
                "updated": datetime.fromtimestamp(stats.st_mtime),
                "time_created": datetime.fromtimestamp(stats.st_ctime),
                "content_type": self._guess_content_type(file_path),
                # Add other metadata similar to GCS for compatibility
                "etag": None,
                "generation": None,
                "metageneration": None,
                "md5_hash": None,
                "storage_class": "STANDARD",
                "metadata": {},
            }

            logger.info(f"Got metadata for {file_path}")
            return metadata
        except Exception as e:
            logger.error(f"Failed to get metadata for {path}: {e}")
            raise StorageError(f"Failed to get file metadata: {e}") from e

    def _guess_content_type(self, file_path: str) -> str:
        """Guess the content type of a file based on its extension."""
        _, ext = os.path.splitext(file_path)
        ext = ext.lower()

        content_types = {
            ".txt": "text/plain",
            ".html": "text/html",
            ".htm": "text/html",
            ".css": "text/css",
            ".js": "application/javascript",
            ".json": "application/json",
            ".xml": "application/xml",
            ".jpg": "image/jpeg",
            ".jpeg": "image/jpeg",
            ".png": "image/png",
            ".gif": "image/gif",
            ".svg": "image/svg+xml",
            ".pdf": "application/pdf",
            ".doc": "application/msword",
            ".docx": (
                "application/vnd.openxmlformats-officedocument."
                "wordprocessingml.document"
            ),
            ".xls": "application/vnd.ms-excel",
            ".xlsx": (
                "application/vnd.openxmlformats-officedocument." "spreadsheetml.sheet"
            ),
            ".ppt": "application/vnd.ms-powerpoint",
            ".pptx": (
                "application/vnd.openxmlformats-officedocument."
                "presentationml.presentation"
            ),
            ".mp3": "audio/mpeg",
            ".wav": "audio/wav",
            ".mp4": "video/mp4",
            ".avi": "video/x-msvideo",
            ".mov": "video/quicktime",
            ".zip": "application/zip",
            ".tar": "application/x-tar",
            ".gz": "application/gzip",
        }

        return content_types.get(ext, "application/octet-stream")

================
File: video_processor/services/__init__.py
================
"""
Services package for external service integrations.
"""

================
File: video_processor/tests/outdated/test_process_video_event.py
================
from video_processor.core.processors.video import process_video_event


def test_skip_non_mp4_files(mocker):
    """Test that process_video_event skips non-MP4 files."""
    # Call the function with a non-MP4 file
    process_video_event("test-bucket", "daily-raw/test_file.txt")

    # Verify that the function returns early
    mock_get_storage_service = mocker.patch(
        "video_processor.core.processors.video.get_storage_service"
    )
    mock_get_storage_service.assert_not_called()


def test_skip_incorrect_path(mocker):
    """Test that process_video_event skips files not in the correct paths."""
    # Call the function with a file in the wrong path
    process_video_event("test-bucket", "wrong-path/test_file.mp4")

    # Verify that the function returns early
    mock_get_storage_service = mocker.patch(
        "video_processor.core.processors.video.get_storage_service"
    )
    mock_get_storage_service.assert_not_called()

================
File: video_processor/tests/unit/test_video_processor.py
================


================
File: video_processor/tests/__init__.py
================
# This file is intentionally left empty to make the directory a Python package

================
File: video_processor/tests/conftest.py
================
"""
Pytest configuration file with common fixtures for testing.

import sys
import os

# Add the parent directory to the path so we can import the module
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
"""

import os
import subprocess
import tempfile
from unittest.mock import MagicMock

import pytest
from flask import Flask


@pytest.fixture
def mock_storage_client():
    """Mock for Google Cloud Storage client."""
    mock_client = MagicMock()
    mock_bucket = MagicMock()
    mock_blob = MagicMock()

    # Set up the chain of mocks
    mock_client.bucket.return_value = mock_bucket
    mock_bucket.blob.return_value = mock_blob

    return mock_client, mock_bucket, mock_blob


@pytest.fixture
def mock_generative_model():
    """Mock for Vertex AI GenerativeModel."""
    mock_model = MagicMock()
    mock_response = MagicMock()
    mock_response.text = "This is a mock response from Gemini API"
    mock_model.generate_content.return_value = mock_response

    return mock_model


@pytest.fixture
def sample_audio_file():
    """Create a temporary WAV file for testing."""
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_file:
        temp_path = temp_file.name

    try:
        # Generate a simple test tone using ffmpeg
        subprocess.run(
            [
                "ffmpeg",
                "-y",  # Overwrite output files without asking
                "-f",
                "lavfi",  # Use libavfilter
                "-i",
                "sine=frequency=440:duration=1",  # Generate a 1-second 440Hz tone
                "-ar",
                "16000",  # Audio sample rate
                "-ac",
                "1",  # Mono audio
                temp_path,
            ],
            check=True,
            capture_output=True,
            text=True,
        )

        yield temp_path
    finally:
        # Clean up the temporary file
        if os.path.exists(temp_path):
            os.unlink(temp_path)


@pytest.fixture
def sample_video_file():
    """Create a temporary MP4 file for testing."""
    with tempfile.NamedTemporaryFile(suffix=".mp4", delete=False) as temp_file:
        temp_path = temp_file.name

    try:
        # Generate a simple test video using ffmpeg
        subprocess.run(
            [
                "ffmpeg",
                "-y",  # Overwrite output files without asking
                "-f",
                "lavfi",  # Use libavfilter
                "-i",
                "sine=frequency=440:duration=1",  # Generate a 1-second 440Hz tone
                "-f",
                "lavfi",  # Use libavfilter for video
                "-i",
                "color=c=blue:s=320x240:d=1",  # Generate a 1-second blue screen
                "-c:a",
                "aac",  # Audio codec
                "-c:v",
                "h264",  # Video codec
                temp_path,
            ],
            check=True,
            capture_output=True,
            text=True,
        )

        yield temp_path
    finally:
        # Clean up the temporary file
        if os.path.exists(temp_path):
            os.unlink(temp_path)


@pytest.fixture
def mock_part():
    """Mock for Vertex AI Part object."""
    mock = MagicMock()
    mock.from_data.return_value = MagicMock()
    return mock


@pytest.fixture
def mock_flask_app():
    """Create a test Flask app."""
    app = Flask(__name__)
    app.testing = True
    return app


@pytest.fixture
def mock_cloud_event():
    """Create a mock Cloud Event for testing."""
    event = MagicMock()
    event.data = {
        "bucket": "test-bucket",
        "name": "daily-raw/test_video.mp4",
        "contentType": "video/mp4",
        "size": "1000000",
    }
    return event


@pytest.fixture
def mock_youtube_credentials():
    """Mock for YouTube API credentials."""
    mock_creds = MagicMock()
    mock_creds.refresh.return_value = None
    return mock_creds


@pytest.fixture
def mock_youtube_service():
    """Mock for YouTube API service."""
    mock_service = MagicMock()
    mock_videos = MagicMock()
    mock_captions = MagicMock()

    # Set up the chain of mocks
    mock_service.videos.return_value = mock_videos
    mock_service.captions.return_value = mock_captions

    # Mock the insert methods
    mock_insert_request = MagicMock()
    mock_insert_request.next_chunk.return_value = (None, {"id": "test_video_id"})
    mock_videos.insert.return_value = mock_insert_request

    mock_caption_request = MagicMock()
    mock_caption_request.execute.return_value = {"id": "test_caption_id"}
    mock_captions.insert.return_value = mock_caption_request

    return mock_service


@pytest.fixture
def mock_secretmanager_client():
    """Mock for Secret Manager client."""
    mock_client = MagicMock()
    mock_response = MagicMock()
    mock_response.payload.data.decode.return_value = "test_secret_value"
    mock_client.access_secret_version.return_value = mock_response
    return mock_client

================
File: video_processor/tests/test_chapters_generation.py
================
"""
Tests for the chapters generation functionality.
"""

import json
import os

# Import the functions to test
import sys
from unittest.mock import MagicMock, patch

# Add the parent directory to the path so we can import the module
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from process_uploaded_video import generate_chapters


def test_generate_chapters_valid_json(mock_generative_model, mock_part):
    """Test the generate_chapters function with valid JSON response."""
    # Create a valid JSON response
    valid_chapters = [
        {"timecode": "00:00", "chapterSummary": "Introduction"},
        {"timecode": "02:30", "chapterSummary": "Main topic"},
        {"timecode": "05:45", "chapterSummary": "Conclusion"},
    ]

    # Set up the mock response
    mock_response = MagicMock()
    mock_response.text = json.dumps(valid_chapters)
    mock_generative_model.generate_content.return_value = mock_response

    # Create a mock audio part
    mock_audio_part = MagicMock()

    # Patch the GenerativeModel class to return our mock
    with patch(
        "video_processor.process_uploaded_video.GenerativeModel",
        return_value=mock_generative_model,
    ):
        # Call the function
        result = generate_chapters(mock_audio_part)

        # Verify the result
        assert len(result) == 3
        assert result[0]["timecode"] == "00:00"
        assert result[0]["chapterSummary"] == "Introduction"
        assert result[1]["timecode"] == "02:30"
        assert result[2]["chapterSummary"] == "Conclusion"

        # Verify the model was called with the correct parameters
        mock_generative_model.generate_content.assert_called_once()
        args, kwargs = mock_generative_model.generate_content.call_args

        # Check that the prompt and audio part were passed correctly
        assert len(args[0]) == 2
        assert "Chapterize the video content" in args[0][0]  # Check part of the prompt
        assert args[0][1] == mock_audio_part  # Check the audio part

        # Check the generation config
        assert "temperature" in kwargs.get("generation_config", {})
        assert kwargs["generation_config"]["temperature"] == 0.6
        assert kwargs["generation_config"]["response_mime_type"] == "application/json"


def test_generate_chapters_invalid_json(mock_generative_model, mock_part):
    """Test the generate_chapters function with invalid JSON response."""
    # Set up the mock response with invalid JSON
    mock_response = MagicMock()
    mock_response.text = "This is not valid JSON"
    mock_generative_model.generate_content.return_value = mock_response

    # Create a mock audio part
    mock_audio_part = MagicMock()

    # Patch the GenerativeModel class to return our mock
    with patch(
        "video_processor.process_uploaded_video.GenerativeModel",
        return_value=mock_generative_model,
    ):
        # Call the function
        result = generate_chapters(mock_audio_part)

        # Verify the result is an empty list for invalid JSON
        assert result == []


def test_generate_chapters_missing_keys(mock_generative_model, mock_part):
    """Test the generate_chapters function with JSON missing required keys."""
    # Create JSON with missing keys
    invalid_chapters = [
        {"timecode": "00:00", "summary": "Introduction"},  # Missing chapterSummary
        {"time": "02:30", "chapterSummary": "Main topic"},  # Missing timecode
        {"other": "field"},  # Missing both required keys
    ]

    # Set up the mock response
    mock_response = MagicMock()
    mock_response.text = json.dumps(invalid_chapters)
    mock_generative_model.generate_content.return_value = mock_response

    # Create a mock audio part
    mock_audio_part = MagicMock()

    # Patch the GenerativeModel class to return our mock
    with patch(
        "video_processor.process_uploaded_video.GenerativeModel",
        return_value=mock_generative_model,
    ):
        # Call the function
        result = generate_chapters(mock_audio_part)

        # Verify the result is an empty list for invalid structure
        assert result == []


def test_generate_chapters_error_handling(mock_generative_model, mock_part):
    """Test error handling in the generate_chapters function."""
    # Set up the mock to raise an exception
    mock_generative_model.generate_content.side_effect = Exception("API error")

    # Create a mock audio part
    mock_audio_part = MagicMock()

    # Patch the GenerativeModel class to return our mock
    with patch(
        "video_processor.process_uploaded_video.GenerativeModel",
        return_value=mock_generative_model,
    ):
        # Call the function - it should handle the exception and return an empty list
        result = generate_chapters(mock_audio_part)

        # Verify the result is an empty list when an exception occurs
        assert result == []

================
File: video_processor/tests/test_firestore_trigger_listener.py
================
import os
from unittest.mock import patch

import firestore_trigger_listener as listener
import pytest
from google.cloud import firestore

# Mark all tests in this module as "integration" so they can be
# selectively run via `pytest -m integration`
pytestmark = pytest.mark.integration

TEST_COLLECTION = "videos_test"


@pytest.fixture(scope="module")
def firestore_client():
    # Use the same credentials as the main app, but a test collection
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = listener.SERVICE_ACCOUNT_PATH
    client = firestore.Client()
    yield client
    # Cleanup: delete all docs in the test collection after tests
    docs = client.collection(TEST_COLLECTION).stream()
    for doc in docs:
        doc.reference.delete()


def create_test_doc(client, doc_id, data):
    doc_ref = client.collection(TEST_COLLECTION).document(doc_id)
    doc_ref.set(data)
    return doc_ref


def update_test_doc(client, doc_id, updates):
    doc_ref = client.collection(TEST_COLLECTION).document(doc_id)
    doc_ref.update(updates)


def get_doc_data(client, doc_id):
    doc_ref = client.collection(TEST_COLLECTION).document(doc_id)
    return doc_ref.get().to_dict()


def run_listener_once(client, last_snapshots):
    # Simulate one iteration of the listener's polling logic
    docs = client.collection(TEST_COLLECTION).stream()
    for doc in docs:
        doc_id = doc.id
        data = doc.to_dict()
        prev = last_snapshots.get(doc_id)

        if prev is not None:
            for key, value in data.items():
                if key == "thumbnails" and "thumbnails" in prev:
                    for idx, thumb in enumerate(value):
                        prev_thumb = (
                            prev["thumbnails"][idx]
                            if idx < len(prev["thumbnails"])
                            else {}
                        )
                        if thumb.get("prompt") != prev_thumb.get("prompt"):
                            listener.regenerate_thumbnail(
                                doc_id, idx, thumb.get("prompt")
                            )
                elif key in ["title", "tags", "description", "scheduledTime"]:
                    if value != prev.get(key):
                        listener.update_metadata(doc_id, {key: value})
        last_snapshots[doc_id] = data


def test_firestore_trigger_listener_metadata_update(firestore_client):
    doc_id = "test_video_1"
    initial_data = {
        "title": "Original Title",
        "tags": ["tag1"],
        "description": "Original description",
        "scheduledTime": "2025-05-01T10:00:00Z",
        "thumbnails": [{"prompt": "original prompt"}],
    }
    create_test_doc(firestore_client, doc_id, initial_data)
    last_snapshots = {doc_id: initial_data.copy()}

    # Simulate a metadata update
    updates = {"title": "New Title"}
    update_test_doc(firestore_client, doc_id, updates)

    with patch.object(
        listener, "update_metadata"
    ) as mock_update_metadata, patch.object(
        listener, "regenerate_thumbnail"
    ) as mock_regen_thumb:
        run_listener_once(firestore_client, last_snapshots)
        mock_update_metadata.assert_called_once_with(doc_id, {"title": "New Title"})
        mock_regen_thumb.assert_not_called()


def test_firestore_trigger_listener_thumbnail_prompt_change(firestore_client):
    doc_id = "test_video_2"
    initial_data = {
        "title": "Title",
        "tags": ["tag1"],
        "description": "desc",
        "scheduledTime": "2025-05-01T10:00:00Z",
        "thumbnails": [{"prompt": "original prompt"}],
    }
    create_test_doc(firestore_client, doc_id, initial_data)
    last_snapshots = {doc_id: initial_data.copy()}

    # Simulate a thumbnail prompt change
    updates = {"thumbnails": [{"prompt": "new prompt"}]}
    update_test_doc(firestore_client, doc_id, updates)

    with patch.object(
        listener, "update_metadata"
    ) as mock_update_metadata, patch.object(
        listener, "regenerate_thumbnail"
    ) as mock_regen_thumb:
        run_listener_once(firestore_client, last_snapshots)
        mock_regen_thumb.assert_called_once_with(doc_id, 0, "new prompt")
        mock_update_metadata.assert_not_called()

================
File: video_processor/tests/test_generate_youtube_token.py
================
"""
Tests for the generate_youtube_token.py module.
"""

import os
import sys
from unittest.mock import MagicMock, patch

# Add the root directory to the path so we can import the module
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")))

from video_processor import generate_youtube_token


def test_save_refresh_token_to_secret(mock_secretmanager_client):
    """Test saving a refresh token to Secret Manager."""
    with patch(
        "video_processor.generate_youtube_token.secretmanager.SecretManagerServiceClient",
        return_value=mock_secretmanager_client,
    ):
        # Call the function
        generate_youtube_token.save_refresh_token_to_secret(
            "test-project", "test-secret", "test-token"
        )

        # Verify the correct API call was made
        mock_secretmanager_client.add_secret_version.assert_called_once_with(
            request={
                "parent": "projects/test-project/secrets/test-secret",
                "payload": {"data": b"test-token"},
            }
        )


def test_main_client_secrets_not_found():
    """Test main function when client secrets file is not found."""
    # Mock the argument parser
    mock_args = MagicMock()
    mock_args.channel = "daily"
    mock_args.save = False

    with patch("argparse.ArgumentParser.parse_args", return_value=mock_args):
        with patch("os.path.exists", return_value=False):
            with patch("builtins.print") as mock_print:
                # Call the function
                generate_youtube_token.main()

                # Verify error message was printed
                mock_print.assert_any_call(
                    f"\nError: Client secrets file not found at "
                    f"'{generate_youtube_token.CLIENT_SECRETS_FILE}'"
                )


def test_main_successful_flow():
    """Test successful flow of the main function."""
    # Mock the argument parser
    mock_args = MagicMock()
    mock_args.channel = "daily"
    mock_args.save = False

    mock_flow = MagicMock()
    mock_credentials = MagicMock()
    mock_credentials.refresh_token = "test-refresh-token"
    mock_flow.credentials = mock_credentials

    # Mock the authorization URL
    mock_flow.authorization_url.return_value = ("https://example.com/auth", None)

    # Mock the fetch_token method
    mock_flow.fetch_token.return_value = None

    with patch("argparse.ArgumentParser.parse_args", return_value=mock_args):
        with patch("os.path.exists", return_value=True):
            with patch(
                "google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file",
                return_value=mock_flow,
            ):
                with patch("builtins.input", return_value="test-code"):
                    with patch("builtins.print") as mock_print:
                        # Disable auto-saving to Secret Manager
                        with patch.object(
                            generate_youtube_token, "SAVE_TO_SECRET_MANAGER", False
                        ):
                            # Call the function
                            generate_youtube_token.main()

                            # Verify the flow was used correctly
                            mock_flow.authorization_url.assert_called_once_with(
                                prompt="consent"
                            )
                            mock_flow.fetch_token.assert_called_once_with(
                                code="test-code"
                            )

                            # Verify the refresh token was printed
                            mock_print.assert_any_call(
                                "Refresh Token: test-refresh-token"
                            )


def test_main_with_secret_manager_saving():
    """Test main function with saving to Secret Manager enabled."""
    # Mock the argument parser
    mock_args = MagicMock()
    mock_args.channel = "daily"
    mock_args.save = True

    # Create a mock secret ID
    mock_secret_id = "youtube-daily-refresh-token"

    mock_flow = MagicMock()
    mock_credentials = MagicMock()
    mock_credentials.refresh_token = "test-refresh-token"
    mock_flow.credentials = mock_credentials

    # Mock the authorization URL
    mock_flow.authorization_url.return_value = ("https://example.com/auth", None)

    # Mock the fetch_token method
    mock_flow.fetch_token.return_value = None

    with patch("argparse.ArgumentParser.parse_args", return_value=mock_args):
        with patch.dict(generate_youtube_token.SECRET_IDS, {"daily": mock_secret_id}):
            with patch("os.path.exists", return_value=True):
                with patch(
                    "google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file",
                    return_value=mock_flow,
                ):
                    with patch("builtins.input", return_value="test-code"):
                        with patch("builtins.print"):
                            with patch(
                                "video_processor.generate_youtube_token.save_refresh_token_to_secret"
                            ) as mock_save:
                                # Call the function
                                generate_youtube_token.main()

                                # Verify save_refresh_token_to_secret was called
                                mock_save.assert_called_once_with(
                                    generate_youtube_token.PROJECT_ID,
                                    mock_secret_id,
                                    "test-refresh-token",
                                )

================
File: video_processor/tests/test_titles_generation.py
================
"""
Tests for the titles and keywords generation functionality.
"""

import json
import os

# Import the functions to test
import sys
from unittest.mock import MagicMock, patch

# Add the parent directory to the path so we can import the module
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from process_uploaded_video import generate_titles


def test_generate_titles_valid_json(mock_generative_model, mock_part):
    """Test the generate_titles function with valid JSON response."""
    # Create a valid JSON response
    valid_title_dict = {
        "Description": "Exciting Video Title",
        "Keywords": "keyword1,keyword2,keyword3,keyword4",
    }

    # Set up the mock response
    mock_response = MagicMock()
    mock_response.text = json.dumps(valid_title_dict)
    mock_generative_model.generate_content.return_value = mock_response

    # Create a mock audio part
    mock_audio_part = MagicMock()

    # Patch the GenerativeModel class to return our mock
    with patch(
        "video_processor.process_uploaded_video.GenerativeModel",
        return_value=mock_generative_model,
    ):
        # Call the function
        result = generate_titles(mock_audio_part)

        # Verify the result
        assert result["Description"] == "Exciting Video Title"
        assert result["Keywords"] == "keyword1,keyword2,keyword3,keyword4"

        # Verify the model was called with the correct parameters
        mock_generative_model.generate_content.assert_called_once()
        args, kwargs = mock_generative_model.generate_content.call_args

        # Check that the prompt and audio part were passed correctly
        assert len(args[0]) == 2
        assert (
            "Please write a 40-character long intriguing title" in args[0][0]
        )  # Check part of the prompt
        assert args[0][1] == mock_audio_part  # Check the audio part

        # Check the generation config
        assert "temperature" in kwargs.get("generation_config", {})
        assert kwargs["generation_config"]["temperature"] == 0.8
        assert kwargs["generation_config"]["response_mime_type"] == "application/json"


def test_generate_titles_invalid_json(mock_generative_model, mock_part):
    """Test the generate_titles function with invalid JSON response."""
    # Set up the mock response with invalid JSON
    mock_response = MagicMock()
    mock_response.text = "This is not valid JSON"
    mock_generative_model.generate_content.return_value = mock_response

    # Create a mock audio part
    mock_audio_part = MagicMock()

    # Patch the GenerativeModel class to return our mock
    with patch(
        "video_processor.process_uploaded_video.GenerativeModel",
        return_value=mock_generative_model,
    ):
        # Call the function
        result = generate_titles(mock_audio_part)

        # Verify the result is the default dictionary for invalid JSON
        assert result["Description"] == "Default Title"
        assert result["Keywords"] == "default,keywords"


def test_generate_titles_missing_keys(mock_generative_model, mock_part):
    """Test the generate_titles function with JSON missing required keys."""
    # Create JSON with missing keys
    invalid_title_dict = {
        "Title": "Wrong Key Name",  # Missing Description
        "Tags": "tag1,tag2,tag3",  # Missing Keywords
    }

    # Set up the mock response
    mock_response = MagicMock()
    mock_response.text = json.dumps(invalid_title_dict)
    mock_generative_model.generate_content.return_value = mock_response

    # Create a mock audio part
    mock_audio_part = MagicMock()

    # Patch the GenerativeModel class to return our mock
    with patch(
        "video_processor.process_uploaded_video.GenerativeModel",
        return_value=mock_generative_model,
    ):
        # Call the function
        result = generate_titles(mock_audio_part)

        # Verify the result is the default dictionary for invalid structure
        assert result["Description"] == "Default Title"
        assert result["Keywords"] == "default,keywords"


def test_generate_titles_error_handling(mock_generative_model, mock_part):
    """Test error handling in the generate_titles function."""
    # Set up the mock to raise an exception
    mock_generative_model.generate_content.side_effect = Exception("API error")

    # Create a mock audio part
    mock_audio_part = MagicMock()

    # Patch the GenerativeModel class to return our mock
    with patch(
        "video_processor.process_uploaded_video.GenerativeModel",
        return_value=mock_generative_model,
    ):
        # Call the function - it should handle the exception and return
        # the default dictionary
        result = generate_titles(mock_audio_part)

        # Verify the result is the default dictionary when an exception occurs
        assert result["Description"] == "Default Title"
        assert result["Keywords"] == "default,keywords"

================
File: video_processor/tests/test_vtt_generation.py
================
"""
Tests for the VTT subtitles generation functionality.
"""

import os

# Import the functions to test
import sys
from unittest.mock import MagicMock, patch

import pytest

# Add the parent directory to the path so we can import the module
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from process_uploaded_video import generate_vtt

# Import Part for mocking
from vertexai.preview.generative_models import Part


@pytest.mark.parametrize(
    "mock_response_text,expected_result",
    [
        (
            "WEBVTT\n\n00:00:00.000 --> 00:00:05.000\nHello world",
            "WEBVTT\n\n00:00:00.000 --> 00:00:05.000\nHello world",
        ),
        (
            "00:00:00.000 --> 00:00:05.000\nMissing WEBVTT header",
            "WEBVTT\n\n00:00:00.000 --> 00:00:05.000\nMissing WEBVTT header".replace(
                "\\n", "\n"
            ),
        ),
        ("  WEBVTT  ", "WEBVTT"),
    ],
)
def test_generate_vtt(
    mock_generative_model, mock_part, mock_response_text, expected_result
):
    """Test the generate_vtt function with various inputs."""
    # Set up the mock response
    mock_response = MagicMock()
    mock_response.text = mock_response_text
    mock_generative_model.generate_content.return_value = mock_response

    # Create a mock audio part that is a proper Part instance
    mock_audio_part = MagicMock(spec=Part)

    # Patch the necessary classes and functions
    with patch(
        "process_uploaded_video.GenerativeModel",
        return_value=mock_generative_model,
    ):
        # Call the function
        result = generate_vtt(mock_audio_part)

        # Verify the result - normalize newlines for comparison
        assert result.replace("\\n", "\n") == expected_result

        # Verify the model was called
        mock_generative_model.generate_content.assert_called_once()


def test_generate_vtt_webvtt_correction():
    """Test that the VTT generator adds WEBVTT header if missing."""
    # Create a mock response without WEBVTT header
    mock_response = MagicMock()
    mock_response.text = "00:00:00.000 --> 00:00:05.000\nThis is a test subtitle"

    mock_model = MagicMock()
    mock_model.generate_content.return_value = mock_response

    # Create a mock audio part that is a proper Part instance
    mock_audio_part = MagicMock(spec=Part)

    # Patch the necessary classes and functions
    with patch(
        "process_uploaded_video.GenerativeModel",
        return_value=mock_model,
    ):
        # Call the function
        result = generate_vtt(mock_audio_part)

        # Verify the result has WEBVTT header added
        assert result.replace("\\n", "\n").startswith("WEBVTT\n\n")
        assert "00:00:00.000 --> 00:00:05.000" in result
        assert "This is a test subtitle" in result


def test_generate_vtt_error_handling(mock_generative_model):
    """Test error handling in the generate_vtt function."""
    # Set up the mock to raise an exception
    mock_generative_model.generate_content.side_effect = Exception("API error")

    # Create a mock audio part that is a proper Part instance
    mock_audio_part = MagicMock(spec=Part)

    # Patch the necessary classes and functions
    with patch(
        "process_uploaded_video.GenerativeModel",
        return_value=mock_generative_model,
    ):
        # Call the function and expect an exception
        with pytest.raises(Exception) as excinfo:
            generate_vtt(mock_audio_part)

        # Verify the exception message
        assert "API error" in str(excinfo.value)

================
File: video_processor/tests/test_youtube_uploader.py
================
"""
Tests for the youtube_uploader.py module.
"""

import os
import sys
import tempfile
from unittest.mock import MagicMock, patch

# Add the root directory to the path so we can import the module
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")))

from video_processor import youtube_uploader


def test_get_secret(mock_secretmanager_client):
    """Test retrieving a secret from Secret Manager."""
    with patch(
        "video_processor.youtube_uploader.secretmanager.SecretManagerServiceClient",
        return_value=mock_secretmanager_client,
    ):
        # Call the function
        result = youtube_uploader.get_secret("test-secret", "test-project")

        # Verify the result
        assert result == "test_secret_value"

        # Verify the correct API call was made
        mock_secretmanager_client.access_secret_version.assert_called_once_with(
            request={
                "name": "projects/test-project/secrets/test-secret/versions/latest"
            }
        )


def test_get_youtube_credentials(mock_secretmanager_client, mock_youtube_credentials):
    """Test building YouTube credentials from stored secrets."""
    with patch(
        "video_processor.youtube_uploader.secretmanager.SecretManagerServiceClient",
        return_value=mock_secretmanager_client,
    ):
        with patch(
            "video_processor.youtube_uploader.Credentials",
            return_value=mock_youtube_credentials,
        ):
            with patch("video_processor.youtube_uploader.Request"):
                # Call the function
                result = youtube_uploader.get_youtube_credentials(
                    {
                        "client_id": "test-client-id",
                        "client_secret": "test-client-secret",
                        "refresh_token": "test-refresh-token",
                    }
                )

                # Verify the result
                assert result == mock_youtube_credentials

                # Verify the refresh method was called
                mock_youtube_credentials.refresh.assert_called_once()


def test_download_blob(mock_storage_client):
    """Test downloading a blob from GCS."""
    mock_client, mock_bucket, mock_blob = mock_storage_client

    with patch(
        "video_processor.youtube_uploader.storage.Client", return_value=mock_client
    ):
        # Call the function
        result = youtube_uploader.download_blob(
            "test-bucket", "test-blob", "/tmp/test-file"
        )

        # Verify the result
        assert result == "/tmp/test-file"

        # Verify the correct API calls were made
        mock_client.bucket.assert_called_once_with("test-bucket")
        mock_bucket.blob.assert_called_once_with("test-blob")
        mock_blob.download_to_filename.assert_called_once_with("/tmp/test-file")


def test_read_blob_content(mock_storage_client):
    """Test reading content from a blob in GCS."""
    mock_client, mock_bucket, mock_blob = mock_storage_client
    mock_blob.download_as_text.return_value = "test content"

    with patch(
        "video_processor.youtube_uploader.storage.Client", return_value=mock_client
    ):
        # Call the function
        result = youtube_uploader.read_blob_content("test-bucket", "test-blob")

        # Verify the result
        assert result == "test content"

        # Verify the correct API calls were made
        mock_client.bucket.assert_called_once_with("test-bucket")
        mock_bucket.blob.assert_called_once_with("test-blob")
        mock_blob.download_as_text.assert_called_once()


def test_upload_video(mock_youtube_service):
    """Test uploading a video to YouTube."""
    # Create a temporary file for testing
    with tempfile.NamedTemporaryFile(suffix=".mp4") as temp_file:
        with patch(
            "video_processor.youtube_uploader.MediaFileUpload"
        ) as mock_media_upload:
            # Call the function
            result = youtube_uploader.upload_video(
                mock_youtube_service, temp_file.name, "Test Title", "Test Description"
            )

            # Verify the result
            assert result == {"id": "test_video_id"}

            # Verify the correct API calls were made
            mock_youtube_service.videos.assert_called_once()
            mock_youtube_service.videos().insert.assert_called_once()
            mock_media_upload.assert_called_once_with(
                temp_file.name, chunksize=-1, resumable=True
            )


def test_upload_captions(mock_youtube_service):
    """Test uploading captions to YouTube."""
    # Create a temporary file for testing
    with tempfile.NamedTemporaryFile(suffix=".vtt") as temp_file:
        with patch(
            "video_processor.youtube_uploader.MediaFileUpload"
        ) as mock_media_upload:
            # Call the function
            result = youtube_uploader.upload_captions(
                mock_youtube_service, "test_video_id", temp_file.name
            )

            # Verify the result
            assert result == {"id": "test_caption_id"}

            # Verify the correct API calls were made
            mock_youtube_service.captions.assert_called_once()
            mock_youtube_service.captions().insert.assert_called_once()
            mock_media_upload.assert_called_once_with(temp_file.name)


def test_upload_to_youtube_daily(
    mock_cloud_event,
    mock_storage_client,
    mock_youtube_credentials,
    mock_youtube_service,
):
    """Test the Cloud Function for uploading to YouTube Daily channel."""
    mock_client, mock_bucket, mock_blob = mock_storage_client

    # Mock the list_blobs method to return a list of blobs
    mock_blobs = [
        MagicMock(name="processed-daily/test_video/video.mp4"),
        MagicMock(name="processed-daily/test_video/description.txt"),
        MagicMock(name="processed-daily/test_video/subtitles.vtt"),
    ]
    mock_blobs[0].name = "processed-daily/test_video/video.mp4"
    mock_blobs[1].name = "processed-daily/test_video/description.txt"
    mock_blobs[2].name = "processed-daily/test_video/subtitles.vtt"

    mock_client.list_blobs.return_value = mock_blobs

    # Set up the cloud event data
    mock_cloud_event.data = {
        "bucket": "test-bucket",
        "name": "processed-daily/test_video/video.mp4",
    }

    with patch(
        "video_processor.youtube_uploader.storage.Client", return_value=mock_client
    ):
        with patch(
            "video_processor.youtube_uploader.get_youtube_credentials",
            return_value=mock_youtube_credentials,
        ):
            with patch(
                "video_processor.youtube_uploader.build",
                return_value=mock_youtube_service,
            ):
                with patch("video_processor.youtube_uploader.download_blob"):
                    with patch(
                        "video_processor.youtube_uploader.upload_video",
                        return_value={"id": "test_video_id"},
                    ):
                        with patch("video_processor.youtube_uploader.upload_captions"):
                            with patch(
                                "video_processor.youtube_uploader.os.path.exists",
                                return_value=True,
                            ):
                                with patch(
                                    "video_processor.youtube_uploader.os.remove"
                                ):
                                    # Call the function
                                    youtube_uploader.upload_to_youtube_daily(
                                        mock_cloud_event
                                    )

                                    # Verify the correct API calls were made
                                    mock_client.list_blobs.assert_called_once_with(
                                        "test-bucket",
                                        prefix="processed-daily/test_video/",
                                    )


def test_upload_to_youtube_main(
    mock_cloud_event,
    mock_storage_client,
    mock_youtube_credentials,
    mock_youtube_service,
):
    """Test the Cloud Function for uploading to YouTube Main channel."""
    mock_client, mock_bucket, mock_blob = mock_storage_client

    # Mock the list_blobs method to return a list of blobs
    mock_blobs = [
        MagicMock(name="processed-main/test_video/video.mp4"),
        MagicMock(name="processed-main/test_video/description.txt"),
        MagicMock(name="processed-main/test_video/subtitles.vtt"),
    ]
    mock_blobs[0].name = "processed-main/test_video/video.mp4"
    mock_blobs[1].name = "processed-main/test_video/description.txt"
    mock_blobs[2].name = "processed-main/test_video/subtitles.vtt"

    mock_client.list_blobs.return_value = mock_blobs

    # Set up the cloud event data
    mock_cloud_event.data = {
        "bucket": "test-bucket",
        "name": "processed-main/test_video/video.mp4",
    }

    with patch(
        "video_processor.youtube_uploader.storage.Client", return_value=mock_client
    ):
        with patch(
            "video_processor.youtube_uploader.get_youtube_credentials",
            return_value=mock_youtube_credentials,
        ):
            with patch(
                "video_processor.youtube_uploader.build",
                return_value=mock_youtube_service,
            ):
                with patch("video_processor.youtube_uploader.download_blob"):
                    with patch(
                        "video_processor.youtube_uploader.upload_video",
                        return_value={"id": "test_video_id"},
                    ):
                        with patch("video_processor.youtube_uploader.upload_captions"):
                            with patch(
                                "video_processor.youtube_uploader.os.path.exists",
                                return_value=True,
                            ):
                                with patch(
                                    "video_processor.youtube_uploader.os.remove"
                                ):
                                    # Call the function
                                    youtube_uploader.upload_to_youtube_main(
                                        mock_cloud_event
                                    )

                                    # Verify the correct API calls were made
                                    mock_client.list_blobs.assert_called_once_with(
                                        "test-bucket",
                                        prefix="processed-main/test_video/",
                                    )

================
File: video_processor/utils/__init__.py
================
"""
Utilities module for shared functionality across the application.
"""

================
File: video_processor/utils/error_handling.py
================
"""
Error handling utilities.
"""

import functools
import traceback
from typing import Any, Callable, Optional, Type, TypeVar, Union, cast

from .logging import get_logger

logger = get_logger(__name__)

# Type variables for better type hinting with decorators
F = TypeVar("F", bound=Callable[..., Any])
R = TypeVar("R")


class ProcessingError(Exception):
    """Base exception for video processing errors."""

    pass


class StorageError(ProcessingError):
    """Exception for storage-related errors."""

    pass


class TranscriptionError(ProcessingError):
    """Exception for transcription-related errors."""

    pass


class VideoProcessingError(ProcessingError):
    """Exception for video processing errors."""

    pass


class ConfigurationError(Exception):
    """Exception for configuration errors."""

    pass


def handle_exceptions(
    fallback_return: Optional[Any] = None,
    exception_types: Optional[
        Union[Type[Exception], tuple[Type[Exception], ...]]
    ] = None,
    log_level: str = "error",
) -> Callable[[F], F]:
    """
    Decorator to handle exceptions in a function.

    Args:
        fallback_return: Value to return if an exception occurs
        exception_types: Exception types to catch (defaults to Exception)
        log_level: Logging level to use when an exception occurs

    Returns:
        Decorated function
    """
    if exception_types is None:
        exception_types = Exception

    def decorator(func: F) -> F:
        @functools.wraps(func)
        def wrapper(*args: Any, **kwargs: Any) -> Any:
            try:
                return func(*args, **kwargs)
            except exception_types as e:
                log_func = getattr(logger, log_level)
                log_func(
                    f"Error in {func.__name__}: {str(e)}\n"
                    f"{''.join(traceback.format_tb(e.__traceback__))}"
                )
                return fallback_return

        return cast(F, wrapper)

    return decorator


def retry(
    max_attempts: int = 3,
    backoff_factor: float = 0.5,
    exception_types: Optional[
        Union[Type[Exception], tuple[Type[Exception], ...]]
    ] = None,
) -> Callable[[F], F]:
    """
    Retry a function on failure with exponential backoff.

    Args:
        max_attempts: Maximum number of attempts
        backoff_factor: Factor for exponential backoff
        exception_types: Exception types to catch and retry on

    Returns:
        Decorated function
    """
    if exception_types is None:
        exception_types = Exception

    def decorator(func: F) -> F:
        @functools.wraps(func)
        def wrapper(*args: Any, **kwargs: Any) -> Any:
            import random
            import time

            attempt = 0
            last_exception = None

            while attempt < max_attempts:
                try:
                    return func(*args, **kwargs)
                except exception_types as e:
                    attempt += 1
                    last_exception = e

                    if attempt >= max_attempts:
                        break

                    # Calculate sleep time with jitter
                    sleep_time = backoff_factor * (2 ** (attempt - 1))
                    sleep_time = sleep_time + (random.random() * sleep_time * 0.5)

                    logger.warning(
                        f"Attempt {attempt}/{max_attempts} failed for "
                        f"{func.__name__}: {e}. "
                        f"Retrying in {sleep_time:.2f}s..."
                    )
                    time.sleep(sleep_time)

            # If we reach here, all attempts failed
            logger.error(f"All {max_attempts} attempts failed for {func.__name__}")
            raise last_exception

        return cast(F, wrapper)

    return decorator

================
File: video_processor/utils/file_handling.py
================
"""
File handling utilities.
"""

import os
import tempfile

from .logging import get_logger

logger = get_logger(__name__)


def ensure_directory_exists(directory_path: str) -> bool:
    """
    Ensure a directory exists, creating it if necessary.

    Args:
        directory_path: Path to the directory

    Returns:
        True if successful, False otherwise
    """
    try:
        os.makedirs(directory_path, exist_ok=True)
        return True
    except Exception as e:
        logger.error(f"Error creating directory {directory_path}: {e}")
        return False


def get_temp_directory() -> str:
    """
    Create and return a temporary directory path.

    Returns:
        Path to a new temporary directory
    """
    return tempfile.mkdtemp()


def get_file_extension(file_path: str) -> str:
    """
    Get the extension of a file.

    Args:
        file_path: Path to the file

    Returns:
        File extension without the dot
    """
    return os.path.splitext(file_path)[1].lstrip(".")


def normalize_filename(filename: str) -> str:
    """
    Normalize a filename by replacing spaces with hyphens.

    Args:
        filename: Original filename

    Returns:
        Normalized filename
    """
    return filename.replace(" ", "-")


def get_safe_path(base_directory: str, *paths: str) -> str:
    """
    Create a safe path by joining path components and normalizing.

    Args:
        base_directory: Base directory
        *paths: Path components to join

    Returns:
        Normalized, safe path
    """
    path = os.path.join(base_directory, *paths)
    return os.path.normpath(path)

================
File: video_processor/utils/logging.py
================
"""
Centralized logging configuration and utilities.
"""

import logging
import sys
from typing import Optional, Union


def configure_logging(
    level: Union[int, str] = logging.INFO,
    format_string: Optional[str] = None,
    logger_name: Optional[str] = None,
) -> logging.Logger:
    """
    Configure logging with consistent format across the application.

    Args:
        level: Logging level
        format_string: Custom format string (if None, uses default)
        logger_name: Name for the logger (if None, uses root logger)

    Returns:
        Logger instance
    """
    if format_string is None:
        format_string = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

    # Get logger by name or root logger
    logger = logging.getLogger(logger_name)
    logger.setLevel(level)

    # Remove existing handlers to avoid duplicates when called multiple times
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)

    # Add console handler with formatting
    handler = logging.StreamHandler(sys.stdout)
    handler.setLevel(level)
    formatter = logging.Formatter(format_string)
    handler.setFormatter(formatter)
    logger.addHandler(handler)

    return logger


def get_logger(name: str) -> logging.Logger:
    """
    Get a logger with the given name.

    Args:
        name: Logger name, typically __name__ from the calling module

    Returns:
        Logger instance
    """
    return logging.getLogger(name)

================
File: video_processor/__init__.py
================
# This file makes the video_processor directory a Python package

================
File: video_processor/.gcloudignore
================
# Ignore local configs and noise
__pycache__/
*.pyc
.venv/
.env
.gcloudignore
.gitinore
client_secret.json

================
File: video_processor/.gitignore
================
# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
dev-debug.log

# Dependency directories
node_modules/

# Environment variables
.env

# Editor directories and files
.idea
.vscode
*.suo
*.ntvs*
*.njsproj
*.sln
*.sw?

# OS specific
.DS_Store

# Task files
tasks.json
tasks/

================
File: video_processor/app.py
================
"""
Main application entry point.
"""

import logging

from .api import create_app
from .config import get_settings
from .utils.logging import configure_logging

# Configure logging
logger = configure_logging(level=logging.INFO)
logger.info("🚀 app.py starting...")

# Create the Flask application
app = create_app()

if __name__ == "__main__":
    logger.info("🏁 Attempting to run Flask development server...")
    try:
        settings = get_settings()
        app.run(host="0.0.0.0", port=settings.port, debug=settings.debug)
        logger.info(f"Flask server should be running on port {settings.port}")
    except Exception:
        logger.exception("❌ Failed to start Flask development server:")
        raise

================
File: video_processor/conftest.py
================
"""
Global pytest fixtures and configuration.
"""

import os
from unittest.mock import MagicMock, patch

import pytest

# Set environment variables for testing
os.environ["GOOGLE_CLOUD_PROJECT"] = "automations-457120"

# For CI environment, we'll use the credentials file created by the workflow
# For local testing, we'll use the credentials file in the credentials directory
credentials_path = os.path.abspath(
    os.path.join(os.path.dirname(__file__), "../credentials/service_account.json")
)
if os.path.exists(credentials_path):
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = credentials_path
else:
    # In CI, the credentials file is created in the root directory
    ci_credentials_path = os.path.abspath(
        os.path.join(
            os.path.dirname(__file__), "../../credentials/service_account.json"
        )
    )
    if os.path.exists(ci_credentials_path):
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = ci_credentials_path
    else:
        # If no credentials file is found, we'll use mock authentication
        from unittest.mock import patch

        import pytest

        @pytest.fixture(autouse=True, scope="session")
        def mock_google_auth():
            """Mock Google Cloud authentication for all tests if no credentials
            file is found."""
            with patch(
                "google.auth.default", return_value=(None, "automations-457120")
            ):
                yield


# For tests that need mock storage client
@pytest.fixture
def mock_storage_client():
    """Mock for Google Cloud Storage client."""
    mock_client = MagicMock()
    mock_bucket = MagicMock()
    mock_blob = MagicMock()

    # Set up the mock chain
    mock_client.bucket.return_value = mock_bucket
    mock_bucket.blob.return_value = mock_blob

    # Return the mock objects for use in tests
    return mock_client, mock_bucket, mock_blob

================
File: video_processor/firestore_trigger_listener.py
================
import os
import time

from google.cloud import firestore

# Path to service account key
SERVICE_ACCOUNT_PATH = os.path.abspath(
    os.path.join(os.path.dirname(__file__), "../../credentials/service_account.json")
)


def regenerate_thumbnail(video_id, thumbnail_idx, prompt):
    # Placeholder: implement actual thumbnail regeneration logic
    print(
        f"[THUMBNAIL] Regenerating thumbnail {thumbnail_idx} for video {video_id} "
        f"with prompt: {prompt}"
    )


def update_metadata(video_id, updated_fields):
    # Placeholder: implement actual metadata update logic
    print(f"[METADATA] Updating metadata for video {video_id}: {updated_fields}")


def main():
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = SERVICE_ACCOUNT_PATH
    db = firestore.Client()

    videos_ref = db.collection("videos")

    # Store last seen document snapshots to detect changes
    last_snapshots = {}

    print("Listening for Firestore document changes in 'videos' collection...")

    while True:
        docs = videos_ref.stream()
        for doc in docs:
            doc_id = doc.id
            data = doc.to_dict()
            prev = last_snapshots.get(doc_id)

            if prev is not None:
                # Compare previous and current data to detect changes
                for key, value in data.items():
                    if key == "thumbnails" and "thumbnails" in prev:
                        # Check for prompt changes in thumbnails array
                        for idx, thumb in enumerate(value):
                            prev_thumb = (
                                prev["thumbnails"][idx]
                                if idx < len(prev["thumbnails"])
                                else {}
                            )
                            if thumb.get("prompt") != prev_thumb.get("prompt"):
                                regenerate_thumbnail(doc_id, idx, thumb.get("prompt"))
                    elif key in ["title", "tags", "description", "scheduledTime"]:
                        if value != prev.get(key):
                            update_metadata(doc_id, {key: value})
                    # Add more field checks as needed

            # Update last snapshot
            last_snapshots[doc_id] = data

        time.sleep(2)  # Poll every 2 seconds


if __name__ == "__main__":
    main()

================
File: video_processor/generate_youtube_token.py
================
import argparse
import os

import google_auth_oauthlib.flow
from google.cloud import secretmanager

# --- Configuration ---
# Download this file from Google Cloud Console > APIs & Services > Credentials
# Important: DO NOT COMMIT THIS FILE OR THE GENERATED TOKEN!
CLIENT_SECRETS_FILE = "./credentials/client_secret.json"  # Path to client secrets file

# The scopes must match exactly what your Cloud Function will need.
SCOPES = ["https://www.googleapis.com/auth/youtube.upload"]
API_SERVICE_NAME = "youtube"
API_VERSION = "v3"

# --- Secret Manager Config (Optional: To save the token directly) ---
# Set these if you want the script to attempt saving the token automatically
# Requires google-cloud-secret-manager library and authentication
SAVE_TO_SECRET_MANAGER = False  # Set to True to enable saving
PROJECT_ID = "automations-457120"

# Secret IDs for each channel
SECRET_IDS = {"daily": "youtube-daily-refresh-token", "main": "youtube-refresh-token"}


def save_refresh_token_to_secret(project_id, secret_id, token):
    """Saves the refresh token to Google Cloud Secret Manager."""
    try:
        client = secretmanager.SecretManagerServiceClient()
        parent = f"projects/{project_id}/secrets/{secret_id}"

        # Add a new secret version with the refresh token payload
        response = client.add_secret_version(
            request={
                "parent": parent,
                "payload": {"data": token.encode("UTF-8")},
            }
        )
        print(f"Saved refresh token to secret manager version: {response.name}")
        print(
            f"IMPORTANT: Ensure your Cloud Function service account has the "
            f"'Secret Manager Secret Accessor' role for {secret_id}."
        )

    except Exception as e:
        print(f"\nError saving token to Secret Manager for secret '{secret_id}': {e}")
        print("Please save the refresh token manually.")


def main():
    # Parse command line arguments
    parser = argparse.ArgumentParser(
        description="Generate YouTube OAuth tokens for video uploads"
    )
    parser.add_argument(
        "--channel",
        choices=["daily", "main"],
        default="daily",
        help="The YouTube channel to generate tokens for (daily or main)",
    )
    parser.add_argument(
        "--save", action="store_true", help="Save the token directly to Secret Manager"
    )
    args = parser.parse_args()

    # Set channel and secret ID based on arguments
    channel = args.channel
    secret_id_to_save = SECRET_IDS[channel]
    save_to_secret_manager = args.save or SAVE_TO_SECRET_MANAGER

    # Print header
    print("\n" + "=" * 80)
    print(f"YouTube OAuth Token Generator for {channel.upper()} Channel")
    print("=" * 80)

    # Check if client secrets file exists
    if not os.path.exists(CLIENT_SECRETS_FILE):
        print(f"\nError: Client secrets file not found at '{CLIENT_SECRETS_FILE}'")
        print("\nTo obtain this file:")
        print("1. Go to https://console.cloud.google.com/apis/credentials")
        print("2. Create an OAuth 2.0 Client ID (Web application type)")
        print("3. Add http://localhost:8080 as an authorized redirect URI")
        print(
            "4. Download the JSON file and save it as 'client_secret.json' "
            "in the docs directory"
        )
        return

    # Create flow instance to manage the OAuth 2.0 Authorization Grant Flow steps.
    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(
        CLIENT_SECRETS_FILE, SCOPES
    )

    # Run the console flow instead of local server
    print("\nStarting OAuth authentication flow...")
    print(
        "\nIMPORTANT: You will need to authenticate with the Google account "
        "that owns the YouTube channel."
    )
    print("\nPlease visit the following URL in your browser:")
    auth_url, _ = flow.authorization_url(prompt="consent")
    print(f"\n{auth_url}\n")
    print("After authorizing, Google will provide you with a code.")
    code = input("Enter the authorization code here: ")
    flow.fetch_token(code=code)
    credentials = flow.credentials
    # credentials = flow.run_local_server(port=0) # Old local server method

    # The credentials object now contains the refresh token.
    refresh_token = credentials.refresh_token

    print("\n" + "=" * 40)
    print("Authentication successful!")
    print("=" * 40)
    print(f"\nChannel: {channel.upper()}")
    print(f"Secret ID: {secret_id_to_save}")
    print(f"Refresh Token: {refresh_token}")
    print(
        "\nIMPORTANT: This refresh token allows offline access to your YouTube account."
    )
    print("Keep it secure and do not commit it to version control!")

    # Optionally save to Secret Manager
    if save_to_secret_manager and refresh_token:
        print("\nAttempting to save refresh token to Secret Manager...")
        print(f"Project: {PROJECT_ID}")
        print(f"Secret ID: {secret_id_to_save}")
        save_refresh_token_to_secret(PROJECT_ID, secret_id_to_save, refresh_token)
    else:
        print("\nAutomatic saving to Secret Manager is disabled.")
        print("\nTo manually save the token:")
        print(
            f"1. Go to https://console.cloud.google.com/security/secret-manager?project={PROJECT_ID}"
        )
        print(f"2. Create or update the secret '{secret_id_to_save}'")
        print("3. Add a new version with the refresh token as the value")
        print("\nYou'll also need to create/update these secrets:")
        print(
            f"- '{secret_id_to_save.replace('refresh_token', 'client_id')}': "
            f"The client ID from your OAuth credentials"
        )
        print(
            f"- '{secret_id_to_save.replace('refresh_token', 'client_secret')}': "
            f"The client secret from your OAuth credentials"
        )


if __name__ == "__main__":
    main()

================
File: video_processor/main.py
================
import json
import logging
import os
from typing import Callable, Optional

from flask import Flask, request

# Import the process_video_event function
from .process_uploaded_video import process_video_event

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)

# Log startup information
logging.info("Starting Video Processor application")
logging.info(f"Testing mode: {os.environ.get('TESTING_MODE', 'false')}")
logging.info(f"Project ID: {os.environ.get('GOOGLE_CLOUD_PROJECT', 'unknown')}")


def create_app(process_func: Optional[Callable] = None) -> Flask:
    """Create and configure the Flask application.

    Args:
        process_func: Optional function to use for processing video events.
                     Useful for testing with a mock function.

    Returns:
        Flask application instance
    """
    flask_app = Flask(__name__)

    # Use the provided process function or default to the real one
    video_processor = process_func if process_func is not None else process_video_event

    @flask_app.route("/", methods=["POST"])
    def handle_gcs_event():
        """Handles incoming CloudEvents from GCS via Eventarc/Cloud Run."""
        event_id = request.headers.get("Ce-Id")
        event_type = request.headers.get("Ce-Type")
        event_source = request.headers.get("Ce-Source")
        event_subject = request.headers.get("Ce-Subject")

        logging.info(
            f"Received CloudEvent: ID={event_id}, "
            f"Type={event_type}, Source={event_source}, "
            f"Subject={event_subject}"
        )

        try:
            # Get the JSON payload (CloudEvent data)
            event_data = request.get_json()

            if not event_data or "bucket" not in event_data or "name" not in event_data:
                logging.error(f"Invalid event payload: {json.dumps(event_data)}")
                return ("Invalid event payload", 400)

            bucket = event_data["bucket"]
            name = event_data["name"]

            logging.info(f"Processing gs://{bucket}/{name}")

            # Call the core processing function using the injected or default processor
            logging.info(f"Calling video_processor function for gs://{bucket}/{name}")
            try:
                video_processor(bucket, name)
                logging.info(f"Successfully processed gs://{bucket}/{name}")
            except Exception as e:
                logging.error(f"Error in video_processor function: {e}", exc_info=True)
                raise
            # Return 2xx response to acknowledge the event
            return ("", 204)

        except Exception as e:
            logging.error(
                f"Error processing event for {event_subject}: {e}", exc_info=True
            )
            # Return 500 to indicate failure, potentially causing Eventarc to retry
            return ("Internal Server Error", 500)

    return flask_app


# Create the application instance
app = create_app()


def run_app(debug=True):
    """Run the Flask application for local development.

    Args:
        debug: Whether to run the app in debug mode
    """
    port = int(os.environ.get("PORT", 8080))
    app.run(host="0.0.0.0", port=port, debug=debug)


if __name__ == "__main__":
    # This section is for local development testing
    # Gunicorn will directly run the 'app' object in production
    run_app(debug=True)

================
File: video_processor/process_uploaded_video.py
================
import json
import logging
import os
import shutil
import subprocess
from unittest.mock import MagicMock

import vertexai
from google.cloud import aiplatform, storage
from vertexai.preview.generative_models import GenerativeModel, Part

PROJECT_ID = os.environ.get("GOOGLE_CLOUD_PROJECT", "automations-457120")
REGION = "us-east1"
MODEL = "gemini-2.0-flash-001"

# Check if we're in testing mode
TESTING_MODE = os.environ.get("TESTING_MODE", "false").lower() == "true"

# Check if we're in real API test mode
REAL_API_TEST = os.environ.get("REAL_API_TEST", "false").lower() == "true"

# Check if we should write outputs to local filesystem
LOCAL_OUTPUT = os.environ.get("LOCAL_OUTPUT", "false").lower() == "true" or TESTING_MODE

# Initialize clients with appropriate configuration
if TESTING_MODE:
    # In testing mode, always use mock clients
    from unittest.mock import MagicMock

    # Create a mock storage client for testing
    storage_client = MagicMock()
    logging.info("Using mock storage client for testing")

    # Create mock bucket and blob methods
    mock_bucket = MagicMock()
    mock_blob = MagicMock()
    storage_client.bucket = MagicMock(return_value=mock_bucket)
    mock_bucket.blob = MagicMock(return_value=mock_blob)
    mock_bucket.copy_blob = MagicMock()
    mock_blob.upload_from_string = MagicMock()
    mock_blob.download_to_filename = MagicMock()
    mock_blob.delete = MagicMock()

    # Skip Vertex AI initialization in testing mode
    logging.info("Skipping Vertex AI initialization in testing mode")
    model = MagicMock()
    # Mock the generate_content method
    model.generate_content = MagicMock(
        return_value=MagicMock(
            text="This is a mock response from Gemini API in testing mode."
        )
    )
else:
    # Normal initialization for production
    storage_client = storage.Client()
    aiplatform.init(project=PROJECT_ID, location=REGION)
    vertexai.init(project=PROJECT_ID, location=REGION)
    # Load the Gemini model - Specify the correct model name
    model = GenerativeModel(MODEL)


def generate_transcript(audio_part):
    """Generates transcript from audio data."""
    if TESTING_MODE:
        # In testing mode, return a mock transcript
        logging.info("TESTING MODE: Returning mock transcript")
        return (
            "This is a mock transcript for testing purposes. "
            "It simulates what would be returned by the Gemini API in production."
        )
    else:
        # Use a model optimized for transcription if needed
        transcription_model = GenerativeModel("gemini-2.0-flash-001")
        prompt = (
            "Generate a transcription of the audio, only extract speech "
            "and ignore background audio."
        )
        response = transcription_model.generate_content(  # Use the specific model
            [prompt, audio_part],
            generation_config={"temperature": 0.2},  # Lower temp for accuracy
        )
        # Add basic error handling or validation if needed
        return response.text.strip()


def generate_vtt(audio_part):
    """Generates VTT subtitles from audio data."""
    if TESTING_MODE:
        # In testing mode, return mock VTT subtitles
        logging.info("TESTING MODE: Returning mock VTT subtitles")
        return """WEBVTT

00:00:00.000 --> 00:00:05.000
This is a mock subtitle for testing purposes.

00:00:05.000 --> 00:00:10.000
It simulates what would be returned by the Gemini API in production."""
    else:
        vtt_model = GenerativeModel("gemini-2.0-flash-001")
        prompt = (
            "Generate subtitles in VTT format for the following audio. "
            "Ensure accurate timing.\\n"
            "Example format:\\n"
            "WEBVTT\\n\\n"
            "00:00:00.000 --> 00:00:05.000\\n"
            "Hello everyone and welcome back.\\n\\n"
            "00:00:05.500 --> 00:00:10.000\\n"
            "Today we are discussing..."
        )
        response = vtt_model.generate_content(
            [prompt, audio_part],
            # VTT can be long
            generation_config={"max_output_tokens": 4096, "temperature": 0.5},
        )
        # Add basic error handling or validation if needed
        # Ensure it starts with WEBVTT
        text = response.text.strip()
        if not text.startswith("WEBVTT"):
            text = "WEBVTT\\n\\n" + text  # Attempt basic correction
        return text


def generate_shownotes(audio_part):
    """Generates detailed shownotes from audio data."""
    if TESTING_MODE:
        # In testing mode, return mock shownotes
        logging.info("TESTING MODE: Returning mock shownotes")
        return """# Mock Shownotes for Testing

## Key Takeaways
- This is a mock shownote for testing purposes
- It simulates what would be returned by the Gemini API in production

## Resources Mentioned
- Example resource 1: https://example.com
- Example resource 2: https://test.com

## Timestamps
- 00:00 - Introduction
- 02:30 - Main topic discussion
- 05:45 - Conclusion"""
    else:
        prompt = (
            "Analyze the following audio and generate detailed shownotes. "
            "Include key takeaways, any mentioned links or resources, "
            "and relevant timestamps for important points."
        )
        shownotes_model = GenerativeModel("gemini-2.0-flash-001")
        response = shownotes_model.generate_content(
            [prompt, audio_part],
            generation_config={"max_output_tokens": 2048, "temperature": 0.7},
        )
        # Add basic error handling or validation if needed
        return response.text.strip()


def generate_chapters(audio_part):
    """Generates timestamped chapters with summaries from audio data."""
    if TESTING_MODE:
        # In testing mode, return mock chapters
        logging.info("TESTING MODE: Returning mock chapters")
        return [
            {"timecode": "00:00", "chapterSummary": "Introduction to the mock video"},
            {
                "timecode": "02:00",
                "chapterSummary": "First main point in the mock video",
            },
            {
                "timecode": "04:00",
                "chapterSummary": "Second main point in the mock video",
            },
            {"timecode": "06:00", "chapterSummary": "Conclusion of the mock video"},
        ]
    else:
        # Updated prompt asking for JSON output
        chapters_model = GenerativeModel("gemini-2.0-flash-001")
        prompt = (
            "Chapterize the video content by grouping the content into chapters "
            "and providing a summary for each chapter. "
            "Please only capture key events and highlights. "
            "If you are not sure about any info, please do not make it up. "
            "Return the result ONLY as a valid JSON array of objects, "
            "where each object "
            'has the keys "timecode" (string, e.g., "00:00") '
            'and "chapterSummary" (string). Aim for chapters roughly '
            "every 2 minutes.\\n"
            "Example JSON output format:\\n"
            "[\\n"
            '  {\\"timecode\\": \\"00:00\\", \\"chapterSummary\\": '
            '\\"Introduction to the topic...\\"},\\n'
            '  {\\"timecode\\": \\"02:01\\", \\"chapterSummary\\": '
            '\\"Discussing the first main point...\\"}\\n'
            "]"
        )
    response = chapters_model.generate_content(  # Use the specific model
        [prompt, audio_part],
        generation_config={
            "temperature": 0.6,
            "response_mime_type": "application/json",
        },  # Request JSON output
    )

    # Parse the JSON response
    try:
        # The response text should be a valid JSON string
        chapter_list = json.loads(response.text)
        # Basic validation: check if it's a list of dicts with required keys
        if not isinstance(chapter_list, list) or not all(
            isinstance(item, dict) and "timecode" in item and "chapterSummary" in item
            for item in chapter_list
        ):
            raise ValueError("Parsed JSON is not a list of chapter objects.")
        return chapter_list
    except (json.JSONDecodeError, ValueError) as e:
        logging.error(
            f"Failed to parse chapters JSON: {e}. Raw response: {response.text}"
        )
        # Return an empty list or raise an error, depending on desired handling
        return []


def generate_titles(audio_part):
    """Generates title and keywords as a dictionary from audio data."""
    if TESTING_MODE:
        # In testing mode, return mock title and keywords
        logging.info("TESTING MODE: Returning mock title and keywords")
        return {
            "Description": "Mock Video Title for Testing Purposes",
            "Keywords": (
                "testing,mock,video,automation,example,demo,sample,"
                "test,keywords,tags"
            ),
        }
    else:
        # Use a specific model if desired
        title_model = GenerativeModel("gemini-2.0-flash-001")
        prompt = (
            "Please write a 40-character long intriguing title for this video "
            "and 10 comma-separated hashtags suitable for YouTube Shorts "
            "based on the audio. Format the response strictly as a valid JSON object "
            "with two keys: 'Description' (containing the title, max 50 characters) "
            "and 'Keywords' (containing the comma-separated hashtags "
            "as a single string)."
        )
    response = title_model.generate_content(  # Use the specific model
        [prompt, audio_part],
        # Request JSON output
        generation_config={
            "temperature": 0.8,
            "response_mime_type": "application/json",
        },
    )

    # Parse the JSON response
    try:
        title_dict = json.loads(response.text)
        # Basic validation
        if (
            not isinstance(title_dict, dict)
            or "Description" not in title_dict
            or "Keywords" not in title_dict
        ):
            raise ValueError("Parsed JSON is not a dictionary with required keys.")
        return title_dict
    except (json.JSONDecodeError, ValueError) as e:
        logging.error(
            f"Failed to parse title/keywords JSON: {e}. Raw response: {response.text}"
        )
        # Return a default dict or raise, depending on desired handling
        return {"Description": "Default Title", "Keywords": "default,keywords"}


def write_blob(bucket_name, blob_path, content):
    if LOCAL_OUTPUT:
        # In local output mode, write to the local filesystem
        # Determine the local path based on environment
        if os.path.exists("/app/test_data"):
            # Docker environment
            base_path = "/app/test_data"
        else:
            # Local environment
            base_path = "test_data"

        local_path = os.path.join(base_path, blob_path)
        # Create the directory if it doesn't exist
        os.makedirs(os.path.dirname(local_path), exist_ok=True)
        # Write the content to the file
        with open(local_path, "w") as f:
            f.write(content)
        logging.info(f"Wrote {blob_path} to local file {local_path}")

    # If not in testing mode or in real API test mode, also write to GCS
    if not TESTING_MODE or REAL_API_TEST:
        # In production mode, write to GCS
        # Assuming storage_client is initialized globally or passed as an arg
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(blob_path)
        blob.upload_from_string(content)
        logging.info(f"Uploaded {blob_path} to bucket {bucket_name}")


def should_process_file(file_name):
    """
    Determine if a file should be processed based on file type and path.

    Args:
        file_name: Name of the file

    Returns:
        bool: True if file should be processed
    """
    # Check if the file is in the daily-raw or main-raw folders
    if not (file_name.startswith("daily-raw/") or file_name.startswith("main-raw/")):
        logging.info(
            f"File {file_name} is not in daily-raw/ or main-raw/ folder. Skipping."
        )
        return False

    # Check if the file is an MP4
    if not file_name.lower().endswith(".mp4"):
        logging.info(f"File {file_name} is not an MP4 file. Skipping.")
        return False

    return True


def setup_output_paths(bucket_name, file_name):
    """
    Set up all the necessary paths for processing.

    Args:
        bucket_name: The bucket name
        file_name: The file name

    Returns:
        tuple: Contains all the necessary paths for processing
    """
    # Determine if it's a daily or main channel video
    channel_type = "daily" if file_name.startswith("daily-raw/") else "main"

    # Extract the base filename without the path or extension
    base_filename = os.path.splitext(os.path.basename(file_name))[0]

    # Set up paths for the output files
    base_folder = "processed-daily" if channel_type == "daily" else "processed-main"
    output_folder = f"{base_folder}/{base_filename}"

    # Local paths for testing
    local_dir = f"local_output/{output_folder}"
    video_path = f"{local_dir}/{base_filename}.mp4"
    audio_path = f"{local_dir}/{base_filename}.wav"
    transcript_path = f"{local_dir}/{base_filename}.txt"
    vtt_path = f"{local_dir}/{base_filename}.vtt"
    shownotes_path = f"{local_dir}/{base_filename}.md"
    chapters_path = f"{local_dir}/{base_filename}.chapters.json"
    titles_path = f"{local_dir}/{base_filename}.titles.json"

    return (
        channel_type,
        base_filename,
        output_folder,
        local_dir,
        video_path,
        audio_path,
        transcript_path,
        vtt_path,
        shownotes_path,
        chapters_path,
        titles_path,
    )


def download_and_setup_local(bucket_name, file_name, video_path, local_dir):
    """
    Download the file from GCS and set up local directory.

    Args:
        bucket_name: The bucket name
        file_name: The file name
        video_path: Path to save the video
        local_dir: Local directory to create

    Returns:
        bool: True if successful
    """
    # Create local output directory if it doesn't exist
    os.makedirs(local_dir, exist_ok=True)

    # Download the video file from GCS
    if not TESTING_MODE or REAL_API_TEST:
        logging.info(f"Downloading gs://{bucket_name}/{file_name} to {video_path}")
        storage_client = storage.Client()
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(file_name)

        try:
            blob.download_to_filename(video_path)
            logging.info(f"Downloaded gs://{bucket_name}/{file_name} to {video_path}")
        except Exception as e:
            logging.error(f"Error downloading file: {e}")
            return False
    else:
        # In testing mode, create a dummy video file
        logging.info(f"TESTING MODE: Creating dummy file at {video_path}")
        os.makedirs(os.path.dirname(video_path), exist_ok=True)
        with open(video_path, "wb") as f:
            f.write(b"Dummy test video file for testing.")

    return True


def move_processed_file(
    file_name, bucket_name, destination_blob_name, source_path=None, dest_path=None
):
    """Move a processed file to the appropriate destination."""
    try:
        if LOCAL_OUTPUT and source_path and dest_path:
            # In local output mode, move the file locally
            os.makedirs(os.path.dirname(dest_path), exist_ok=True)
            if os.path.exists(source_path):
                shutil.copy2(source_path, dest_path)
                if not TESTING_MODE:
                    os.remove(source_path)
                logging.info(
                    f"Successfully moved local file from {source_path} to {dest_path}"
                )
            else:
                logging.warning(
                    f"Source file {source_path} does not exist. Skipping move."
                )
        else:
            if not TESTING_MODE or REAL_API_TEST:
                # In production mode, move the file in GCS
                # blob refers to the original blob object fetched earlier for download
                storage_client = storage.Client()
                bucket = storage_client.bucket(bucket_name)
                blob = bucket.blob(file_name)
                bucket.copy_blob(blob, bucket, destination_blob_name)
                blob.delete()  # Delete the original blob after successful copy
                logging.info(
                    f"Successfully moved gs://{bucket_name}/{file_name} to gs://{bucket_name}/{destination_blob_name}"
                )
    except Exception as move_error:
        # Log error but don't fail the entire process if move fails
        logging.error(
            f"Failed to move original file {file_name} to "
            f"{destination_blob_name}: {move_error}"
        )


def handle_processing_results(
    bucket_name,
    output_folder,
    base_filename,
    local_dir,
    video_path,
    audio_path,
    transcript_path,
    vtt_path,
    shownotes_path,
    chapters_path,
    titles_path,
):
    """Handle all processing results and output files."""
    try:
        # Define additional GCS paths
        bucket = None
        if not TESTING_MODE or REAL_API_TEST:
            storage_client = storage.Client()
            bucket = storage_client.bucket(bucket_name)

        # Upload all files to GCS if not in local-only mode
        if not LOCAL_OUTPUT:
            try:
                # Upload video to the processed folder in GCS
                if os.path.exists(video_path) and (not TESTING_MODE or REAL_API_TEST):
                    video_blob = bucket.blob(f"{output_folder}/{base_filename}.mp4")
                    video_blob.upload_from_filename(video_path)

                # Upload metadata files to GCS
                for local_path, filename in [
                    (audio_path, f"{base_filename}.wav"),
                    (transcript_path, f"{base_filename}.txt"),
                    (vtt_path, f"{base_filename}.vtt"),
                    (shownotes_path, f"{base_filename}.md"),
                    (chapters_path, f"{base_filename}.chapters.json"),
                    (titles_path, f"{base_filename}.titles.json"),
                ]:
                    if os.path.exists(local_path) and (
                        not TESTING_MODE or REAL_API_TEST
                    ):
                        metadata_blob = bucket.blob(f"{output_folder}/{filename}")
                        metadata_blob.upload_from_filename(local_path)
                        logging.info(
                            f"Uploaded {local_path} to gs://{bucket_name}/{output_folder}/{filename}"
                        )

                # The YouTube uploader will be triggered automatically by the GCS event
                # when the files are uploaded to the processed-daily/ or
                # processed-main/ folders
                logging.info("YouTube uploader will be triggered by GCS event.")
            except Exception as upload_error:
                logging.error(f"Error uploading processed files: {upload_error}")
                return False

        return True
    except Exception as e:
        logging.error(f"Error handling processing results: {e}")
        return False


def extract_audio(video_path, output_path=None, sample_rate=16000, channels=1):
    """
    Extract audio from a video file using ffmpeg.

    Args:
        video_path: Path to the video file
        output_path: Path to save the audio file (if None, uses video path with .wav)
        sample_rate: Audio sample rate in Hz
        channels: Number of audio channels (1=mono, 2=stereo)

    Returns:
        Path to the output audio file
    """
    if output_path is None:
        # Generate output path based on video path
        output_path = os.path.splitext(video_path)[0] + ".wav"

    logging.info(f"Extracting audio from {video_path} to {output_path}")

    try:
        subprocess.run(
            [
                "ffmpeg",
                "-y",  # Overwrite output files without asking
                "-i",
                video_path,
                "-vn",  # No video output
                "-acodec",
                "pcm_s16le",  # Standard WAV format
                "-ar",
                str(sample_rate),  # Audio sample rate
                "-ac",
                str(channels),  # Number of audio channels
                output_path,
            ],
            check=True,  # Raise exception on non-zero exit code
            capture_output=True,  # Capture stderr/stdout
            text=True,  # Decode stderr/stdout as text
        )
        logging.info("Audio extraction complete")
        return output_path
    except subprocess.CalledProcessError as e:
        logging.error(f"ffmpeg failed: {e}\nStderr: {e.stderr}")
        raise
    except Exception as e:
        logging.error(f"Error during audio extraction: {e}")
        raise


def process_audio_and_generate_content(
    video_path,
    audio_path,
    transcript_path,
    vtt_path,
    shownotes_path,
    chapters_path,
    titles_path,
):
    """
    Process audio and generate content using AI.

    Args:
        video_path: Path to the video file
        audio_path: Path to the audio file
        transcript_path: Path to save the transcript
        vtt_path: Path to save the VTT subtitles
        shownotes_path: Path to save the shownotes
        chapters_path: Path to save the chapters data
        titles_path: Path to save the titles data

    Returns:
        bool: True if successful
    """
    try:
        # Extract audio from video
        extract_audio(video_path, audio_path)
        logging.info(f"Extracted audio from {video_path} to {audio_path}")

        # Load the audio file for AI processing
        audio_part = None
        if os.path.exists(audio_path) and os.path.getsize(audio_path) > 0:
            try:
                with open(audio_path, "rb") as f:
                    audio_data = f.read()
                    audio_part = Part.from_data(audio_data, mime_type="audio/wav")
            except Exception as e:
                logging.error(f"Error loading audio file: {e}")
                # Create dummy audio data for testing
                if TESTING_MODE:
                    audio_part = Part.from_data(
                        b"Dummy audio data for testing", mime_type="audio/wav"
                    )

        # Generate transcript
        transcript = generate_transcript(audio_part)
        if transcript:
            with open(transcript_path, "w") as f:
                f.write(transcript)
            logging.info(f"Saved transcript to {transcript_path}")

        # Generate VTT subtitles
        vtt = generate_vtt(audio_part)
        if vtt:
            with open(vtt_path, "w") as f:
                f.write(vtt)
            logging.info(f"Saved VTT subtitles to {vtt_path}")

        # Generate shownotes
        shownotes = generate_shownotes(audio_part)
        if shownotes:
            with open(shownotes_path, "w") as f:
                f.write(shownotes)
            logging.info(f"Saved shownotes to {shownotes_path}")

        # Generate chapters
        chapters = generate_chapters(audio_part)
        if chapters:
            with open(chapters_path, "w") as f:
                json.dump(chapters, f, indent=2)
            logging.info(f"Saved chapters to {chapters_path}")

        # Generate title and keywords
        titles_data = generate_titles(audio_part)
        if titles_data:
            with open(titles_path, "w") as f:
                json.dump(titles_data, f, indent=2)
            logging.info(f"Saved titles to {titles_path}")

        return True
    except Exception as e:
        logging.error(f"Error processing audio and generating content: {e}")
        logging.exception("Stack trace:")
        return False


# Rename function and change signature to accept bucket and file name directly
def process_video_event(bucket_name, file_name):
    logging.info(f"Processing video event for gs://{bucket_name}/{file_name}")

    # Skip files that don't need processing
    if not should_process_file(file_name):
        return

    # Set up all the paths
    (
        channel_type,
        base_filename,
        output_folder,
        local_dir,
        video_path,
        audio_path,
        transcript_path,
        vtt_path,
        shownotes_path,
        chapters_path,
        titles_path,
    ) = setup_output_paths(bucket_name, file_name)

    # Download the file from GCS and set up local directory
    if not download_and_setup_local(bucket_name, file_name, video_path, local_dir):
        logging.error("Failed to download and set up files. Aborting.")
        return

    # --- Process the video file and generate content ---
    if not process_audio_and_generate_content(
        video_path,
        audio_path,
        transcript_path,
        vtt_path,
        shownotes_path,
        chapters_path,
        titles_path,
    ):
        logging.error("Failed to process video and generate content. Aborting.")
        return

    # --- Move the original video file to a processed folder ---
    original_video_destination = f"raw-processed/{file_name}"
    move_processed_file(file_name, bucket_name, original_video_destination)

    # --- Handle the processing results ---
    if not handle_processing_results(
        bucket_name,
        output_folder,
        base_filename,
        local_dir,
        video_path,
        audio_path,
        transcript_path,
        vtt_path,
        shownotes_path,
        chapters_path,
        titles_path,
    ):
        logging.error("Failed to handle processing results.")
        return False

    logging.info(f"Completed processing for {file_name}")
    return True

================
File: video_processor/pytest.ini
================
[pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts = --verbose --cov=video_processor --cov=.. --cov-report=term-missing

# Custom markers
markers =
    integration: integration tests that rely on external services or emulators

================
File: video_processor/README.md
================
# Video Processor Service

A modular service for processing videos, extracting audio, generating transcripts, subtitles, and more with AI-powered features.

## Project Structure

The project follows a modular architecture design to improve maintainability, testability, and developer experience:

```
backend/video_processor/
├── api/                  # API endpoints and controllers
├── config/               # Configuration management
├── core/                 # Core domain logic
│   ├── models/           # Domain models
│   └── processors/       # Processing components
├── services/             # External service integrations
│   ├── ai/               # AI model integrations
│   ├── storage/          # Storage service (GCS, local)
│   └── youtube/          # YouTube API integration
├── utils/                # Shared utilities
└── tests/                # Tests organized by type
    ├── unit/             # Unit tests
    ├── integration/      # Integration tests
    └── e2e/              # End-to-end tests
```

## Architecture Overview

```mermaid
flowchart TD
    A[Upload MP4 to GCS Bucket] --> B[Eventarc Trigger]
    B --> C[Cloud Run Service]
    C --> D[Download Video]
    D --> E[Extract Audio with ffmpeg]
    E --> F[Create Part Object for Audio]
    F --> G[Process with Gemini API]
    G --> H1[Generate Transcript]
    G --> H2[Generate VTT Subtitles]
    G --> H3[Generate Shownotes]
    G --> H4[Generate Chapters]
    G --> H5[Generate Title & Keywords]
    H1 & H2 & H3 & H4 & H5 --> I[Upload Results to GCS]
    I --> J[Move Original Video to Processed Folder]
    I --> K[Trigger YouTube Upload Function]
```

## Key Design Features

1. **Dependency Injection**
   - Services and components receive dependencies rather than creating them
   - Improves testability and flexibility

2. **Interface-Based Design**
   - Components work with interfaces instead of concrete implementations
   - Enables easy swapping of implementations (e.g., GCS vs. local storage)

3. **Centralized Configuration**
   - Environment variables managed through dedicated system
   - Default values and validation in one place

4. **Error Handling**
   - Consistent error handling with custom exceptions
   - Retry mechanisms for transient failures

5. **Logging**
   - Structured logging throughout the application
   - Consistent format and levels

## Development Workflow

### Local Development

1. Set up your environment:

```bash
# Create a virtual environment
python -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Set environment variables for local testing
export TESTING_MODE=true
export LOCAL_OUTPUT=true
```

2. Run the service locally:

```bash
python -m video_processor.app
```

3. Test a video upload:
   - Use the `scripts/simulate_firestore_update.py` script, or
   - Make a POST request to the service

### Running Tests

Tests are organized by type:

```bash
# Run unit tests
pytest tests/unit

# Run integration tests
pytest tests/integration

# Run end-to-end tests
pytest tests/e2e

# Run comprehensive test
python scripts/run_comprehensive_test.py
```

## Deployment

The service can be deployed to Google Cloud Run:

```bash
# Deploy to Cloud Run
./deploy.sh

# Deploy in dry-run mode (no actual deployment)
./deploy.sh --dry-run

# Skip tests during deployment
./deploy.sh --skip-tests
```

## Service Components

### Storage Service

The `StorageService` interface abstracts storage operations, with implementations for:

- **GCSStorageService**: Google Cloud Storage
- **LocalStorageService**: Local filesystem (for testing and development)

### Video Processing

The core processing pipeline:

1. **Download Video**: Get video from storage
2. **Extract Audio**: Convert video to audio
3. **Generate Transcript**: Create full transcript
4. **Generate Subtitles**: Create subtitles in VTT format
5. **Generate Shownotes**: Create detailed notes
6. **Generate Chapters**: Create timestamped chapters
7. **Generate Title/Keywords**: Create optimized metadata
8. **Upload Results**: Store all outputs

### YouTube Integration

Handles automatic uploading to YouTube channels:

- Authentication using OAuth2
- Separate channel configurations
- Caption and metadata management

## API Integration

### Gemini API

The module uses Google's Gemini API through Vertex AI to process audio and generate various metadata:

1. **Transcript**: Full text transcript of the audio
2. **VTT Subtitles**: WebVTT format subtitles with timestamps
3. **Shownotes**: Detailed notes about the content
4. **Chapters**: Timestamped chapters with summaries
5. **Title & Keywords**: Engaging title and relevant keywords

### Audio Format Requirements

When sending audio to Gemini API:
- Use the `Part.from_data()` method to create a properly formatted Part object
- Specify the correct MIME type (e.g., "audio/wav")
- The audio should be in a supported format (WAV, MP3, etc.)

## Future Improvements

1. Add more robust error handling for different types of audio files and formats
2. Implement retry logic for API calls
3. Increase test coverage for edge cases and error conditions
4. Consider adding support for additional audio formats
5. Optimize audio extraction parameters for better quality
6. Add integration tests with actual GCS and Gemini API (using test credentials)
7. Implement CI/CD pipeline for automated testing
8. Add performance tests to measure processing time for different file sizes
9. Create a test data generator for creating test audio/video files with known content

================
File: video_processor/setup_youtube_secrets.py
================
#!/usr/bin/env python3
"""
Script to set up YouTube API secrets in Google Cloud Secret Manager.
This script helps create or update the necessary secrets for the YouTube uploader.
"""

import argparse
import json

from google.cloud import secretmanager

# Project ID
PROJECT_ID = "automations-457120"

# Secret IDs for each channel
DAILY_SECRETS = {
    "client_id": "youtube-daily-client-id",
    "client_secret": "youtube-daily-client-secret",
    "refresh_token": "youtube-daily-refresh-token",
}

MAIN_SECRETS = {
    "client_id": "youtube-main-client-id",
    "client_secret": "youtube-main-client-secret",
    "refresh_token": "youtube-refresh-token",
}


def create_or_update_secret(project_id, secret_id, secret_value):
    """Creates a new secret or updates an existing one in Secret Manager.

    Args:
        project_id: Google Cloud project ID
        secret_id: ID of the secret to create or update
        secret_value: Value to store in the secret

    Returns:
        bool: True if successful, False otherwise
    """
    client = secretmanager.SecretManagerServiceClient()
    parent = f"projects/{project_id}"

    # Check if the secret already exists
    try:
        secret_path = f"{parent}/secrets/{secret_id}"
        client.get_secret(request={"name": secret_path})
        secret_exists = True
        print(f"Secret {secret_id} already exists.")
    except Exception:
        secret_exists = False
        print(f"Secret {secret_id} does not exist yet.")

    try:
        # Create the secret if it doesn't exist
        if not secret_exists:
            print(f"Creating secret {secret_id}...")
            client.create_secret(
                request={
                    "parent": parent,
                    "secret_id": secret_id,
                    "secret": {"replication": {"automatic": {}}},
                }
            )

        # Add the new secret version
        print(f"Adding new version to secret {secret_id}...")
        response = client.add_secret_version(
            request={
                "parent": f"{parent}/secrets/{secret_id}",
                "payload": {"data": secret_value.encode("UTF-8")},
            }
        )
        print(f"Added secret version: {response.name}")
        return True
    except Exception as e:
        print(f"Error creating/updating secret {secret_id}: {e}")
        return False


def setup_channel_secrets(channel, client_id, client_secret, refresh_token):
    """Sets up all secrets for a specific channel.

    Args:
        channel: Channel name ('daily' or 'main')
        client_id: OAuth client ID
        client_secret: OAuth client secret
        refresh_token: OAuth refresh token

    Returns:
        bool: True if all secrets were set up successfully, False otherwise
    """
    secrets = DAILY_SECRETS if channel == "daily" else MAIN_SECRETS

    success = True

    # Set up client ID
    if client_id:
        if not create_or_update_secret(PROJECT_ID, secrets["client_id"], client_id):
            success = False

    # Set up client secret
    if client_secret:
        if not create_or_update_secret(
            PROJECT_ID, secrets["client_secret"], client_secret
        ):
            success = False

    # Set up refresh token
    if refresh_token:
        if not create_or_update_secret(
            PROJECT_ID, secrets["refresh_token"], refresh_token
        ):
            success = False

    return success


def load_client_secrets(file_path):
    """Loads client ID and secret from a client_secret.json file.

    Args:
        file_path: Path to the client_secret.json file

    Returns:
        tuple: (client_id, client_secret) or (None, None) if loading fails
    """
    try:
        with open(file_path, "r") as f:
            data = json.load(f)

        # Extract client ID and secret based on the file format
        if "web" in data:
            return data["web"]["client_id"], data["web"]["client_secret"]
        elif "installed" in data:
            return data["installed"]["client_id"], data["installed"]["client_secret"]
        else:
            print(f"Unknown format in {file_path}")
            return None, None
    except Exception as e:
        print(f"Error loading client secrets from {file_path}: {e}")
        return None, None


def main():
    parser = argparse.ArgumentParser(
        description="Set up YouTube API secrets in Secret Manager"
    )
    parser.add_argument(
        "--channel",
        choices=["daily", "main", "both"],
        default="both",
        help="Which channel to set up secrets for",
    )
    parser.add_argument(
        "--client-secrets-file",
        help="Path to client_secret.json file from Google Cloud Console",
    )
    parser.add_argument("--client-id", help="OAuth client ID")
    parser.add_argument("--client-secret", help="OAuth client secret")
    parser.add_argument("--refresh-token", help="OAuth refresh token")

    args = parser.parse_args()

    # Load client ID and secret from file if provided
    client_id = args.client_id
    client_secret = args.client_secret

    if args.client_secrets_file:
        file_client_id, file_client_secret = load_client_secrets(
            args.client_secrets_file
        )
        if file_client_id and file_client_secret:
            client_id = client_id or file_client_id
            client_secret = client_secret or file_client_secret

    # Check if we have at least one value to set
    if not any([client_id, client_secret, args.refresh_token]):
        print("Error: No values provided to store in Secret Manager.")
        print(
            "Please provide at least one of: --client-id, --client-secret, "
            "--refresh-token, or --client-secrets-file"
        )
        return 1

    # Set up secrets for the specified channel(s)
    if args.channel in ["daily", "both"]:
        print("\n=== Setting up secrets for DAILY channel ===")
        if setup_channel_secrets("daily", client_id, client_secret, args.refresh_token):
            print("✅ Successfully set up secrets for DAILY channel")
        else:
            print("❌ Failed to set up some secrets for DAILY channel")

    if args.channel in ["main", "both"]:
        print("\n=== Setting up secrets for MAIN channel ===")
        if setup_channel_secrets("main", client_id, client_secret, args.refresh_token):
            print("✅ Successfully set up secrets for MAIN channel")
        else:
            print("❌ Failed to set up some secrets for MAIN channel")

    return 0


if __name__ == "__main__":
    exit(main())

================
File: video_processor/setup.py
================
from setuptools import find_packages, setup

setup(
    name="video_processor",
    version="0.1.0",
    packages=find_packages(),
    install_requires=[
        "google-cloud-storage==2.10.0",
        "google-cloud-aiplatform",
        "vertexai",
        "flask",
        "gunicorn",
    ],
    extras_require={
        "dev": [
            "pytest",
            "pytest-mock",
            "pytest-cov",
        ],
    },
)

================
File: video_processor/test_audio_processing.py
================
#!/usr/bin/env python3
"""
Test script to verify the audio processing with Gemini API.
"""

import logging
import os
import subprocess
import tempfile

import vertexai
from vertexai.preview.generative_models import GenerativeModel, Part

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)

# Set project ID directly for testing
PROJECT_ID = "automations-457120"  # Use the same project ID as in the main code
REGION = "us-central1"


def test_audio_processing():
    """Test audio processing with a sample WAV file."""
    # Initialize Vertex AI
    vertexai.init(project=PROJECT_ID, location=REGION)
    # Create a temporary directory for our test
    with tempfile.TemporaryDirectory() as tmpdir:
        # Create a simple test audio file using ffmpeg
        test_audio_path = os.path.join(tmpdir, "test_audio.wav")

        # Generate a simple test tone using ffmpeg
        logging.info(f"Generating test audio file at {test_audio_path}...")
        try:
            subprocess.run(
                [
                    "ffmpeg",
                    "-y",  # Overwrite output files without asking
                    "-f",
                    "lavfi",  # Use libavfilter
                    "-i",
                    "sine=frequency=440:duration=5",  # Generate a 5-second 440Hz tone
                    "-ar",
                    "16000",  # Audio sample rate
                    "-ac",
                    "1",  # Mono audio
                    test_audio_path,
                ],
                check=True,
                capture_output=True,
                text=True,
            )
            logging.info("Test audio generation complete.")
        except subprocess.CalledProcessError as e:
            logging.error(f"ffmpeg failed: {e}\nStderr: {e.stderr}")
            return

        # Read the audio file and create a proper Part object
        logging.info("Reading audio file and creating Part object...")
        try:
            with open(test_audio_path, "rb") as f:
                audio_bytes = f.read()

            # Create a Part object with the correct MIME type
            audio_part = Part.from_data(mime_type="audio/wav", data=audio_bytes)

            # Test the transcript generation with our own implementation
            logging.info("Testing transcript generation...")

            # Define a simple transcript function for testing
            def generate_test_transcript(audio_part):
                """Test function to generate a transcript from audio data."""
                transcription_model = GenerativeModel("gemini-2.0-flash-001")
                prompt = (
                    "Generate a transcription of the audio, only extract speech "
                    "and ignore background audio."
                )
                response = transcription_model.generate_content(
                    [prompt, audio_part],
                    generation_config={"temperature": 0.2},
                )
                return response.text.strip()

            transcript = generate_test_transcript(audio_part)
            logging.info(f"Generated transcript: {transcript}")

            logging.info("Test completed successfully!")
        except Exception as e:
            logging.error(f"Error during testing: {e}")


if __name__ == "__main__":
    test_audio_processing()

================
File: video_processor/test_process_video.py
================
#!/usr/bin/env python3
"""
Test script to verify the video processing with the fixed code.
"""

import logging
import os
import subprocess
import tempfile

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)


def test_process_video():
    """Test the process_video_event function with a sample MP4 file."""
    # Create a temporary directory for our test
    with tempfile.TemporaryDirectory() as tmpdir:
        # Create a simple test video file using ffmpeg
        test_video_path = os.path.join(tmpdir, "test_video.mp4")

        # Generate a simple test video using ffmpeg
        logging.info(f"Generating test video file at {test_video_path}...")
        try:
            subprocess.run(
                [
                    "ffmpeg",
                    "-y",  # Overwrite output files without asking
                    "-f",
                    "lavfi",  # Use libavfilter
                    "-i",
                    "sine=frequency=440:duration=5",  # Generate a 5-second 440Hz tone
                    "-f",
                    "lavfi",  # Use libavfilter for video
                    "-i",
                    "color=c=blue:s=640x480:d=5",  # Generate a 5-second blue screen
                    "-c:a",
                    "aac",  # Audio codec
                    "-c:v",
                    "h264",  # Video codec
                    test_video_path,
                ],
                check=True,
                capture_output=True,
                text=True,
            )
            logging.info("Test video generation complete.")
        except subprocess.CalledProcessError as e:
            logging.error(f"ffmpeg failed: {e}\nStderr: {e.stderr}")
            return

        # Create a mock GCS bucket and upload the test video
        bucket_name = "test-bucket"
        file_name = "daily-raw/test_video.mp4"

        # Mock the GCS operations for testing
        logging.info("Testing process_video_event function...")
        try:
            # In a real test, we would upload the file to GCS and then call
            # process_video_event
            # For this test, we'll just log that we would call the function
            logging.info(f"Would call process_video_event({bucket_name}, {file_name})")
            logging.info("Test completed successfully!")
        except Exception as e:
            logging.error(f"Error during testing: {e}")


if __name__ == "__main__":
    test_process_video()

================
File: video_processor/youtube_uploader_README.md
================
# YouTube Uploader

This component handles uploading processed videos to YouTube channels. It supports uploading to both a "Daily" channel and a "Main" channel.

## Overview

The YouTube uploader is implemented as a Cloud Function that is triggered by Cloud Storage events when a processed video is ready. It handles:

1. Detecting when a processed video is available
2. Downloading the video and associated metadata (description, captions, etc.)
3. Authenticating with the YouTube API
4. Uploading the video to the appropriate YouTube channel
5. Adding captions if available
6. Creating a marker file to prevent duplicate uploads

## Setup Instructions

### 1. Create OAuth Credentials

1. Go to the [Google Cloud Console](https://console.cloud.google.com/apis/credentials)
2. Create an OAuth 2.0 Client ID (Web application type)
3. Add `http://localhost:8080` as an authorized redirect URI
4. Download the JSON file and save it as `credentials/client_secret.json`

### 2. Generate OAuth Tokens

Run the token generator script for each channel:

```bash
# For the Daily channel
python -m video_processor.generate_youtube_token --channel daily

# For the Main channel
python -m video_processor.generate_youtube_token --channel main
```

Follow the prompts to authorize the application and obtain refresh tokens.

### 3. Set Up Secret Manager

Store the OAuth credentials in Secret Manager:

```bash
# Using the setup script
python -m video_processor.setup_youtube_secrets --client-secrets-file credentials/client_secret.json --refresh-token YOUR_REFRESH_TOKEN

# Or manually via the Google Cloud Console
# Go to: https://console.cloud.google.com/security/secret-manager
```

The following secrets need to be created:
- `youtube-daily-client-id`
- `youtube-daily-client-secret`
- `youtube-daily-refresh-token`
- `youtube-main-client-id`
- `youtube-main-client-secret`
- `youtube-refresh-token`

### 4. Deploy the Cloud Functions

The YouTube uploader is deployed as two separate Cloud Functions:

1. `upload-to-youtube-daily`: Triggered by files in the `processed-daily/` folder
2. `upload-to-youtube-main`: Triggered by files in the `processed-main/` folder

## Usage

The YouTube uploader is automatically triggered when a processed video is available. The workflow is:

1. Upload a raw video to the `daily-raw/` or `main-raw/` folder in the GCS bucket
2. The video processor processes the video and generates metadata
3. The processed files are stored in `processed-daily/` or `processed-main/`
4. The YouTube uploader is triggered and uploads the video to the appropriate channel

### Configuration Options

The YouTube uploader can be configured using environment variables:

- `DEFAULT_PRIVACY_STATUS`: Sets the default privacy status for uploaded videos (default: "unlisted")
  - Options: "private", "unlisted", "public"
  - Example: `DEFAULT_PRIVACY_STATUS=unlisted`

## File Structure

- `video_processor/youtube_uploader.py`: Main implementation of the YouTube uploader
- `video_processor/generate_youtube_token.py`: Script to generate OAuth tokens
- `video_processor/setup_youtube_secrets.py`: Script to set up Secret Manager secrets

## Troubleshooting

### Common Issues

1. **Authentication Errors**: Check that the OAuth credentials are correctly stored in Secret Manager.
2. **Missing Files**: Ensure that the video file and metadata files are correctly named and stored in the GCS bucket.
3. **Duplicate Uploads**: The uploader creates a marker file to prevent duplicate uploads. If you need to force a re-upload, delete the `uploaded.marker` file in the processed folder.

### Logs

Check the Cloud Functions logs for detailed error messages:

```bash
gcloud functions logs read upload-to-youtube-daily
gcloud functions logs read upload-to-youtube-main
```

## Testing

Run the unit tests:

```bash
cd video_processor
python -m pytest tests/test_youtube_uploader.py -v
```

================
File: video_processor/youtube_uploader.py
================
import json
import logging
import os
import random  # Add random for jitter
import tempfile
import time  # Add time for sleep
import traceback

import functions_framework
from google.auth.transport.requests import Request
from google.cloud import secretmanager, storage
from google.oauth2.credentials import Credentials
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from googleapiclient.http import MediaFileUpload

# Configure logging
logging.basicConfig(level=logging.INFO)

# --- Configuration (Fetch from Secrets/Env Vars) ---
PROJECT_ID = os.environ.get("GOOGLE_CLOUD_PROJECT", "automations-457120")

# Default privacy status for uploaded videos
DEFAULT_PRIVACY_STATUS = os.environ.get("DEFAULT_PRIVACY_STATUS", "unlisted")

# Secret Manager IDs (Assumed names, need to be created)
DAILY_SECRETS = {
    "client_id": "youtube-daily-client-id",
    "client_secret": "youtube-daily-client-secret",
    "refresh_token": "youtube-daily-refresh-token",
}
MAIN_SECRETS = {
    "client_id": "youtube-main-client-id",
    "client_secret": "youtube-main-client-secret",
    "refresh_token": "youtube-refresh-token",
}

# YouTube API details
API_SERVICE_NAME = "youtube"
API_VERSION = "v3"
SCOPES = ["https://www.googleapis.com/auth/youtube.upload"]

# --- Helper Functions ---


def get_secret(secret_id, project_id, version_id="latest"):
    """Retrieves a secret version from Google Cloud Secret Manager."""
    try:
        client = secretmanager.SecretManagerServiceClient()
        name = f"projects/{project_id}/secrets/{secret_id}/versions/{version_id}"
        response = client.access_secret_version(request={"name": name})
        return response.payload.data.decode("UTF-8")
    except Exception as e:
        logging.error(f"Failed to retrieve secret {secret_id}: {e}")
        raise


def get_youtube_credentials(secret_config):
    """Builds credentials object from stored refresh token."""
    try:
        client_id = get_secret(secret_config["client_id"], PROJECT_ID)
        client_secret = get_secret(secret_config["client_secret"], PROJECT_ID)
        refresh_token = get_secret(secret_config["refresh_token"], PROJECT_ID)

        credentials = Credentials(
            None,  # No access token initially
            refresh_token=refresh_token,
            token_uri="https://oauth2.googleapis.com/token",
            client_id=client_id,
            client_secret=client_secret,
            scopes=SCOPES,
        )

        # Refresh the credentials to get an access token
        # This requires the google.auth.transport.requests library
        try:
            credentials.refresh(Request())
        except Exception as e:
            logging.error(f"Failed to refresh YouTube credentials: {e}")
            # Potentially indicates an expired/revoked refresh token
            # This requires manual re-authentication outside the function
            raise RuntimeError(
                "Could not refresh YouTube credentials. Refresh token might be invalid."
            ) from e

        return credentials

    except Exception as e:
        logging.error(f"Error building YouTube credentials: {e}")
        raise


def download_blob(bucket_name, source_blob_name, destination_file_name):
    """Downloads a blob from the bucket."""
    try:
        storage_client = storage.Client()
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(source_blob_name)

        logging.info(
            "Downloading gs://{}/{} to {}".format(
                bucket_name, source_blob_name, destination_file_name
            )
        )
        blob.download_to_filename(destination_file_name)
        logging.info("Download complete.")
        return destination_file_name
    except Exception as e:
        logging.error(f"Failed to download blob {source_blob_name}: {e}")
        raise


def read_blob_content(bucket_name, source_blob_name):
    """Reads the content of a text blob from the bucket."""
    try:
        storage_client = storage.Client()
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(source_blob_name)
        logging.info(f"Reading content from gs://{bucket_name}/{source_blob_name}")
        content = blob.download_as_text()
        logging.info(f"Read {len(content)} bytes.")
        return content
    except Exception as e:
        # Might fail if file doesn't exist, handle gracefully
        logging.warning(f"Could not read blob {source_blob_name}: {e}")
        return None


def check_upload_marker(bucket_name, folder_path):
    """Checks if a video has already been uploaded by looking for a marker file.

    Args:
        bucket_name: The GCS bucket name
        folder_path: The folder path in the bucket

    Returns:
        bool: True if the marker exists (video already uploaded), False otherwise
    """
    marker_path = folder_path + "uploaded.marker"
    try:
        storage_client = storage.Client()
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(marker_path)
        exists = blob.exists()
        if exists:
            logging.info(f"Upload marker found at gs://{bucket_name}/{marker_path}")
        return exists
    except Exception as e:
        logging.warning(f"Error checking upload marker: {e}")
        return False


def create_upload_marker(bucket_name, folder_path, video_id):
    """Creates a marker file after successful upload to prevent duplicate uploads.

    Args:
        bucket_name: The GCS bucket name
        folder_path: The folder path in the bucket
        video_id: The YouTube video ID of the uploaded video

    Returns:
        bool: True if marker was created successfully, False otherwise
    """
    marker_path = folder_path + "uploaded.marker"
    marker_content = json.dumps(
        {
            "video_id": video_id,
            "upload_time": time.strftime("%Y-%m-%d %H:%M:%S"),
            "status": "uploaded",
        }
    )

    try:
        storage_client = storage.Client()
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(marker_path)
        blob.upload_from_string(marker_content)
        logging.info(f"Created upload marker at gs://{bucket_name}/{marker_path}")
        return True
    except Exception as e:
        logging.error(f"Failed to create upload marker: {e}")
        return False


def upload_video(
    youtube,
    local_file_path,  # Now expects a local path
    title,
    description,
    privacy_status=None,  # Will use DEFAULT_PRIVACY_STATUS if None
    category_id="22",  # Default: People & Blogs
    tags=None,
    max_retries=5,  # Max retries for 5xx errors
    initial_backoff=1,  # Initial wait time in seconds
    max_backoff=16,  # Max wait time
    backoff_factor=2,  # Factor to increase wait time
):
    """Uploads a video from a local file path to YouTube with retries."""
    if tags is None:
        tags = []
    logging.info(f"Starting upload for local file: {local_file_path}")

    # Use DEFAULT_PRIVACY_STATUS if privacy_status is None
    if privacy_status is None:
        privacy_status = DEFAULT_PRIVACY_STATUS

    logging.info(f"Setting video privacy status to: {privacy_status}")

    body = dict(
        snippet=dict(
            title=title, description=description, tags=tags, categoryId=category_id
        ),
        status=dict(privacyStatus=privacy_status),
    )

    # Perform the upload
    insert_request = youtube.videos().insert(
        part=",".join(body.keys()),
        body=body,
        media_body=MediaFileUpload(local_file_path, chunksize=-1, resumable=True),
    )

    response = None
    retries = 0
    backoff = initial_backoff
    while response is None:
        try:
            status, response = insert_request.next_chunk()
            if status:
                logging.info(f"Uploaded {int(status.progress() * 100)}%")
            # Reset retry count on successful chunk
            retries = 0
            backoff = initial_backoff
        except HttpError as e:
            if e.resp.status in [500, 502, 503, 504]:
                retries += 1
                if retries > max_retries:
                    logging.error(
                        "An HTTP error {} occurred and max retries reached: {}".format(
                            e.resp.status, e.content
                        )
                    )
                    raise  # Max retries exceeded
                else:
                    # Exponential backoff with jitter
                    sleep_time = min(backoff + (random.random() * 0.5), max_backoff)
                    logging.warning(
                        "HTTP error {} occurred (Attempt {}/{}). "
                        "Retrying in {:.2f}s...".format(
                            e.resp.status, retries, max_retries, sleep_time
                        )
                    )
                    time.sleep(sleep_time)
                    backoff = min(backoff * backoff_factor, max_backoff)
                    # Continue loop to call next_chunk() again
            elif e.resp.status == 404:
                logging.error(
                    "HTTP error 404 occurred: {}. "
                    "Upload session lost, requires restart.".format(e.content)
                )
                raise  # Indicate restart needed
            else:
                # Non-retryable HTTP error
                logging.error(
                    f"An non-retryable HTTP error {e.resp.status} occurred: {e.content}"
                )
                raise  # Fail immediately
        except Exception as e:
            logging.error(f"An unexpected error occurred during upload chunk: {e}")
            raise  # Fail immediately for other errors

    logging.info(f"Upload successful! Video ID: {response.get('id')}")
    return response


def upload_captions(
    youtube,
    video_id,
    caption_file_path,
    language="en",
    max_retries=2,  # Fewer retries for captions
    initial_backoff=1,
    max_backoff=8,
    backoff_factor=2,
):
    """Uploads captions/subtitles with retries for transient errors."""
    logging.info(f"Uploading captions from {caption_file_path} for video ID {video_id}")

    insert_request = youtube.captions().insert(
        part="snippet",
        body={
            "snippet": {
                "videoId": video_id,
                "language": language,
                "name": f"{language} Captions",
                "isDraft": False,
            }
        },
        media_body=MediaFileUpload(caption_file_path),
        # Note: Resumable upload is default for media uploads
        # but explicit retry logic is needed for robustness.
    )

    response = None
    retries = 0
    backoff = initial_backoff
    while retries <= max_retries:
        try:
            response = insert_request.execute()
            logging.info(f"Caption upload successful! Caption ID: {response.get('id')}")
            return response  # Success
        except HttpError as e:
            if e.resp.status in [500, 502, 503, 504] and retries < max_retries:
                retries += 1
                sleep_time = min(backoff + (random.random() * 0.5), max_backoff)
                logging.warning(
                    "Error on caption upload (Attempt {}/{}). "
                    "Retrying in {:.2f}s...".format(retries, max_retries, sleep_time)
                )
                time.sleep(sleep_time)
                backoff = min(backoff * backoff_factor, max_backoff)
                # Continue loop to retry execute()
            else:
                # Max retries reached or non-retryable HTTP error
                logging.error(
                    "Failed uploading captions: {}. "
                    "Max retries ({}) reached or error non-retryable.".format(
                        e.content, max_retries
                    )
                )
                return None  # Fail gracefully for captions
        except Exception as e:
            logging.error(f"An unexpected error occurred during caption upload: {e}")
            return None  # Fail gracefully

    # Should not be reached if logic is correct, but as a fallback:
    logging.error(f"Caption upload failed after {max_retries} retries.")
    return None


def get_video_folder_path(bucket_name, blob_name):
    """
    Get the folder path for the video files.

    Args:
        bucket_name: Name of the bucket
        blob_name: Name of the blob that triggered the event

    Returns:
        str: Folder path for the video files
    """
    # If the trigger was for a video file, get the folder it's in
    folder_path = "/".join(blob_name.split("/")[:-1])
    logging.info(f"Folder path: {folder_path}")
    return folder_path


def find_folder_files(storage_client, bucket_name, folder_path, expected_extensions):
    """
    Find files in a folder with specific extensions.

    Args:
        storage_client: Storage client
        bucket_name: Name of the bucket
        folder_path: Path to the folder
        expected_extensions: Dict mapping file types to extensions

    Returns:
        dict: Mapping of file types to blob names
    """
    # List all blobs in the folder
    blobs = storage_client.list_blobs(bucket_name, prefix=folder_path)

    # Find the necessary files
    found_files = {}
    for blob in blobs:
        for file_type, extension in expected_extensions.items():
            if blob.name.endswith(extension):
                found_files[file_type] = blob.name
                logging.info(f"Found {file_type}: {blob.name}")

    return found_files


def process_youtube_upload(cloud_event, channel_type):
    """
    Process YouTube upload for a specific channel type.

    Args:
        cloud_event: Cloud event that triggered the function
        channel_type: Type of channel ("daily" or "main")

    Returns:
        None
    """
    try:
        # Get the bucket and blob name from the cloud event
        bucket_name = cloud_event.data["bucket"]
        blob_name = cloud_event.data["name"]
        logging.info(
            f"Processing {channel_type} channel upload: gs://{bucket_name}/{blob_name}"
        )

        # Skip if this is not a video file
        if not blob_name.endswith(".mp4"):
            logging.info(f"Skipping non-video file: {blob_name}")
            return

        # Make sure it's in the correct folder
        prefix = f"processed-{channel_type}/"
        if not blob_name.startswith(prefix):
            logging.info(f"Skipping file not in {prefix} folder: {blob_name}")
            return

        # Get the folder path for the video files
        folder_path = get_video_folder_path(bucket_name, blob_name)

        # Create a storage client
        storage_client = storage.Client()

        # Find video, description, and subtitles files
        expected_extensions = {
            "video": "video.mp4",
            "description": "description.txt",
            "subtitles": "subtitles.vtt",
        }

        found_files = find_folder_files(
            storage_client, bucket_name, folder_path, expected_extensions
        )

        # Make sure we have the video file
        video_blob_name = found_files.get("video")
        if video_blob_name is None:
            logging.warning(f"No video file found in folder {folder_path}. Exiting.")
            # If the trigger was for a metadata file, another trigger might
            # come for the video.
            # If the trigger was for the video, but it's not found (??),
            # something is wrong.
            return  # Stop processing

        # We need at minimum a video file and description
        description_blob_name = found_files.get("description")
        if description_blob_name is None:
            logging.warning(
                f"No description file found in folder {folder_path}. Exiting."
            )
            return  # Stop processing

        # Get the description text
        description_text = read_blob_content(bucket_name, description_blob_name)

        # Parse the description - first line is the title, rest is description
        lines = description_text.strip().split("\n")
        video_title = lines[0]
        video_description = "\n".join(lines[1:]) if len(lines) > 1 else ""

        # Get YouTube credentials
        secret_prefix = f"youtube-{channel_type}"
        youtube_creds = {
            "client_id": get_secret(f"{secret_prefix}-client-id", PROJECT_ID),
            "client_secret": get_secret(f"{secret_prefix}-client-secret", PROJECT_ID),
            "refresh_token": get_secret(f"{secret_prefix}-refresh-token", PROJECT_ID),
        }

        credentials = get_youtube_credentials(youtube_creds)

        # Create a YouTube API client
        youtube = build("youtube", "v3", credentials=credentials)

        # Download the video file
        with tempfile.TemporaryDirectory() as temp_dir:
            video_path = os.path.join(temp_dir, "video.mp4")
            download_blob(bucket_name, video_blob_name, video_path)

            # Upload the video to YouTube
            upload_response = upload_video(
                youtube, video_path, video_title, video_description
            )

            # Get the video ID from the response
            if "id" in upload_response:
                video_id = upload_response["id"]
                logging.info(
                    "Uploaded video to YouTube {} channel. Video ID: {}".format(
                        channel_type, video_id
                    )
                )

                # Upload captions if available
                subtitles_blob_name = found_files.get("subtitles")
                if subtitles_blob_name:
                    subtitles_path = os.path.join(temp_dir, "subtitles.vtt")
                    download_blob(bucket_name, subtitles_blob_name, subtitles_path)

                    # Upload captions to YouTube
                    captions_response = upload_captions(
                        youtube, video_id, subtitles_path
                    )
                    logging.info(
                        "Uploaded captions to YouTube. Caption ID: {}".format(
                            captions_response.get("id")
                        )
                    )
                else:
                    logging.info("No subtitles file found. Skipping captions upload.")

                # Clean up temporary files
                if os.path.exists(video_path):
                    os.remove(video_path)
                if subtitles_blob_name and os.path.exists(subtitles_path):
                    os.remove(subtitles_path)
            else:
                logging.error(
                    "Could not get video ID from upload response. "
                    "Upload may have failed."
                )

    except Exception as e:
        logging.error(f"Error in upload_to_youtube_{channel_type}: {e}")
        traceback.print_exc()
        raise


@functions_framework.cloud_event
def upload_to_youtube_daily(cloud_event):
    """Triggers YouTube upload for the Daily channel based on GCS events."""
    process_youtube_upload(cloud_event, "daily")


@functions_framework.cloud_event
def upload_to_youtube_main(cloud_event):
    """Triggers YouTube upload for the Main channel based on GCS events."""
    process_youtube_upload(cloud_event, "main")

================
File: deploy.sh
================
#!/bin/bash
# Deployment script for the Video Processor application

# Exit on any error
set -e

# Configuration
PROJECT_ID="automations-457120"
REGION="us-east1"
SERVICE_NAME="video-processor"
ARTIFACT_REGISTRY="us-east1-docker.pkg.dev/${PROJECT_ID}/cloud-run-source-deploy"
IMAGE_NAME="${ARTIFACT_REGISTRY}/${SERVICE_NAME}"
IMAGE_TAG=$(date +%Y%m%d-%H%M%S)

# Colors for output
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Print with color
print_green() { echo -e "${GREEN}$1${NC}"; }
print_yellow() { echo -e "${YELLOW}$1${NC}"; }
print_red() { echo -e "${RED}$1${NC}"; }
print_blue() { echo -e "${BLUE}$1${NC}"; }

# Log file for deployment
LOG_DIR="logs"
LOG_FILE="${LOG_DIR}/deploy-$(date +%Y%m%d-%H%M%S).log"

# Create logs directory if it doesn't exist
mkdir -p ${LOG_DIR}

# Function to log messages to both console and log file
log() {
    echo "$(date +"%Y-%m-%d %H:%M:%S") - $1" | tee -a "${LOG_FILE}"
}

# Function to handle errors
handle_error() {
    print_red "\n====== ERROR ======"
    print_red "An error occurred during deployment at step: $1"
    print_red "Check the log file for details: ${LOG_FILE}"
    print_red "==================\n"
    exit 1
}

# Trap errors
trap 'handle_error "${BASH_COMMAND}"' ERR

# Parse command line arguments
DRY_RUN=false
SKIP_TESTS=false
VERBOSE=false

while [[ $# -gt 0 ]]; do
    case $1 in
        --dry-run)
            DRY_RUN=true
            shift
            ;;
        --skip-tests)
            SKIP_TESTS=true
            shift
            ;;
        --verbose)
            VERBOSE=true
            shift
            ;;
        *)
            print_red "Unknown option: $1"
            echo "Usage: $0 [--dry-run] [--skip-tests] [--verbose]"
            exit 1
            ;;
    esac
done

if [ "$DRY_RUN" = true ]; then
    print_blue "=== DRY RUN MODE - No actual deployment will occur ==="
fi

# Check if gcloud is installed
log "Checking if gcloud CLI is installed..."
if ! command -v gcloud &> /dev/null; then
    print_red "gcloud CLI is not installed. Please install it first."
    exit 1
fi
log "✓ gcloud CLI is installed"

# Check if docker is installed
log "Checking if Docker is installed..."
if ! command -v docker &> /dev/null; then
    print_red "Docker is not installed. Please install it first."
    exit 1
fi
log "✓ Docker is installed"

# Ensure we're authenticated to GCP
log "Checking GCP authentication..."
GCP_ACCOUNT=$(gcloud auth list --filter=status:ACTIVE --format="value(account)")
if [ -z "$GCP_ACCOUNT" ]; then
    print_red "Not authenticated to GCP. Please run 'gcloud auth login' first."
    exit 1
fi
log "✓ Authenticated to GCP as $GCP_ACCOUNT"

# Set the GCP project
log "Setting GCP project to ${PROJECT_ID}..."
gcloud config set project ${PROJECT_ID}
log "✓ GCP project set to ${PROJECT_ID}"

# Validate project configuration
log "Validating project configuration..."
PROJECT_NUMBER=$(gcloud projects describe ${PROJECT_ID} --format="value(projectNumber)")
if [ -z "$PROJECT_NUMBER" ]; then
    print_red "Failed to get project number. Check if the project ID is correct."
    exit 1
fi
log "✓ Project number: ${PROJECT_NUMBER}"

# Check if the service account exists
SA_EMAIL="${SERVICE_NAME}-sa@${PROJECT_ID}.iam.gserviceaccount.com"
log "Checking if service account ${SA_EMAIL} exists..."
if ! gcloud iam service-accounts describe ${SA_EMAIL} &> /dev/null; then
    log "Service account does not exist. Creating it..."
    if [ "$DRY_RUN" = false ]; then
        gcloud iam service-accounts create "${SERVICE_NAME}-sa" \
            --display-name="${SERVICE_NAME} Service Account"
        log "✓ Service account created"
    else
        log "[DRY RUN] Would create service account ${SA_EMAIL}"
    fi
else
    log "✓ Service account exists"
fi

# Run tests before deployment
if [ "$SKIP_TESTS" = false ]; then
    log "Running tests..."
    cd video_processor
    python -m pytest tests/test_youtube_uploader.py tests/test_generate_youtube_token.py tests/test_main.py -v | tee -a "../${LOG_FILE}" || {
        print_red "Tests failed. Aborting deployment."
        cd ..
        exit 1
    }
    cd ..
    log "✓ All tests passed"
else
    log "Skipping tests as requested"
fi

# Build and test Docker image locally before deployment
log "Building Docker image locally for testing..."
LOCAL_IMAGE_NAME="${SERVICE_NAME}-local:${IMAGE_TAG}"
if [ "$DRY_RUN" = false ]; then
    docker build -t ${LOCAL_IMAGE_NAME} . | tee -a "${LOG_FILE}"
    log "✓ Docker image built successfully"

    # Run the Docker image locally for a quick test
    log "Running Docker image locally for a quick test..."
    CONTAINER_ID=$(docker run -d -p 8080:8080 ${LOCAL_IMAGE_NAME})
    sleep 5  # Give the container time to start

    # Check if the container is running
    if docker ps -q --filter "id=${CONTAINER_ID}" | grep -q .; then
        log "✓ Container is running"

        # Send a simple request to check if the app is responding
        log "Sending a test request to the container..."
        if curl -s -o /dev/null -w "%{http_code}" http://localhost:8080/ | grep -q "200\|204\|404"; then
            log "✓ Container is responding to requests"
        else
            print_yellow "Warning: Container is not responding to requests. This might be expected if the app only accepts POST requests."
        fi

        # Stop and remove the container
        docker stop ${CONTAINER_ID} > /dev/null
        docker rm ${CONTAINER_ID} > /dev/null
        log "Stopped and removed test container"
    else
        print_red "Container failed to start. Check the Docker logs."
        docker logs ${CONTAINER_ID} | tee -a "${LOG_FILE}"
        exit 1
    fi
else
    log "[DRY RUN] Would build and test Docker image ${LOCAL_IMAGE_NAME}"
fi

# Deploy to Cloud Run
log "Deploying to Cloud Run from source..."
if [ "$DRY_RUN" = false ]; then
    # Use a deployment ID to track this deployment
    DEPLOY_ID=$(date +%Y%m%d-%H%M%S)
    log "Deployment ID: ${DEPLOY_ID}"

    # Deploy to Cloud Run
    gcloud run deploy ${SERVICE_NAME} \
        --source . \
        --platform managed \
        --region ${REGION} \
        --allow-unauthenticated \
        --memory 2Gi \
        --cpu 2 \
        --timeout 3600 \
        --concurrency 10 \
        --labels="deploy-id=${DEPLOY_ID}" | tee -a "${LOG_FILE}"

    log "✓ Deployment to Cloud Run completed"

    # Get the service URL
    SERVICE_URL=$(gcloud run services describe ${SERVICE_NAME} --region=${REGION} --format='value(status.url)')
    log "✓ Service URL: ${SERVICE_URL}"

    # Check if the service is responding
    log "Checking if the service is responding..."
    if curl -s -o /dev/null -w "%{http_code}" ${SERVICE_URL} | grep -q "200\|204\|404"; then
        log "✓ Service is responding to requests"
    else
        print_yellow "Warning: Service is not responding to requests. This might be expected if the app only accepts POST requests."
    fi
else
    log "[DRY RUN] Would deploy to Cloud Run with the following command:"
    log "gcloud run deploy ${SERVICE_NAME} --source . --platform managed --region ${REGION} ..."
fi

# Set up Eventarc trigger (if it doesn't exist)
log "Checking if Eventarc trigger exists..."
if ! gcloud eventarc triggers describe video-processor-trigger --location=${REGION} &> /dev/null; then
    log "Creating Eventarc trigger..."
    if [ "$DRY_RUN" = false ]; then
        gcloud eventarc triggers create video-processor-trigger \
            --location=${REGION} \
            --destination-run-service=${SERVICE_NAME} \
            --destination-run-region=${REGION} \
            --event-filters="type=google.cloud.storage.object.v1.finalized" \
            --event-filters="bucket=automations-youtube-videos-2025" \
            --service-account="${SA_EMAIL}" | tee -a "${LOG_FILE}"
        log "✓ Eventarc trigger created"
    else
        log "[DRY RUN] Would create Eventarc trigger"
    fi
else
    log "✓ Eventarc trigger already exists"
fi

# Check deployment logs
log "Fetching recent logs from Cloud Run service..."
if [ "$DRY_RUN" = false ]; then
    gcloud logging read "resource.type=cloud_run_revision AND resource.labels.service_name=${SERVICE_NAME}" \
        --limit=10 \
        --format="table(timestamp, severity, textPayload)" | tee -a "${LOG_FILE}"
else
    log "[DRY RUN] Would fetch logs from Cloud Run service"
fi

print_green "\n====== DEPLOYMENT SUMMARY ======"
print_green "✓ Deployment completed successfully!"
if [ "$DRY_RUN" = false ]; then
    print_green "✓ Service URL: ${SERVICE_URL}"
    print_green "✓ Log file: ${LOG_FILE}"
    print_green "To view logs in real-time, run:"
    print_blue "  gcloud logging read 'resource.type=cloud_run_revision AND resource.labels.service_name=${SERVICE_NAME}' --limit=50 --format='table(timestamp, severity, textPayload)' --follow"
else
    print_blue "This was a dry run. No actual deployment was performed."
fi
print_green "==============================\n"

================
File: Dockerfile
================
# Use an official Python runtime as the base image
FROM python:3.11

# Install ffmpeg system package and other tools
RUN apt-get update && \
    apt-get install -y --no-install-recommends ffmpeg && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /app

# Copy requirements and install dependencies
COPY requirements.txt .
RUN pip install --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt

# Copy the application code
COPY . .

# Expose port Cloud Run expects
EXPOSE 8080
ENV PORT=8080

# Command to run the main script using Gunicorn
CMD exec gunicorn --bind :$PORT --workers 1 --threads 8 --timeout 0 video_processor.main:app

================
File: Dockerfile.mock
================
# Use an official Python runtime as the base image
FROM python:3.11-slim

# Set working directory
WORKDIR /app

# Install Flask and other dependencies
RUN pip install --no-cache-dir flask requests

# Copy the mock service script
COPY scripts/mock_gcs_service.py /app/

# Expose port
EXPOSE 8081
ENV PORT=8081

# Command to run the mock service
CMD ["python", "mock_gcs_service.py"]

================
File: Makefile
================
lint:
	ruff check .

format:
	black .
	ruff check --fix .

test:
	pytest

check: format lint test

install-dev:
	pip install -r requirements.txt

.PHONY: lint format test check install-dev

================
File: pyproject.toml
================
# backend/pyproject.toml

[tool.black]
line-length = 88
# You can add other Black options here if needed:
# https://black.readthedocs.io/en/stable/usage_and_configuration/the_basics.html#configuration-via-a-file

[tool.ruff]
# Match Black's line length
line-length = 88
# Specify the Python version(s) you are targeting
# Adjust this based on your venv (seems like 3.12 or 3.13 from the structure)
target-version = "py312"

[tool.ruff.lint]
# Select the rules Ruff should enforce. This is a good starting set.
# E/W: pycodestyle errors/warnings
# F: Pyflakes
# I: isort (import sorting)
# C90: McCabe complexity
# B: flake8-bugbear (potential bugs)
# TID: flake8-tidy-imports (cleaner imports)
# For all rules, see: https://docs.astral.sh/ruff/rules/
select = ["E", "W", "F", "I", "C90", "B", "TID"]
ignore = [] # Add rule codes here if you need to disable specific ones

# Define directories/files Ruff should ignore
exclude = [
    ".bzr", ".direnv", ".eggs", ".git", ".hg", ".mypy_cache", ".nox",
    ".pants.d", ".ruff_cache", ".svn", ".tox", ".venv", "__pypackages__",
    "_build", "buck-out", "build", "dist", "node_modules", "venv",
    "tests/outdated", # Added based on your structure
    "test_data"      # Added based on your structure
]

[tool.ruff.lint.isort]
# Helps Ruff sort your project's imports correctly.
# List your main source directories here.
known-first-party = ["video_processor"] # Adjust if your main module is named differently

# If you wanted Ruff to format instead of Black, you'd uncomment and configure this:
# [tool.ruff.format]
# quote-style = "double"
# indent-style = "space"
# skip-magic-trailing-comma = false
# line-ending = "lf"

================
File: python_settings.txt
================
{
    // ... (keep all existing non-Python settings)

    "[python]": {
        "editor.defaultFormatter": "ms-python.black-formatter",
        "editor.formatOnSave": true,
        "editor.codeActionsOnSave": {
            "source.fixAll.ruff": "explicit",
            "source.organizeImports.ruff": "explicit"
        }
    },
    "python.linting.enabled": true,
    "python.linting.ruffEnabled": true,
    "python.linting.pylintEnabled": false,
    "python.linting.flake8Enabled": false,
    "python.formatting.provider": "none",
    
    // Additional Python settings
    "files.associations": {
        "*.py": "python",
        "pyproject.toml": "toml",
        "requirements.txt": "pip-requirements"
    },
    
    // Match Black/Ruff configuration
    "black-formatter.args": ["--line-length", "88"],
    "ruff.lint.args": ["--line-length", "88"],
    
    // Python-specific exclusions (already in files.watcherExclude)
    "python.analysis.exclude": [
        "**/venv/**",
        "**/.venv/**",
        "**/node_modules/**"
    ]
}

================
File: README.md
================
# Video Processor Backend

## Development Setup (Mac)

1. **Python Version Management with pyenv**:
   ```bash
   # Ensure you're using the correct Python version
   pyenv install 3.12.7  # If not already installed
   pyenv local 3.12.7    # Sets Python version for this directory
   ```

2. **Virtual Environment Setup**:
   ```bash
   # Create venv using pyenv's Python
   python -m venv venv
   source venv/bin/activate
   ```

3. **Install Dependencies**:
   ```bash
   make install-dev
   ```

## VS Code Python Interpreter Setup

When VS Code asks you to select a Python interpreter:

1. **For Mac Users**:
   - Select the interpreter that shows "(pyenv)" in its path
   - It should be `.../pyenv/versions/3.12.7/bin/python`
   - This ensures you're using the pyenv-managed Python version
   - DO NOT select system Python or other versions

2. **After pyenv selection**:
   - VS Code will automatically find the virtual environment in `backend/venv`
   - You'll see "venv" in the status bar once it's properly configured

## Development Tools

We use specific versions of development tools to ensure consistency:

1. **Python Version**:
   - Python 3.12.7 (managed via pyenv)
   - Used for all backend development

2. **Code Quality Tools**:
   - Black v24.1.1 (formatter)
   - Ruff v0.2.1 (linter)
   - pytest 7.4.0 (testing)

These versions are pinned in requirements.txt for consistency across the team.

### Available Commands

Use `make` commands to run code quality tools:

```bash
# Format code (Black + Ruff)
make format

# Run linter checks
make lint

# Run tests
make test

# Run all checks (format, lint, test)
make check
```

### Tool Configuration

- Black and Ruff configurations are in `pyproject.toml`
- VS Code settings are in `.vscode/settings.json`
- Extension recommendations are in `.vscode/extensions.json`

## Troubleshooting

If you encounter Python/VS Code issues:

1. **Wrong Python Version**:
   ```bash
   # Check your current Python version
   python --version  # Should show 3.12.7
   
   # If incorrect, ensure pyenv is set:
   pyenv local 3.12.7
   ```

2. **Virtual Environment Issues**:
   ```bash
   # Recreate the virtual environment if needed
   deactivate  # If already in a venv
   rm -rf venv
   python -m venv venv
   source venv/bin/activate
   pip install -r requirements.txt
   ```

3. **VS Code Integration**:
   - Command Palette → "Python: Select Interpreter"
   - Choose the pyenv version (3.12.7) from the list
   - Look for the path containing "pyenv/versions/3.12.7"
   - VS Code should then detect and use the venv automatically
   - Reload VS Code if settings aren't taking effect

================
File: requirements.txt
================
# Core dependencies
Flask
requests>=2.20.0
google-cloud-secret-manager>=2.16.0
google-cloud-storage==2.10.0
vertexai
ffmpeg-python
google-cloud-aiplatform
gunicorn

# YouTube API dependencies
google-api-python-client>=2.80.0
google-auth-oauthlib>=1.0.0
google-auth-httplib2>=0.1.0
python-dotenv>=1.0.0

# Cloud Functions dependencies
functions-framework

# Testing dependencies
pytest==7.4.0
pytest-mock==3.11.1
pytest-cov==4.1.0

# Data validation
pydantic>=2.0.0

# Development tools - versions pinned for consistency
black==24.1.1        # Code formatter
ruff==0.2.1         # Fast linter and import sorter
