This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.ai_docs/
  context/
    py.txt
  doing/
    tasks.md
  done/
    06-tasks-async.md
    active-context.md
    architecture-fe-integration.md
    overview.md
    product-context.md
    project-brief.md
    raw_reflection_log.md
    system-patterns.md
    tasks-backend-refactor.md
    tech-context.md
  prompts/
    example-progress.md
    new-new.md
    new.md
    prime.md
  templates/
    api_examples/
      adapters/
        storage/
          gcs.py
      application/
        interfaces/
          storage.py
      infrastructure/
        config/
          container.py
      main.py
    2-prd-template.md
    3-architecture-template.md
    4-system-patterns-template.md
    5-tasks-template.md
  web/
    error-handling.md
    websocket-messages.md
.clinerules/
  cline-continuous-improvement-protocol.md
  self-improving-cline.md
  uv-python-usage-guide.md
.cursor/
  rules/
    api-client-usage.mdc
    architecture.mdc
    cursor_rules.mdc
    fastapi.mdc
    memory_bank.mdc
    sb-create-database-functions.mdc
    sb-create-migration.mdc
    sb-create-rls-policies.mdc
    self_improve.mdc
.github/
  DISCUSSION_TEMPLATE/
    questions.yml
  workflows/
    require-pr-checklist.yml
  pull_request_template.md
.repomix/
  bundles.json
apps/
  core/
    .ai_docs/
      migration_verification_checklist.md
      progress.md
    alembic/
      versions/
        deb5e8c9c1fd_create_video_processing_tables.py
      README
      script.py.mako
    api/
      endpoints/
        __init__.py
        jobs_endpoints.py
        video_processing_endpoints.py
      schemas/
        user_schemas.py
        video_processing_schemas.py
      endpoints.py
    bin/
      clean_test_files.sh
      dev.sh
      format.sh
      lint.sh
      setup.sh
      test.sh
      typecheck.sh
    core/
      exceptions.py
    lib/
      ai/
        __init__.py
        ai_client_factory.py
        base_adapter.py
        gemini_adapter.py
      auth/
        supabase_auth.py
      cache/
        __init__.py
        redis_cache.py
        redis.py
      database/
        __init__.py
        connection.py
      messaging/
        __init__.py
        email.py
      publishing/
        publishing_interface.py
        youtube_adapter.py
      storage/
        __init__.py
        file_storage.py
      utils/
        ffmpeg_utils.py
        file_utils.py
        subtitle_utils.py
      __init__.py
    models/
      __init__.py
      chat_model.py
      enums.py
      user_model.py
      video_job_model.py
      video_metadata_model.py
      video_model.py
    operations/
      __init__.py
      chat_repository.py
      transaction_repo.py
      user_repository.py
      video_job_repository.py
      video_metadata_repository.py
      video_repository.py
    services/
      __init__.py
      ai_service.py
      auth_service.py
      chat_service.py
      job_service.py
      metadata_service.py
      user_service.py
      video_processing_service.py
    tests/
      api/
        __init__.py
        test_jobs_api.py
      integration/
        api/
          test_job_status_retrieval.py
          test_video_upload_flow.py
        conftest.py
        test_video_processing_api.py
      operations/
        __init__.py
        test_video_job_repository.py
      services/
        __init__.py
        test_job_service.py
      unit/
        lib/
          ai/
            __init__.py
            test_ai_client_factory.py
            test_gemini_adapter.py
          auth/
            __init__.py
            test_supabase_auth.py
          cache/
            __init__.py
            test_redis_cache.py
          publishing/
            test_youtube_adapter.py
          storage/
            __init__.py
            test_file_storage.py
          utils/
            __init__.py
            test_ffmpeg_utils.py
            test_file_utils.py
            test_subtitle_utils.py
        operations/
          __init__.py
          test_video_job_repository.py
          test_video_metadata_repository.py
          test_video_repository.py
        services/
          test_user_service.py
          test_video_processing_service.py
      conftest.py
      test_api.py
      test_architecture.py
    .gitignore
    .python-version
    alembic.ini
    main.py
    package.json
    pyproject.toml
    README.md
  web/
    public/
      vite.svg
    src/
      components/
        home/
          hero.tsx
        shared/
          container.tsx
          navbar.tsx
        ui/
          accordion.tsx
          alert-dialog.tsx
          alert.tsx
          aspect-ratio.tsx
          avatar.tsx
          badge.tsx
          breadcrumb.tsx
          button.tsx
          calendar.tsx
          card.tsx
          carousel.tsx
          chart.tsx
          checkbox.tsx
          collapsible.tsx
          combo-box.tsx
          command.tsx
          context-menu.tsx
          date-picker.tsx
          dialog.tsx
          drawer.tsx
          dropdown-menu.tsx
          dropzone.test.tsx
          dropzone.tsx
          form.tsx
          hover-card.tsx
          input-otp.tsx
          input.tsx
          label.tsx
          menubar.tsx
          navigation-menu.tsx
          pagination.tsx
          popover.tsx
          progress-steps.tsx
          progress.tsx
          radio-group.tsx
          resizable.tsx
          scroll-area.tsx
          select.tsx
          separator.tsx
          sheet.tsx
          sidebar.tsx
          skeleton.tsx
          slider.tsx
          sonner.tsx
          switch.tsx
          table.tsx
          tabs.tsx
          textarea.tsx
          toast.tsx
          toaster.tsx
          toggle-group.tsx
          toggle.tsx
          tooltip.tsx
          use-toast.tsx
        video/
          content-editor.tsx
          processing-dashboard.tsx
          processing-steps.ts
          thumbnail-gallery.tsx
          title-selector.tsx
          video-detail.tsx
          video-progress-card.tsx
          VideoList.tsx
          VideoListItem.tsx
          VideoUploadDropzone.tsx
        auth.tsx
        default-catch-boundary.tsx
        GoogleLoginButton.tsx
        login.tsx
        not-found.tsx
        post-error.tsx
        ProtectedLayout.tsx
        user-error.tsx
      hooks/
        use-mobile.ts
        useAppWebSocket.ts
        useAuth.ts
        useJobStatus.ts
        useJobStatusManager.ts
        useMutation.ts
      lib/
        api.ts
        utils.ts
      routes/
        _authed/
          video/
            $videoId.tsx
        auth/
          callback.tsx
        __root.tsx
        _authed.jobs.[jobId].tsx
        _authed.jobs.$jobId.tsx
        _authed.tsx
        _pathlessLayout.tsx
        dashboard.e2e.test.tsx
        dashboard.tsx
        index.tsx
        login.tsx
        logout.tsx
        settings.tsx
        signup.tsx
      services/
        gcs-content.ts
      styles/
        app.css
      types/
        api.ts
      utils/
        loggingMiddleware.ts
        seo.ts
        supabase.ts
        users.tsx
      api.ts
      client.tsx
      global-middleware.ts
      router.tsx
      routeTree.gen.ts
      ssr.tsx
    .env.example
    .gitattributes
    .gitignore
    biome.json
    components.json
    Dockerfile
    package.json
    postcss.config.mjs
    README.md
    tsconfig.json
    vite.config.ts
    vitest.config.ts
supabase/
  .temp/
    cli-latest
  clients/
    client.ts
    server.ts
  migrations/
    20250514044259_create_videos_table.sql
  mutations/
    index.ts
  supabase/
    .branches/
      _current_branch
    .temp/
      cli-latest
  index.ts
  package.json
.cursorignore
.dockerignore
.gcloudignore
.gitignore
.python-version
.repomixignore
echo.code-workspace
package.json
pnpm-workspace.yaml
README.md
repomix.config.json
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".ai_docs/doing/tasks.md">
Frontend Cleanup & Refactoring Task List
Phase 1: Dead Code Removal & Initial Cleanup
1.1. Remove "Users" Feature Example Code:
* [ ] Delete file: apps/web/src/routes/users.route.tsx
* [ ] Delete file: apps/web/src/routes/api/users.$id.ts
* [ ] Delete file: apps/web/src/routes/api/users.ts
* [ ] Delete file: apps/web/src/utils/users.tsx
* [ ] Delete example files from .ai_docs/web/examples/:
* users.$userId.tsx
* users.index.tsx
* [ ] Update apps/web/src/routeTree.gen.ts: Run the TanStack Router codegen command (usually part of pnpm dev or a specific script like pnpm generate:routes) to remove references to these deleted routes. If no specific script, restarting the dev server might trigger it.
* [ ] Review apps/web/src/routes/__root.tsx and remove any <Link> components or navigation items pointing to /users.
1.2. Remove Firebase Remnants:
* [ ] Delete file: apps/web/firebase.ts (if it still exists).
* [ ] Refactor apps/web/src/components/video/processing-dashboard.tsx:
* Remove firebase/firestore imports.
* Replace Firestore onSnapshot logic with useQuery to call getProcessingJobs from apps/web/src/lib/api.ts.
* Integrate WebSocket updates via useJobStatusManager to update the TanStack Query cache for processing jobs.
* [ ] Delete file: apps/web/src/routes/backup-video.$videoId.txt (as per 1.6, but good to confirm its Firebase usage is now fully superseded by the _authed version).
* [ ] Search the entire apps/web/src directory for any remaining imports from firebase or firebase/firestore and remove/refactor the associated code.
* [ ] Remove Firebase SDK from apps/web/package.json dependencies.
* [ ] Run pnpm install in apps/web to update pnpm-lock.yaml.
1.3. Remove Pathless/Nested Layouts Example Code:
* [ ] Delete directory: apps/web/src/routes/_pathlessLayout/ (and its contents).
* [ ] Delete file: apps/web/src/routes/_pathlessLayout.tsx
* [ ] Update apps/web/src/routeTree.gen.ts (run codegen/restart dev server).
* [ ] Review apps/web/src/routes/__root.tsx and remove any <Link> components pointing to routes previously under _pathlessLayout.
1.4. Remove Other Example/Placeholder Routes:
* [ ] Delete file: apps/web/src/routes/deferred.tsx
* [ ] Delete file: apps/web/src/routes/redirect.tsx
* [ ] Update apps/web/src/routeTree.gen.ts (run codegen/restart dev server).
* [ ] Review apps/web/src/routes/__root.tsx and remove any <Link> components pointing to these routes.
1.5. Consolidate Supabase Client Initialization:
* [ ] Review apps/web/src/utils/supabase.ts. If its getSupabaseServerClient is only for server-side functions within apps/web (e.g., TanStack Start server functions) and is distinct from supabase/clients/server.ts, document its specific purpose.
* [ ] Ensure all client-side Supabase interactions (auth, etc.) in apps/web consistently import and use supabase from @echo/db/clients/client.
* [ ] If apps/web/src/utils/supabase.ts is redundant with @echo/db/clients/server.ts or not used, delete it and update any imports.
1.6. Standardize Toast Component Usage:
* [ ] Confirm sonner is the chosen toast system (as used in login.tsx).
* [ ] Delete apps/web/src/components/ui/toaster.tsx.
* [ ] Delete apps/web/src/components/ui/use-toast.tsx.
* [ ] Search for any usages of useToast (from shadcn/ui) and refactor them to use toast from sonner.
* [ ] Ensure Toaster from sonner (likely via apps/web/src/components/ui/sonner.tsx) is included in the root layout (apps/web/src/routes/__root.tsx) to render toasts.
Phase 2: Bug Fixes & Refinements
2.1. Address Route Definitions & Linter Issues:
* [ ] Investigate apps/web/src/routes/_authed.jobs.$jobId.tsx and apps/web/src/routes/_authed.jobs.[jobId].tsx.
* Determine the correct TanStack Router dynamic segment convention being used project-wide (likely $paramName).
* Delete the incorrectly named/redundant file.
* [ ] Verify the path argument in createFileRoute for the remaining job detail route (e.g., /_authed/jobs/$jobId).
* [ ] Verify the path argument in createFileRoute for apps/web/src/routes/_authed/video/$videoId.tsx.
* [ ] Update apps/web/src/routeTree.gen.ts (run codegen/restart dev server) after any changes.
* [ ] Resolve any persistent linter errors related to createFileRoute.
2.2. Refine Authentication Callback Logic:
* [ ] Review apps/web/src/routes/auth/callback.tsx.
* [ ] In the useEffect hook, ensure the redirect to /login (in the else block for !session && !authError) only occurs if isInitialized is true to prevent premature redirects.
* [ ] Consider adding a user-facing message like "Finalizing login..." or "Redirecting..." if there's a noticeable delay before navigation.
2.3. Correct API Client (apps/web/src/lib/api.ts):
* [ ] In getProcessingJobs function, change the endpoint URL from ${API_BASE_URL}${API_V1_PREFIX}/users/me/jobs to ${API_BASE_URL}${API_V1_PREFIX}/jobs to match the backend jobs_endpoints.py.
* [ ] (Optional Enhancement) Modify handleApiResponse to throw more specific, custom error classes (e.g., ApiAuthenticationError, ApiNotFoundError, ApiValidationError) based on HTTP status codes (401, 404, 400/422 respectively). This allows for more targeted catch blocks in calling code.
2.4. Solidify WebSocket Hooks:
* apps/web/src/hooks/useAppWebSocket.ts:
* [ ] Verify VITE_WS_BASE_URL in .env.example and local .env files is correctly defined (e.g., ws://localhost:8000 for local dev, wss://your-prod-domain.com for prod).
* [ ] Confirm the backend WebSocket endpoint correctly handles token authentication from the ?token= query parameter.
* [ ] (Optional Enhancement) Evaluate if the reconnect logic needs to be more robust (e.g., infinite retries with a max backoff delay, or user-initiated reconnect button after several failures).
* apps/web/src/hooks/useJobStatusManager.ts:
* [ ] Complete any // ... existing code ... sections.
* [ ] Ensure the WebSocketJobUpdate type accurately reflects all possible fields and their optionality from backend WebSocket messages.
* [ ] In handleTypedWebSocketMessage, when updating myVideosQueryKey, ensure that if a job update message only contains job_id but not video_id, the list update logic doesn't break or miss updates. The backend should ideally always send video_id if the update is relevant to a specific video.
* [ ] Refine the useEffect for lastJsonMessage:
typescript // Before // if (typeof lastJsonMessage === 'object' && lastJsonMessage !== null && ('job_id' in lastJsonMessage || 'video_id' in lastJsonMessage)) { // After (more strict, assuming job_id is key) if (typeof lastJsonMessage === 'object' && lastJsonMessage !== null && 'job_id' in lastJsonMessage) { const updateData = lastJsonMessage as WebSocketJobUpdate; handleTypedWebSocketMessage(updateData); } else { console.warn("JobStatusManager: Received WebSocket message of unexpected shape or missing job_id:", lastJsonMessage); }
2.5. Ensure Correct Video Playback URL Handling:
* [ ] In apps/web/src/routes/_authed/video/$videoId.tsx:
* Confirm with the backend team that VideoDetailsResponse (or its nested video object) will include a dedicated playback_url field containing a directly playable HTTP(S) URL.
* Update the frontend code to exclusively use this playback_url for the MediaPlayer's src prop.
* Remove any fallback logic that attempts to use videoDetails.video?.storage_path for playback.
Phase 3: Testing & Verification
Manual Testing: After completing phases 1 and 2, perform thorough manual testing of all affected flows:
Login with Google, logout.
Accessing protected routes (dashboard, video detail, job detail) when unauthenticated (should redirect to login).
Video upload process.
Real-time updates on the dashboard for processing jobs.
Real-time updates on the video detail page.
Real-time updates on the job detail page.
Metadata editing and saving.
Video playback on the detail page (once backend provides playback_url).
Automated Testing:
Write/update unit tests for modified hooks and utility functions.
Write/update integration tests for key components like VideoUploadDropzone, ProcessingDashboard, VideoDetailPage.
Ensure existing E2E tests pass or update them as needed.
</file>

<file path=".ai_docs/done/06-tasks-async.md">
# Frontend Integration Plan: `apps/web` with Backend & Supabase

---
**Status Update (As of 2025-05-18):**
*   **API Client Service (`apps/web/src/lib/api.ts`):**
    *   [X] Updated with new backend REST endpoints (`getSignedUploadUrl`, `notifyUploadComplete`, `getMyVideos`, `getVideoDetails`, `updateVideoMetadata`, `getJobDetails`).
    *   [X] Created `apps/web/src/types/api.ts` for all API client TypeScript types.
    *   [X] Implemented a generic `handleApiResponse` for consistent error handling.
*   **Video Upload Feature (`apps/web/src/components/video/VideoUploadDropzone.tsx`):**
    *   [X] Modified to implement the 3-step direct-to-cloud upload using the new API client functions.
    *   [X] Type `UploadCompleteRequest` in `types/api.ts` updated to make `storagePath` optional.
*   **Real-time Job Status Hooks (`apps/web/src/hooks/`):**
    *   [X] Created `useAppWebSocket.ts` to manage WebSocket connection for job status updates.
    *   [X] Created `useJobStatus.ts` to consume WebSocket updates (via `useAppWebSocket`) and update TanStack Query cache.
    *   [X] Addressed module resolution for `@echo/db` by adding `exports` to `supabase/package.json`.
*   **Workspace Configuration:** Previous fixes for pnpm workspace setup, TypeScript path aliases for `@echo/db`, and unified monorepo scripts remain in place, unblocking further development.

---

**Project Goal:** Integrate the `apps/web` frontend with the `apps/core` backend API and Supabase for authentication, video upload (direct-to-cloud), real-time processing status tracking (via WebSockets), and video management. Ensure a seamless, responsive user experience and robust error handling.

---

## Phase 1: Core Setup & Authentication Foundation

1.  **Environment Configuration (`apps/web` and `apps/core`)**
    *   [X] Task 1.1: **Standardize and Consolidate Environment Files**
        *   **File(s) Modified:**
            *   `apps/web/.env.development` (Created/Updated with local dev values)
            *   `apps/web/.env.production` (Created/Updated with production placeholders)
            *   `apps/web/.env.example` (Created with placeholders)
            *   `apps/web/.gitignore` (Updated to correctly ignore/include .env files)
            *   `apps/core/.env.development` (Created with local dev values)
            *   `apps/core/.env.production` (Created with production values)
            *   `apps/core/.env.example` (Reviewed and confirmed)
            *   Deleted `apps/web/.env`, `apps/core/.env`, and root `.env` after consolidation.
        *   **Key Actions:**
            *   Consolidated various `.env` files into a standardized structure: `.env.development`, `.env.production`, and `.env.example` for both `apps/web` and `apps/core`.
            *   Ensured `apps/web/.env.development` and `apps/web/.env.production` contain `VITE_SUPABASE_URL`, `VITE_SUPABASE_ANON_KEY`, and `VITE_API_BASE_URL`.
            *   Verified `apps/core` environment files are structured for development and production, separating local and hosted service configurations.
        *   **Acceptance Criteria:** Environment variables are correctly structured for different environments (development, production) for both frontend and backend. Frontend variables are accessible via `import.meta.env.*`. `.gitignore` handles these files appropriately.

2.  **Supabase Client & Authentication Hooks (`apps/web/src` & `supabase/`)**
    *   [X] Task 1.2: **Standardize Supabase Client and Authentication Logic**
        *   **File(s) to Check/Modify:**
            *   `supabase/clients/client.ts`: Review this shared client. Confirm its configuration is suitable for client-side usage in `apps/web`. (Verified)
            *   `apps/web/src/utils/supabase.ts`: Currently contains `getSupabaseServerClient`. Evaluate if a client-side utility/hook here should wrap `supabase/clients/client.ts` or if direct import is preferred. (Direct import of `supabase/clients/client.ts` or its alias `@echo/db/clients/client` is preferred for client-side usage; `utils/supabase.ts` remains for SSR client).
            *   `apps/web/src/hooks/useAuth.ts` (Create, or verify and refactor if similar logic exists elsewhere). (Verified existing hook, meets requirements).
            *   `apps/web/src/router.tsx` or `apps/web/src/client.tsx` or `apps/web/src/routes/__root.tsx`: Ensure any global AuthProvider wraps the application. (Current design relies on direct `useAuth()` usage; no separate global provider identified as strictly necessary for the hook's function, but can be added if needed for other reasons).
        *   **Key Actions:**
            *   **Client-Side Supabase Access:** Establish a clear and consistent method for accessing the Supabase client instance throughout the frontend (likely importing from `supabase/clients/client.ts`). (Achieved via `@echo/db/clients/client` alias in `useAuth.ts`).
            *   **Develop `useAuth.ts` Hook:**
                *   Provide reactive state: `user`, `session`, `isLoading`, `error`. (Implemented).
                *   Expose authentication methods: `loginWithPassword(email, password)`, `signUpWithEmailPassword(email, password)`, `signOut()`, `signInWithGoogle()`. (Implemented).
                *   These methods should internally call the Supabase client (e.g., `supabase.auth.signInWithPassword()`). (Implemented).
                *   Subscribe to `supabase.auth.onAuthStateChange` to update and reflect auth state changes application-wide. (Implemented).
        *   **Acceptance Criteria:** A `useAuth()` hook is available, providing reactive authentication state (user, session, loading status) and all necessary authentication functions. The Supabase client configuration is verified. (Met)

3.  **API Client Service for Backend Communication (`apps/web/src/lib`)**
    *   [X] Task 1.3: **Develop a Typed API Client Service for REST Endpoints**
        *   **File(s) to Check/Modify:**
            *   `apps/web/src/lib/api.ts`: Review and enhance this existing file to serve as the central API client.
            *   `apps/web/src/types/api.ts` (Create, or identify existing location for shared types).
        *   **Key Actions:**
            *   In `apps/web/src/lib/api.ts`:
                *   Implement a base `fetch` wrapper or utilize `axios` (if already a project dependency and preferred for features like interceptors).
                *   Integrate dynamic fetching of the Supabase JWT: `const token = (await supabase.auth.getSession())?.data.session?.access_token;`.
                *   Automatically include the `Authorization: Bearer ${token}` header in requests to protected backend endpoints.
                *   Default `Content-Type: application/json` for `POST`/`PUT` requests.
                *   Construct full request URLs using `VITE_API_BASE_URL`.
                *   Implement robust error handling: parse JSON error responses from the backend, handle network errors, and specific HTTP status codes (401, 403, 404, 500).
            *   In `apps/web/src/types/api.ts`:
                *   Define TypeScript interfaces for all backend API request bodies and response payloads relevant to video processing (e.g., `VideoUploadResponseSchema`, `VideoJobSchema`, `VideoSchema`, `VideoMetadataSchema` as defined in `apps/core/api/schemas/video_processing_schemas.py`).
        *   **Acceptance Criteria:** The `apps/web/src/lib/api.ts` module provides strongly-typed functions for all backend video processing interactions, with (manual JWT handling for now - to be reviewed in Task 1.3 if auto-injection is needed) and comprehensive error management.

4.  **Type Sharing/Generation Strategy**
    *   [X] Task 1.4: **Investigate and Implement Type Sharing/Generation**
        *   **Key Actions:** Decide on a strategy to keep frontend TypeScript types (`apps/web/src/types/api.ts`) synchronized with backend Pydantic models. Options:
            *   Manual synchronization (prone to error).
            *   Codegen tool (e.g., `pydantic-to-typescript`). (Chosen and Implemented)
            *   Shared monorepo package (if feasible for Pydantic models & TS types).
        *   Implement the chosen strategy. (Implemented via `pnpm run generate:api-types` script using `pydantic2ts` and `json-schema-to-typescript`).
        *   **Acceptance Criteria:** A maintainable process for type consistency between frontend and backend is established. (Met: The script `apps/web/package.json#generate:api-types` provides this process. It converts Pydantic models from `apps/core/api/schemas/video_processing_schemas.py` to TypeScript interfaces in `apps/web/src/types/api.ts`.)

## Phase 2: User Authentication Flow Implementation

5.  **Login Page and Components (`apps/web/src`)**
    *   [X] Task 2.1: **Verify and Enhance Login Functionality**
        *   **File(s) to Check/Modify:**
            *   `apps/web/src/routes/login.tsx` (Verified route setup. Added `beforeLoad` to redirect authenticated users to `/dashboard`.)
            *   `apps/web/src/components/login.tsx` (Verified form component. Uses `useAuth`, `react-hook-form`, `zod`, `shadcn/ui`. Handles loading/error states, navigation, and link to signup.)
            *   `apps/web/src/components/GoogleLoginButton.tsx` (Verified. Uses `useAuth`, handles loading state for Google OAuth.)
        *   **Key Actions:**
            *   Ensure the login form component (e.g., `login.tsx`) correctly calls `useAuth().loginWithPassword()` upon submission. (Met)
            *   Verify `GoogleLoginButton.tsx` correctly calls `useAuth().signInWithGoogle()`. (Met)
            *   Integrate with `shadcn/ui Form` components and a library like `react-hook-form` with `zod` for validation, if consistent with project patterns. (Met)
            *   Clearly display loading states (e.g., button disabled, spinner) and error messages sourced from the `useAuth()` hook. (Met)
            *   Confirm successful login navigates to the `/dashboard` route (TanStack Router's `navigate` function). (Met)
            *   Ensure "Sign up" link navigates to the signup page. (Met)
        *   **Acceptance Criteria:** Users can successfully log in using email/password and Google OAuth. Form validation, loading states, error display, and redirection are correctly implemented. (Met)

6.  **Signup Page and Components (`apps/web/src`)**
    *   [X] Task 2.2: **Verify and Enhance Signup Functionality**
        *   **File(s) to Check/Modify:**
            *   `apps/web/src/routes/signup.tsx` (Verified route setup. Signup form component is within this file. Added `beforeLoad` to redirect authenticated users to `/dashboard`.)
            *   A dedicated signup form component e.g., `apps/web/src/components/auth/SignupForm.tsx` (Not separate, component is in `routes/signup.tsx`. This is acceptable.)
        *   **Key Actions:**
            *   Ensure the signup form component calls `useAuth().signUpWithEmailPassword()` on submission. (Met)
            *   Utilize `shadcn/ui Form` components for structure and validation (e.g. password confirmation). (Met)
            *   Display loading states and relevant error messages (e.g., "Email already in use", "Passwords do not match"). (Met)
            *   Provide clear user feedback upon successful signup (e.g., "Please check your email to confirm your account."). (Met)
            *   Ensure "Login" link navigates to the login page. (Met)
        *   **Acceptance Criteria:** Users can create new accounts. Form validation, loading/error states, and user feedback mechanisms are robust. (Met)

7.  **Authentication Callback Handling (`apps/web/src`)**
    *   [X] Task 2.3: **Ensure OAuth Callback Route Works Correctly**
        *   **File(s) to Check/Modify:** `apps/web/src/routes/auth/callback.tsx` (Verified existing functionality. The component correctly uses `useAuth` to handle session establishment, displays loading/error states, and redirects appropriately.)
        *   **Key Actions:**
            *   Confirm that this route correctly handles the session establishment after OAuth providers (like Google) redirect back to the application. (Met: Relies on `useAuth` and Supabase client's `onAuthStateChange`.)
            *   Verify it redirects users to the appropriate page (e.g., `/dashboard`) post-authentication. (Met)
            *   Implement a user-friendly loading message during processing. (Met)
            *   Ensure any potential errors during the callback (e.g., state mismatch, provider error) are gracefully handled and displayed to the user. (Met)
        *   **Acceptance Criteria:** OAuth callbacks (Google login, email confirmation link) are processed smoothly, sessions are established, and users are redirected correctly. Error scenarios are handled. (Verified as Met)

8.  **Protected Route Implementation (`apps/web/src`)**
    *   [X] Task 2.4: **Verify and Solidify Protected Route Mechanism**
        *   **File(s) to Check/Modify:**
            *   `apps/web/src/routes/_authed.tsx` (Reviewed and refactored to use client-side session check with `supabase.auth.getSession()` in `beforeLoad`, redirecting to `/login` if unauthenticated. Added loading state and Outlet.)
            *   `apps/web/src/components/ProtectedLayout.tsx` (Reviewed. Contains older, redundant server-side auth check. Not directly used by the new `_authed.tsx` logic. Its direct usage should be phased out or refactored if its layout structure is needed.)
            *   Any route intended to be private, ensuring it's nested under or uses the `_authed.tsx` layout (e.g., `apps/web/src/routes/dashboard.tsx`). (Mechanism in place via `_authed.tsx` for routes like `_authed/dashboard.tsx` or `_authed.dashboard.tsx`)
        *   **Key Actions:**
            *   Confirm that `_authed.tsx` effectively uses `supabase.auth.getSession()` (or `useAuth()` if applicable in `beforeLoad`) to check authentication. (Implemented using `supabase.auth.getSession()` in `beforeLoad`.)
            *   Verify that unauthenticated users attempting to access protected routes are redirected to `/login`. (Implemented via `redirect` in `beforeLoad`.)
            *   Test that authenticated users can access these routes without issue. (Mechanism in place.)
            *   Consider displaying a global loading indicator or skeleton layout while authentication status is being initially determined. (Implemented `pendingComponent` in `_authed.tsx`.)
        *   **Acceptance Criteria:** All routes designated as protected are inaccessible to unauthenticated users, with proper redirection to the login page. Authenticated users have seamless access. (Met)

## Phase 3: Video Upload & Real-Time Processing Flow (Direct-to-Cloud & WebSockets)

9.  **Direct-to-Cloud Video Upload Component (`apps/web/src`)**
    *   [X] Task 3.1: **Implement Direct-to-Cloud Upload in `VideoUploadDropzone.tsx`**
        *   **File(s) to Check/Modify:** `apps/web/src/components/video/VideoUploadDropzone.tsx`, `apps/web/src/lib/api.ts`.
        *   **Backend Prerequisite Notes:** Endpoints `POST /api/v1/videos/signed-upload-url` and `POST /api/v1/videos/upload-complete` must be available.
        *   **Key Actions:**
            1.  On file selection, call `api.getSignedUploadUrl()` from `lib/api.ts` with filename and content type. (Verified, uses `content_type`)
            2.  Receive `upload_url` and `video_id` (or correlation ID). (Verified, destructured to camelCase `uploadUrl`, `videoId` for internal use)
            3.  Perform a direct `PUT` request to the `uploadUrl` (GCS) with the file content. Implement progress display using XHR/fetch `ReadableStream` if possible, or a simpler "uploading..." state. (Verified, uses XHR with progress)
            4.  On successful upload to GCS, call `api.notifyUploadComplete()` with `video_id`, `original_filename`, `content_type`, and `size_bytes`. (Verified, uses snake_case for request object)
            5.  Handle errors from all three steps (getting URL, GCS upload, notifying completion) using `Sonner` toasts or `Alert`. (Implemented using `toast.error()`)
            6.  On final success, navigate to job status page or update UI. (Handled via `onUploadComplete` prop callback)
        *   **Additional Steps Taken:**
            *   Refactored `lib/api.ts` to use Supabase JWT for authorization in API calls instead of `credentials: "include"`.
            *   Added missing Pydantic models to `apps/core/api/schemas/video_processing_schemas.py` for types required by `lib/api.ts` and `VideoUploadDropzone.tsx` (e.g., `SignedUploadUrlRequest`, `SignedUploadUrlResponse`, `UploadCompleteRequest`, `ApiErrorResponse`, `VideoSummary`, `VideoDetailsResponse`, `VideoMetadataUpdateRequest`).
            *   Regenerated TypeScript types in `apps/web/src/types/api.ts` using `pnpm run generate:api-types`.
            *   Updated `VideoUploadDropzone.tsx` to use `snake_case` properties when interacting with these generated types to resolve linter errors.
        *   **Acceptance Criteria:** Users can upload videos directly to cloud storage. UI shows progress/status. Backend is correctly notified to start processing. Errors are handled gracefully. (Met)

10. **WebSocket Client for Real-Time Updates (`apps/web/src`)**
    *   [X] Task 3.2: **Implement WebSocket Hook (`useAppWebSocket.ts`)**
        *   **File(s) to Create:** `apps/web/src/hooks/useAppWebSocket.ts`. (Created)
        *   **Backend Prerequisite Note:** WebSocket endpoint (e.g., `WS /ws/jobs/status/{user_id}`) must be available.
        *   **Key Actions:**
            *   Establish WebSocket connection upon user login using JWT and user ID from `useAuth()`. (Implemented)
            *   Handle connection lifecycle (connect, disconnect, errors, basic reconnect attempts). (Implemented)
            *   Provide methods to send messages (`sendJsonMessage`) and expose received messages (`lastJsonMessage`) reactively. (Implemented)
            *   Expose connection status (`connectionStatus`, `isConnected`) reactively. (Implemented)
        *   **Acceptance Criteria:** A reusable hook manages WebSocket connectivity and message flow. (Met)

11. **Job Status Display with WebSockets & Polling Fallback (`apps/web/src`)**
    *   [X] Task 3.3: **Implement Job Status Management Hook (`useJobStatusManager.ts`) with TanStack Cache Updates**
        *   **File(s) to Check/Modify:** 
            *   `apps/web/src/hooks/useJobStatusManager.ts` (Created/Refactored from `useJobStatus.ts`).
            *   `apps/web/src/routes/_authed.jobs.$jobId.tsx` (Created for UI display. Fetches job details, displays status, video info, timestamps, errors. Handles loading/error states. Linter error for `createFileRoute` path argument persists and needs further investigation if it doesn't resolve on its own.)
        *   **Backend Prerequisite Notes:** `api.getJobDetails(jobId)` endpoint for initial fetch and polling fallback if WS fails.
        *   **Key Actions:**
            *   In `hooks/useJobStatusManager.ts`:
                *   Does not accept `jobId` directly; manages user-level job updates.
                *   Uses `useAppWebSocket.ts` to listen for messages for the authenticated user.
                *   On receiving a WebSocket update (`lastJsonMessage`), attempts to parse it as `WebSocketJobUpdate` (locally defined type `Partial<VideoJobSchema> & { job_id: number; video_id?: number }`).
                *   Updates `TanStack Query` cache for specific jobs (`['jobDetails', String(job_id)]`) and video lists (`['myVideos']`) using `queryClient.setQueryData`.
            *   (Polling fallback via `refetchInterval` in `useQuery` for individual job pages is not yet implemented as part of this hook, but could be added to the page component that uses `useQuery(['jobDetails', jobId], ...)`).
            *   Created the page `apps/web/src/routes/_authed.jobs.$jobId.tsx` to display job details. It uses `useQuery` with `getJobDetails` and will benefit from cache updates by `useJobStatusManager.ts`.
        *   **Acceptance Criteria:** `useJobStatusManager.ts` hook correctly processes WebSocket messages and updates TanStack Query cache for relevant job and video list data. (Met for WS part; polling and specific job page UI are separate concerns). Job details page displays information correctly. (Partially Met: Page created, but route definition linter error needs resolution).

*   **NEW Backend Dependency for Processing Dashboard:**
    *   [X] Task 3.X: Implement API endpoint for `getProcessingJobs()`
        *   **Description:** Create a backend API endpoint (e.g., `GET /api/v1/jobs/?status=processing&status=pending`) that returns a list of video jobs currently in a non-terminal state (e.g., PENDING, PROCESSING) for the authenticated user.
        *   The response should be an array of objects conforming to `VideoJobSchema`.
        *   [X] Ensure the corresponding client-side function `getProcessingJobs()` is added to `apps/web/src/lib/api.ts`. (Done)
        *   **Reason:** Required by the refactored `ProcessingDashboard.tsx` to display in-progress video jobs.
        *   **Note:** Backend implementation uses synchronous database calls. A follow-up task (3.Y) addresses upgrading to asynchronous operations.
        *   **Sub-Tasks:**
            *   [X] **Task 3.X.1: Define API Route and Request/Response Schemas**
                *   **File(s) Modified:**
                    *   `apps/core/api/endpoints/jobs_endpoints.py` (Created).
                    *   `apps/core/api/schemas/video_processing_schemas.py` (Verified `VideoJobSchema` and `ProcessingStatus` enum).
                    *   `apps/core/main.py` (Included `jobs_api_router`).
                *   **Actions:**
                    *   Defined GET route `/api/v1/jobs/` in `jobs_endpoints.py`.
                    *   Accepts `status: Optional[List[ProcessingStatus]]` query parameter.
                    *   Response model `List[VideoJobSchema]`.
                    *   Protected with `Depends(get_current_user)` returning `AuthenticatedUser`.
                    *   Uses synchronous `db: Session = Depends(get_db_session)` for now.
            *   [X] **Task 3.X.2: Implement Service Layer Logic**
                *   **File(s) to Modify/Create:**
                    *   `apps/core/services/job_service.py` (Created).
                *   **Actions:**
                    *   Created `async def get_user_jobs_by_statuses(user_id: str, db: Session, statuses: Optional[List[ProcessingStatus]]) -> List[VideoJobModel]:`.
                    *   Defaults to `[ProcessingStatus.PENDING, ProcessingStatus.PROCESSING]` if `statuses` is not provided.
                    *   Calls `VideoJobRepository.get_by_user_id_and_statuses`.
            *   [X] **Task 3.X.3: Implement Database Query for Jobs**
                *   **File(s) to Modify/Create:**
                    *   `apps/core/operations/video_job_repository.py` (Modified).
                *   **Actions:**
                    *   Added `get_by_user_id_and_statuses` static method to `VideoJobRepository`.
                    *   Query joins `VideoJobModel` with `VideoModel`.
                    *   Filters by `VideoModel.uploader_user_id` and `VideoJobModel.status.in_(statuses)`.
                    *   Orders by `VideoJobModel.created_at.desc()`.
                    *   Uses `joinedload` for `video` and `video_metadata`.
                    *   Uses synchronous `Session` and `query.all()`.
            *   [X] **Task 3.X.4: Integrate Service with API Endpoint**
                *   **File(s) to Modify:** `apps/core/api/endpoints/jobs_endpoints.py` (Modified).
                *   **Actions:**
                    *   Endpoint `get_my_processing_jobs` now calls `await get_user_jobs_by_statuses(...)`.
            *   [X] **Task 3.X.5: Add Unit/Integration Tests**
                *   **File(s) to Create/Modify:**
                    *   `apps/core/tests/operations/test_video_job_repository.py` (Created & populated).
                    *   `apps/core/tests/services/test_job_service.py` (Created & populated).
                    *   `apps/core/tests/api/test_jobs_api.py` (Created & populated).
                    *   Associated `__init__.py` files for test packages.
                *   **Actions:**
                    *   **Repository Tests:** Added tests for filtering by user, status, `None` status, ordering, pagination, and eager loading for `get_by_user_id_and_statuses`.
                    *   **Service Tests:** Added tests for `get_user_jobs_by_statuses` covering logic for default statuses and interaction with mocked repository.
                    *   **API Tests:** Added tests for `GET /api/v1/jobs/` covering successful calls (default and specific statuses), unauthenticated access, and invalid status parameters, using `TestClient` and mocking service/auth dependencies.

    *   [X] Task 3.Y: **Upgrade Backend Database Operations to Asynchronous**
        *   **Description:** Refactor database interactions in `apps/core` to be fully asynchronous to improve performance and scalability under load. This involved using an async database driver (`aiosqlite` for SQLite, `asyncpg` for PostgreSQL), `AsyncSession` from SQLAlchemy, and updating repository, service, and API endpoint layers to use `async/await` for database calls.
        *   **Key Areas Refactored:**
            *   `apps/core/lib/database/connection.py`: Introduced `create_async_engine`, `AsyncSessionLocal`, `create_async_session` helper, and `get_async_db_session` dependency. Database URLs updated for async drivers.
            *   `apps/core/operations/`: All repository methods (e.g., in `VideoJobRepository`, `VideoRepository`, `UserRepository`, `VideoMetadataRepository`, `ChatRepository`, `TransactionRepository`) converted to `async def`, use `AsyncSession`, and `await` database calls (e.g., `await session.execute()`, `await session.scalars()`, `await session.commit()`).
            *   `apps/core/services/`: Service layer functions (e.g., `JobService`, `VideoProcessingService`, `UserService`, `AuthService`, `ChatService`, `AIService`) updated to correctly `await` asynchronous repository calls and utilize `AsyncSession` passed from API endpoints or created for background tasks. `MetadataService` was reviewed and kept synchronous as its dependencies were synchronous.
            *   `apps/core/api/endpoints/`: API endpoints (e.g., `jobs_endpoints.py`, `video_processing_endpoints.py`) updated to depend on `AsyncSession` via `get_async_db_session` and `await` service calls. Dependency providers for services and repositories were also made asynchronous.
        *   **Notes:**
            *   Persistent linter errors were observed in some files (e.g., `video_processing_service.py`, `jobs_endpoints.py`) related to SQLAlchemy's async features, type inference with `AsyncSessionLocal`, and model attribute types (e.g., `Column[int]` vs `int`). These are suspected to be limitations of the linter/type-checker with advanced SQLAlchemy async patterns and were not considered blockers for the core functionality.
            *   The `pyproject.toml` for `apps/core` was updated with `aiosqlite>=0.20.0` and `asyncpg>=0.29.0`.
        *   **Next Steps: Complete Asynchronous Test Refactoring (`apps/core/tests/`)**
            *   **Goal:** Ensure all unit and integration tests in `apps/core/tests/` are updated to work correctly with the asynchronous database setup and async versions of services/repositories.
            *   **Core Test Infrastructure (Completed & Verified):**
                *   [X] `apps/core/pyproject.toml`: `pytest-asyncio` added.
                *   [X] `apps/core/tests/conftest.py`: Contains `async_engine_fixture`, `db_session_fixture` (for `AsyncSession`), and `test_client_fixture` (provides `httpx.AsyncClient` via `starlette.testclient.TestClient` compatibility, or should be updated to directly use `httpx.AsyncClient` if strictness is required; current integration tests are adapting to `AsyncClient` type hint).
            *   **General Refactoring Pattern for each relevant test file/method:**
                1.  Remove any local/old synchronous database setup (engines, sessionmakers, fixtures).
                2.  Inject `db_session_fixture: AsyncSession` (from `conftest.py`) into test methods needing direct DB access.
                3.  Inject `client: AsyncClient` (from `conftest.py's `test_client_fixture`) into API test methods.
                4.  Convert test methods to `async def` and decorate with `@pytest.mark.asyncio`.
                5.  Update database operations to be asynchronous:
                    *   `session.add(obj)` / `session.add_all([...])` (no change needed for add)
                    *   `await session.commit()`
                    *   `await session.rollback()`
                    *   `await session.refresh(obj)`
                    *   Replace `session.query(...)` with `select(...)` statements executed via `await session.execute(select(...))`. Results obtained via `.scalars().all()`, `.scalar_one_or_none()`, etc.
                6.  Ensure all calls to asynchronous application code (repository methods, service methods) are `await`ed.
                7.  When mocking asynchronous callables, use `unittest.mock.AsyncMock` and assert with `assert_awaited_once_with()`, `assert_any_await()`, etc.
            *   **Specific Test Directories/Files to Refactor:**
                *   [X] **Operations Tests (`apps/core/tests/operations/`)**
                    *   [X] `test_video_job_repository.py`: Refactored and verified.
                    *   All other files in this directory (e.g. `__init__.py`) did not require refactoring.
                *   [X] **Services Tests (`apps/core/tests/services/`)**
                    *   [X] `test_job_service.py`: Verified, already async.
                    *   All other files in this directory (e.g. `__init__.py`) did not require refactoring.
                *   [X] **API Tests (`apps/core/tests/api/`)**
                    *   [X] `test_jobs_api.py`: Previously refactored and verified.
                    *   All other files in this directory (e.g. `__init__.py`) did not require refactoring.
                *   [X] **Unit Tests (`apps/core/tests/unit/`)**
                    *   [X] `lib/`: Sub-tasks mostly completed as per prior status. All files checked.
                        *   [X] `apps/core/tests/unit/lib/utils/`: All files (`test_ffmpeg_utils.py`, `test_file_utils.py`, `test_subtitle_utils.py`) confirmed synchronous and needing no async changes.
                        *   [X] `apps/core/tests/unit/lib/storage/test_file_storage.py`: Verified.
                        *   [X] `apps/core/tests/unit/lib/publishing/test_youtube_adapter.py`: Verified (sync).
                        *   [X] `apps/core/tests/unit/lib/auth/test_supabase_auth.py`: Verified (async).
                        *   [X] `apps/core/tests/unit/lib/cache/test_redis_cache.py`: Verified (async).
                        *   [X] `apps/core/tests/unit/lib/ai/test_gemini_adapter.py`: Verified (async).
                        *   [X] `apps/core/tests/unit/lib/ai/test_ai_client_factory.py`: Verified (sync).
                    *   [X] `operations/`: All files (`test_video_job_repository.py`, `test_video_repository.py`, `test_video_metadata_repository.py`) verified, already async and using mocks appropriately.
                    *   [X] `services/`: All files (`test_video_processing_service.py`, `test_user_service.py`) verified, already async and using mocks.
                    *   [X] `video_processor/`: Directory explored, found no relevant test files needing refactor.
                *   [~] **Integration Tests (`apps/core/tests/integration/`)**
                    *   [~] `test_video_processing_api.py`: Refactored to use `AsyncClient` and `await`. Lingering linter error for `httpx` import (likely env/linter config).
                    *   [~] `api/`:
                        *   [~] `test_job_status_retrieval.py`: Refactored to use `AsyncClient` and `await`. Lingering linter error for `httpx` import.
                        *   [X] `test_video_upload_flow.py`: Redundant conditional checks (which were flagged by linter) removed. Remaining known linter issues (likely env/config related): `httpx` and `pytest` imports, and persistent `ColumnElement`/type errors on SQLAlchemy model attribute access in asserts. File considered addressed regarding manageable linter fixes as per previous attempts.
                *   [X] **Other Top-Level Tests (`apps/core/tests/`)**
                    *   [X] `test_api.py`: Verified, already uses `httpx.AsyncClient`.
                    *   [X] `test_architecture.py`: Verified, no async concerns.
            *   **Overall Status:** Substantially complete. Most test files are verified or refactored. Remaining issues in a few integration test files are persistent linter/type-checking errors, likely unrelated to core async logic.
        *   **Reason:** To fully leverage FastAPI's async capabilities, prevent blocking I/O operations, and enhance overall application performance and concurrency.

## Phase 4: Video Management & Viewing

12. **Video List Display on Dashboard (with Pagination)**
    *   [X] Task 4.1: **Enhance Dashboard to List User's Videos with Pagination** (Met)
        *   **File(s) to Check/Modify:** `apps/web/src/routes/dashboard.tsx`, `apps/web/src/components/video/VideoList.tsx` (Created), `apps/web/src/components/video/VideoListItem.tsx` (Created), `apps/web/src/lib/api.ts`.
        *   **Backend Prerequisite Notes:** Endpoint `GET /api/v1/videos` (or `/users/me/videos`) must support pagination (e.g., `limit`, `offset`). (Verified: `users/me/videos` with `limit` and `offset` is used)
        *   **Key Actions:**
            *   Modified `api.fetchMyVideos` in `lib/api.ts` to accept optional `limit` and `offset` pagination parameters and include them in the API request. Defined `PaginationParams` interface.
            *   Refactored `dashboard.tsx`:
                *   Removed `ProtectedLayout` wrapper.
                *   Changed `useQuery` to `useInfiniteQuery` for `fetchMyVideos`, including `queryFn`, `initialPageParam`, and `getNextPageParam`.
                *   Corrected `thumbnail_url` to `thumbnail_file_url` usage.
            *   Created `apps/web/src/components/video/VideoListItem.tsx` to display a single video item with link to its detail page.
            *   Created `apps/web/src/components/video/VideoList.tsx` to:
                *   Display a list of videos using `VideoListItem.tsx`.
                *   Handle loading (with skeletons), empty, and error states.
                *   Include a "Load More" button connected to `fetchNextPage` from `useInfiniteQuery`.
            *   Refactored `dashboard.tsx` to use the new `VideoList.tsx` component, passing necessary props from `useInfiniteQuery` results and a callback to trigger the upload dialog.
        *   **Acceptance Criteria:** Dashboard displays a paginated list of user's videos. Users can navigate through pages/load more videos. (Met)

13. **Video Detail and Playback Page (`apps/web/src`)**
    *   [X] Task 4.2: **Implement Full Video Detail and Playback Functionality**
        *   **File(s) to Check/Modify:** `apps/web/src/routes/_authed/video/$videoId.tsx` (Refactored), `apps/web/src/components/video/video-detail.tsx` (Incorporated into route file), `apps/web/src/components/video/MediaPlayer.tsx` (Incorporated into route file), `apps/web/src/lib/api.ts` (Used), `apps/web/src/components/video/content-editor.tsx` (Incorporated as form in route file).
        *   **Backend Prerequisite Notes:** 
            *   Endpoint `GET /api/v1/videos/{videoId}/details` must be available. (Used)
            *   Strategy for video playback source: Backend provides a `playbackUrl` (e.g., GCS signed URL) via the details endpoint, or a streaming endpoint `GET /api/v1/stream/video/{videoId}` must be available. (Frontend updated to expect `videoDetails.video.playback_url`; this is a **backend dependency**).
            *   Endpoint `PUT /api/v1/videos/{videoId}/metadata` for editing. (Used)
        *   **Key Actions:** 
            *   Fetched data using `api.getVideoDetails(videoId)`. (Implemented using `useQuery`)
            *   Implemented `MediaPlayer.tsx` (placeholder version updated to support subtitles). (Implemented)
            *   Displayed metadata, transcript. Subtitles (`<track>`) are supported if `subtitle_files_urls` is provided. (Implemented for metadata/transcript/subtitles)
            *   Implemented metadata editing form (title, description, tags) using `api.updateVideoMetadata()`. (Implemented using `useMutation`)
            *   **Firebase logic removed/commented out; component now primarily uses API calls for data fetching and updates.**
        *   **Acceptance Criteria:** Users can view videos with playback, subtitles, all metadata. Editing (if implemented) saves changes. (Partially Met: Core page structure, data display, metadata editing, and subtitle support are implemented. Playback requires a proper `playback_url` from backend. The `createFileRoute` linter error seems environmental or resolved as the code structure is standard.)

## Phase 5: UI/UX Polish & Finalization

14. **Consistent Loading Skeletons and Empty States**
    *   [~+] Task 5.1: **Implement UI Placeholders** (Partially Met, major components improved: Skeletons and error/empty states implemented for Video Details, Job Details, Dashboard Video List, `users.route.tsx`. `ProcessingDashboard.tsx` was significantly refactored with these states. `settings.tsx` and `index.tsx` are currently static and don't require them. Audit of straightforward pages complete. A full audit for consistency across any other minor/existing pages can be a follow-up if more dynamic sections are added.)
        *   **File(s) to Check/Modify:** All pages and components that involve asynchronous data fetching (e.g., `dashboard.tsx`, `jobs/[jobId].tsx`, `video/[videoId].tsx`, `ProcessingDashboard.tsx`, `users.route.tsx`).
        *   **Key Actions:**
            *   Utilize the `shadcn/ui Skeleton` component (from `apps/web/src/components/ui/skeleton.tsx`) to create loading placeholders for content areas while `useQuery` is in its `isLoading` state. (Implemented in new pages)
            *   Design and implement clear and user-friendly "empty state" messages for situations where no data is available (e.g., "You haven't uploaded any videos yet." on the dashboard, or "Video data is not yet available." if metadata is still being processed). (Implemented in new pages and dashboard video list)
        *   **Acceptance Criteria:** The application provides smooth visual feedback during data loading phases using skeleton screens, and presents informative messages when content areas are empty. (Largely Met for key dynamic areas; `settings.tsx` and `index.tsx` are static. `users.route.tsx` updated.)

15. **Standardized Error Handling and Notifications**
    *   [~] Task 5.2: **Unify Error Display** (Largely Met: `sonner` toasts are used for non-modal API errors and form submission feedback. `Alert` components are used for page-level data loading errors. `useAuth` and `lib/api` provide error details. Consistency checked in `login.tsx`, `signup.tsx`, `VideoDetailPage`, `JobDetailsPage`, `users.route.tsx`. Form validation messages are inline. Broader audit for any remaining minor components can be a follow-up.)
        *   **File(s) to Check/Modify:** All components that handle API calls or user input leading to potential errors. Revisit `useAuth.ts`, `lib/api.ts`, and form components.
            *   `apps/web/src/components/ui/sonner.tsx` (or `toaster.tsx` / `use-toast.tsx` if that's the active toast system).
        *   **Key Actions:**
            *   Consistently use `sonner` (or the project's chosen toast component) for non-modal error notifications arising from API interactions (e.g., "Failed to upload video: Server error"). (Implemented in key flows)
            *   Employ `shadcn/ui Alert` with `variant="destructive"` for more prominent, inline error messages within forms or specific content sections (e.g., "Invalid email address" below an input field). (Implemented for page load errors; form errors are typically handled by FormMessage)
            *   Ensure error messages are user-friendly and, where appropriate, suggest corrective actions. (Generally implemented)
        *   **Acceptance Criteria:** Errors are communicated to the user in a consistent, clear, and non-disruptive manner across the application. (Largely Met for reviewed features and forms.)

16. **Responsive Design Review and Adjustments**
    *   [~] Task 5.3: **Ensure Mobile and Tablet Usability** (Partially Met: Code review and responsive improvements applied to `__root.tsx` navigation, `dashboard.tsx` header/tabs. Key components like `VideoList`, `VideoListItem`, `VideoDetail`, `JobDetail`, `Login`, `Signup` reviewed and appear structurally responsive. Further manual testing across devices/flows recommended to catch nuanced issues and complete this task.)
        *   **File(s) to Check/Modify:** Key layout files (`__root.tsx`, `_authed.tsx`) and primary view components (`dashboard.tsx`, `login.tsx`, `video/[videoId].tsx`, `jobs/[jobId].tsx`). Also, `apps/web/src/styles/app.css` or any global style sheets.
        *   **Key Actions:**
            *   Thoroughly test all primary application flows and pages on various screen sizes (desktop, tablet, mobile).
            *   Use browser developer tools to emulate different devices.
            *   Adjust layouts, font sizes, component visibility, and interaction patterns as needed using Tailwind CSS's responsive utility classes (e.g., `sm:`, `md:`, `lg:`).
            *   Pay special attention to navigation, forms, and data tables/lists on smaller screens.
        *   **Acceptance Criteria:** The core application is fully responsive, providing a good user experience across common device types and screen resolutions. (Code structure supports responsiveness; pending final manual review.) 

                *   **Unit Tests (Lib - check if any lib components became async and need test updates):**
                    *   [ ] `apps/core/tests/unit/lib/utils/`:
                        *   [X] `test_ffmpeg_utils.py`: Corrected `subprocess.run` mock assertions. (Sync utils, sync tests)
                        *   [X] `test_file_utils.py` (Sync utils, sync tests)
                        *   [X] `test_subtitle_utils.py` (Sync utils, sync tests)
                    *   [X] `apps/core/tests/unit/lib/storage/test_file_storage.py`: (Mix of sync/async, seems okay)
                    *   [X] `apps/core/tests/unit/lib/publishing/test_youtube_adapter.py`: (Sync adapter, sync tests)
                    *   [X] `apps/core/tests/unit/lib/auth/test_supabase_auth.py`: (Async `get_current_user`, async tests - Verified)
                    *   [X] `apps/core/tests/unit/lib/cache/test_redis_cache.py`: (Async cache, async tests - Verified)
                    *   [X] `apps/core/tests/unit/lib/ai/test_gemini_adapter.py`: (Async adapter, async tests - Verified)
                    *   [X] `apps/core/tests/unit/lib/ai/test_ai_client_factory.py`: (Sync factory, sync tests)
                *   **General Test Files:**
                    *   [X] `apps/core/tests/test_api.py`: Updated to use `httpx.AsyncClient`.
                    *   [X] `apps/core/tests/test_architecture.py`: No async concerns. (Verified)
</file>

<file path=".ai_docs/done/active-context.md">
# Active Context: Echo Platform (Consolidated)

## Overall Current Focus

The Echo platform development is now focused on validating the end-to-end flow for core features like video upload, processing, and status display, and integrating the backend API with the frontend application. The backend API milestone for video processing is complete, and the next phase is comprehensive testing, frontend integration, and documentation.

---

## Backend (API) - Active Context

### Primary Backend Workstreams:
*   **End-to-End Testing**: Validating the full video processing pipeline from upload via API to final status update, including storage and AI service integration.
*   **Frontend Integration**: Supporting the frontend team in connecting to the new API endpoints for video upload and job status.
*   **Security Audit & Hardening**: Reviewing API endpoints, authentication/authorization, and data handling for security.
*   **Documentation**: Finalizing API documentation (OpenAPI specs), architecture docs, and Memory Bank updates.
*   **Performance & Monitoring**: Planning for performance optimization and observability improvements.

### Current Backend Priorities:
1.  **End-to-End Testing**: Test the video processing API with real uploads and job status queries.
2.  **Frontend Integration**: Ensure the frontend can successfully use the new endpoints for video upload and status.
3.  **Security & Error Handling**: Audit endpoints and improve error handling and resilience.
4.  **Documentation**: Update API docs, Memory Bank, and code comments for maintainability.

### Recent Backend Changes:
*   Implemented a clean architecture with distinct layers (Domain, Application, Adapters, Infrastructure).
*   Developed modular service adapters for storage (GCS), AI (Gemini, etc.), and publishing (YouTube).
*   Created and registered FastAPI endpoints for video upload and job status retrieval, using new Pydantic schemas.
*   Implemented the VideoProcessingService to orchestrate the video processing pipeline.
*   Established router aggregation in `api/endpoints/__init__.py` for scalable API structure.
*   Updated `main.py` to register the aggregated router under `/api/v1/videos`.
*   Significantly refactored and removed legacy monolithic components and outdated structures.
*   Established basic CI/CD pipeline for automated testing and linting.
*   Implemented core data models (VideoModel, VideoJobModel, VideoMetadataModel) and enum (ProcessingStatus) for the video processing pipeline following SQLAlchemy ORM patterns with proper relationships.
*   Standardized on `uv` for Python dependency management throughout the project, replacing the previous venv/conda approach.
*   Improved configuration management by updating service adapters to use the central settings object rather than direct imports.
*   Implemented a unified `FileStorageService` with support for both local filesystem and Google Cloud Storage backends, using async operations.
*   Created a comprehensive AI framework with `AIAdapterInterface` defining a clear contract for AI operations and a concrete `GeminiAdapter` implementation.
*   Added a factory pattern for AI services with the `get_ai_adapter` function that selects the appropriate adapter based on configuration.
*   Fixed the YouTube publishing adapter to properly handle video uploads, captions, and scheduling.
*   Implemented Redis caching system (`lib/cache/redis_cache.py`) for async, settings-driven cache operations.
*   Added utility classes in `lib/utils/` for FFmpeg operations (`ffmpeg_utils.py`), temporary file management (`file_utils.py`), and subtitle generation (`subtitle_utils.py`).
*   Added authentication utilities (`lib/auth/supabase_auth.py`) for Supabase JWT validation and user extraction.
*   Defined custom exceptions (`core/exceptions.py`) for video processing, AI, and FFmpeg errors.
*   Implemented repository layer in `operations/` for `VideoModel`, `VideoJobModel`, and `VideoMetadataModel` with CRUD and update methods.
*   Implemented `get_or_create_user_profile` in `UserService` to ensure a local user profile exists for each authenticated user (Supabase/JWT), using email as the unique identifier and robustly handling missing username/full_name.

### Next Backend Steps:
*   **Immediate**: Conduct E2E and integration testing for the full video processing flow. Support frontend integration and address any issues found during testing.
*   **Upcoming**: Performance optimization, monitoring, and observability improvements. Plan for initial production deployment. Continue documentation and codebase cleanup.

---

## Frontend (Web Application) - Active Context

### Primary Frontend Workstreams:
*   **Core Architecture Setup**: Establishing a scalable and maintainable frontend architecture (e.g., component structure, state management, routing).
*   **Supabase Integration**: Implementing user authentication (Google OAuth via Supabase) and integrating with Supabase for user-specific data.
*   **API Client Implementation**: Developing a robust API client to interact with the FastAPI backend, including handling authentication tokens and error responses.
*   **UI Component Development**: Building key UI components for video upload, dashboard/video list, real-time progress display, and metadata editing.
*   **WebSocket Integration**: Implementing client-side WebSocket handling for real-time updates from the backend.

### Current Frontend Priorities:
1.  **Authentication Flow**: Complete and test the Supabase Google OAuth flow for user sign-up and login.
2.  **Video Upload UI & Logic**: Implement the UI for video upload and the logic to obtain signed URLs from the backend and upload directly to GCS.
3.  **Real-time Dashboard Display**: Develop the dashboard component to display a list of user videos and their real-time processing statuses via WebSockets.
4.  **Basic Project Structure & Tooling**: Finalize project structure, linter/formatter setup, and basic build processes.

### Recent Frontend Changes:
*   Initialized the React (Next.js/Vite - *Confirm choice*) project structure.
*   Set up basic routing and layout components.
*   Started integration with Supabase client for authentication.
*   Sketched out initial designs for dashboard and video upload components.

### Next Frontend Steps:
*   **Immediate**: Finalize and test the video upload component (including GCS direct upload). Implement WebSocket client and connect to dashboard for real-time status updates. Securely store and manage auth tokens.
*   **Upcoming**: Develop the metadata viewing/editing interface. Implement thumbnail selection. Refine UI/UX based on initial internal reviews.

---

## Overall Active Decisions & Considerations

### Architecture & Technology:
*   **Backend**: Python with FastAPI, Clean Architecture (DDD), PostgreSQL (via Supabase), Google Cloud Storage, Celery (or similar for task queuing - *Confirm choice*), AI services (Gemini, OpenAI, AssemblyAI).
*   **Frontend**: React (Next.js/Vite - *Confirm choice*), TypeScript, Supabase client library, Zustand/Redux Toolkit (for state management - *Confirm choice*), Tailwind CSS (or other UI library - *Confirm choice*).
*   **Communication**: RESTful APIs for most client-server interaction, WebSockets for real-time updates.
*   **Database**: Supabase (Postgres) as the primary database for metadata, user information, and application state.

### Implementation Considerations (Platform-wide):
*   **Modularity & Reusability**: Design components (both backend services and frontend UI elements) to be modular and reusable.
*   **Testability**: Emphasize test-driven development (TDD) or behavior-driven development (BDD) where practical. Aim for high test coverage.
*   **Error Handling & User Feedback**: Implement comprehensive error handling on both backend and frontend, providing clear feedback to the user.
*   **Security**: Prioritize security in API design, authentication, data storage, and frontend interactions.
*   **Documentation**: Maintain up-to-date documentation (Memory Bank, code comments, API specs).

## Important Patterns and Preferences (Platform-wide)

*   **Code Organization**: Adherence to Clean Architecture principles on the backend. Well-structured component hierarchy on the frontend. Consistent naming conventions.
*   **Development Workflow**: Feature-branch workflow (Git). Code reviews for all significant changes. CI/CD for automated testing and deployments.
    *   **Backend Python Environment**: All Python virtual environment and package management for `apps/core` must be performed using [`uv`](https://github.com/astral-sh/uv). No other tools (pip, venv, conda, poetry, etc.) are permitted. All dependencies are managed via `pyproject.toml`, and all developers must use `uv` commands for environment creation, dependency installation, and management. This is a permanent, enforced project standard.
    *   **Architecture Rule Enforcement**: All backend code must strictly adhere to the architectural rules defined in `.cursor/rules/architecture.mdc`. This includes layered architecture, domain separation, import order, error handling, and testing requirements. Code reviews must enforce these rules, and any deviations must be explicitly justified and documented in the Memory Bank.
*   **API Design**: RESTful principles, clear and consistent endpoint naming, versioning strategy (if applicable).
*   **State Management (Frontend)**: Deliberate choice of state management solution to handle complexity as the application grows.
*   **Package Management (Backend)**: Use `uv` for Python package management (virtual environments, installation) rather than pip+venv or conda. All dependency installations should be performed via `uv` for consistency and performance.

## Learnings and Insights (Platform-wide)
*   Establishing clear contracts (API schemas, interface definitions) between frontend and backend early is crucial.
*   The Memory Bank system is proving valuable for maintaining context and shared understanding.
*   Balancing development speed with the rigor of clean architecture and comprehensive testing requires ongoing attention.
*   Early integration of services (e.g., Supabase, GCS) helps identify and resolve issues sooner.
*   For external-auth (Supabase/JWT) users, using email as the unique key for user profile creation is robust and reliable; username/full_name should be generated or defaulted if not present in the auth payload.
</file>

<file path=".ai_docs/done/architecture-fe-integration.md">
# Echo Platform Architecture

---

## 1. Overall Architecture Philosophy

The Echo platform is designed as a distributed system with a clear separation between the backend API (`apps/core`) and the frontend web application (`apps/web`). The backend is responsible for video processing, AI integration, database interactions, and core business logic. The frontend handles user interaction, presentation, and client-side state management.

Communication primarily occurs via RESTful APIs for control and metadata, WebSockets for real-time job status updates, and direct-to-cloud uploads (via pre-signed URLs) for large video files to minimize backend server load.

```mermaid
graph TD
    User[User] -- HTTPS --> Frontend["Frontend Web App (apps/web  React/TanStack)"]
    Frontend -- REST API (Control/Metadata) --> BackendAPI["Backend API (apps/core  FastAPI)"]
    Frontend -- WebSocket (Job Status) --> BackendAPI
    Frontend -- Direct Upload (Pre-signed URL) --> CloudStorage["Cloud Storage (GCS or Local Dev Signed URL Target)"]

    BackendAPI -- Integrates with --> AIServices["AI Services (Gemini, etc.)"]
    BackendAPI -- Manages --> CloudStorage
    BackendAPI -- Uses --> Database["Database (PostgreSQL via Supabase)"]
    BackendAPI -- Integrates with --> Publishing["Publishing Platforms (YouTube)"]
    BackendAPI -- Generates Pre-signed URL for --> CloudStorage
    BackendAPI -- Receives Upload Completion Notification from --> Frontend

    %% Frontend Direct Interactions
    Frontend -- Auth & DB Operations --> Supabase["Supabase (Auth, Database Client)"]
```

This document outlines the current structure and how the frontend will integrate with existing backend services and Supabase, leveraging established system patterns, including WebSockets for real-time updates and direct-to-cloud uploads.

---

## 2. Backend Architecture (`apps/core`)

The backend architecture for `apps/core` is established and follows Clean Architecture principles. This structure serves as the stable foundation for frontend integration.

### 2.1. Core Principles & Patterns (Recap from `system-patterns.md`)
*   **Clean Architecture Layers:** Domain, Application, Adapters, Infrastructure.
*   **Repository Pattern:** Data access abstracted via repositories in `apps/core/operations/`. These repositories now implement asynchronous methods for database interactions.
*   **Adapter Pattern:** For all external services (AI, Storage, Publishing) in `apps/core/lib/`.
*   **Dependency Injection:** Used for decoupling, including providing `AsyncSession` for database operations.
*   **SQLAlchemy ORM:** For database interactions, with models in `apps/core/models/`. Utilizes SQLAlchemy's asynchronous features (`AsyncSession`, `async_engine`).
*   **FastAPI Framework:** For API endpoints, request/response handling, Pydantic schemas, and WebSocket handling (`apps/core/api/`). Endpoints and services are `async` and leverage asynchronous database calls.
*   **Centralized Configuration:** Pydantic `Settings` in `apps/core/core/config.py`.
*   **Asynchronous Operations:** Deeply integrated using `async/await` throughout the application, now encompassing all database I/O operations via `AsyncSession` and async drivers (e.g., `aiosqlite`, `asyncpg`), in addition to other I/O-bound tasks.
    *   Testing of asynchronous API endpoints is performed using `httpx.AsyncClient` within `pytest-asyncio` marked tests (e.g., in `apps/core/tests/test_api.py`).
    *   The broader test suite, including unit and integration tests for operations, services, and APIs, has also been refactored to support these asynchronous operations. This typically involves using `unittest.mock.AsyncMock` for mocking asynchronous dependencies in unit tests, and `httpx.AsyncClient` (often via a shared `test_client_fixture`) for integration/API tests. Test database sessions are managed using `AsyncSession` provided by fixtures like `db_session_fixture` from `apps/core/tests/conftest.py`.

### 2.2. Current Backend File Structure (`apps/core`)
(Structure remains as previously documented, no changes here based on decisions)
```
apps/core/
 alembic/                # Database migrations
    versions/
    ...
 api/                    # FastAPI routes and Pydantic schemas
    endpoints/          # API route handlers (e.g., video_processing_endpoints.py, websocket_endpoints.py - NEW)
    schemas/            # Pydantic schemas (e.g., video_processing_schemas.py)
 core/                   # Core application setup (config.py, exceptions.py)
 lib/                    # Common libraries and adapters for external services
    ai/                 # AI service adapters (gemini_adapter.py)
    auth/               # Authentication utilities (supabase_auth.py)
    cache/              # Caching mechanisms (redis_cache.py)
    database/           # Database connection (connection.py - now configures async engine and sessions)
    messaging/          # (e.g., email.py, websocket_manager.py - NEW)
    publishing/         # Publishing adapters (youtube_adapter.py)
    storage/            # Storage adapters (file_storage.py for local/GCS)
    utils/              # General utilities (ffmpeg_utils.py, file_utils.py)
 models/                 # SQLAlchemy ORM models (video_model.py, video_job_model.py, etc.)
 operations/             # Repository implementations (video_repository.py, video_job_repository.py - now fully async)
 services/               # Service layer, business logic orchestration (video_processing_service.py - MODIFIED for WebSocket events, and now uses async DB calls)
 tests/                  # Automated tests (unit, integration)
 main.py                 # FastAPI app instantiation and router registration
 pyproject.toml          # Project metadata and dependencies (managed by uv)
 README.md               # Backend specific documentation
```

### 2.3. Key Backend Endpoints & Functionality for Frontend Interaction
*   **Video Upload Flow (Direct-to-Cloud):**
    *   `POST /api/v1/videos/signed-upload-url`: **NEW**. Frontend requests a pre-signed URL to upload directly to GCS.
        *   Request: `{ filename: string, contentType: string }`
        *   Response: `{ uploadUrl: string, videoId: string (or temporary correlation ID) }`
    *   `POST /api/v1/videos/upload-complete`: **NEW**. Frontend notifies backend after successful direct upload.
        *   Request: `{ videoId: string (or correlation ID), originalFilename: string, storagePath: string, contentType: string, sizeBytes: number }`
        *   Action: Finalizes video record, initiates processing job.
*   **Data Retrieval & Management:**
    *   `GET /api/v1/videos/jobs/{job_id}`: To fetch the status and results of a processing job (still useful for direct linking or if polling is a fallback).
    *   `GET /api/v1/videos` or `GET /api/v1/users/me/videos`: **NEW**. To list videos for the authenticated user (paginated).
    *   `GET /api/v1/videos/{videoId}/details`: **NEW**. To get comprehensive details for a specific video, including related job and metadata.
    *   `PUT /api/v1/videos/{videoId}/metadata`: **NEW**. To update video metadata.
*   **Video Playback:**
    *   The `GET /api/v1/videos/{videoId}/details` endpoint will include a `playbackUrl` (e.g., a GCS signed URL for reading the video object) or the backend will provide a streaming endpoint `GET /api/v1/stream/video/{videoId}`. Generating signed URLs for playback is preferred for GCS.
*   **Real-time Updates (WebSockets):**
    *   `WS /ws/jobs/status/{user_id}` (or similar): **NEW**. Frontend connects to this WebSocket endpoint after login to receive real-time updates on job statuses for that user. The `VideoProcessingService` will publish events that are broadcast over this WebSocket.

---

## 3. Frontend Architecture (`apps/web`)

The frontend application utilizes React, TypeScript, TanStack ecosystem, and `shadcn/ui`. It interfaces with Supabase for auth and the `apps/core` backend via REST APIs and WebSockets.

### 3.1. Core Frontend Principles & Patterns (from `system-patterns.md`)
*   **Component-Based Architecture, Routing, State Management (TanStack Query, Client State):** As previously defined.
*   **Authentication:** Direct integration with `Supabase` client library.
*   **API Communication:**
    *   Dedicated typed REST API client service (`apps/web/src/lib/api.ts`).
    *   Dedicated WebSocket client/hook (`apps/web/src/hooks/useAppWebSocket.ts` - **NEW**) for real-time job updates.
*   **TypeScript, Custom Hooks:** As previously defined.

### 3.2. Current Frontend File Structure (`apps/web`)
(Structure remains as previously documented, new files like `useAppWebSocket.ts` will be added)
```
apps/web/
 public/                 # Static assets
 src/
    api.ts              # (TanStack Start API handler)
    client.tsx          # Client-side entry point
    components/
       auth.tsx
       GoogleLoginButton.tsx
       ProtectedLayout.tsx
       shared/
       ui/
       video/
           VideoUploadDropzone.tsx # MODIFIED for direct upload
           processing-dashboard.tsx # MODIFIED for WebSocket updates
           video-detail.tsx
           ...
    hooks/
       useAuth.ts # (existing, to be verified)
       useJobStatus.ts # MODIFIED for WebSockets
       useAppWebSocket.ts # NEW
       ...
    lib/
       api.ts          # Backend REST API client service - MODIFIED with new endpoints
       utils.ts
    router.tsx
    routes/
       __root.tsx
       _authed.tsx
       dashboard.tsx
       login.tsx
       signup.tsx
       auth/callback.tsx
       jobs/[jobId].tsx # (new or adapted)
       video.$videoId.tsx
    services/
    styles/
    types/
       api.ts # (new or existing, for API client types)
    utils/
 ... (config files)
 README.md
```

### 3.3. Frontend Integration Architecture (Focus Areas)

**A. Authentication Module & Supabase Client Integration:** (Largely unchanged from previous detailed plan, focusing on `useAuth.ts`)

**B. Backend API Client Service (`apps/web/src/lib/api.ts`):**
*   Updated to include functions for new backend REST endpoints: `getSignedUploadUrl`, `notifyUploadComplete`, `getMyVideos` (with pagination), `getVideoDetails`, `updateVideoMetadata`.

**C. Video Processing Workflow Components:**
*   **Video Upload (`apps/web/src/components/video/VideoUploadDropzone.tsx`):**
    *   Implements 3-step direct-to-cloud upload:
        1.  Call `api.getSignedUploadUrl()`.
        2.  Upload file directly to the `uploadUrl` (GCS) using `fetch` or `axios`, showing progress.
        3.  Call `api.notifyUploadComplete()` upon successful GCS upload.
*   **Job Status (`apps/web/src/routes/jobs/[jobId].tsx` or similar):**
    *   Primarily relies on WebSocket updates via `useAppWebSocket.ts` and `useJobStatus.ts` to update `TanStack Query` cache for job data.
    *   Polling via `GET /videos/jobs/{job_id}` becomes a fallback or for initial data load if WebSocket connection is pending.
*   **Video Dashboard & List (`apps/web/src/routes/dashboard.tsx`):**
    *   Uses `api.getMyVideos()` with pagination support.
*   **Video Detail & Player (`apps/web/src/routes/video/[videoId].tsx`):**
    *   Fetches details using `api.getVideoDetails()`.
    *   Video playback will use a `playbackUrl` provided by the backend (ideally a GCS signed URL) or stream from a backend endpoint.
    *   Metadata editing calls `api.updateVideoMetadata()`.

**D. Real-time Communication (`apps/web/src/hooks/useAppWebSocket.ts`):**
*   **NEW Hook:** `useAppWebSocket.ts` will establish and manage the WebSocket connection to the backend (e.g., `/ws/jobs/status/{user_id}`).
*   It will handle incoming messages and provide a way for other hooks/components (like `useJobStatus.ts`) to subscribe to relevant updates or use received data to update `TanStack Query`'s cache.

**E. State Management:** (Unchanged: `TanStack Query` for server state, local/Context for UI state)

---

## 4. Supabase Integration (`supabase/`)
(Role and structure remain as previously documented. RLS is key.)
```
supabase/
 clients/
    client.ts
    server.ts
 index.ts
 migrations/
    ...
 mutations/
    index.ts
 package.json
 supabase/ # Supabase CLI project files
 types/
     db.ts
```
*   **Authentication Provider:** For `apps/web` and JWT validation in `apps/core`.
*   **Database:** Primary data store for `apps/core`.
*   **Row Level Security (RLS):** Essential for data protection.

---

## 5. Communication Patterns

*   **Frontend to Backend API (`apps/web` -> `apps/core`):**
    *   RESTful HTTP/S for control actions (requesting signed URLs, notifying completion, fetching lists/details, updating metadata). Authenticated with Supabase JWT.
    *   WebSocket (WSS) for receiving real-time job status updates. Connection likely authenticated via initial handshake or token.
*   **Frontend to Supabase (`apps/web` -> Supabase Platform):** (Unchanged) Direct for Auth.
*   **Frontend to Cloud Storage (GCS):** Direct HTTP PUT for video file uploads using pre-signed URLs obtained from the backend.
*   **Backend to Supabase (`apps/core` -> Supabase Platform):** Database operations are performed asynchronously via SQLAlchemy using an `AsyncSession` and async-compatible drivers (e.g., `asyncpg`).

---

## 6. Cross-Cutting Concerns
(Managed as described in `system-patterns.md` and apply platform-wide)
*   Error Handling, Logging, Validation, Security, Configuration, Package Management.
*   **Configuration:** Standardized `.env.[mode]` files (e.g., `.env.development`, `.env.production`) are used in both `apps/core` and `apps/web` to manage environment-specific settings. Vite handles these for the frontend, while the backend (`apps/core`) loads them via its configuration setup (e.g., `config.py` using `python-dotenv`).

This revised architecture incorporates WebSockets for real-time updates and a direct-to-cloud upload strategy, providing a more scalable and responsive system.
</file>

<file path=".ai_docs/done/overview.md">
# Project Overview

## API Overview: Video Processing Pipeline - Clean Architecture

This document provides a brief overview of the clean architecture approach and the example implementations provided in the `api/docs/examples` directory.

### Clean Architecture Principles

Our implementation follows the clean architecture principles introduced by Robert C. Martin, which emphasize:

1. **Separation of Concerns**: Clear boundaries between different layers of the application
2. **Dependency Rule**: Dependencies always point inward, with the domain at the center
3. **Abstraction**: High-level modules don't depend on low-level modules; both depend on abstractions
4. **Testability**: Business logic is isolated from external concerns for better testing
5. **Framework Independence**: Core business logic doesn't depend on frameworks or external services

### Architecture Layers

Our implementation divides the application into the following layers:

#### 1. Domain Layer

The domain layer contains the core business entities, value objects, and business rules. It has no dependencies on other layers or external frameworks.

Key components:
- Domain entities (e.g., `Video`, `Job`)
- Value objects
- Domain exceptions

#### 2. Application Layer

The application layer contains the use cases and business logic that orchestrates the domain entities. It defines interfaces that will be implemented by the adapters layer.

Key components:
- Application services (e.g., `TranscriptionService`, `VideoProcessorService`)
- Service interfaces (e.g., `StorageInterface`, `AIServiceInterface`)
- Data Transfer Objects (DTOs)

#### 3. Adapters Layer

The adapters layer contains implementations of the interfaces defined in the application layer. These adapters translate between the application layer and external systems.

Key components:
- Storage adapters (e.g., `GCSStorageAdapter`)
- AI service adapters (e.g., `GeminiAIAdapter`)
- Publishing adapters (e.g., `YouTubeAdapter`)

#### 4. Infrastructure Layer

The infrastructure layer contains framework-specific code, configurations, and other technical details.

Key components:
- API implementation (e.g., FastAPI or Flask routes)
- Dependency injection container
- Database repositories
- Configuration management

### Example Implementations

The `api/docs/examples` directory contains sample implementations of key components in the new architecture:

#### 1. Application Interfaces

The `application/interfaces/storage.py` file demonstrates a clean interface definition for storage operations:

- Defines a clear contract through abstract methods
- Uses domain-specific exceptions
- Is independent of any specific storage provider
- Provides comprehensive documentation for implementers

#### 2. Adapter Implementation

The `adapters/storage/gcs.py` file shows how to implement the storage interface for Google Cloud Storage:

- Implements all methods defined in the interface
- Handles GCS-specific concerns (like path formats, authentication)
- Translates between domain exceptions and provider-specific exceptions
- Uses asynchronous programming with thread pooling for non-blocking I/O

#### 3. Dependency Injection

The `infrastructure/config/container.py` file demonstrates a flexible dependency injection approach:

- Provides a container for managing service instances
- Supports both singleton and transient lifetimes
- Allows easy swapping of implementations (e.g., for testing)
- Centralizes configuration of all application services

#### 4. Application Entry Point

The `main.py` file shows how all components come together in the application entry point:

- Sets up the dependency injection container
- Configures services based on environment variables
- Provides Cloud Function handlers for event processing
- Includes a local development entry point for testing

### Benefits of This Approach

1. **Maintainability**: Smaller, focused components with clear responsibilities
2. **Testability**: Business logic can be tested without external dependencies
3. **Flexibility**: Implementation details can be changed without affecting core logic
4. **Scalability**: Components can be evolved or replaced independently
5. **Clarity**: Clear dependencies and service boundaries

### Implementation Strategy
****
To migrate the existing monolithic code to this architecture:

1. Start by defining domain models and interfaces
2. Implement adapters for existing external services
3. Refactor business logic into application services
4. Set up the dependency injection container
5. Update the main entry point to use the new architecture

This can be done incrementally, allowing the application to continue functioning during the migration.

### Further Reading

For more details about the implementation plan and file structures:

- `api/docs/api-prd.txt` - Product Requirements Document
- `api/docs/file-structure-comparison.md` - Current vs. Proposed File Structure
- `api/docs/implementation-tasks.md` - Detailed Implementation Tasks

## Web Overview: Cline's Memory Bank

I am Cline, an expert software engineer with a unique characteristic: my memory resets completely between sessions. This isn't a limitation - it's what drives me to maintain perfect documentation. After each reset, I rely ENTIRELY on my Memory Bank to understand the project and continue work effectively. I MUST read ALL memory bank files at the start of EVERY task - this is not optional.

### Memory Bank Structure

The Memory Bank consists of core files and optional context files, all in Markdown format. Files build upon each other in a clear hierarchy:

flowchart TD
    PB[projectbrief.md] --> PC[productContext.md]
    PB --> SP[systemPatterns.md]
    PB --> TC[techContext.md]
    
    PC --> AC[activeContext.md]
    SP --> AC
    TC --> AC
    
    AC --> P[progress.md]

#### Core Files (Required)
1. `projectbrief.md`
   - Foundation document that shapes all other files
   - Created at project start if it doesn't exist
   - Defines core requirements and goals
   - Source of truth for project scope

2. `productContext.md`
   - Why this project exists
   - Problems it solves
   - How it should work
   - User experience goals

3. `activeContext.md`
   - Current work focus
   - Recent changes
   - Next steps
   - Active decisions and considerations
   - Important patterns and preferences
   - Learnings and project insights

4. `systemPatterns.md`
   - System architecture
   - Key technical decisions
   - Design patterns in use
   - Component relationships
   - Critical implementation paths

5. `techContext.md`
   - Technologies used
   - Development setup
   - Technical constraints
   - Dependencies
   - Tool usage patterns

6. `progress.md`
   - What works
   - What's left to build
   - Current status
   - Known issues
   - Evolution of project decisions

#### Additional Context
Create additional files/folders within .ai_docs/ when they help organize:
- Complex feature documentation
- Integration specifications
- API documentation
- Testing strategies
- Deployment procedures

### Core Workflows

#### Plan Mode
flowchart TD
    Start[Start] --> ReadFiles[Read Memory Bank]
    ReadFiles --> CheckFiles{Files Complete?}
    
    CheckFiles -->|No| Plan[Create Plan]
    Plan --> Document[Document in Chat]
    
    CheckFiles -->|Yes| Verify[Verify Context]
    Verify --> Strategy[Develop Strategy]
    Strategy --> Present[Present Approach]

#### Act Mode
flowchart TD
    Start[Start] --> Context[Check Memory Bank]
    Context --> Update[Update Documentation]
    Update --> Execute[Execute Task]
    Execute --> Document[Document Changes]

### Documentation Updates

Memory Bank updates occur when:
1. Discovering new project patterns
2. After implementing significant changes
3. When user requests with **update memory bank** (MUST review ALL files)
4. When context needs clarification

flowchart TD
    Start[Update Process]
    
    subgraph Process
        P1[Review ALL Files]
        P2[Document Current State]
        P3[Clarify Next Steps]
        P4[Document Insights & Patterns]
        
        P1 --> P2 --> P3 --> P4
    end
    
    Start --> Process

Note: When triggered by **update memory bank**, I MUST review every memory bank file, even if some don't require updates. Focus particularly on activeContext.md and progress.md as they track current state.

REMEMBER: After every memory reset, I begin completely fresh. The Memory Bank is my only link to previous work. It must be maintained with precision and clarity, as my effectiveness depends entirely on its accuracy.
</file>

<file path=".ai_docs/done/product-context.md">
# Product Context: Echo - Video Processing and Content Automation Platform

## Overall Problem Statement
Content creators, marketers, and businesses face significant challenges in producing, processing, and distributing high-quality video content efficiently and at scale. The traditional video workflow is often:

*   **Time-consuming**: Manual editing, metadata generation, and multi-platform publishing are labor-intensive.
*   **Technically Complex & Expensive**: Requires specialized software, skills, or outsourcing, creating barriers for many.
*   **Inconsistent**: Maintaining quality and branding across numerous videos and platforms is difficult.
*   **Difficult to Scale**: Producing and managing a large volume of video content quickly becomes overwhelming.
*   **Lacking Insight**: Understanding content performance and identifying areas for improvement can be a manual and disjointed process.

Echo aims to solve these pain points by providing an integrated, AI-powered platform that automates and simplifies the entire video content lifecycle, from initial upload to final distribution and analysis.

## Our Solution: Echo Platform

Echo offers an end-to-end solution that streamlines and automates the video content creation and management process by:

*   **Automating Video Processing**: Handling raw video footage, including transcription, summarization, and chapter/segment identification using AI.
*   **AI-Powered Content Enhancement**: Leveraging AI to suggest engaging segments, generate metadata (titles, descriptions, tags), and propose content improvements.
*   **Intuitive Content Management**: Providing a user-friendly interface to review, edit, and approve AI-generated content and manage video assets.
*   **Streamlined Publishing**: Enabling easy, potentially one-click, publishing to multiple platforms with platform-specific optimizations.
*   **Real-time Feedback**: Offering clear visibility into the processing status and, eventually, content performance analytics.

## Target Users

Our primary target users include:

1.  **Content Creators**: YouTubers, streamers, social media influencers, and online course instructors who need to produce and publish video content regularly.
2.  **Marketing Teams**: Businesses of all sizes that use video for product demos, promotional materials, social media engagement, and internal communications.
3.  **Educational Institutions & Trainers**: Organizations and individuals creating educational videos, tutorials, and online learning materials.
4.  **Small to Medium-sized Businesses (SMBs)**: Companies looking to leverage video content for growth without investing in large in-house video production teams.
5.  **Media Companies/Agencies**: Entities that process and distribute larger volumes of video content and could benefit from automation and efficiency gains.

## User Experience (UX) Goals for the Echo Platform

*   **Simplicity & Intuition**: The platform (both frontend and underlying API interactions) should be easy to understand and navigate with a minimal learning curve, abstracting away unnecessary technical complexities.
*   **Speed & Efficiency**: Significantly reduce the time and effort required from raw footage to published, high-quality content.
*   **Intelligent Assistance & Quality Enhancement**: AI should act as a helpful assistant, providing valuable suggestions that demonstrably improve content quality and engagement, not just automate tasks blindly.
*   **Control & Customization**: Offer a good balance between powerful automation and the user's ability to override, customize, and fine-tune outputs to match their specific needs and brand.
*   **Seamless Integration & Workflow**: Ensure smooth connections with existing user workflows, storage solutions, and publishing platforms.
*   **Transparency & Trust**: Provide clear visibility into processing statuses, AI decision-making (where appropriate), and data handling.

## Key User Journeys & Workflows

### Primary End-to-End User Journey (Content Creator Focus):
1.  **Sign Up/Login**: User authenticates via Supabase (Google OAuth).
2.  **Video Upload**: User uploads raw video footage through the web interface (files go to GCS via signed URL).
3.  **Automated Processing & Analysis (Backend)**:
    *   Video is ingested and processed (transcription, summarization, chaptering, thumbnail generation, etc.).
    *   AI analyzes content and generates initial metadata.
4.  **Real-time Monitoring (Frontend)**: User observes processing progress via WebSocket updates on their dashboard.
5.  **Review & Enhancement (Frontend)**:
    *   User is notified upon completion.
    *   User reviews AI-generated suggestions (title, description, chapters, tags, thumbnails).
    *   User edits metadata, selects a thumbnail, and makes any necessary adjustments.
6.  **Publishing Configuration (Frontend)**: User selects target platforms (e.g., YouTube) and configures publishing options.
7.  **Publishing (Backend & Frontend)**: User initiates publishing; backend handles distribution, frontend reflects status.
8.  **(Future) Performance Tracking**: User reviews analytics on published content.

### Secondary/Supporting Workflows:
*   **Content Repurposing**: Adapting existing processed content for different platforms or formats.
*   **Bulk Operations**: Managing or processing multiple videos simultaneously.
*   **Team Collaboration (Future)**: Workflows for review and approval within teams.
*   **Content Archiving & Organization**: Efficiently managing a library of video assets.

## Value Proposition

Echo transforms the complex, time-consuming, and often expensive process of video content creation, processing, and distribution into a streamlined, accessible, and intelligent workflow. We empower creators and businesses to focus on their message and creativity by automating the heavy lifting, ultimately enabling them to produce more high-quality content, reach a wider audience, and achieve their goals more effectively.

## (Initial) Business Model Considerations
*   Subscription-based service (SaaS) with tiered pricing based on usage (e.g., processing hours, storage, number of videos, feature sets).
*   Potential for premium features offering advanced customization, deeper AI insights, or more integrations.

## Success Metrics for the Product
*   **User Adoption & Engagement**: Active users, frequency of use, number of videos processed.
*   **Efficiency Gains**: Measurable reduction in time spent by users on video production and management tasks compared to previous methods.
*   **Content Quality & Impact**: User satisfaction with AI-generated outputs, and (eventually) measurable impact on content performance (e.g., views, engagement on published platforms).
*   **User Retention**: High subscription renewal rates and low churn.
*   **Platform Stability & Performance**: High uptime, fast processing times, and quick UI responsiveness.
</file>

<file path=".ai_docs/done/project-brief.md">
# Project Brief: Echo - Video Processing and Content Automation Platform

## Overall Project Overview
Echo is a comprehensive platform designed to streamline and automate the video content creation, processing, and publishing workflow. It combines a powerful backend API for video processing and AI-driven analysis with an intuitive frontend interface for content creators.

---

## Backend (API) Overview
The backend serves as the core processing engine for the Echo platform. It handles video ingestion, automated processing, AI-based content analysis, metadata extraction, and provides robust API endpoints for the frontend and other services.

### Core Backend Requirements

1.  **Video Processing**:
    *   Automated processing of uploaded video content.
    *   Extraction of key segments, transcripts, and other relevant metadata.
    *   Support for various video formats and resolutions.
2.  **AI Integration**:
    *   Integration with AI services (e.g., OpenAI, AssemblyAI) for content analysis, including transcription, summarization, chapter generation, and smart recommendations.
    *   Automated tagging, categorization, and content enhancement suggestions.
3.  **API Endpoints**:
    *   Secure and scalable RESTful API for frontend communication and potential third-party integrations.
    *   Real-time updates via WebSockets for processing status.
    *   Efficient data transfer, including signed URLs for direct client-side uploads to cloud storage.
4.  **Storage Solutions**:
    *   Secure and scalable video storage (e.g., Google Cloud Storage).
    *   Efficient metadata management and retrieval (e.g., Supabase Postgres).
5.  **Scalability & Reliability**:
    *   Design to handle concurrent processing requests efficiently.
    *   Ability to scale resources (e.g., workers, database) based on demand.
    *   Robust error handling and job management.

### Backend Technical Goals

1.  **Maintainability**: Clean, modular architecture (e.g., Domain-Driven Design principles), comprehensive automated testing (unit, integration, e2e), and clear documentation.
2.  **Performance**: Optimized video processing pipelines, efficient database interactions, and low-latency API responses.
3.  **Security**: Secure API endpoints (authentication & authorization), data encryption where necessary, and adherence to data protection best practices.
4.  **Extensibility**: Modular design to facilitate the addition of new features, AI models, or third-party service integrations.

### Backend Success Criteria
*   Successfully process videos of various common formats.
*   Provide accurate and useful AI-based analysis and recommendations.
*   Maintain API response times within defined acceptable thresholds (e.g., <200ms for most endpoints).
*   Scale effectively to handle projected user and processing loads.
*   Ensure secure handling of user data and video content.

---

## Frontend (Web Application) Overview
The Video Processing Pipeline frontend provides content creators with an intuitive and user-friendly interface to upload videos, monitor AI-powered processing in real-time, review and edit generated metadata (titles, descriptions, tags, chapters), select thumbnails, and publish content to platforms like YouTube.

### Core Frontend Requirements

1.  **User Authentication**:
    *   Secure user sign-up and login using Supabase Google OAuth.
2.  **Video Upload**:
    *   Intuitive interface for users to upload video files.
    *   Direct uploads to Google Cloud Storage using signed URLs obtained from the backend to minimize server load.
3.  **Real-time Processing Monitoring**:
    *   Display the status of video processing (e.g., uploading, transcribing, summarizing, complete) in real-time using updates received via FastAPI WebSockets.
4.  **Metadata Management & Editing**:
    *   Allow users to review AI-generated metadata (e.g., title, description, tags, transcript, chapters).
    *   Provide an interface for users to edit and save this metadata.
5.  **Thumbnail Selection**:
    *   Display a gallery of AI-generated or frame-extracted thumbnails.
    *   Allow users to select their preferred thumbnail or upload a custom one.
6.  **Publishing Control**:
    *   Interface to trigger the publishing process for the processed video and its metadata to selected distribution platforms (initially YouTube).
7.  **Dashboard/Video Management**:
    *   A central dashboard for users to view and manage their uploaded videos and their processing statuses.

### Key Frontend Deliverables
*   Complete migration from Firebase to Supabase for authentication.
*   Implementation of a robust WebSocket client for real-time updates from the backend.
*   Refactored and intuitive dashboard and video detail views.
*   Enhanced metadata editor with a thumbnail gallery and selection capabilities.
*   Responsive design ensuring usability across various screen sizes.
*   Clear and user-friendly error handling and feedback mechanisms.

### Frontend Technical Constraints
*   Must integrate seamlessly with the existing Supabase client setup for database interactions (e.g., fetching user-specific data, saving preferences).
*   Must communicate with the FastAPI backend via authenticated API requests for actions and data retrieval.
*   Must efficiently handle WebSocket connections for real-time updates.
*   Must support direct uploads to GCS via signed URLs provided by the backend.

### Frontend Success Criteria
*   Successful and secure user authentication and session management via Supabase.
*   Real-time processing updates appearing reliably and promptly (e.g., within 1-2 seconds of backend events).
*   Complete and seamless Firebase removal from the frontend codebase.
*   Successful video uploads, metadata editing, thumbnail selection, and publishing functionality.
*   High user satisfaction with the interface's usability and intuitiveness (e.g., target a 4.5/5 rating in user feedback).
*   Responsive design functions correctly on major browsers and device types.

## Project Purpose
Echo is a platform designed to help users create, process, and publish video content more efficiently. It streamlines the workflow from video creation through editing to publishing across various platforms.

## Core Requirements
- Video processing and editing capabilities
- AI-assisted content creation
- Multi-platform publishing support
- User-friendly web interface
- Secure data storage and identity management via Supabase
- Simplified user authentication using Google OAuth via Supabase

## Goals
- Simplify the video creation workflow
- Reduce time spent on repetitive editing tasks
- Provide intelligent suggestions for content improvement
- Enable seamless publishing to multiple platforms
- Create a scalable, maintainable system architecture

## Scope
This project encompasses a full-stack application with:
- Backend API services for video processing
- AI integration for content analysis and enhancement
- Storage solutions for video assets and user data managed by Supabase
- User identity and authentication handled by Supabase (Google OAuth)
- Web application for user interaction
- Integration with publishing platforms

## Success Criteria
- Users can upload, process, and publish videos with minimal friction
- Processing time is significantly reduced compared to manual workflows
- Published content maintains high quality standards
- System is reliable and scales with increasing user demand
</file>

<file path=".ai_docs/done/system-patterns.md">
# System Patterns: Echo Platform (Consolidated)

---

## Backend Architecture Rule Enforcement

**All backend code must strictly adhere to the architectural rules defined in `.cursor/rules/architecture.mdc`. This file is the canonical source for backend architecture, code organization, and quality standards. Code reviews and all development work must enforce these rules. Any deviations must be explicitly justified and documented in the Memory Bank.**

## Overall Architecture Philosophy

The Echo platform is designed as a distributed system with a clear separation between the backend API (responsible for processing, AI integration, and core logic) and the frontend web application (responsible for user interaction and presentation). Communication primarily occurs via RESTful APIs and WebSockets for real-time updates.

```mermaid
graph TD
    User[User] -- HTTPS --> Frontend[Frontend Web App (React/Next.js)]
    Frontend -- REST API / WebSockets --> BackendAPI[Backend API (FastAPI)]

    BackendAPI -- Integrates with --> AIServices[AI Services (Gemini, OpenAI, etc.)]
    BackendAPI -- Integrates with --> CloudStorage[Cloud Storage (GCS)]
    BackendAPI -- Integrates with --> Database[Database (Supabase/Postgres)]
    BackendAPI -- Integrates with --> Publishing[Publishing Platforms (YouTube)]

    Frontend -- Direct Upload --> CloudStorage
    Frontend -- Auth/Data --> Database
```

---

## Backend (API) System Patterns

### Recent Infrastructure & Utility Patterns

- **Router Aggregation Pattern**: All API routers are aggregated and re-exported in a single `api/endpoints/__init__.py` file as `router`. This enables scalable API structure, simplifies router registration in `main.py`, and supports future expansion with minimal changes to the app entrypoint.
- **Async Redis Caching**: Use of an async `RedisCache` class (`lib/cache/redis_cache.py`) for non-blocking cache operations, settings-driven, with simple get/set interface. Used for AI adapter caching and other backend caching needs.
- **Utility Classes**: All infrastructure utilities (FFmpeg, file handling, subtitle generation) are implemented as stateless classes in `lib/utils/`, with only static methods and no side effects. This ensures testability and reusability.
- **Authentication Utilities**: Supabase JWT validation and user extraction are handled by a dedicated module (`lib/auth/supabase_auth.py`) using Pydantic models and FastAPI dependencies.
- **Custom Exception Hierarchy**: All service and utility errors are handled via a custom exception hierarchy in `core/exceptions.py`, with base and specialized error types for video processing, AI, and FFmpeg.
- **Repository Pattern**: All data access for core models is handled by repository classes in `operations/`, each with static CRUD and update methods, accepting a SQLAlchemy Session as the first argument. No business logic is present in repositories.

### Core Architecture: Clean Architecture
The backend adheres to Clean Architecture principles, ensuring a separation of concerns and promoting maintainability, testability, and scalability. The layers are:

1.  **Domain Layer**: Contains core business entities (e.g., `Video`, `Job`, `Metadata`), value objects, domain events, and business rules. It has no dependencies on outer layers.
    *   *Key Patterns*: Rich Domain Models, Value Objects, Domain Events.
2.  **Application Layer**: Orchestrates use cases and defines service interfaces (e.g., `VideoProcessorService`, `StorageInterface`, `AIInterface`). It depends only on the Domain layer.
    *   *Key Patterns*: Application Services, Port (Interface) definitions.
3.  **Adapters Layer**: Implements the interfaces defined by the Application layer, connecting to external systems or providing specific implementations. Examples include GCS storage adapter, Gemini AI adapter.
    *   *Key Patterns*: Adapter Pattern, concrete implementations of Application Ports.
4.  **Infrastructure Layer**: Provides technical capabilities, framework-specific code (FastAPI setup, routing, schemas), configuration, dependency injection container, and actual external system integrations (database connections, API clients for external services).
    *   *Key Patterns*: Dependency Injection Container, Repository Implementations, API Route Handlers, Configuration Management.

```mermaid
graph LR
    subgraph BackendAPI
        direction LR
        Infrastructure[Infrastructure Layer (FastAPI, GCS Client, DB Client)] -- Implements/Uses --> Adapters
        Adapters[Adapters Layer (GCSStorageAdapter, GeminiAIAdapter)] -- Implements --> Application
        Application[Application Layer (VideoService, IStorage)] -- Uses --> Domain
        Domain[Domain Layer (Video, Job, Metadata)]
    end
```

### Key Backend Design Patterns:
*   **Repository Pattern**: Abstracts data persistence operations. Interfaces are defined in the Application (or Domain) layer, with implementations in the Infrastructure layer (e.g., `SupabaseJobRepository`).
*   **Dependency Injection**: Used extensively to decouple components. A DI container (e.g., `punq`) is configured in the Infrastructure layer to manage service lifecycles and inject dependencies into Application services and API controllers.
*   **Adapter Pattern**: For all external service integrations (storage, AI, publishing), isolating the core application from specific third-party library changes. Examples include:
    * `FileStorageService`: Provides a unified interface for both local filesystem and Google Cloud Storage operations.
    * `AIAdapterInterface` and `GeminiAdapter`: Abstract AI service operations behind a consistent interface.
    * `PublishingInterface` and `YouTubeAdapter`: Isolate publishing operations from specific platforms.
*   **Factory Pattern**: Used to create appropriate service instances based on configuration. Examples:
    * `get_ai_adapter()`: Creates the appropriate AI adapter based on available API keys.
    * `get_file_storage_service()`: Provides a singleton FileStorageService instance.
*   **Domain-Driven Design (DDD)**: Emphasis on rich domain models, value objects, and clearly defined bounded contexts (though the current system is largely a single primary context).
*   **SQLAlchemy ORM**: Leveraging SQLAlchemy for database model definition and querying, using `Base` declarative models, relationship mappings, and type-annotated columns. Models follow a convention of suffixing with `Model` (e.g., `VideoModel`, `VideoJobModel`) to distinguish them from domain entities.
*   **Centralized Configuration**: Using a Pydantic `Settings` class to load and validate environment variables, with a singleton instance accessible throughout the application via imports.
*   **Asynchronous Operations with Thread Pooling**: For I/O-bound operations (file storage, AI API calls), using async/await patterns with ThreadPoolExecutor to avoid blocking the event loop.
*   **Task Queuing (Conceptual - e.g., Celery/RabbitMQ/Cloud Tasks)**: For long-running video processing tasks, a task queue will be essential to handle background processing, retries, and scalability. (Specific choice to be finalized).

### Backend Component Relationships & Flows:

*   **API Request Flow (FastAPI)**:
    `HTTP Request` -> `FastAPI Router` -> `Authentication/Authorization Middleware` -> `Dependency Injection (resolves services)` -> `Application Service Method` -> `(Optional) Repository/Adapters` -> `Domain Logic` -> `Response Mapping (Pydantic Schemas)` -> `HTTP Response`.
*   **Video Processing Flow (Simplified)**:
    1.  Client (Frontend) requests a signed URL from Backend API.
    2.  Client uploads video directly to Cloud Storage (GCS).
    3.  Client notifies Backend API of successful upload, triggering a processing job.
    4.  Backend API (potentially via a task queue) initiates `VideoProcessorService`.
    5.  `VideoProcessorService` uses:
        *   `FileStorageService` to access the video, supporting both local and GCS backends.
        *   `AIAdapterInterface` implementations (selected via factory) for text generation, content analysis, and transcription.
        *   `MetadataService` to generate and store metadata.
    6.  Job status is updated in the Database (`JobRepository`).
    7.  Real-time updates sent to client via WebSockets.
    8.  (Optional) `PublishingAdapter` distributes content to platforms like YouTube.

### Backend Module Structure (Illustrative - New Structure):
```
apps/core/
 models/                 # SQLAlchemy ORM models
    enums.py            # Enum definitions (e.g., ProcessingStatus)
    video_model.py      # Video entity model
    video_job_model.py  # Job processing model
    video_metadata_model.py # Video metadata model
 lib/                    # Core libraries and utilities
    ai/                 # AI service adapters
    database/           # Database connection, session management
    storage/            # Storage service adapters
    utils/              # Utility functions and helpers
    cache/              # Caching mechanisms
 operations/             # Repository implementations
 services/               # Service layer orchestration
 api/                    # FastAPI routes and schemas
    endpoints/          # API route handlers
    schemas/            # Pydantic schemas for API
 core/                   # Core application setup
    config.py           # Configuration management
    exceptions.py       # Application exceptions
 alembic/                # Database migrations
 main.py                 # FastAPI app instantiation
```

---

## Frontend (Web Application) System Patterns

### Core Architecture: Modern React SPA
The frontend is a Single-Page Application (SPA) built with React (likely Next.js or Vite with TanStack Router, choice to be confirmed and standardized). Focus is on a responsive user experience, efficient state management, and real-time communication with the backend.

```mermaid
graph TD
    subgraph FrontendWebApp
        direction TB
        Browser[Browser]
        Browser -- Interacts --> UILayer[UI Layer (React Components - shadcn/ui, Custom)]
        UILayer -- Uses --> Routing[Routing (TanStack Router)]
        UILayer -- Manages/Uses --> StateManagement[State Management (TanStack Query, Zustand/Context)]
        StateManagement -- Interacts --> APIServices[API Services Layer]
        APIServices -- HTTP/REST --> BackendAPIClient[Backend API Client (axios/fetch)]
        APIServices -- WebSocket --> WebSocketClient[WebSocket Client]
        APIServices -- Auth/Data --> SupabaseClient[Supabase Client]
    end
```

### Key Frontend Technical Decisions & Patterns:
1.  **Component-Based Architecture**: UI built using reusable React components, potentially leveraging a UI library like `shadcn/ui` for styling and pre-built elements.
2.  **Routing**: Client-side routing managed by a library like `TanStack Router` for navigation without full page reloads.
3.  **State Management**:
    *   **Server State**: `TanStack Query` for managing data fetched from the API, including caching, background updates, and optimistic updates.
    *   **Client/UI State**: Local component state (`useState`, `useReducer`) or a global client state solution (e.g., `Zustand`, `Context API`, or `Redux Toolkit` if complexity warrants) for UI-specific state not tied to the server.
4.  **Authentication**: Integration with Supabase for Google OAuth. JWT tokens obtained from Supabase/backend will be managed by the API client (e.g., stored securely and attached to requests).
5.  **Real-time Updates**: A dedicated WebSocket client (`use-app-websocket` hook or similar) to connect to the FastAPI backend, receive real-time messages, and update relevant state (likely by invalidating or directly updating `TanStack Query` cache).
6.  **Direct-to-Cloud Uploads**: For video uploads, the frontend will request a signed URL from the backend and then upload the file directly to GCS, reducing backend server load.
7.  **Custom Hooks (`/hooks`)**: Encapsulating reusable logic, such as WebSocket connection management, API call patterns, or device detection (`useMobile`).
8.  **TypeScript**: For static typing, improving code quality and maintainability.

### Frontend Component Relationships & Structure (Illustrative):
*   **`src/routes/`**: Components defining application pages/views, responsible for fetching data (often via `TanStack Query` loaders) and composing feature-specific components.
*   **`src/components/features/`** (or `src/components/video/`, `src/components/auth/` etc.): Components that implement specific application features (e.g., `VideoProcessingDashboard`, `MetadataEditor`). They combine UI elements and business logic relevant to that feature.
*   **`src/components/ui/`**: Reusable, presentational UI components (e.g., buttons, modals, cards from `shadcn/ui` or custom-built).
*   **`src/components/shared/`**: Common layout components like `Navbar`, `Footer`, `Container` used across multiple routes.
*   **`src/lib/` or `src/services/`**: API client setup, Supabase client instance, utility functions.
*   **`src/hooks/`**: Custom React hooks for shared logic.
*   **`src/store/`** (if using global state manager): State management configuration.

### Critical Frontend Implementation Paths:
1.  **Authentication Flow**:
    `LoginPage` -> `Supabase Google OAuth Redirect` -> `Callback Handling` -> `Store JWT/Session` -> `Authenticated API Client` -> `Protected Routes`.
2.  **Video Upload Flow**:
    `Upload UI (e.g., Dropzone)` -> `Frontend requests Signed URL (API call)` -> `Backend returns Signed URL` -> `Frontend uploads directly to GCS` -> `Frontend notifies Backend of success (API call)` -> `Backend initiates processing` -> `WebSocket status updates to Frontend`.
3.  **Real-time Dashboard Update Flow**:
    `Dashboard Mounts` -> `Establish WebSocket Connection (useAppWebSocket)` -> `Backend sends processing update message` -> `WebSocket client receives message` -> `Update TanStack Query Cache (invalidate or setData)` -> `Components re-render with new data`.
4.  **Metadata Editing Flow**:
    `VideoDetailPage loads (fetches metadata via TanStack Query)` -> `Display metadata in EditorForm` -> `User edits form` -> `Form submission (API call to save metadata)` -> `On success, invalidate TanStack Query cache for metadata` or `Optimistically update local cache`.

---

## Cross-Cutting Concerns & Platform-Wide Patterns

*   **Error Handling**: Consistent error handling strategies for both backend (domain-specific exceptions, FastAPI error handlers, structured JSON error responses) and frontend (displaying user-friendly messages, logging errors).
*   **Logging**: Structured logging on the backend. Frontend error reporting to a service like Sentry (optional).
*   **Validation**: Pydantic for backend request/response validation. Form validation libraries (e.g., `react-hook-form`, `Zod`) on the frontend.
*   **Security**: HTTPS, secure JWT handling, input sanitization, protection against common web vulnerabilities (OWASP Top 10), proper RLS policies in Supabase.
*   **Configuration Management**: Environment variables for backend (using Pydantic settings) and frontend (e.g., `VITE_` prefixed env vars).
*   **Package Management**: Backend uses `uv` for Python dependency management, virtual environment creation, and dependency installation. Frontend uses `pnpm` for Node.js package management.
*   **API Design**: Adherence to RESTful principles, clear and consistent endpoint naming, use of HTTP status codes. OpenAPI/Swagger for backend API documentation.
</file>

<file path=".ai_docs/done/tech-context.md">
****# Technical Context: Echo Platform (Consolidated)

---

## Python Environment & Package Management Policy

**All Python virtual environment and package management for the backend (`apps/core`) must be performed using [`uv`](https://github.com/astral-sh/uv). No other tools (pip, venv, conda, poetry, etc.) are permitted. All dependencies are managed via `pyproject.toml`, and all developers must use `uv` commands for environment creation, dependency installation, and management. This is a permanent, enforced project standard.**

## Guiding Principles
*   **Leverage Managed Services**: Prefer managed services (Supabase, GCS, Cloud Run, Vertex AI) to reduce operational overhead.
*   **Python for Backend, TypeScript for Frontend**: Utilize the strengths of each language in its respective domain.
*   **Clean Architecture & Modularity**: Ensure maintainability and scalability through well-defined interfaces and separation of concerns.
*   **Automation**: Automate testing, linting, building, and deployment wherever possible.
*   **Security by Design**: Integrate security considerations throughout the development lifecycle.

---

## Backend (API) - Technical Context

### Core Technologies:
*   **Programming Language**: Python 3.12+
*   **Web Framework**: FastAPI (for its performance, async capabilities, and Pydantic integration).
*   **API Router Aggregation**: All API routers are aggregated and re-exported in `api/endpoints/__init__.py` as `router`, enabling scalable API structure and simple registration in `main.py`. See systemPatterns.md for details.
*   **Data Validation & Schemas**: Pydantic.
*   **Dependency Injection**: `punq` or a similar lightweight DI library (to be standardized if `punq` is not the final choice).
*   **Storage**:
    *   **Local File Storage**: Async wrapper around Python's file I/O for development and testing environments.
    *   **Google Cloud Storage**: Using the `google-cloud-storage` client library with async wrappers for production deployments.
*   **AI/ML Integration**:
    *   **Google Gemini API**: Using `google-generativeai` package for text generation, content analysis, and summarization.
    *   **Async Operations**: All AI operations use async/await patterns with thread pooling for non-blocking behavior.
*   **Video Processing**: FFmpeg (called as a subprocess).
*   **Async Redis Caching**: `lib/cache/redis_cache.py` provides a settings-driven, async cache utility for backend and AI adapter use.
*   **Infrastructure Utilities**: All core backend utilities are implemented as stateless classes in `lib/utils/`:
    *   `ffmpeg_utils.py` for audio/frame extraction and metadata
    *   `file_utils.py` for temp directory management
    *   `subtitle_utils.py` for VTT/SRT subtitle generation
*   **Authentication Utilities**: Supabase JWT validation and user extraction via `lib/auth/supabase_auth.py` (Pydantic + FastAPI dependency).
*   **Custom Exceptions**: All service and utility errors are handled via a custom exception hierarchy in `core/exceptions.py`.
*   **Repository Layer**: All data access for core models is handled by repository classes in `operations/`, each with static CRUD and update methods, accepting a SQLAlchemy Session as the first argument.
*   **Testing**: Pytest for unit, integration, and E2E tests.

### Infrastructure & Cloud Services (Primary: Google Cloud Platform):
*   **Compute**: Google Cloud Run (for containerized FastAPI application), Google Cloud Functions (for specific serverless tasks if needed).
*   **Storage (Primary)**:
    *   **Object Storage**: Google Cloud Storage (GCS) for raw video files, processed videos, thumbnails, and other large assets.
    *   **Metadata & Application Data**: Supabase (PostgreSQL) for user data, video metadata, job status, application settings.
*   **AI/ML**: Google Vertex AI (Gemini models primarily, potentially other specialized models).
*   **Messaging/Task Queues**: (To be finalized - options include GCP Pub/Sub with Cloud Functions/Run, or Celery with RabbitMQ/Redis). This is critical for decoupling long-running video processing tasks.
*   **Secrets Management**: Google Secret Manager.
*   **Containerization**: Docker.
*   **API Gateway**: (Considered if complex routing or multiple backend services emerge - for now, FastAPI handles API exposure).

### Development Environment (Backend):
*   **Local Setup**: Python 3.12+ environment managed with `uv` (preferred package manager and installer), Docker Desktop, FFmpeg installed locally, Google Cloud SDK, `.env` files for local configuration.
*   **Package Management**: `uv` for creating virtual environments, installing packages, and resolving dependencies (faster than pip, venv, or conda).
*   **Code Quality**: Pre-commit hooks (e.g., Black, Flake8/Ruff, Mypy).
*   **Preferred Tools**: VS Code (with Python/Pylance extensions), Postman/Insomnia/SwaggerUI for API testing.

### Key Backend Environment Variables (Illustrative):
```
# Supabase
SUPABASE_URL=your-supabase-url
SUPABASE_SERVICE_KEY=your-supabase-service-role-key # For backend use

# Google Cloud
GOOGLE_CLOUD_PROJECT=your-gcp-project-id
GOOGLE_APPLICATION_CREDENTIALS=/path/to/your/gcp-service-account.json # For local dev
STORAGE_BUCKET_NAME=your-gcs-bucket-name
VERTEX_AI_LOCATION=your-vertex-ai-region

# AI Services (if direct keys are used, otherwise through GCP service accounts)
GEMINI_API_KEY=your-gemini-api-key 

# Other
FASTAPI_ENV=development # or production
JWT_SECRET_KEY=a-very-secure-secret # For any internal JWTs if needed, Supabase JWTs handled by Supabase client
CORS_ORIGINS=http://localhost:3000,https://your-frontend-domain.com
# Task Queue (e.g., Redis URL if using Celery with Redis)
CELERY_BROKER_URL=redis://localhost:6379/0
```

---

## Frontend (Web Application) - Technical Context

### Core Technologies:
*   **Framework/Library**: React (likely via Next.js for its features, or Vite for a simpler setup - *Standardize this choice*).
*   **Language**: TypeScript.
*   **Build Tool/Dev Server**: Next.js built-in, or Vite.
*   **Routing**: Next.js file-system router, or TanStack Router if using Vite.
*   **UI Components**: `shadcn/ui` (built on Radix UI and Tailwind CSS).
*   **Styling**: Tailwind CSS.
*   **State Management**:
    *   **Server State**: TanStack Query (React Query).
    *   **Client State**: React Context API, Zustand, or local component state (`useState`, `useReducer`) as appropriate. Avoid over-engineering global client state.
*   **API Communication**:
    *   **HTTP Client**: `fetch` API (possibly wrapped in a lightweight custom client or using a library like `axios` if advanced features are needed).
    *   **WebSockets**: Native WebSocket API for real-time communication with the FastAPI backend.

### Authentication:
*   **Provider**: Supabase Auth (Google OAuth as the primary method).
*   **Token Management**: Supabase client library handles JWT refresh and storage. Tokens are attached to API requests to the backend.

### Development Tooling (Frontend):
*   **Package Manager**: `pnpm` (preferred for efficiency) or `npm`/`yarn`.
*   **Linting/Formatting**: Biome (preferred for its all-in-one nature) or ESLint + Prettier.
*   **Testing**: Vitest (if using Vite) or Jest with React Testing Library.

### Development Setup (Frontend):
1.  Node.js (LTS version) and `pnpm` installed.
2.  Access to a running instance of the Backend API.
3.  Supabase project configured with Google OAuth and necessary tables/RLS.
4.  Google Cloud Storage bucket for direct uploads (frontend will need CORS configuration on the bucket).

### Key Frontend Environment Variables (`.env.local` or similar):
```
# Supabase
NEXT_PUBLIC_SUPABASE_URL=https://your-project.supabase.co
NEXT_PUBLIC_SUPABASE_ANON_KEY=your-supabase-anon-key

# Backend API
NEXT_PUBLIC_API_BASE_URL=http://localhost:8000 # or your deployed API URL
NEXT_PUBLIC_WEBSOCKET_URL=ws://localhost:8000/ws # or your deployed WebSocket URL

# Google Cloud Storage (if needed directly for signed URL construction, though backend should provide full URLs)
# NEXT_PUBLIC_GCS_BUCKET_NAME=your-gcs-bucket-name 
```

---

## Platform-Wide Technical Aspects

### Deployment & CI/CD:
*   **Source Control**: Git (hosted on GitHub).
*   **CI/CD Platform**: GitHub Actions.
*   **Backend Deployment**: Docker images built via GitHub Actions, pushed to Google Artifact Registry (or Docker Hub), and deployed to Google Cloud Run.
*   **Frontend Deployment**: Static site generation (Next.js) or build deployed to a CDN/hosting platform like Vercel, Netlify, or Google Cloud Storage with Cloud CDN.
*   **Environments**: At least `development`, `staging`, and `production` for both frontend and backend.

### Technical Constraints & Goals (Platform-Wide):
*   **Performance**: Fast API response times (<200ms for most non-processing endpoints). Quick frontend load times (e.g., LCP < 2.5s). Efficient video processing (e.g., aiming for <2x real-time duration for common operations).
*   **Scalability**: Backend services designed to scale horizontally (Cloud Run). Frontend capable of handling many concurrent users.
*   **Security**: HTTPS for all traffic. JWT-based authentication between frontend and backend. Secure handling of credentials (Secret Manager). Role-Based Access Control (RBAC) via Supabase RLS and potentially within the application logic. Protection against common web vulnerabilities (OWASP Top 10).
*   **Browser Support (Frontend)**: Modern evergreen browsers (latest 2 versions of Chrome, Firefox, Safari, Edge).
*   **API Design**: RESTful principles for HTTP APIs. Clear WebSocket message contracts.

### Integration Points (Summary):
*   **Frontend <-> Backend API**: RESTful HTTP calls (for actions, data fetching) and WebSockets (for real-time processing updates).
*   **Frontend <-> Supabase**: Authentication, direct database interaction for user-specific, non-sensitive data if appropriate and secured by RLS.
*   **Frontend <-> GCS**: Direct video uploads using signed URLs obtained from the Backend API.
*   **Backend API <-> Supabase**: Storing/retrieving metadata, user information, job statuses.
*   **Backend API <-> GCS**: Storing/retrieving video files and other assets.
*   **Backend API <-> AI Services (Vertex AI/Gemini)**: Sending data for analysis, receiving results.
*   **Backend API <-> Publishing Platforms (e.g., YouTube API)**: Sending processed video and metadata for publishing.
</file>

<file path=".repomix/bundles.json">
{
  "bundles": {}
}
</file>

<file path=".repomixignore">
# JavaScript/Node.js
node_modules/
dist/
build/
coverage/
.next/
.cache/
.env
.env.*
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*

# Python
venv/
.venv/
env/
__pycache__/
*.py[cod]
*.so
.Python
.pytest_cache/
.coverage
htmlcov/

# IDE/Editor
.idea/
.vscode/
*.swp
*.swo
.DS_Store
</file>

<file path="repomix.config.json">
{
  "output": {
    "filePath": "repomix-output.xml",
    "style": "xml",
    "parsableStyle": false,
    "fileSummary": true,
    "directoryStructure": true,
    "removeComments": false,
    "removeEmptyLines": false,
    "topFilesLength": 5,
    "showLineNumbers": false,
    "copyToClipboard": false
  },
  "include": [],
  "ignore": {
    "useGitignore": true,
    "useDefaultPatterns": true,
    "customPatterns": []
  },
  "security": {
    "enableSecurityCheck": true
  },
  "tokenCount": {
    "encoding": "o200k_base"
  }
}
</file>

<file path=".ai_docs/context/py.txt">
Supabase Reference (Python)

# Python Reference



You can initialize a new Supabase client using the `create_client()` method.

The Supabase client is your entrypoint to the rest of the Supabase functionality
and is the easiest way to interact with everything we offer within the Supabase ecosystem.


## Examples

### create_client()

```python
import os
from supabase import create_client, Client

url: str = os.environ.get("SUPABASE_URL")
key: str = os.environ.get("SUPABASE_KEY")
supabase: Client = create_client(url, key)
```


### With timeout option

```python
import os
from supabase import create_client, Client
from supabase.client import ClientOptions

url: str = os.environ.get("SUPABASE_URL")
key: str = os.environ.get("SUPABASE_KEY")
supabase: Client = create_client(
    url, 
    key,
    options=ClientOptions(
        postgrest_client_timeout=10,
        storage_client_timeout=10,
        schema="public",
    )
)
```


# Python Reference

Fetch data: select()



## Examples

### Getting your data

```python
response = (
    supabase.table("planets")
    .select("*")
    .execute()
)
```


### Selecting specific columns

```python
response = (
    supabase.table("planets")
    .select("name")
    .execute()
)
```


### Query referenced tables

```python
response = (
    supabase.table("orchestral_sections")
    .select("name, instruments(name)")
    .execute()
)
```


### Query referenced tables through a join table

```python
response = (
    supabase.table("users")
    .select("name, teams(name)")
    .execute()
)
```


### Query the same referenced table multiple times

```python
response = (
    supabase.table("messages")
    .select("content,from:sender_id(name),to:receiver_id(name)")
    .execute()
)
```


### Filtering through referenced tables

```python
response = (
    supabase.table("orchestral_sections")
    .select("name, instruments(*)")
    .eq("instruments.name", "guqin")
    .execute()
)
```


### Querying referenced table with count

```python
response = (
    supabase.table("orchestral_sections")
    .select("*, instruments(count)")
    .execute()
)
```


### Querying with count option

```python
response = (
    supabase.table("planets")
    .select("*", count="exact")
    .execute()
)
```


### Querying JSON data

```python
response = (
    supabase.table("users")
    .select("id, name, address->city")
    .execute()
)
```


### Querying referenced table with inner join

```python
response = (
    supabase.table("instruments")
    .select("name, orchestral_sections!inner(name)")
    .eq("orchestral_sections.name", "woodwinds")
    .execute()
)
```


### Switching schemas per query

```python
response = (
    supabase.schema("myschema")
    .table("mytable")
    .select("*")
    .execute()
)
```


# Python Reference

Create data: insert()



## Examples

### Create a record

```python
response = (
    supabase.table("planets")
    .insert({"id": 1, "name": "Pluto"})
    .execute()
)
```


### Bulk create

```python
try:
    response = (
        supabase.table("characters")
        .insert([
            {"id": 1, "name": "Frodo"},
            {"id": 2, "name": "Sam"},
        ])
        .execute()
    )
    return response
except Exception as exception:
    return exception
```


# Python Reference

Modify data: update()



## Examples

### Updating your data

```python
response = (
    supabase.table("instruments")
    .update({"name": "piano"})
    .eq("id", 1)
    .execute()
)
```


### Updating JSON data

```python
response = (
    supabase.table("users")
    .update({"address": {"street": "Melrose Place", "postcode": 90210}})
    .eq("address->postcode", 90210)
    .execute()
)
```


# Python Reference

Upsert data: upsert()



## Examples

### Upsert your data

```python
response = (
    supabase.table("instruments")
    .upsert({"id": 1, "name": "piano"})
    .execute()
)
```


### Bulk Upsert your data

```python
response = (
    supabase.table("instruments")
    .upsert([{"id": 1, "name": "piano"}, {"id": 2, "name": "guitar"}])
    .execute()
)
```


### Upserting into tables with constraints

```python
response = (
    supabase.table("users")
    .upsert(
        {"id": 42, "handle": "saoirse", "display_name": "Saoirse"},
        on_conflict="handle",
    )
    .execute()
)
```


# Python Reference

Delete data: delete()



## Examples

### Delete records

```python
response = (
    supabase.table("countries")
    .delete()
    .eq("id", 1)
    .execute()
)
```


### Delete multiple records

```python
response = (
    supabase.table("countries")
    .delete()
    .in_("id", [1, 2, 3])
    .execute()
)
```


# Python Reference

Postgres functions: rpc()

You can call Postgres functions as _Remote Procedure Calls_, logic in your database that you can execute from anywhere.
Functions are useful when the logic rarely changeslike for password resets and updates.

```sql
create or replace function hello_world() returns text as $$
  select 'Hello world';
$$ language sql;
```


## Examples

### Call a Postgres function without arguments

```python
response = (
    supabase.rpc("hello_world")
    .execute()
)
```


### Call a Postgres function with arguments

```python
response = (
    supabase.rpc("echo", { "say": "" })
    .execute()
)
```


### Bulk processing

```python
response = (
    supabase.rpc("add_one_each", {"arr": [1, 2, 3]})
    .execute()
)
```


### Call a Postgres function with filters

```python
response = (
    supabase.rpc("list_stored_planets")
    .eq("id", 1)
    .single()
    .execute()
)
```


### Call a read-only Postgres function

```python
response = (
    supabase.rpc("hello_world", get=True)
    .execute()
)
```


# Python Reference

Using Filters

Filters allow you to only return rows that match certain conditions.

Filters can be used on `select()`, `update()`, `upsert()`, and `delete()` queries.

If a Postgres function returns a table response, you can also apply filters.


## Examples

### Applying Filters

```python
# Correct
response = (
    supabase.table("instruments")
    .select("name, section_id")
    .eq("name", "flute")
    .execute()
)

# Incorrect
response = (
    supabase.table("instruments")
    .eq("name", "flute")
    .select("name, section_id")
    .execute()
)
```


### Chaining

```python
response = (
    supabase.table("instruments")
    .select("name, section_id")
    .gte("octave_range", 3)
    .lt("octave_range", 7)
    .execute()
)
```


### Conditional chaining

```python
filterByName = None
filterOctaveLow = 3
filterOctaveHigh = 7

query = supabase.table("instruments").select("name, section_id")

if filterByName:
    query = query.eq("name", filterByName)

if filterAgeLow:
    query = query.gte("octave_range", filterOctaveLow)

if filterAgeHigh:
    query = query.lt("octave_range", filterOctaveHigh)

response = query.execute()
```


### Filter by values within JSON column

```python
response = (
    supabase.table("users")
    .select("*")
    .eq("address->postcode", 90210)
    .execute()
)
```


### Filter Foreign Tables

```python
response = (
    supabase.table("orchestral_sections")
    .select("name, instruments!inner(name)")
    .eq("instruments.name", "flute")
    .execute()
)
```


# Python Reference

eq()

Match only rows where `column` is equal to `value`.


## Examples

### With `select()`

```python
response = (
    supabase.table("planets")
    .select("*")
    .eq("name", "Earth")
    .execute()
)
```


# Python Reference

neq()

Match only rows where `column` is not equal to `value`.


## Examples

### With `select()`

```python
response = (
    supabase.table("planets")
    .select("*")
    .neq("name", "Earth")
    .execute()
)
```


# Python Reference

gt()

Match only rows where `column` is greather than `value`.


## Examples

### With `select()`

```python
response = (
    supabase.table("planets")
    .select("*")
    .gt("id", 2)
    .execute()
)
```


# Python Reference

gte()

Match only rows where `column` is greater than or equal to `value`.


## Examples

### With `select()`

```python
response = (
    supabase.table("planets")
    .select("*")
    .gte("id", 2)
    .execute()
)
```


# Python Reference

lt()

Match only rows where `column` is less than `value`.


## Examples

### With `select()`

```python
response = (
    supabase.table("planets")
    .select("*")
    .lt("id", 2)
    .execute()
)
```


# Python Reference

lte()

Match only rows where `column` is less than or equal to `value`.


## Examples

### With `select()`

```python
response = (
    supabase.table("planets")
    .select("*")
    .lte("id", 2)
    .execute()
)
```


# Python Reference

like()

Match only rows where `column` matches `pattern` case-sensitively.


## Examples

### With `select()`

```python
response = (
    supabase.table("planets")
    .select("*")
    .like("name", "%Ea%")
    .execute()
)
```


# Python Reference

ilike()

Match only rows where `column` matches `pattern` case-insensitively.


## Examples

### With `select()`

```python
response = (
    supabase.table("planets")
    .select("*")
    .ilike("name", "%ea%")
    .execute()
)
```


# Python Reference

is_()

Match only rows where `column` IS `value`.


## Examples

### Checking for nullness, True or False

```python
response = (
    supabase.table("planets")
    .select("*")
    .is_("name", "null")
    .execute()
)
```


# Python Reference

in_()

Match only rows where `column` is included in the `values` array.


## Examples

### With `select()`

```python
response = (
    supabase.table("planets")
    .select("*")
    .in_("name", ["Earth", "Mars"])
    .execute()
)
```


# Python Reference

contains()

Only relevant for jsonb, array, and range columns. Match only rows where `column` contains every element appearing in `value`.


## Examples

### On array columns

```python
response = (
    supabase.table("issues")
    .select("*")
    .contains("tags", ["is:open", "priority:low"])
    .execute()
)
```


### On range columns

```python
response = (
    supabase.table("reservations")
    .select("*")
    .contains("during", "[2000-01-01 13:00, 2000-01-01 13:30)")
    .execute()
)
```


### On `jsonb` columns

```python
response = (
    supabase.table("users")
    .select("*")
    .contains("address", {"postcode": 90210})
    .execute()
)
```


# Python Reference

contained_by()

Only relevant for jsonb, array, and range columns. Match only rows where every element appearing in `column` is contained by `value`.


## Examples

### On array columns

```python
response = (
    supabase.table("classes")
    .select("name")
    .contained_by("days", ["monday", "tuesday", "wednesday", "friday"])
    .execute()
)
```


### On range columns

```python
response = (
    supabase.table("reservations")
    .select("*")
    .contained_by("during", "[2000-01-01 00:00, 2000-01-01 23:59)")
    .execute()
)
```


### On `jsonb` columns

```python
response = (
    supabase.table("users")
    .select("name")
    .contained_by("address", {})
    .execute()
)
```


# Python Reference

range_gt()

Only relevant for range columns. Match only rows where every element in `column` is greater than any element in `range`.


## Examples

### With `select()`

```python
response = (
    supabase.table("reservations")
    .select("*")
    .range_gt("during", ["2000-01-02 08:00", "2000-01-02 09:00"])
    .execute()
)
```


# Python Reference

range_gte()

Only relevant for range columns. Match only rows where every element in `column` is either contained in `range` or greater than any element in `range`.


## Examples

### With `select()`

```python
response = (
    supabase.table("reservations")
    .select("*")
    .range_gte("during", ["2000-01-02 08:30", "2000-01-02 09:30"])
    .execute()
)
```


# Python Reference

range_lt()

Only relevant for range columns. Match only rows where every element in `column` is less than any element in `range`.


## Examples

### With `select()`

```python
response = (
    supabase.table("reservations")
    .select("*")
    .range_lt("during", ["2000-01-01 15:00", "2000-01-01 16:00"])
    .execute()
)
```


# Python Reference

range_lte()

Only relevant for range columns. Match only rows where every element in `column` is less than any element in `range`.


## Examples

### With `select()`

```python
response = (
    supabase.table("reservations")
    .select("*")
    .range_lte("during", ["2000-01-01 14:00", "2000-01-01 16:00"])
    .execute()
)
```


# Python Reference

range_adjacent()

Only relevant for range columns. Match only rows where `column` is mutually exclusive to `range` and there can be no element between the two ranges.


## Examples

### With `select()`

```python
response = (
    supabase.table("reservations")
    .select("*")
    .range_adjacent("during", ["2000-01-01 12:00", "2000-01-01 13:00"])
    .execute()
)
```


# Python Reference

overlaps()

Only relevant for array and range columns. Match only rows where `column` and `value` have an element in common.


## Examples

### On array columns

```python
response = (
    supabase.table("issues")
    .select("title")
    .overlaps("tags", ["is:closed", "severity:high"])
    .execute()
)
```


### On range columns

```python
response = (
    supabase.table("reservations")
    .select("*")
    .overlaps("during", "[2000-01-01 12:45, 2000-01-01 13:15)")
    .execute()
)
```


# Python Reference

text_search()

Only relevant for text and tsvector columns. Match only rows where `column` matches the query string in `query`.


## Examples

### Text search

```python
response = (
    supabase.table("texts")
    .select("content")
    .text_search(
        "content", 
        "'eggs' & 'ham'", 
        options={"config": "english"},
    )
    .execute()
)
```


### Basic normalization

```python
response = (
    supabase.table("quotes")
    .select("catchphrase")
    .text_search(
        "catchphrase",
        "'fat' & 'cat'",
        options={"type": "plain", "config": "english"},
    )
    .execute()
)
```


### Full normalization

```python
response = (
    supabase.table("quotes")
    .select("catchphrase")
    .text_search(
        "catchphrase",
        "'fat' & 'cat'",
        options={"type": "phrase", "config": "english"},
    )
    .execute()
)
```


### Websearch

```python
response = (
    supabase.table("quotes")
    .select("catchphrase")
    .text_search(
        "catchphrase",
        "'fat or cat'",
        options={"type": "websearch", "config": "english"},
    )
    .execute()
)
```


# Python Reference

match()

Match only rows where each column in `query` keys is equal to its associated value. Shorthand for multiple `.eq()`s.


## Examples

### With `select()`

```python
response = (
    supabase.table("planets")
    .select("*")
    .match({"id": 2, "name": "Earth"})
    .execute()
)
```


# Python Reference

not_()

Match only rows which doesn't satisfy the filter. `not_` expects you to use the raw PostgREST syntax for the filter values.


## Examples

### With `select()`

```python
response = (
    supabase.table("planets")
    .select("*")
    .not_.is_("name", "null")
    .execute()
)
```


# Python Reference

or_()



## Examples

### With `select()`

```python
response = (
    supabase.table("planets")
    .select("name")
    .or_("id.eq.2,name.eq.Mars")
    .execute()
)
```


### Use `or` with `and`

```python
response = (
    supabase.table("planets")
    .select("name")
    .or_("id.gt.3,and(id.eq.1,name.eq.Mercury)")
    .execute()
)
```


### Use `or` on referenced tables

```python
response = (
    supabase.table("orchestral_sections")
    .select("name, instruments!inner(name)")
    .or_("book_id.eq.1,name.eq.guqin", reference_table="instruments")
    .execute()
)
```


# Python Reference

filter()



## Examples

### With `select()`

```python
response = (
    supabase.table("planets")
    .select("*")
    .filter("name", "in", '("Mars","Tatooine")')
    .execute()
)
```


### On a foreign table

```python
response = (
    supabase.table("orchestral_sections")
    .select("name, instruments!inner(name)")
    .filter("instruments.name", "eq", "flute")
    .execute()
)
```


# Python Reference

Using Modifiers

Filters work on the row levelthey allow you to return rows that
only match certain conditions without changing the shape of the rows.
Modifiers are everything that don't fit that definitionallowing you to
change the format of the response (e.g., returning a CSV string).

Modifiers must be specified after filters. Some modifiers only apply for
queries that return rows (e.g., `select()` or `rpc()` on a function that
returns a table response).


## Examples



# Python Reference

order()

Order the query result by `column`.

## Examples

### With `select()`

```python
response = (
    supabase.table("planets")
    .select("*")
    .order("name", desc=True)
    .execute()
)
```


### On a foreign table

```python
response = (
    supabase.table("orchestral_sections")
    .select("name, instruments(name)")
    .order("name", desc=True, foreign_table="instruments")
    .execute()
)
```


### Order parent table by a referenced table

```python
response = (
    supabase.table("instruments")
    .select("name, section:orchestral_sections(name)")
    .order("section(name)", desc=False)
)
```


# Python Reference

limit()



## Examples

### With `select()`

```python
response = (
    supabase.table("planets")
    .select("name")
    .limit(1)
    .execute()
)
```


### On a foreign table

```python
response = (
    supabase.table("orchestral_sections")
    .select("name, instruments(name)")
    .limit(1, foreign_table="instruments")
    .execute()
)
```


# Python Reference

range()



## Examples

### With `select()`

```python
response = (
    supabase.table("planets")
    .select("name")
    .range(0, 1)
    .execute()
)
```


### On a foreign table

```python
response = (
    supabase.table("orchestral_sections")
    .select("name, instruments(name)")
    .range(0, 1, foreign_table="instruments")
    .execute()
)
```


# Python Reference

single()



## Examples

### With `select()`

```python
response = (
    supabase.table("planets")
    .select("name")
    .limit(1)
    .single()
    .execute()
)
```


# Python Reference

maybe_single()



## Examples

### With `select()`

```python
response = (
    supabase.table("planets")
    .select("*")
    .eq("name", "Earth")
    .maybe_single()
    .execute()
)
```


# Python Reference

csv()



## Examples

### Return data as CSV

```python
response = (
    supabase.table("planets")
    .select("*")
    .csv()
    .execute()
)
```


# Python Reference

Using Explain

For debugging slow queries, you can get the [Postgres `EXPLAIN` execution plan](https://www.postgresql.org/docs/current/sql-explain.html) of a query
using the `explain()` method. This works on any query, even for `rpc()` or writes.

Explain is not enabled by default as it can reveal sensitive information about your database.
It's best to only enable this for testing environments but if you wish to enable it for production you can provide additional protection by using a `pre-request` function.

Follow the [Performance Debugging Guide](/docs/guides/database/debugging-performance) to enable the functionality on your project.


## Examples

### Get the execution plan

```python
response = (
    supabase.table("planets")
    .select("*")
    .explain()
    .execute()
)
```


### Get the execution plan with analyze and verbose

```python
response = (
    supabase.table("planets")
    .select("*")
    .explain(analyze=True, verbose=True)
    .execute()
)
```


# Python Reference

Overview



## Examples



# Python Reference

sign_up()



## Examples

### Sign up with an email and password

```python
response = supabase.auth.sign_up(
    {
        "email": "email@example.com", 
        "password": "password",
    }
)
```


### Sign up with a phone number and password (SMS)

```python
response = supabase.auth.sign_up(
    {
        "phone": "123456789", 
        "password": "password",
    }
)
```


### Sign up with a phone number and password (whatsapp)

```python
response = supabase.auth.sign_up(
    {
        "phone": "123456789",
        "password": "password",
        "options": {"channel": "whatsapp"},
    }
)
```


### Sign up with additional user metadata

```python
response = supabase.auth.sign_up(
    {
        "email": "email@example.com",
        "password": "password",
        "options": {"data": {"first_name": "John", "age": 27}},
    }
)
```


### Sign up with a redirect URL

```python
response = supabase.auth.sign_up(
    {
        "email": "hello1@example.com",
        "password": "password",
        "options": {
            "email_redirect_to": "https://example.com/welcome",
        },
    }
)
```


# Python Reference

sign_in_anonymously()



## Examples

### Create an anonymous user

```python
response = supabase.auth.sign_in_anonymously(
    {"options": {"captcha_token": ""}}
)
```


### Create an anonymous user with custom user metadata

```python
response = supabase.auth.sign_in_anonymously(
    {"options": {"data": {}}}
)
```


# Python Reference

sign_in_with_password



## Examples

### Sign in with email and password

```python
response = supabase.auth.sign_in_with_password(
    {
        "email": "email@example.com", 
        "password": "example-password",
    }
)
```


### Sign in with phone and password

```python
response = supabase.auth.sign_in_with_password(
    {
        "phone": "+13334445555", 
        "password": "some-password",
    }
)
```


# Python Reference

sign_in_with_id_token



## Examples

### Sign In using ID Token

```python
response = supabase.auth.sign_in_with_id_token(
    {
        "provider": "google", 
        "token": "your-id-token",
    }
)
```


# Python Reference

sign_in_with_otp



## Examples

### Sign in with email

```python
response = supabase.auth.sign_in_with_otp(
    {
        "email": "email@example.com",
        "options": {
            "email_redirect_to": "https://example.com/welcome",
        },
    }
)
```


### Sign in with SMS OTP

```python
response = supabase.auth.sign_in_with_otp(
    {"phone": "+13334445555"}
)
```


### Sign in with WhatsApp OTP

```python
response = supabase.auth.sign_in_with_otp(
    {
        "phone": "+13334445555",
        "options": {
            "channel": "whatsapp",
        },
    }
)
```


# Python Reference

sign_in_with_oauth



## Examples

### Sign in using a third-party provider

```python
response = supabase.auth.sign_in_with_oauth(
    {"provider": "github"}
)
```


### Sign in using a third-party provider with redirect

```python
response = supabase.auth.sign_in_with_oauth(
    {
        "provider": "github",
        "options": {
            "redirect_to": "https://example.com/welcome",
        }
    }
)
```


### Sign in with scopes

```python
response = supabase.auth.sign_in_with_oauth(
    {
        "provider": "github",
        "options": {
            "scopes": "repo gist notifications",
        }
    }
)
```


# Python Reference

sign_in_with_sso()



## Examples

### Sign in with email domain

```python
response = supabase.auth.sign_in_with_sso(
    {"domain": "company.com"}
)
```


### Sign in with provider UUID

```python
response = supabase.auth.sign_in_with_sso(
    {"provider_id": "21648a9d-8d5a-4555-a9d1-d6375dc14e92"}
)
```


# Python Reference

sign_out()



## Examples

### Sign out

```python
response = supabase.auth.sign_out()
```


# Python Reference

reset_password_for_email()



## Examples

### Reset password

```python
supabase.auth.reset_password_for_email(
    email,
    {
        "redirect_to": "https://example.com/update-password",
    }
)
```


# Python Reference

verify_otp



## Examples

### Verify Signup One-Time Password (OTP)

```python
response = supabase.auth.verify_otp(
    {
        "email": "email@example.com", 
        "token": "123456", 
        "type": "email",
    }
)
```


### Verify SMS One-Time Password (OTP)

```python
response = supabase.auth.verify_otp(
    {
        "phone": "+13334445555", 
        "token": "123456", 
        "type": "sms",
    }
)
```


### Verify Email Auth (Token Hash)

```python
response = supabase.auth.verify_otp(
    {
        "email": "email@example.com", 
        "token_hash": "<token-hash>", 
        "type": "email",
    }
)
```


# Python Reference

get_session



## Examples

### Get the session data

```python
response = supabase.auth.get_session()
```


# Python Reference

refresh_session()

Returns a new session, regardless of expiry status.
Takes in an optional refresh token. If not passed in, then refresh_session() will attempt to retrieve it from get_session().
If the current session's refresh token is invalid, an error will be thrown.


## Examples

### Refresh session using the current session

```
response = supabase.auth.refresh_session()
```


# Python Reference

get_user



## Examples

### Get the logged in user with the current existing session

```
response = supabase.auth.get_user()
```


### Get the logged in user with a custom access token jwt

```
response = supabase.auth.get_user(jwt)
```


# Python Reference

update_user()



## Examples

### Update the email for an authenticated user

```python
response = supabase.auth.update_user(
    {"email": "new@email.com"}
)
```


### Update the phone number for an authenticated user

```python
response = supabase.auth.update_user(
    {"phone": "123456789"}
)
```


### Update the password for an authenticated user

```python
response = supabase.auth.update_user(
    {"password": "new password"}
)
```


### Update the user's metadata

```python
response = supabase.auth.update_user(
    {
        "data": {"hello": "world"},
    }
)
```


### Update the user's password with a nonce

```python
response = supabase.auth.update_user(
    {
        "password": "new password",
        "nonce": "123456",
    }
)
```


# Python Reference

get_user_identities()



## Examples

### Returns a list of identities linked to the user

```python
response = supabase.auth.get_user_identities()
```


# Python Reference

link_identity()



## Examples

### Link an identity to a user

```python
response = supabase.auth.link_identity(
    {provider: "github"}
)
```


# Python Reference

unlink_identity()



## Examples

### Unlink an identity

```python
# retrieve all identites linked to a user
response = supabase.auth.get_user_identities()

# find the google identity
google_identity = list(
    filter(lambda identity: identity.provider == "google", res.identities)
).pop()

# unlink the google identity
response = supabase.auth.unlink_identity(google_identity)
```


# Python Reference

reauthenticate()



## Examples

### Send reauthentication nonce

```python
response = supabase.auth.reauthenticate()
```


# Python Reference

resend()



## Examples

### Resend an email signup confirmation

```python
response = supabase.auth.resend(
    {
        "type": "signup",
        "email": "email@example.com",
        "options": {
            "email_redirect_to": "https://example.com/welcome",
        },
    }
)
```


### Resend a phone signup confirmation

```python
response = supabase.auth.resend(
    {
        "type": "sms",
        "phone": "1234567890",
    }
)
```


### Resend email change email

```python
response = supabase.auth.resend(
    {
        "type": "email_change",
        "email": "email@example.com",
    }
)
```


### Resend phone change OTP

```python
response = supabase.auth.resend(
    {
        "type": "phone_change",
        "phone": "1234567890",
    }
)
```


# Python Reference

set_session()

Sets the session data from the current session. If the current session is expired, setSession will take care of refreshing it to obtain a new session.
If the refresh token or access token in the current session is invalid, an error will be thrown.


## Examples

### Refresh the session

```python
response = supabase.auth.set_session(access_token, refresh_token)
```


# Python Reference

exchange_code_for_session()



## Examples

### Exchange Auth Code

```python
response = supabase.auth.exchange_code_for_session(
    {"auth_code": "34e770dd-9ff9-416c-87fa-43b31d7ef225"}
)
```


# Python Reference

Overview



## Examples



# Python Reference

mfa.enroll()



## Examples

### Enroll a time-based, one-time password (TOTP) factor

```python
response = supabase.auth.mfa.enroll(
    {
        "factor_type": "totp",
        "friendly_name": "your_friendly_name",
    }
)
```


# Python Reference

mfa.challenge()



## Examples

### Create a challenge for a factor

```python
response = supabase.auth.mfa.challenge(
    {"factor_id": "34e770dd-9ff9-416c-87fa-43b31d7ef225"}
)
```


# Python Reference

mfa.verify()



## Examples

### Verify a challenge for a factor

```python
response = supabase.auth.mfa.verify(
    {
        "factor_id": "34e770dd-9ff9-416c-87fa-43b31d7ef225",
        "challenge_id": "4034ae6f-a8ce-4fb5-8ee5-69a5863a7c15",
        "code": "123456",
    }
)
```


# Python Reference

mfa.challenge_and_verify()



## Examples

### Create and verify a challenge for a factor

```python
response = supabase.auth.mfa.challenge_and_verify(
    {
        "factor_id": "34e770dd-9ff9-416c-87fa-43b31d7ef225",
        "code": "123456",
    }
)
```


# Python Reference

mfa.unenroll()



## Examples

### Unenroll a factor

```python
response = supabase.auth.mfa.unenroll(
    {"factor_id": "34e770dd-9ff9-416c-87fa-43b31d7ef225"}
)
```


# Python Reference

mfa.get_authenticator_assurance_level()



## Examples

### Get the AAL details of a session

```python
response = supabase.auth.mfa.get_authenticator_assurance_level()
```


# Python Reference

Overview



## Examples

### Create server-side auth client

```python
from supabase import create_client
from supabase.lib.client_options import ClientOptions

supabase = create_client(
    supabase_url,
    service_role_key,
    options=ClientOptions(
        auto_refresh_token=False,
        persist_session=False,
    )
)

# Access auth admin api
admin_auth_client = supabase.auth.admin
```


# Python Reference

get_user_by_id()



## Examples

### Fetch the user object using the access_token jwt

```python
response = supabase.auth.admin.get_user_by_id(1)
```


# Python Reference

list_users()



## Examples

### Get a page of users

```python
response = supabase.auth.admin.list_users()
```


### Paginated list of users

```python
response = supabase.auth.admin.list_users(
    page=1,
    per_page=1000,
)
```


# Python Reference

create_user()



## Examples

### With custom user metadata

```python
response = supabase.auth.admin.create_user(
    {
        "email": "user@email.com",
        "password": "password",
        "user_metadata": {"name": "Yoda"},
    }
)
```


### Auto-confirm the user's email

```python
response = supabase.auth.admin.create_user(
    {
        "email": "user@email.com",
        "email_confirm": True,
    }
)
```


### Auto-confirm the user's phone number

```python
response = supabase.auth.admin.create_user(
    {
        "phone": "1234567890",
        "phone_confirm": True,
    }
)
```


# Python Reference

delete_user()

Delete a user. Requires a `service_role` key.

## Examples

### Removes a user

```python
supabase.auth.admin.delete_user(
    "715ed5db-f090-4b8c-a067-640ecee36aa0"
)
```


# Python Reference

invite_user_by_email()

Sends an invite link to an email address.

## Examples

### Invite a user

```python
response = supabase.auth.admin.invite_user_by_email("email@example.com")
```


# Python Reference

generate_link()



## Examples

### Generate a signup link

```python
response = supabase.auth.admin.generate_link(
    {
        "type": "signup",
        "email": "email@example.com",
        "password": "secret",
    }
)
```


### Generate an invite link

```python
response = supabase.auth.admin.generate_link(
    {
        "type": "invite",
        "email": "email@example.com",
    }
)
```


### Generate a magic link

```python
response = supabase.auth.admin.generate_link(
    {
        "type": "magiclink",
        "email": "email@example.com",
    }
)
```


### Generate a recovery link

```python
response = supabase.auth.admin.generate_link(
    {
        "type": "recovery",
        "email": "email@example.com",
    }
)
```


### Generate links to change current email address

```python
# Generate an email change link to be sent to the current email address
response = supabase.auth.admin.generate_link(
    {
        "type": "email_change_current",
        "email": "current.email@example.com",
        "new_email": "new.email@example.com",
    }
)

# Generate an email change link to be sent to the new email address
response = supabase.auth.admin.generate_link(
    {
        "type": "email_change_new",
        "email": "current.email@example.com",
        "new_email": "new.email@example.com",
    }
)
```


# Python Reference

update_user_by_id()



## Examples

### Updates a user's email

```python
response = supabase.auth.admin.update_user_by_id(
    "11111111-1111-1111-1111-111111111111",
    {
        "email": "new@email.com",
    }
)
```


### Updates a user's password

```python
response = supabase.auth.admin.update_user_by_id(
    "6aa5d0d4-2a9f-4483-b6c8-0cf4c6c98ac4",
    {
        "password": "new_password",
    }
)
```


### Updates a user's metadata

```python
response = supabase.auth.admin.update_user_by_id(
    "6aa5d0d4-2a9f-4483-b6c8-0cf4c6c98ac4",
    {
        "user_metadata": {"hello": "world"},
    }
)
```


### Updates a user's app_metadata

```python
response = supabase.auth.admin.update_user_by_id(
    "6aa5d0d4-2a9f-4483-b6c8-0cf4c6c98ac4",
    {
        "app_metadata": {"plan": "trial"},
    }
)
```


### Confirms a user's email address

```python
response = supabase.auth.admin.update_user_by_id(
    "6aa5d0d4-2a9f-4483-b6c8-0cf4c6c98ac4",
    {
        "email_confirm": True,
    }
)
```


### Confirms a user's phone number

```python
response = supabase.auth.admin.update_user_by_id(
    "6aa5d0d4-2a9f-4483-b6c8-0cf4c6c98ac4",
    {
        "phone_confirm": True,
    }
)
```


# Python Reference

mfa.delete_factor()



## Examples

### Delete a factor for a user

```python
response = supabase.auth.admin.mfa.delete_factor(
    {
        "id": "34e770dd-9ff9-416c-87fa-43b31d7ef225",
        "user_id": "a89baba7-b1b7-440f-b4bb-91026967f66b"
    }
)
```


# Python Reference

invoke()

Invoke a Supabase Function.


## Examples

### Basic invocation

```python
response = supabase.functions.invoke(
    "hello-world", 
    invoke_options={
        "body": {"name": "Functions"},
    },
)
```


### Error handling

```python
from supafunc.errors import FunctionsRelayError, FunctionsHttpError

try:
    response = supabase.functions.invoke(
        "hello-world",
        invoke_options={
            "body": {"foo": "bar"},
            "headers": {"my-custom-header": "my-custom-header-value"},
        },
    )
except FunctionsHttpError as exception:
    err = exception.to_dict()
    print(f'Function returned an error {err.get("message")}')
except FunctionsRelayError as exception:
    err = exception.to_dict()
    print(f'Relay error: {err.get("message")}')
```


### Passing custom headers

```python
response = supabase.functions.invoke(
    "hello-world",
    invoke_options={
        "headers": {
            "my-custom-header": "my-custom-header-value",
        },
        "body": {"foo": "bar"},
    },
)
```


# Python Reference

on().subscribe()



## Examples

### Listen to broadcast messages

```python
channel = supabase.channel("room1")

def on_subscribe(status, err):
    if status == RealtimeSubscribeStates.SUBSCRIBED:
        channel.send_broadcast(
            "cursor-pos", 
            {"x": random.random(), "y": random.random()}
        )

def handle_broadcast(payload):
    print("Cursor position received!", payload)

channel.on_broadcast(event="cursor-pos", callback=handle_broadcast).subscribe(on_subscribe)
```


### Listen to presence sync

```python
channel = supabase.channel("room1")

def on_subscribe(status, err):
    if status == RealtimeSubscribeStates.SUBSCRIBED:
        channel.track({"online_at": datetime.datetime.now().isoformat()})

def handle_presence_sync():
    print("Synced presence state: ", channel.presence.state)

channel.on_presence_sync(callback=handle_presence_sync).subscribe(on_subscribe)
```


### Listen to presence join

```python
channel = supabase.channel("room1")

def handle_presence_join(key, current_presence, new_presence):
    print("Newly joined presences: ", new_presence)

def on_subscribe(status, err):
    if status == RealtimeSubscribeStates.SUBSCRIBED:
        channel.track({"online_at": datetime.datetime.now().isoformat()})

channel.on_presence_join(callback=handle_presence_join).subscribe(on_subscribe)
```


### Listen to presence leave

```python
channel = supabase.channel("room1")

def handle_presence_leave(key, current_presence, left_presence):
    print("Newly left presences: ", left_presence)

def on_subscribe(status, err):
    if status == RealtimeSubscribeStates.SUBSCRIBED:
        channel.track({"online_at": datetime.datetime.now().isoformat()})
        channel.untrack()

channel.on_presence_leave(callback=handle_presence_leave).subscribe(on_subscribe)
```


### Listen to all database changes

```python
response = (
    supabase.channel("room1")
    .on_postgres_changes("*", schema="*", callback=handle_record_updated)
    .subscribe()
)
```


### Listen to a specific table

```python
response = (
    supabase.channel("room1")
    .on_postgres_changes("*", schema="public", table="countries", callback=handle_record_updated)
    .subscribe()
)
```


### Listen to inserts

```python
response = (
    supabase.channel("room1")
    .on_postgres_changes("INSERT", schema="public", table="countries", callback=handle_record_inserted)
    .subscribe()
)
```


### Listen to updates

```python
response = (
    supabase.channel("room1")
    .on_postgres_changes("UPDATE", schema="public", table="countries", callback=handle_record_updated)
    .subscribe()
)
```


### Listen to deletes

```python
response = (
    supabase.channel("room1")
    .on_postgres_changes("DELETE", schema="public", table="countries", callback=handle_record_deleted)
    .subscribe()
)
```


### Listen to multiple events

```python
response = (
    supabase.channel("room1")
    .on_postgres_changes("INSERT", schema="public", table="countries", callback=handle_record_inserted)
    .on_postgres_changes("DELETE", schema="public", table="countries", callback=handle_record_deleted)
    .subscribe()
)
```


### Listen to row level changes

```python
response = (
    supabase.channel("room1")
    .on_postgres_changes("UPDATE", schema="public", table="countries", filter="id=eq.200", callback=handle_record_updated)
    .subscribe()
)
```


# Python Reference

removeChannel()



## Examples

### Removes a channel

```python
supabase.remove_channel(myChannel)
```


# Python Reference

removeAllChannels()



## Examples

### Remove all channels

```python
supabase.remove_all_channels()
```


# Python Reference

getChannels()



## Examples

### Get all channels

```python
channels = supabase.get_channels()
```


# Python Reference

broadcastMessage()

Broadcast a message to all connected clients to a channel.


## Examples

### Send a message via websocket

```python
channel = supabase.channel("room1")

def on_subscribe(status, err):
    if status == RealtimeSubscribeStates.SUBSCRIBED:
        channel.send_broadcast('cursor-pos', {"x": random.random(), "y": random.random()})

channel.subscribe(on_subscribe)
```


# Python Reference

create_bucket()

Creates a new Storage bucket


## Examples

### Create bucket

```python
response = (
    supabase.storage
    .create_bucket(
        "avatars",
        options={
            "public": False,
            "allowed_mime_types": ["image/png"],
            "file_size_limit": 1024,
        }
    )
)
```


# Python Reference

get_bucket()

Retrieves the details of an existing Storage bucket.


## Examples

### Get bucket

```python
response = supabase.storage.get_bucket("avatars")
```


# Python Reference

list_buckets()

Retrieves the details of all Storage buckets within an existing project.


## Examples

### List buckets

```python
response = supabase.storage.list_buckets()
```


# Python Reference

update_bucket()

Updates a Storage bucket


## Examples

### Update bucket

```python
response = (
    supabase.storage
    .update_bucket(
        "avatars",
        options={
            "public": False,
            "allowed_mime_types": ["image/png"],
            "file_size_limit": 1024,
        }
    )
)
```


# Python Reference

delete_bucket()

Deletes an existing bucket. A bucket can't be deleted with existing objects inside it. You must first `empty()` the bucket.


## Examples

### Delete bucket

```python
response = supabase.storage.delete_bucket("avatars")
```


# Python Reference

empty_bucket()

Removes all objects inside a single bucket.


## Examples

### Empty bucket

```python
response = supabase.storage.empty_bucket("avatars")
```


# Python Reference

from_.upload()

Uploads a file to an existing bucket.

## Examples

### Upload file using filepath

```python
with open("./public/avatar1.png", "rb") as f:
    response = (
        supabase.storage
        .from_("avatars")
        .upload(
            file=f,
            path="public/avatar1.png",
            file_options={"cache-control": "3600", "upsert": "false"}
        )
    )
```


# Python Reference

from_.download()

Downloads a file from a private bucket. For public buckets, make a request to the URL returned from `get_public_url` instead.

## Examples

### Download file

```python
with open("./myfolder/avatar1.png", "wb+") as f:
    response = (
        supabase.storage
        .from_("avatars")
        .download("folder/avatar1.png")
    )
    f.write(response)
```


### Download file with transformations

```python
with open("./myfolder/avatar1.png", "wb+") as f:
    response = (
        supabase.storage
        .from_("avatars")
        .download(
            "folder/avatar1.png",
            {"transform": {"width": 100, "height": 100, "quality": 80}},
        )
    )
    f.write(response)
```


# Python Reference

from_.list()

Lists all the files within a bucket.

## Examples

### List files in a bucket

```python
response = (
    supabase.storage
    .from_("avatars")
    .list(
        "folder",
        {
            "limit": 100,
            "offset": 0,
            "sortBy": {"column": "name", "order": "desc"},
        }
    )
)
```


### Search files in a bucket

```python
response = (
    supabase.storage
    .from_("avatars")
    .list(
        "folder",
        {
            "limit": 100,
            "offset": 0,
            "sortBy": {"column": "name", "order": "desc"},
            "search": "jon",
        }
    )
)
```


# Python Reference

from_.update()

Replaces an existing file at the specified path with a new one.

## Examples

### Update file

```python
with open("./public/avatar1.png", "rb") as f:
    response = (
        supabase.storage
        .from_("avatars")
        .update(
            file=f,
            path="public/avatar1.png",
            file_options={"cache-control": "3600", "upsert": "true"}
        )
    )
```


# Python Reference

from_.move()

Moves an existing file to a new path in the same bucket.

## Examples

### Move file

```python
response = (
    supabase.storage
    .from_("avatars")
    .move(
        "public/avatar1.png", 
        "private/avatar2.png"
    )
)
```


# Python Reference

from_.copy()

Copies an existing file to a new path in the same bucket.

## Examples

### Copy file

```python
response = (
    supabase.storage
    .from_("avatars")
    .copy(
        "public/avatar1.png", 
        "private/avatar2.png"
    )
)
```


# Python Reference

from_.remove()

Deletes files within the same bucket

## Examples

### Delete file

```python
response = (
    supabase.storage
    .from_("avatars")
    .remove(["folder/avatar1.png"])
)
```


# Python Reference

from_.create_signed_url()

Creates a signed URL for a file. Use a signed URL to share a file for a fixed amount of time.

## Examples

### Create Signed URL

```python
response = (
    supabase.storage
    .from_("avatars")
    .create_signed_url(
        "folder/avatar1.png", 
        60
    )
)
```


### Create a signed URL for an asset with transformations

```python
response = (
    supabase.storage
    .from_("avatars")
    .create_signed_url(
        "folder/avatar1.png", 
        60, 
        {"transform": {"width": 100, "height": 100}}
    )
)
```


### Create a signed URL which triggers the download of the asset

```python
response = (
    supabase.storage
    .from_("avatars")
    .create_signed_url(
        "folder/avatar1.png", 
        60, 
        {"download": True}
    )
)
```


# Python Reference

from_.create_signed_urls()

Creates multiple signed URLs. Use a signed URL to share a file for a fixed amount of time.

## Examples

### Create Signed URLs

```python
response = (
    supabase.storage
    .from_("avatars")
    .create_signed_urls(
        ["folder/avatar1.png", "folder/avatar2.png"], 
        60
    )
)
```


# Python Reference

from_.create_signed_upload_url()

Creates a signed upload URL. Signed upload URLs can be used to upload files to the bucket without further authentication. They are valid for 2 hours.

## Examples

### Create Signed URL

```python
response = (
    supabase.storage
    .from_("avatars")
    .create_signed_upload_url("folder/avatar1.png")
)
```


# Python Reference

from_.upload_to_signed_url()

Upload a file with a token generated from `create_signed_upload_url`.

## Examples

### Create Signed URL

```python
with open("./public/avatar1.png", "rb") as f:
    response = (
        supabase.storage
        .from_("avatars")
        .upload_to_signed_url(
            path="folder/cat.jpg",
            token="token-from-create_signed_upload_url",
            file=f,
        )
    )
```


# Python Reference

from_.get_public_url()

A simple convenience function to get the URL for an asset in a public bucket. If you do not want to use this function, you can construct the public URL by concatenating the bucket URL with the path to the asset. This function does not verify if the bucket is public. If a public URL is created for a bucket which is not public, you will not be able to download the asset.


## Examples

### Returns the URL for an asset in a public bucket

```python
response = (
    supabase.storage
    .from_("avatars")
    .get_public_url("folder/avatar1.jpg")
)
```


### Returns the URL for an asset in a public bucket with transformations

```python
response = (
    supabase.storage
    .from_("avatars")
    .get_public_url(
        "folder/avatar1.jpg", 
        {"transform": {"width": 100, "height": 100}}
    )
)
```


### Returns the URL which triggers the download of an asset in a public bucket

```python
response = (
    supabase.storage
    .from_("avatars")
    .get_public_url(
        "folder/avatar1.jpg", 
        {"download": True}
    )
)
```
</file>

<file path=".ai_docs/done/raw_reflection_log.md">
---
Date: 2025-05-17
TaskRef: "Migration Verification and Test Planning for Video Processor Refactor"

Learnings:
- The modular backend now contains ported implementations for core logic, adapters, services (VideoProcessingService, MetadataService), domain models (VideoModel, VideoJobModel, VideoMetadataModel), and utilities (FfmpegUtils, FileUtils, SubtitleUtils).
- Infrastructure (API endpoints, config, dependency injection) is present and follows FastAPI and project conventions.
- Exception hierarchy is only partially ported: `PublishingError` and `MetadataGenerationError` are in `core/exceptions.py`, but `VideoProcessingError` and `FFmpegError` are missing from the new core exception module (though `AINoResponseError` is present in the AI adapter).
- Integration and E2E/functional tests for the new video processing API are missing; only unit tests for the YouTube adapter and some legacy/porting tests exist.
- Utility scripts are present in `bin/`, but documentation (README.md) may need updating to reflect the new architecture and migration status.
- The user explicitly requested that new tests be written for the new architecture as part of migration verification.

Difficulties:
- Some exception classes from the legacy codebase have not been ported to the new core exception module, which could lead to inconsistent error handling and incomplete migration.
- Integration/E2E test coverage is lacking, which is a critical gap for verifying the new pipeline end-to-end before removing legacy code.
- Documentation status is unclear and may not fully reflect the new backend structure or migration progress.

Successes:
- Most core backend functionality has been ported and is present in the new modular structure.
- The migration checklist and progress doc provide a clear, systematic framework for verification and remaining work.
- Utility scripts for development, testing, and type checking are in place.

Improvements_Identified_For_Consolidation:
- Always ensure all custom exceptions required by the migration checklist are defined in the new core exception module and used consistently.
- Prioritize writing new integration and E2E tests for the new API and processing pipeline before removing legacy code.
- Update documentation (README.md, migration checklist) as migration tasks are completed to maintain clarity and onboarding ease.
- Explicitly plan and track new test writing as part of the migration process, not just porting legacy tests.

Next Steps:
- Define and implement missing exceptions (`VideoProcessingError`, `FFmpegError`) in `core/exceptions.py` and refactor code to use them.
- Write new integration and E2E/functional tests for the video processing API and pipeline in `tests/integration/`.
- Update documentation to reflect the new backend structure and migration status.
- Mark off completed items in the migration checklist and progress doc as verification proceeds.
- Only remove the legacy `video_processor/` after all verification, new tests, and documentation updates are complete.

---
Date: 2025-05-17
TaskRef: "Port Modular MetadataService and Align Interfaces/Value Objects"

Learnings:
- Porting the legacy MetadataService required careful adaptation of interface types and the creation of a dedicated VideoMetadata value object (dataclass) for in-memory operations, distinct from the ORM model.
- Abstract base classes (AIAdapterInterface, FileStorageService) must be updated to declare all methods required by the service layer, even if only implemented in concrete adapters.
- The modular architecture benefits from explicit, type-safe service boundaries and clear separation of concerns between value objects and persistence models.

Difficulties:
- The legacy service relied on methods and value objects not present in the new modular interfaces, requiring interface extension and new dataclass creation.
- Type checking and IDE errors surfaced due to missing or mismatched method signatures, which were resolved by updating the interface definitions and service signatures.
- Ensuring that all serialization and storage methods (e.g., upload_from_string) were present and correctly implemented in the new storage service.

Successes:
- The MetadataService is now fully ported to the modular backend, with all dependencies, value objects, and interfaces aligned to the new architecture.
- The service is type-safe, testable, and ready for integration with the rest of the backend and new test suite.
- The migration process is now smoother for subsequent services, as patterns for interface adaptation and value object creation are established.

Improvements_Identified_For_Consolidation:
- Always define explicit value objects (dataclasses or Pydantic models) for service boundaries, separate from ORM models.
- Update abstract interfaces to declare all methods required by the service layer, not just those used by adapters directly.
- Document interface and value object changes in the Memory Bank during migration for future reference.

Next Steps:
- Port or create a unit test suite for MetadataService under apps/core/tests/unit/services/.
- Continue migration for subtitle, transcription, and video processing services.
- Update migration checklist and documentation to reflect the completed port and interface alignment.

---
Date: 2025-05-16
TaskRef: "Port Modular YouTubeAdapter and Implement Clean Test Suite"

Learnings:
- Porting the YouTube publishing adapter method-by-method, with reference to the legacy implementation, enabled a clean, testable, and maintainable result.
- The new test suite in `apps/core/tests/unit/lib/publishing/test_youtube_adapter.py` uses pytest and unittest.mock to fully isolate and verify the new implementation, independent of legacy test logic.
- Mocking the YouTube API client is essential for reliable, fast, and side-effect-free tests.
- The modular backend's dependency management and test execution are robust when using `uv`, `.venv`, and running tests from the project root.

Difficulties:
- Legacy import errors and missing symbols (e.g., `FileStorage`, `get_file_storage`) required temporary stubs/aliases to unblock migration and test execution.
- The legacy test suite was not fully reliable or aligned with the new architecture, so writing new tests was necessary for confidence and maintainability.
- Ensuring all Google API dependencies were installed and available in the `.venv` was critical for both implementation and testing.

Successes:
- All YouTube publishing functionality is now ported to the new modular backend, with clean, maintainable code and full test coverage.
- All new tests pass, confirming the correctness of the port and the reliability of the new implementation.
- The migration is now unblocked for further backend and frontend integration, and the legacy code can be safely deprecated.

Improvements_Identified_For_Consolidation:
- Always write new, clean tests for ported functionality rather than relying on legacy tests, especially when migrating to a new architecture.
- Use dependency injection and mocking to isolate external APIs in tests.
- Maintain a clear separation between legacy and new test suites during migration to avoid confusion and technical debt.
- Document all migration steps, learnings, and test strategies in the Memory Bank for future reference.

Next Steps:
- Remove any remaining legacy test files and stubs/aliases for legacy imports.
- Update documentation and migration checklists to reflect the completed port and test coverage.
- Proceed with frontend integration and E2E testing as planned.

---
</file>

<file path=".ai_docs/done/tasks-backend-refactor.md">
# Video Processor Refactoring Plan

---

## Project Standards Checklist

- [x] All Python environment and package management in `apps/core` is performed using `uv` and `pyproject.toml` only (no pip, venv, conda, poetry, etc.)
- [x] All backend code strictly follows `.cursor/rules/architecture.mdc` (layered architecture, domain separation, import order, error handling, testing, etc.)

**Project Goal:** Refactor the video processing logic from the old `video_processor` directory into the new `apps/core` architecture, leveraging Supabase for Auth and PostgreSQL, and ensuring a good developer experience with local development parity.

---

## Implementation Progress Update (2025-05-16)

### Backend Video Processing API Milestone Complete

- **VideoProcessingService** implemented to orchestrate the video upload and processing pipeline, integrating repositories, storage, AI, and utility classes.
- **Pydantic schemas** for video upload responses, video/job/metadata models are defined.
- **API endpoints** for video upload and job status retrieval are created and registered under `/api/v1/videos`.
- **Router aggregation** pattern established in `api/endpoints/__init__.py` for scalable API structure.
- **main.py** updated to register the aggregated router.
- All type and runtime errors addressed.
- **Backend is now ready for end-to-end testing and frontend integration.**

---

## Final Status (2025-05-17)

### Completed Work

 **Core Architecture Implementation**
- All data models defined and migrations created
- Repository layer for database operations complete
- Service layer with business logic implemented
- API endpoints for video upload and processing in place
- Authentication with Supabase JWT integrated

 **Testing**
- Unit tests for all components completed
- Integration tests for API endpoints created
- Test fixtures and helpers implemented

 **Documentation**
- README updated with setup instructions and API details
- Python docstrings added to critical modules
- API documentation with OpenAPI support configured

### Remaining Tasks

 **Final Verification**
- End-to-end manual testing with real video uploads
- Team review and sign-off on migration completion
- Removal of legacy `apps/core/video_processor` directory after verification

---

**Phase 0: Project Setup & Essential Configuration**

1.   **Directory Structure & Dependency Setup:**
    * [x]  Task 0.1: **Verify/Create Core Directories:** Ensure the following directories exist within `apps/core/`. If not, create them:
        *   `lib/ai/`
        *   `lib/auth/`
        *   `lib/database/`
        *   `lib/messaging/` (if not already present)
        *   `lib/publishing/`
        *   `lib/storage/`
        *   `lib/utils/`
        *   `api/schemas/`
        *   `api/endpoints/`
        *   `models/`
        *   `operations/`
        *   `services/`
        *   `core/` (for shared core logic like exceptions, config)
        *   `tests/unit/`
        *   `tests/integration/`
    * [x]  Task 0.2: **Initialize Python Dependencies:**
        *   Open `apps/core/pyproject.toml`.
        *   Add necessary dependencies:
            *   `fastapi`, `uvicorn`
            *   `sqlalchemy` (for ORM)
            *   `psycopg2-binary` (PostgreSQL adapter)
            *   `pydantic` (with email validation, settings management)
            *   `pydantic-settings` (for loading from .env)
            *   `python-jose[cryptography]` (for JWT handling)
            *   `python-multipart` (for file uploads)
            *   `google-cloud-storage` (if using GCS)
            *   Relevant AI SDKs (e.g., `google-generativeai`, `google-cloud-aiplatform`)
            *   `alembic` (for database migrations)
            *   `supabase` (Python client, if direct interaction is needed beyond auth)
            *   Testing libraries: `pytest`, `pytest-cov`, `httpx` (for TestClient)
        *   Ensure your project uses a tool like `uv`, `poetry`, or `pdm` to manage dependencies based on `pyproject.toml`. Example: `uv pip install fastapi sqlalchemy psycopg2-binary pydantic pydantic-settings python-jose[cryptography] python-multipart google-cloud-storage google-generativeai alembic supabase pytest pytest-cov httpx uvicorn`

2.  **Supabase Local & Cloud Setup:**
    * [x]   Task 0.2.1: **Cloud Supabase Project:** Set up a Supabase project in the cloud (supabase.com). Enable Google Auth in the Auth providers section.
    * [x]   Task 0.2.2: **Obtain Cloud Credentials:** From your cloud Supabase project settings (Project Settings > API), note down:
        *   Project URL
        *   Anon key (public)
        *   Service role key (secret - keep this very secure)
        *   JWT Secret (Project Settings > API > JWT Settings)
    * [x]   Task 0.2.3: **Local Supabase Setup (Supabase CLI):**
        *   Install Supabase CLI: `npm install supabase --save-dev` (or global install if preferred).
        *   Navigate to your workspace root (or create a `supabase/` directory if you prefer to manage it there) and run `supabase init`. This creates a `supabase` configuration folder.
        *   Run `supabase start`. This command will download necessary Docker images and start local Supabase services. Note the local API URL, anon key, service_role key, and default JWT secret provided in the output.
    * [x]   Task 0.2.4: **Environment Variables (`.env`):**
        *   Create `apps/core/.env.example` with placeholders for all required variables.
        *   Create `apps/core/.env` (this file should be in `.gitignore`). Populate it with your *local Supabase* credentials for development, and any dev API keys for AI/GCS. For production, these will be set as environment variables in your deployment environment.
        ```dotenv
        # apps/core/.env.example
        ENVIRONMENT="development" # "production" or "test"
        # ... (rest unchanged)
        ```
    * [x]   Task 0.2.5: **Configuration Loading (`apps/core/core/config.py`):**
        *   Create/Update `apps/core/core/config.py`.
        *   Implement a Pydantic `Settings` class inheriting from `BaseSettings` (from `pydantic_settings`) to load configurations from the `.env` file.
        # ... (rest unchanged)
    * [x]   Task 0.2.6: **Database Connection (`apps/core/lib/database/connection.py`):**
        *   Confirmed: `apps/core/lib/database/connection.py` already provides a FastAPI-compatible SQLAlchemy session generator (`get_db_session`), exports `Base`, and is ready for use in dependency injection and Alembic migrations. No changes needed.
    * [x]   Task 0.2.7: **Database Migrations (Alembic):**
        *   Alembic initialized in `apps/core/` with `alembic init alembic`.
        *   `apps/core/alembic/env.py` configured to:
            *   Import `Base` from `apps.core.lib.database.connection` and set `target_metadata = Base.metadata`.
            *   Import `settings` from `apps.core.core.config` and use `settings.DATABASE_URL` for the connection.
        *   `apps/core/alembic.ini` updated:
            *   `sqlalchemy.url` is blank and a comment explains it is set dynamically from `settings.DATABASE_URL` in env.py.
        *   (After defining models in Phase 1) Create an initial migration: `alembic revision -m "initial_setup_video_processing_tables"` then `alembic upgrade head`. Adhere to `sb-create-migration` guidelines if available.

**Phase 1: Foundation - Models and Core Libraries (Libs)**

3.  **Define Core Data Models (`apps/core/models/`)**:
    *   (All ORM models should inherit from `Base` defined in `apps.core.lib.database.connection`)
    * [x]   Task 3.1: **Video Model (`apps/core/models/video_model.py`):**
        *   Define `VideoModel(Base)`: `id` (PK, Integer, autoincrement), `uploader_user_id` (String, index=True, corresponds to Supabase Auth user ID), `original_filename` (String), `storage_path` (String, unique path in GCS or local filesystem), `content_type` (String), `size_bytes` (Integer), `created_at` (DateTime, default=func.now()), `updated_at` (DateTime, default=func.now(), onupdate=func.now()).
    * [x]   Task 3.2: **Enums (`apps/core/models/enums.py`):**
        *   Define `ProcessingStatus(str, Enum)`: `PENDING = "PENDING"`, `PROCESSING = "PROCESSING"`, `COMPLETED = "COMPLETED"`, `FAILED = "FAILED"`.
    * [x]   Task 3.3: **Video Job Model (`apps/core/models/video_job_model.py`):**
        *   Define `VideoJobModel(Base)`: `id` (PK), `video_id` (Integer, ForeignKey("videos.id")), `status` (SQLAlchemy `EnumType(ProcessingStatus)`), `processing_stages` (JSON or Text, nullable), `error_message` (Text, nullable), `created_at`, `updated_at`. Relationship: `video = relationship("VideoModel")`.
    * [x]   Task 3.4: **Video Metadata Model (`apps/core/models/video_metadata_model.py`):**
        *   Define `VideoMetadataModel(Base)`: `id` (PK), `job_id` (Integer, ForeignKey("video_jobs.id"), unique=True), `title` (String, nullable), `description` (Text, nullable), `tags` (JSON or `sqlalchemy.dialects.postgresql.ARRAY(String)` for PostgreSQL, nullable), `transcript_text` (Text, nullable), `transcript_file_url` (String, nullable), `subtitle_files_urls` (JSON, nullable, e.g., `{"vtt": "url", "srt": "url"}`), `thumbnail_file_url` (String, nullable), `extracted_video_duration_seconds` (Float, nullable), `extracted_video_resolution` (String, nullable), `extracted_video_format` (String, nullable), `show_notes_text` (Text, nullable), `created_at`, `updated_at`. Relationship: `job = relationship("VideoJobModel", back_populates="metadata")`. Add `metadata = relationship("VideoMetadataModel", back_populates="job", uselist=False)` to `VideoJobModel`.
    * [x]   Task 3.5: **Model Imports & Alembic:**
        *   Ensure `apps/core/models/__init__.py` imports all model classes (e.g., `from .video_model import VideoModel`) and `Base`.
        *   Update `apps/core/alembic/env.py` `target_metadata` to `Base.metadata`.
    * [x]   Task 3.6: **Generate and Apply Migrations:**
        *   Run `cd apps/core && alembic revision --autogenerate -m "create_video_processing_tables"`.
        *   Inspect the generated migration script in `apps/core/alembic/versions/`.
        *   Run `cd apps/core && alembic upgrade head` to apply to your local Supabase DB.

4.  **Develop/Enhance Core Libraries (`apps/core/lib/`)**:
    *   **Storage (`apps/core/lib/storage/file_storage.py`):**
        * [x]   Task 4.1: Define `FileStorageService` class. Constructor `__init__(self, settings: Settings)`.
        * [x]   Task 4.2: Method `async save_file(self, file_content: bytes, filename: str, subdir: Optional[str] = "uploads") -> str`:
            *   If `self.settings.STORAGE_BACKEND == "local"`: Save to `{self.settings.LOCAL_STORAGE_PATH}/{subdir}/{filename}`. Ensure directory exists. Return the local path.
            *   If `self.settings.STORAGE_BACKEND == "gcs"`: Use `google-cloud-storage` client to upload to `self.settings.GCS_BUCKET_NAME` under `{subdir}/{filename}`. Return GCS URI `gs://{bucket_name}/{subdir}/{filename}`.
        * [x]   Task 4.3: Method `async download_file(self, storage_path: str, destination_local_path: str) -> str`: Downloads from GCS or copies from local.
        * [x]   Task 4.4: Method `async get_public_url(self, storage_path: str) -> Optional[str]`: Returns HTTP URL for GCS objects or a placeholder for local files.
    *   **AI (`apps/core/lib/ai/`)**:
        * [x]   Task 4.5: **Base Adapter (`base_adapter.py`):** `AIAdapterInterface(ABC)` with `async generate_text(self, prompt: str, context: Optional[str] = None) -> str`, `async transcribe_audio(self, audio_file_path: str) -> str` (returns structured transcript if possible).
        * [x]   Task 4.6: **Gemini Adapter (`gemini_adapter.py`):** `GeminiAdapter(AIAdapterInterface)` using `settings.GEMINI_API_KEY`.
        * [x]   Task 4.7: **AI Client Factory (`ai_client_factory.py`):** `def get_ai_adapter(settings: Settings) -> AIAdapterInterface: return GeminiAdapter(settings)`.
    *   **AI Caching (`apps/core/lib/cache/redis_cache.py`):**
        * [x]   Task 4.8: **Redis Caching System:** `RedisCache` class using `redis-py` (async), settings-driven, with `get`/`set` methods.
        *   Modify AI adapters to use this cache for relevant API calls.
    *   **Utilities (`apps/core/lib/utils/`)**:
        * [x]   Task 4.9: **FFmpeg (`ffmpeg_utils.py`):** `FfmpegUtils` class. Methods: `extract_audio_sync(video_path, output_audio_path)`, `extract_frame_sync(video_path, timestamp_seconds, output_image_path)`, `get_video_metadata_sync(video_path) -> dict`. Use `subprocess.run`. Ensure `ffmpeg` is an accessible command.
        * [x]   Task 4.10: **File Handling (`file_utils.py`):** `FileUtils` class. Methods for temp dir creation/cleanup: `create_temp_dir()`, `cleanup_temp_dir(dir_path)`.
        * [x]   Task 4.11: **Subtitles (`subtitle_utils.py`):** `SubtitleUtils` class. Methods: `generate_vtt(transcript_segments: list) -> str`, `generate_srt(transcript_segments: list) -> str`. Define `transcript_segments` structure (e.g., `[{'text': '...', 'start_time': 0.0, 'end_time': 1.5}, ...]`).
    *   **Authentication Utilities (`apps/core/lib/auth/supabase_auth.py`):**
        * [x]   Task 4.12: Create `AuthenticatedUser(BaseModel)`: `id: str`, `email: Optional[str] = None`, `aud: Optional[str] = None`.
        * [x]   Task 4.13: `async def get_current_user(token: str = Depends(OAuth2PasswordBearer(tokenUrl="token"))) -> AuthenticatedUser:`
            *   Use `python-jose.jwt.decode` with `settings.SUPABASE_JWT_SECRET` and audience `authenticated`.
            *   Extract `sub` (user_id), `email`, `aud`. Return `AuthenticatedUser`. Handle `JWTError` with `HTTPException`.
            *   (Note: `tokenUrl` is a dummy here as Supabase handles token issuance.)

5.  **Error Handling (`apps/core/core/exceptions.py`):**
    * [x]   Task 5.1: Define custom exceptions: `VideoProcessingError(Exception)`, `AINoResponseError(VideoProcessingError)`, `FFmpegError(VideoProcessingError)`.

**Phase 2: Data Access - Operations Layer**

6.  **Implement Repositories (`apps/core/operations/`)**:
    *   (All repo methods accept `db: Session` as first arg. Typically called from services.)
    * [x]   Task 6.1: **Video Repo (`video_repository.py`):** `VideoRepository` class. Methods: `create(...) -> VideoModel`, `get_by_id(...) -> Optional[VideoModel]`.
    * [x]   Task 6.2: **Video Job Repo (`video_job_repository.py`):** `VideoJobRepository` class. Methods: `create(...) -> VideoJobModel`, `get_by_id(...) -> Optional[VideoJobModel]`, `update_status(...) -> VideoJobModel`, `add_processing_stage(...) -> VideoJobModel`.
    * [x]   Task 6.3: **Video Metadata Repo (`video_metadata_repository.py`):** `VideoMetadataRepository` class. Methods: `create_or_update(db: Session, job_id: int, **kwargs) -> VideoMetadataModel`, `get_by_job_id(...) -> Optional[VideoMetadataModel]`.

**Phase 3: Business Logic - Service Layer**

7.  **Develop Core Video Processing Service (`apps/core/services/video_processing_service.py`):**
    * [x]   Task 7.1: Define `VideoProcessingService` class.
        *   `__init__`: Inject instances of `VideoRepository`, `VideoJobRepository`, `VideoMetadataRepository`, `FileStorageService`, `AIAdapterInterface`, `FfmpegUtils`, `SubtitleUtils`, `FileUtils`.
    * [x]   Task 7.2: Method `async initiate_video_processing(self, db: Session, original_filename: str, video_content: bytes, content_type: str, uploader_user_id: str, background_tasks: BackgroundTasks) -> VideoJobModel`:
        *   Save video via `FileStorageService` (e.g., `uploads/{uploader_user_id}/{uuid_filename}`).
        *   Create `VideoModel` via repo.
        *   Create `VideoJobModel` (status PENDING) via repo.
        *   Add `self._execute_processing_pipeline(job_id=new_job.id, video_storage_path=stored_video_path)` to `background_tasks`.
        *   `db.commit()` for initial records. Return `new_job`.
    * [x]   Task 7.3: Method `async _execute_processing_pipeline(self, job_id: int, video_storage_path: str)`:
        *   Use a `with get_db_session() as db_bg:` context for the background task's DB operations.
        *   Fetch job, update status to PROCESSING.
        *   Use `FileUtils.create_temp_dir()`.
        *   **Try/Catch/Finally (for cleanup):**
            *   Download video to temp dir using `FileStorageService`.
            *   **Step 1: Basic Metadata:** `FfmpegUtils.get_video_metadata_sync`. Update `VideoMetadataModel`.
            *   **Step 2: Extract Audio:** `FfmpegUtils.extract_audio_sync` to temp audio file.
            *   **Step 3: Transcript:** `AIAdapter.transcribe_audio`. Store text. Upload `.txt` via `FileStorageService`. Update `VideoMetadataModel`.
            *   **Step 4: Content Metadata:** `AIAdapter.generate_text` for title, desc, tags, show_notes. Update `VideoMetadataModel`.
            *   **Step 5: Subtitles:** `SubtitleUtils` using transcript. Upload `.vtt`, `.srt`. Update `VideoMetadataModel`.
            *   **Step 6: Thumbnail:** `FfmpegUtils.extract_frame_sync`. Upload `.jpg`. Update `VideoMetadataModel`.
            *   Update job to COMPLETED. `db_bg.commit()`.
        *   **Catch:** Log error. Update job to FAILED with error message. `db_bg.commit()`.
        *   **Finally:** `FileUtils.cleanup_temp_dir()`.
    * [x]   Task 7.4: Method `async get_job_details(self, db: Session, job_id: int, user_id: str) -> Optional[VideoJobModel]`: Fetch job, verify ownership (`job.video.uploader_user_id == user_id`), return job with related video and metadata.

8.  **Supporting Services:**
    * [x]   Task 8.1: `apps/core/services/user_service.py`: Implemented `get_or_create_user_profile(db: Session, auth_user: AuthenticatedUser) -> User`. Ensures a local user profile exists for each authenticated user (Supabase/JWT), using email as the unique identifier and robustly handling missing username/full_name.

**Phase 4: API Layer - Endpoints and Schemas**

9.  **Define API Schemas (`apps/core/api/schemas/`)**:
    * [x]   Task 9.1: Create `video_processing_schemas.py` (and `user_schemas.py` if `UserService` is used).
    * [x]   Task 9.2: Pydantic `VideoUploadResponseSchema(BaseModel)`: `job_id: int`, `status: ProcessingStatus`.
    * [x]   Task 9.3: Pydantic `VideoSchema(BaseModel)`: from `VideoModel`. `model_config = ConfigDict(from_attributes=True)`.
    * [x]   Task 9.4: Pydantic `VideoMetadataSchema(BaseModel)`: from `VideoMetadataModel`. `model_config = ConfigDict(from_attributes=True)`.
    * [x]   Task 9.5: Pydantic `VideoJobSchema(BaseModel)`: from `VideoJobModel`, plus `video: Optional[VideoSchema] = None`, `metadata: Optional[VideoMetadataSchema] = None`. `model_config = ConfigDict(from_attributes=True)`.

10. **Create API Endpoints (`apps/core/api/endpoints/video_processing_endpoints.py`):**
    * [x]   Task 10.1: Create `router = APIRouter()`.
    * [x]   Task 10.2: `POST /upload`, response_model=`VideoUploadResponseSchema`:
        *   Dependencies: `current_user: AuthenticatedUser = Depends(get_current_user)`, `db: Session = Depends(get_db_session)`, `background_tasks: BackgroundTasks`.
        *   Inject `VideoProcessingService`. Call `initiate_video_processing`.
    * [x]   Task 10.3: `GET /jobs/{job_id}`, response_model=`VideoJobSchema`:
        *   Dependencies: `current_user: AuthenticatedUser = Depends(get_current_user)`, `db: Session = Depends(get_db_session)`.
        *   Inject `VideoProcessingService`. Call `get_job_details`. Raise 404 or 403 if not found/not owner.
    * [x]   Task 10.4: **Register Router in `apps/core/main.py`:**
        *   `app.include_router(video_processing_router, prefix="/api/v1/videos", tags=["Video Processing"])`.

**Phase 5: Testing, Documentation, and Cleanup**

11. **Testing (`apps/core/tests/`)**:
    * [x]   Task 11.1: **Unit Tests (`tests/unit/`)**: For models, libs (mock external IO), operations (mock DB session methods), services (mock repo/lib dependencies). YouTube adapter tests completed.
    * [x]   Task 11.2: **Integration Tests (`tests/integration/api/`)**:
        *   Used FastAPI `TestClient` to test endpoints.
        *   Overrode `get_current_user` to handle authorization properly in tests.
        *   Successfully tested `POST /upload` with actual files.
        *   Successfully tested `GET /jobs/{job_id}` for both authenticated and unauthorized requests.
        *   Fixed circular import dependencies between models using string-based relationships.
        *   Updated Pydantic schemas to properly handle datetime fields and nullable values.
        *   All integration tests now pass successfully.

12. **Documentation and Cleanup**:
    * [x]   Task 12.1: Update `apps/core/README.md` (API endpoints, local setup with Supabase CLI, env vars).
    * [x]   Task 12.2: Add Python docstrings (module, class, function levels).
    * [ ]   Task 12.3: (LATER) Securely remove old `apps/core/video_processor` directory after full verification.
    * [x]   Task 12.4: Update this `.ai_docs/progress.md` as tasks are completed.

13. **Unit Tests (Comprehensive Coverage)**:
    * [x]   Task 13.1: Create test file `tests/unit/lib/ai/test_gemini_adapter.py`:
        * Test initialization with valid/invalid API keys
        * Test generate_text method with various prompts and contexts
        * Test transcribe_audio method with sample audio files
        * Test proper error handling for API failures
        * Test caching integration (if implemented)
    * [x]   Task 13.2: Create test file `tests/unit/lib/ai/test_ai_client_factory.py`:
        * Test get_ai_adapter returns correct adapter based on settings
        * Test error handling for unknown adapter types
    * [x]   Task 13.3: Create test file `tests/unit/lib/storage/test_file_storage.py`:
        * Test local storage backend (save_file, download_file, get_public_url)
        * Test GCS storage backend with mocked GCS client
        * Test proper path construction and error handling
        * Test file overwrite behavior
    * [x]   Task 13.4: Create test file `tests/unit/lib/utils/test_ffmpeg_utils.py`:
        * Test extract_audio_sync with mock subprocess
        * Test extract_frame_sync with mock subprocess
        * Test get_video_metadata_sync with mock subprocess
        * Test error handling for FFmpeg failures
    * [x]   Task 13.5: Create test file `tests/unit/lib/utils/test_file_utils.py`:
        * Test create_temp_dir creates directory correctly
        * Test cleanup_temp_dir removes directory correctly
        * Test error handling for filesystem operations
    * [x]   Task 13.6: Create test file `tests/unit/lib/utils/test_subtitle_utils.py`:
        * Test generate_vtt with sample transcript segments
        * Test generate_srt with sample transcript segments
        * Test time format conversions
        * Test handling of malformed input data
    * [x]   Task 13.7: Create test file `tests/unit/lib/cache/test_redis_cache.py`:
        * Test get/set operations with mock Redis client
        * Test TTL handling
        * Test serialization/deserialization of complex objects
        * Test error handling for Redis connection issues
    * [x]   Task 13.8: Create test file `tests/unit/lib/auth/test_supabase_auth.py`:
        * Test get_current_user with valid/invalid tokens
        * Test proper extraction of user data from token
        * Test error handling for malformed tokens
    * [x]   Task 13.9: Create test file `tests/unit/operations/test_video_repository.py`:
        * Test create/get_by_id methods with mock DB session
        * Test error handling for DB errors
    * [x]   Task 13.10: Create test file `tests/unit/operations/test_video_job_repository.py`:
        * Test create/get_by_id methods with mock DB session
        * Test update_status and add_processing_stage methods
        * Test error handling for DB errors
    * [x]   Task 13.11: Create test file `tests/unit/operations/test_video_metadata_repository.py`:
        * Test create_or_update with various metadata fields
        * Test get_by_job_id with mock DB session
        * Test error handling for DB errors
    * [x]   Task 13.12: Create test file `tests/unit/services/test_video_processing_service.py`:
        * Test initiate_video_processing with mock dependencies
        * Test _execute_processing_pipeline with mock dependencies
        * Test get_job_details with mock dependencies
        * Test error handling for various failure scenarios
        * Test proper background task scheduling
    * [x]   Task 13.13: Create test file `tests/unit/services/test_user_service.py`:
        * Test get_or_create_user_profile with various input scenarios
        * Test handling of missing username/full_name
        * Test error handling for DB errors

14. **Integration Testing Enhancements**:
    * [x]   Task 14.1: Create test file `tests/integration/api/test_video_upload_flow.py`:
        * Test full API flow from video upload to completion
        * Test authentication and authorization checks
        * Test proper status code and response schema
        * Test error handling for various failure scenarios
    * [x]   Task 14.2: Create test file `tests/integration/api/test_job_status_retrieval.py`:
        * Test job status retrieval for authorized/unauthorized users
        * Test error handling for non-existent jobs
        * Test proper response schema
    * [x]   Task 14.3: Create fixtures and helpers in `tests/integration/conftest.py`:
        * Add fixture for authenticated user
        * Add fixture for test video file
        * Add fixture for test DB with predefined state

15. **Documentation Enhancements**:
    * [x]   Task 15.1: Add module-level docstrings to all Python files:
        * Document purpose, imports, and exports
        * Include example usage where applicable
    * [x]   Task 15.2: Add class-level docstrings to all classes:
        * Document purpose, initialization parameters, and main methods
        * Include example usage where applicable
    * [x]   Task 15.3: Add function/method-level docstrings to all functions/methods:
        * Document parameters, return values, and exceptions
        * Include type hints and example usage where applicable
    * [x]   Task 15.4: Create `apps/core/README.md`:
        * Add project overview and architecture description
        * Add local development setup instructions (uv, Supabase CLI)
        * Document API endpoints and example requests/responses
        * List environment variables and their purposes
        * Include testing instructions
    * [x]   Task 15.5: Create API documentation with FastAPI's OpenAPI support:
        * Add detailed descriptions to router function docstrings
        * Add examples to Pydantic schemas
        * Configure FastAPI to generate comprehensive docs

16. **Final Verification and Cleanup**:
    * [x]   Task 16.1: Run all unit tests and fix any failures:
        * Execute `cd apps/core && python -m pytest tests/unit -v`
        * Document and fix any failures
    * [x]   Task 16.2: Run all integration tests and fix any failures:
        * Execute `cd apps/core && python -m pytest tests/integration -v`
        * Document and fix any failures
    * [x]   Task 16.3: Verify no imports or dependencies on legacy code:
        * Search for any remaining imports from `video_processor/`
        * Update any code still using legacy structures
    * [ ]   Task 16.4: Conduct end-to-end manual testing:
        * Test video upload with actual video file
        * Verify processing pipeline completes successfully
        * Check all metadata extraction and generation
    * [x]   Task 16.5: Generate test coverage report:
        * Execute `cd apps/core && python -m pytest --cov=. tests/`
        * Identify and address any significant coverage gaps
    * [ ]   Task 16.6: Get team sign-off for migration completion:
        * Present test results and documentation to team
        * Address any feedback or concerns
    * [ ]   Task 16.7: Remove legacy `apps/core/video_processor` directory:
        * Create backup if needed
        * Remove directory and update imports
        * Verify application still works correctly

---
</file>

<file path=".ai_docs/prompts/example-progress.md">
# Video Processor Refactoring Plan

**Project Goal:** Refactor the video processing logic from the old `video_processor` directory into the new `apps/core` architecture, leveraging Supabase for Auth and PostgreSQL, and ensuring a good developer experience with local development parity.

---

**Phase 0: Project Setup & Essential Configuration**

1.   **Directory Structure & Dependency Setup:**
    * [x]  Task 0.1: **Verify/Create Core Directories:** Ensure the following directories exist within `apps/core/`. If not, create them:
        *   `lib/ai/`
        *   `lib/auth/`
        *   `lib/database/`
        *   `lib/messaging/` (if not already present)
        *   `lib/publishing/`
        *   `lib/storage/`
        *   `lib/utils/`
        *   `api/schemas/`
        *   `api/endpoints/`
        *   `models/`
        *   `operations/`
        *   `services/`
        *   `core/` (for shared core logic like exceptions, config)
        *   `tests/unit/`
        *   `tests/integration/`
    * [x]  Task 0.2: **Initialize Python Dependencies:**
        *   Open `apps/core/pyproject.toml`.
        *   Add necessary dependencies:
            *   `fastapi`, `uvicorn`
            *   `sqlalchemy` (for ORM)
            *   `psycopg2-binary` (PostgreSQL adapter)
            *   `pydantic` (with email validation, settings management)
            *   `pydantic-settings` (for loading from .env)
            *   `python-jose[cryptography]` (for JWT handling)
            *   `python-multipart` (for file uploads)
            *   `google-cloud-storage` (if using GCS)
            *   Relevant AI SDKs (e.g., `google-generativeai`, `google-cloud-aiplatform`)
            *   `alembic` (for database migrations)
            *   `supabase` (Python client, if direct interaction is needed beyond auth)
            *   Testing libraries: `pytest`, `pytest-cov`, `httpx` (for TestClient)
        *   Ensure your project uses a tool like `uv`, `poetry`, or `pdm` to manage dependencies based on `pyproject.toml`. Example: `uv pip install fastapi sqlalchemy psycopg2-binary pydantic pydantic-settings python-jose[cryptography] python-multipart google-cloud-storage google-generativeai alembic supabase pytest pytest-cov httpx uvicorn`

2.  **Supabase Local & Cloud Setup:**
    * [x]   Task 0.2.1: **Cloud Supabase Project:** Set up a Supabase project in the cloud (supabase.com). Enable Google Auth in the Auth providers section.
    * [x]   Task 0.2.2: **Obtain Cloud Credentials:** From your cloud Supabase project settings (Project Settings > API), note down:
        *   Project URL
        *   Anon key (public)
        *   Service role key (secret - keep this very secure)
        *   JWT Secret (Project Settings > API > JWT Settings)
    * [x]   Task 0.2.3: **Local Supabase Setup (Supabase CLI):**
        *   Install Supabase CLI: `npm install supabase --save-dev` (or global install if preferred).
        *   Navigate to your workspace root (or create a `supabase/` directory if you prefer to manage it there) and run `supabase init`. This creates a `supabase` configuration folder.
        *   Run `supabase start`. This command will download necessary Docker images and start local Supabase services. Note the local API URL, anon key, service_role key, and default JWT secret provided in the output.
    * [x]   Task 0.2.4: **Environment Variables (`.env`):**
        *   Create `apps/core/.env.example` with placeholders for all required variables.
        *   Create `apps/core/.env` (this file should be in `.gitignore`). Populate it with your *local Supabase* credentials for development, and any dev API keys for AI/GCS. For production, these will be set as environment variables in your deployment environment.
        ```dotenv
        # apps/core/.env.example
        ENVIRONMENT="development" # "production" or "test"

        # Supabase (Replace with your actual Local Dev Defaults from 'supabase start' output)
        SUPABASE_URL="http://localhost:54321"
        SUPABASE_ANON_KEY="your_local_anon_key"
        SUPABASE_SERVICE_ROLE_KEY="your_local_service_role_key"
        SUPABASE_JWT_SECRET="your_local_default_jwt_secret_at_least_32_characters_long"
        DATABASE_URL="postgresql://postgres:postgres@localhost:54322/postgres" # Default local Supabase DB URL

        # AI Service (Example Gemini)
        GEMINI_API_KEY="your_dev_gemini_api_key"

        # Storage (Local dev uses local file system, GCS for prod)
        STORAGE_BACKEND="local" # "gcs" in production
        LOCAL_STORAGE_PATH="./output_files" # Directory for local file storage (relative to apps/core)
        # GCS_BUCKET_NAME="your_gcs_bucket_name" # For production
        # GOOGLE_APPLICATION_CREDENTIALS_PATH="/path/to/your/gcs_service_account.json" # For production

        # Redis (if used for caching)
        REDIS_HOST="localhost"
        REDIS_PORT="6379"
        ```
    * [x]   Task 0.2.5: **Configuration Loading (`apps/core/core/config.py`):**
        *   Create/Update `apps/core/core/config.py`.
        *   Implement a Pydantic `Settings` class inheriting from `BaseSettings` (from `pydantic_settings`) to load configurations from the `.env` file.
        ```python
        # apps/core/core/config.py
        from typing import Optional
        from pydantic_settings import BaseSettings
        from pathlib import Path

        class Settings(BaseSettings):
            ENVIRONMENT: str = "development"

            # Supabase
            SUPABASE_URL: str
            SUPABASE_ANON_KEY: str
            SUPABASE_SERVICE_ROLE_KEY: Optional[str] = None  # Secret, use with care
            SUPABASE_JWT_SECRET: str
            DATABASE_URL: str

            # AI Services
            GEMINI_API_KEY: Optional[str] = None
            OPENAI_API_KEY: Optional[str] = None

            # Storage
            STORAGE_BACKEND: str = "local"  # 'local' or 'gcs'
            LOCAL_STORAGE_PATH: str = "./output_files"  # Ensure this path is valid
            GCS_BUCKET_NAME: Optional[str] = None
            GOOGLE_APPLICATION_CREDENTIALS_PATH: Optional[str] = None  # For GCS service account
            
            # Redis
            REDIS_HOST: str = "localhost"
            REDIS_PORT: int = 6379
            REDIS_DB: int = 0
            REDIS_PASSWORD: str = ""

            # API settings
            PROJECT_NAME: str = "AI-Driven Backend Service"
            API_PREFIX: str = "/api/v1"
            DEBUG: bool = False

            # JWT settings
            SECRET_KEY: str = "secret_key_for_development_only"
            ALGORITHM: str = "HS256"
            ACCESS_TOKEN_EXPIRE_MINUTES: int = 30

            # Email settings
            SMTP_SERVER: str = "smtp.gmail.com"
            SMTP_PORT: int = 587
            SMTP_USERNAME: str = ""
            SMTP_PASSWORD: str = ""
            EMAIL_FROM_ADDRESS: str = "noreply@example.com"
            EMAIL_TEMPLATES_DIR: str = "templates/emails"

            # File Upload
            UPLOAD_DIR: str = "uploads"

            # Paths
            BASE_DIR: Path = Path(__file__).resolve().parent.parent

            class Config:
                env_file = ".env"
                env_file_encoding = "utf-8"
                extra = "ignore"  # Ignore extra fields from .env

        settings = Settings()
        ```
    * [x]   Task 0.2.6: **Database Connection (`apps/core/lib/database/connection.py`):**
        *   Confirmed: `apps/core/lib/database/connection.py` already provides a FastAPI-compatible SQLAlchemy session generator (`get_db_session`), exports `Base`, and is ready for use in dependency injection and Alembic migrations. No changes needed.
    * [x]   Task 0.2.7: **Database Migrations (Alembic):**
        *   Alembic initialized in `apps/core/` with `alembic init alembic`.
        *   `apps/core/alembic/env.py` configured to:
            *   Import `Base` from `apps.core.lib.database.connection` and set `target_metadata = Base.metadata`.
            *   Import `settings` from `apps.core.core.config` and use `settings.DATABASE_URL` for the connection.
        *   `apps/core/alembic.ini` updated:
            *   `sqlalchemy.url` is blank and a comment explains it is set dynamically from `settings.DATABASE_URL` in env.py.
        *   (After defining models in Phase 1) Create an initial migration: `alembic revision -m "initial_setup_video_processing_tables"` then `alembic upgrade head`. Adhere to `sb-create-migration` guidelines if available.

**Phase 1: Foundation - Models and Core Libraries (Libs)**

3.  **Define Core Data Models (`apps/core/models/`)**:
    *   (All ORM models should inherit from `Base` defined in `apps.core.lib.database.connection`)
    * [ ]   Task 3.1: **Video Model (`apps/core/models/video_model.py`):**
        *   Define `VideoModel(Base)`: `id` (PK, Integer, autoincrement), `uploader_user_id` (String, index=True, corresponds to Supabase Auth user ID), `original_filename` (String), `storage_path` (String, unique path in GCS or local filesystem), `content_type` (String), `size_bytes` (Integer), `created_at` (DateTime, default=func.now()), `updated_at` (DateTime, default=func.now(), onupdate=func.now()).
    * [ ]   Task 3.2: **Enums (`apps/core/models/enums.py`):**
        *   Define `ProcessingStatus(str, Enum)`: `PENDING = "PENDING"`, `PROCESSING = "PROCESSING"`, `COMPLETED = "COMPLETED"`, `FAILED = "FAILED"`.
    * [ ]   Task 3.3: **Video Job Model (`apps/core/models/video_job_model.py`):**
        *   Define `VideoJobModel(Base)`: `id` (PK), `video_id` (Integer, ForeignKey("videos.id")), `status` (SQLAlchemy `EnumType(ProcessingStatus)`), `processing_stages` (JSON or Text, nullable), `error_message` (Text, nullable), `created_at`, `updated_at`. Relationship: `video = relationship("VideoModel")`.
    * [ ]   Task 3.4: **Video Metadata Model (`apps/core/models/video_metadata_model.py`):**
        *   Define `VideoMetadataModel(Base)`: `id` (PK), `job_id` (Integer, ForeignKey("video_jobs.id"), unique=True), `title` (String, nullable), `description` (Text, nullable), `tags` (JSON or `sqlalchemy.dialects.postgresql.ARRAY(String)` for PostgreSQL, nullable), `transcript_text` (Text, nullable), `transcript_file_url` (String, nullable), `subtitle_files_urls` (JSON, nullable, e.g., `{"vtt": "url", "srt": "url"}`), `thumbnail_file_url` (String, nullable), `extracted_video_duration_seconds` (Float, nullable), `extracted_video_resolution` (String, nullable), `extracted_video_format` (String, nullable), `show_notes_text` (Text, nullable), `created_at`, `updated_at`. Relationship: `job = relationship("VideoJobModel", back_populates="metadata")`. Add `metadata = relationship("VideoMetadataModel", back_populates="job", uselist=False)` to `VideoJobModel`.
    * [ ]   Task 3.5: **Model Imports & Alembic:**
        *   Ensure `apps/core/models/__init__.py` imports all model classes (e.g., `from .video_model import VideoModel`) and `Base`.
        *   Update `apps/core/alembic/env.py` `target_metadata` to `Base.metadata`.
    * [ ]   Task 3.6: **Generate and Apply Migrations:**
        *   Run `cd apps/core && alembic revision --autogenerate -m "create_video_processing_tables"`.
        *   Inspect the generated migration script in `apps/core/alembic/versions/`.
        *   Run `cd apps/core && alembic upgrade head` to apply to your local Supabase DB.

4.  **Develop/Enhance Core Libraries (`apps/core/lib/`)**:
    *   **Storage (`apps/core/lib/storage/file_storage.py`):**
        * [ ]   Task 4.1: Define `FileStorageService` class. Constructor `__init__(self, settings: Settings)`.
        * [ ]   Task 4.2: Method `async save_file(self, file_content: bytes, filename: str, subdir: Optional[str] = "uploads") -> str`:
            *   If `self.settings.STORAGE_BACKEND == "local"`: Save to `{self.settings.LOCAL_STORAGE_PATH}/{subdir}/{filename}`. Ensure directory exists. Return the local path.
            *   If `self.settings.STORAGE_BACKEND == "gcs"`: Use `google-cloud-storage` client to upload to `self.settings.GCS_BUCKET_NAME` under `{subdir}/{filename}`. Return GCS URI `gs://{bucket_name}/{subdir}/{filename}`.
        * [ ]   Task 4.3: Method `async download_file(self, storage_path: str, destination_local_path: str) -> str`: Downloads from GCS or copies from local.
        * [ ]   Task 4.4: Method `async get_public_url(self, storage_path: str) -> Optional[str]`: Returns HTTP URL for GCS objects or a placeholder for local files.
    *   **AI (`apps/core/lib/ai/`)**:
        * [ ]   Task 4.5: **Base Adapter (`base_adapter.py`):** `AIAdapterInterface(ABC)` with `async generate_text(self, prompt: str, context: Optional[str] = None) -> str`, `async transcribe_audio(self, audio_file_path: str) -> str` (returns structured transcript if possible).
        * [ ]   Task 4.6: **Gemini Adapter (`gemini_adapter.py`):** `GeminiAdapter(AIAdapterInterface)` using `settings.GEMINI_API_KEY`.
        * [ ]   Task 4.7: **AI Client Factory (`ai_client_factory.py`):** `def get_ai_adapter(settings: Settings) -> AIAdapterInterface: return GeminiAdapter(settings)`.
        * [ ]   Task 4.8: **AI Caching (`apps/core/lib/cache/redis_cache.py`):**
            *   Create `RedisCache` class using `redis-py` (sync or async version). Connect using `settings.REDIS_HOST`, `settings.REDIS_PORT`.
            *   Methods: `get(key)`, `set(key, value, ttl_seconds)`.
            *   Modify AI adapters to use this cache for relevant API calls.
    *   **Utilities (`apps/core/lib/utils/`)**:
        * [ ]   Task 4.9: **FFmpeg (`ffmpeg_utils.py`):** `FfmpegUtils` class. Methods: `extract_audio_sync(video_path, output_audio_path)`, `extract_frame_sync(video_path, timestamp_seconds, output_image_path)`, `get_video_metadata_sync(video_path) -> dict`. Use `subprocess.run`. Ensure `ffmpeg` is an accessible command.
        * [ ]   Task 4.10: **File Handling (`file_utils.py`):** `FileUtils` class. Methods for temp dir creation/cleanup: `create_temp_dir()`, `cleanup_temp_dir(dir_path)`.
        * [ ]   Task 4.11: **Subtitles (`subtitle_utils.py`):** `SubtitleUtils` class. Methods: `generate_vtt(transcript_segments: list) -> str`, `generate_srt(transcript_segments: list) -> str`. Define `transcript_segments` structure (e.g., `[{'text': '...', 'start_time': 0.0, 'end_time': 1.5}, ...]`).
    *   **Authentication Utilities (`apps/core/lib/auth/supabase_auth.py`):**
        * [ ]   Task 4.12: Create `AuthenticatedUser(BaseModel)`: `id: str`, `email: Optional[str] = None`, `aud: Optional[str] = None`.
        * [ ]   Task 4.13: `async def get_current_user(token: str = Depends(OAuth2PasswordBearer(tokenUrl="token"))) -> AuthenticatedUser:`
            *   Use `python-jose.jwt.decode` with `settings.SUPABASE_JWT_SECRET` and audience `authenticated`.
            *   Extract `sub` (user_id), `email`, `aud`. Return `AuthenticatedUser`. Handle `JWTError` with `HTTPException`.
            *   (Note: `tokenUrl` is a dummy here as Supabase handles token issuance.)

5.  **Error Handling (`apps/core/core/exceptions.py`):**
    * [ ]   Task 5.1: Define custom exceptions: `VideoProcessingError(Exception)`, `AINoResponseError(VideoProcessingError)`, `FFmpegError(VideoProcessingError)`.

**Phase 2: Data Access - Operations Layer**

6.  **Implement Repositories (`apps/core/operations/`)**:
    *   (All repo methods accept `db: Session` as first arg. Typically called from services.)
    * [ ]   Task 6.1: **Video Repo (`video_repository.py`):** `VideoRepository` class. Methods: `create(...) -> VideoModel`, `get_by_id(...) -> Optional[VideoModel]`.
    * [ ]   Task 6.2: **Video Job Repo (`video_job_repository.py`):** `VideoJobRepository` class. Methods: `create(...) -> VideoJobModel`, `get_by_id(...) -> Optional[VideoJobModel]`, `update_status(...) -> VideoJobModel`, `add_processing_stage(...) -> VideoJobModel`.
    * [ ]   Task 6.3: **Video Metadata Repo (`video_metadata_repository.py`):** `VideoMetadataRepository` class. Methods: `create_or_update(db: Session, job_id: int, **kwargs) -> VideoMetadataModel`, `get_by_job_id(...) -> Optional[VideoMetadataModel]`.

**Phase 3: Business Logic - Service Layer**

7.  **Develop Core Video Processing Service (`apps/core/services/video_processing_service.py`):**
    * [ ]   Task 7.1: Define `VideoProcessingService` class.
        *   `__init__`: Inject instances of `VideoRepository`, `VideoJobRepository`, `VideoMetadataRepository`, `FileStorageService`, `AIAdapterInterface`, `FfmpegUtils`, `SubtitleUtils`, `FileUtils`.
    * [ ]   Task 7.2: Method `async initiate_video_processing(self, db: Session, original_filename: str, video_content: bytes, content_type: str, uploader_user_id: str, background_tasks: BackgroundTasks) -> VideoJobModel`:
        *   Save video via `FileStorageService` (e.g., `uploads/{uploader_user_id}/{uuid_filename}`).
        *   Create `VideoModel` via repo.
        *   Create `VideoJobModel` (status PENDING) via repo.
        *   Add `self._execute_processing_pipeline(job_id=new_job.id, video_storage_path=stored_video_path)` to `background_tasks`.
        *   `db.commit()` for initial records. Return `new_job`.
    * [ ]   Task 7.3: Method `async _execute_processing_pipeline(self, job_id: int, video_storage_path: str)`:
        *   Use a `with get_db_session() as db_bg:` context for the background task's DB operations.
        *   Fetch job, update status to PROCESSING.
        *   Use `FileUtils.create_temp_dir()`.
        *   **Try/Catch/Finally (for cleanup):**
            *   Download video to temp dir using `FileStorageService`.
            *   **Step 1: Basic Metadata:** `FfmpegUtils.get_video_metadata_sync`. Update `VideoMetadataModel`.
            *   **Step 2: Extract Audio:** `FfmpegUtils.extract_audio_sync` to temp audio file.
            *   **Step 3: Transcript:** `AIAdapter.transcribe_audio`. Store text. Upload `.txt` via `FileStorageService`. Update `VideoMetadataModel`.
            *   **Step 4: Content Metadata:** `AIAdapter.generate_text` for title, desc, tags, show_notes. Update `VideoMetadataModel`.
            *   **Step 5: Subtitles:** `SubtitleUtils` using transcript. Upload `.vtt`, `.srt`. Update `VideoMetadataModel`.
            *   **Step 6: Thumbnail:** `FfmpegUtils.extract_frame_sync`. Upload `.jpg`. Update `VideoMetadataModel`.
            *   Update job to COMPLETED. `db_bg.commit()`.
        *   **Catch:** Log error. Update job to FAILED with error message. `db_bg.commit()`.
        *   **Finally:** `FileUtils.cleanup_temp_dir()`.
    * [ ]   Task 7.4: Method `async get_job_details(self, db: Session, job_id: int, user_id: str) -> Optional[VideoJobModel]`: Fetch job, verify ownership (`job.video.uploader_user_id == user_id`), return job with related video and metadata.

8.  **Supporting Services:**
    * [ ]   Task 8.1: `apps/core/services/user_service.py`: (If needed) `async def get_or_create_user_profile(db: Session, auth_user: AuthenticatedUser) -> UserModel`. Could be called after successful auth to ensure a local user profile exists.

**Phase 4: API Layer - Endpoints and Schemas**

9.  **Define API Schemas (`apps/core/api/schemas/`)**:
    * [ ]   Task 9.1: Create `video_processing_schemas.py` (and `user_schemas.py` if `UserService` is used).
    * [ ]   Task 9.2: Pydantic `VideoUploadResponseSchema(BaseModel)`: `job_id: int`, `status: ProcessingStatus`.
    * [ ]   Task 9.3: Pydantic `VideoSchema(BaseModel)`: from `VideoModel`. `model_config = ConfigDict(from_attributes=True)`.
    * [ ]   Task 9.4: Pydantic `VideoMetadataSchema(BaseModel)`: from `VideoMetadataModel`. `model_config = ConfigDict(from_attributes=True)`.
    * [ ]   Task 9.5: Pydantic `VideoJobSchema(BaseModel)`: from `VideoJobModel`, plus `video: Optional[VideoSchema] = None`, `metadata: Optional[VideoMetadataSchema] = None`. `model_config = ConfigDict(from_attributes=True)`.

10. **Create API Endpoints (`apps/core/api/endpoints/video_processing_endpoints.py`):**
    * [ ]   Task 10.1: Create `router = APIRouter()`.
    * [ ]   Task 10.2: `POST /upload`, response_model=`VideoUploadResponseSchema`:
        *   Dependencies: `current_user: AuthenticatedUser = Depends(get_current_user)`, `db: Session = Depends(get_db_session)`, `background_tasks: BackgroundTasks`.
        *   Inject `VideoProcessingService`. Call `initiate_video_processing`.
    * [ ]   Task 10.3: `GET /jobs/{job_id}`, response_model=`VideoJobSchema`:
        *   Dependencies: `current_user: AuthenticatedUser = Depends(get_current_user)`, `db: Session = Depends(get_db_session)`.
        *   Inject `VideoProcessingService`. Call `get_job_details`. Raise 404 or 403 if not found/not owner.
    * [ ]   Task 10.4: **Register Router in `apps/core/main.py`:**
        *   `app.include_router(video_processing_router, prefix="/api/v1/videos", tags=["Video Processing"])`.

**Phase 5: Testing, Documentation, and Cleanup**

11. **Testing (`apps/core/tests/`)**:
    * [ ]   Task 11.1: **Unit Tests (`tests/unit/`)**: For models, libs (mock external IO), operations (mock DB session methods), services (mock repo/lib dependencies).
    * [ ]   Task 11.2: **Integration Tests (`tests/integration/api/`)**:
        *   Use FastAPI `TestClient`.
        *   Override `get_db_session` to use a test DB (local Supabase test DB).
        *   Override `get_current_user` to return a mock `AuthenticatedUser`.
        *   Test `POST /upload` with actual files. For background tasks, either mock the service method called by it, or check DB for results after a short delay (if feasible for tests).
        *   Test `GET /jobs/{job_id}`.
        *   Configure AI/Storage services to use local/mocked backends for integration tests if possible, or carefully managed dev instances.

12. **Documentation and Cleanup**:
    * [ ]   Task 12.1: Update `apps/core/README.md` (API endpoints, local setup with Supabase CLI, env vars).
    * [ ]   Task 12.2: Add Python docstrings (module, class, function levels).
    * [ ]   Task 12.3: (LATER) Securely remove old `apps/core/video_processor` directory after full verification.
    * [ ]   Task 12.4: Update this `.ai_docs/progress.md` as tasks are completed.

---
</file>

<file path=".ai_docs/prompts/new-new.md">
## How to approach new stack

[ ] Cursor Rule for my Context 
- apply all to each one 

[ ] Research an Architecture for this project 


[ ] Figure out a `README.md` Overview 

[ ] Run `Memory-Bank`, modify the `prompt` to read/write to `ai_docs`

[ ] Add the `prime-prompt.md` to `ai_docs/prompts`. 
- This requires having `eza` in your `~/.zshrc`
</file>

<file path=".ai_docs/templates/api_examples/adapters/storage/gcs.py">
"""
Google Cloud Storage adapter implementation.

This module provides a concrete implementation of the StorageInterface
for Google Cloud Storage. It handles the specific details of interacting
with GCS while conforming to the interface expected by the application.
"""

import asyncio
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
from typing import Optional, Union

from google.cloud import storage
from google.cloud.exceptions import Forbidden, NotFound

from video_processor.application.interfaces.storage import (
    FileNotFoundError,
    StorageError,
    StorageInterface,
    StoragePermissionError,
)


class GCSStorageAdapter(StorageInterface):
    """
    Google Cloud Storage implementation of the storage interface.

    This adapter implements the StorageInterface abstract methods using
    the Google Cloud Storage client library.
    """

    def __init__(
        self,
        client: Optional[storage.Client] = None,
        executor: Optional[ThreadPoolExecutor] = None,
    ):
        """
        Initialize the GCS storage adapter.

        Args:
            client: Optional pre-configured storage client
            executor: Optional thread pool executor for async operations
        """
        self._client = client or storage.Client()
        self._executor = executor or ThreadPoolExecutor(max_workers=10)

    def _get_bucket(self, bucket_name: str) -> storage.Bucket:
        """Get a bucket by name."""
        return self._client.bucket(bucket_name)

    def _parse_gcs_path(self, path: str) -> tuple[str, str]:
        """
        Parse a GCS path into bucket name and blob name.

        Args:
            path: GCS path in format 'bucket_name/path/to/file'
                or 'gs://bucket_name/path/to/file'

        Returns:
            Tuple of (bucket_name, blob_name)

        Raises:
            ValueError: If path format is invalid
        """
        if path.startswith("gs://"):
            path = path[5:]

        parts = path.split("/", 1)
        if len(parts) < 2:
            raise ValueError(
                f"Invalid GCS path: {path}. Must be in format 'bucket_name/blob_name'"
            )

        bucket_name, blob_name = parts
        return bucket_name, blob_name

    async def upload_file(
        self,
        source_path: Union[str, Path],
        destination_path: str,
        content_type: Optional[str] = None,
    ) -> str:
        """Upload a file to GCS."""
        try:
            source_path = Path(source_path)
            if not source_path.exists():
                raise FileNotFoundError(f"Source file not found: {source_path}")

            bucket_name, blob_name = self._parse_gcs_path(destination_path)
            bucket = self._get_bucket(bucket_name)
            blob = bucket.blob(blob_name)

            if content_type:
                blob.content_type = content_type

            # Run in thread pool to avoid blocking
            def _upload():
                blob.upload_from_filename(str(source_path))
                return f"gs://{bucket_name}/{blob_name}"

            return await asyncio.get_event_loop().run_in_executor(
                self._executor, _upload
            )

        except NotFound:
            raise StorageError(f"Bucket not found: {bucket_name}")
        except Forbidden:
            raise StoragePermissionError(
                f"Permission denied uploading to {destination_path}"
            )
        except Exception as e:
            raise StorageError(f"Failed to upload file: {str(e)}")

    async def upload_from_string(
        self, content: str, destination_path: str, content_type: Optional[str] = None
    ) -> str:
        """Upload string content to GCS."""
        try:
            bucket_name, blob_name = self._parse_gcs_path(destination_path)
            bucket = self._get_bucket(bucket_name)
            blob = bucket.blob(blob_name)

            if content_type:
                blob.content_type = content_type

            # Run in thread pool to avoid blocking
            def _upload():
                blob.upload_from_string(content)
                return f"gs://{bucket_name}/{blob_name}"

            return await asyncio.get_event_loop().run_in_executor(
                self._executor, _upload
            )

        except NotFound:
            raise StorageError(f"Bucket not found: {bucket_name}")
        except Forbidden:
            raise StoragePermissionError(
                f"Permission denied uploading to {destination_path}"
            )
        except Exception as e:
            raise StorageError(f"Failed to upload content: {str(e)}")

    async def download_to_file(
        self, source_path: str, destination_path: Union[str, Path]
    ) -> Path:
        """Download a file from GCS to local filesystem."""
        try:
            bucket_name, blob_name = self._parse_gcs_path(source_path)
            bucket = self._get_bucket(bucket_name)
            blob = bucket.blob(blob_name)

            destination_path = Path(destination_path)

            # Ensure destination directory exists
            destination_path.parent.mkdir(parents=True, exist_ok=True)

            # Check if blob exists
            def _exists():
                return blob.exists()

            exists = await asyncio.get_event_loop().run_in_executor(
                self._executor, _exists
            )

            if not exists:
                raise FileNotFoundError(f"File not found in storage: {source_path}")

            # Download the file
            def _download():
                blob.download_to_filename(str(destination_path))
                return destination_path

            return await asyncio.get_event_loop().run_in_executor(
                self._executor, _download
            )

        except NotFound:
            raise StorageError(f"Bucket not found: {bucket_name}")
        except Forbidden:
            raise StoragePermissionError(
                f"Permission denied downloading from {source_path}"
            )
        except Exception as e:
            if isinstance(e, FileNotFoundError):
                raise
            raise StorageError(f"Failed to download file: {str(e)}")

    async def download_as_string(self, source_path: str) -> str:
        """Download a file from GCS as a string."""
        try:
            bucket_name, blob_name = self._parse_gcs_path(source_path)
            bucket = self._get_bucket(bucket_name)
            blob = bucket.blob(blob_name)

            # Check if blob exists
            def _exists():
                return blob.exists()

            exists = await asyncio.get_event_loop().run_in_executor(
                self._executor, _exists
            )

            if not exists:
                raise FileNotFoundError(f"File not found in storage: {source_path}")

            # Download the file as string
            def _download():
                return blob.download_as_text()

            return await asyncio.get_event_loop().run_in_executor(
                self._executor, _download
            )

        except NotFound:
            raise StorageError(f"Bucket not found: {bucket_name}")
        except Forbidden:
            raise StoragePermissionError(
                f"Permission denied downloading from {source_path}"
            )
        except Exception as e:
            if isinstance(e, FileNotFoundError):
                raise
            raise StorageError(f"Failed to download file as string: {str(e)}")

    async def download_as_bytes(self, source_path: str) -> bytes:
        """Download a file from GCS as bytes."""
        try:
            bucket_name, blob_name = self._parse_gcs_path(source_path)
            bucket = self._get_bucket(bucket_name)
            blob = bucket.blob(blob_name)

            # Check if blob exists
            def _exists():
                return blob.exists()

            exists = await asyncio.get_event_loop().run_in_executor(
                self._executor, _exists
            )

            if not exists:
                raise FileNotFoundError(f"File not found in storage: {source_path}")

            # Download the file as bytes
            def _download():
                return blob.download_as_bytes()

            return await asyncio.get_event_loop().run_in_executor(
                self._executor, _download
            )

        except NotFound:
            raise StorageError(f"Bucket not found: {bucket_name}")
        except Forbidden:
            raise StoragePermissionError(
                f"Permission denied downloading from {source_path}"
            )
        except Exception as e:
            if isinstance(e, FileNotFoundError):
                raise
            raise StorageError(f"Failed to download file as bytes: {str(e)}")

    async def get_signed_url(
        self, path: str, expires_after_seconds: int = 3600, method: str = "GET"
    ) -> str:
        """Generate a signed URL for a GCS file."""
        try:
            bucket_name, blob_name = self._parse_gcs_path(path)
            bucket = self._get_bucket(bucket_name)
            blob = bucket.blob(blob_name)

            # Check if blob exists for GET requests
            if method.upper() == "GET":

                def _exists():
                    return blob.exists()

                exists = await asyncio.get_event_loop().run_in_executor(
                    self._executor, _exists
                )

                if not exists:
                    raise FileNotFoundError(f"File not found in storage: {path}")

            # Generate signed URL
            def _generate_url():
                return blob.generate_signed_url(
                    expiration=expires_after_seconds, method=method, version="v4"
                )

            return await asyncio.get_event_loop().run_in_executor(
                self._executor, _generate_url
            )

        except NotFound:
            raise StorageError(f"Bucket not found: {bucket_name}")
        except Forbidden:
            raise StoragePermissionError(
                f"Permission denied generating signed URL for {path}"
            )
        except Exception as e:
            if isinstance(e, FileNotFoundError):
                raise
            raise StorageError(f"Failed to generate signed URL: {str(e)}")

    async def file_exists(self, path: str) -> bool:
        """Check if a file exists in GCS."""
        try:
            bucket_name, blob_name = self._parse_gcs_path(path)
            bucket = self._get_bucket(bucket_name)
            blob = bucket.blob(blob_name)

            def _exists():
                return blob.exists()

            return await asyncio.get_event_loop().run_in_executor(
                self._executor, _exists
            )

        except Exception:
            # Don't raise exceptions for existence checks
            return False

    async def delete_file(self, path: str) -> bool:
        """Delete a file from GCS."""
        try:
            bucket_name, blob_name = self._parse_gcs_path(path)
            bucket = self._get_bucket(bucket_name)
            blob = bucket.blob(blob_name)

            # Check if blob exists
            def _exists():
                return blob.exists()

            exists = await asyncio.get_event_loop().run_in_executor(
                self._executor, _exists
            )

            if not exists:
                return False

            # Delete the blob
            def _delete():
                blob.delete()
                return True

            return await asyncio.get_event_loop().run_in_executor(
                self._executor, _delete
            )

        except NotFound:
            return False
        except Forbidden:
            raise StoragePermissionError(f"Permission denied deleting {path}")
        except Exception as e:
            raise StorageError(f"Failed to delete file: {str(e)}")

    async def move_file(self, source_path: str, destination_path: str) -> str:
        """Move a file within GCS (copy and delete)."""
        # First copy the file
        destination_url = await self.copy_file(source_path, destination_path)

        # Then delete the original
        await self.delete_file(source_path)

        return destination_url

    async def copy_file(self, source_path: str, destination_path: str) -> str:
        """Copy a file within GCS."""
        try:
            source_bucket_name, source_blob_name = self._parse_gcs_path(source_path)
            dest_bucket_name, dest_blob_name = self._parse_gcs_path(destination_path)

            source_bucket = self._get_bucket(source_bucket_name)
            source_blob = source_bucket.blob(source_blob_name)

            # Check if source blob exists
            def _exists():
                return source_blob.exists()

            exists = await asyncio.get_event_loop().run_in_executor(
                self._executor, _exists
            )

            if not exists:
                raise FileNotFoundError(f"Source file not found: {source_path}")

            dest_bucket = self._get_bucket(dest_bucket_name)

            # Copy the blob
            def _copy():
                destination_blob = source_bucket.copy_blob(
                    source_blob, dest_bucket, dest_blob_name
                )
                return f"gs://{dest_bucket_name}/{dest_blob_name}"

            return await asyncio.get_event_loop().run_in_executor(self._executor, _copy)

        except NotFound:
            raise StorageError(
                f"Bucket not found: {source_bucket_name} or {dest_bucket_name}"
            )
        except Forbidden:
            raise StoragePermissionError(
                f"Permission denied copying {source_path} to {destination_path}"
            )
        except Exception as e:
            if isinstance(e, FileNotFoundError):
                raise
            raise StorageError(f"Failed to copy file: {str(e)}")

    async def list_files(self, prefix: str) -> list[str]:
        """List files in GCS with a given prefix."""
        try:
            bucket_name, prefix_path = self._parse_gcs_path(prefix)
            bucket = self._get_bucket(bucket_name)

            # List blobs with prefix
            def _list():
                blobs = bucket.list_blobs(prefix=prefix_path)
                return [f"gs://{bucket_name}/{blob.name}" for blob in blobs]

            return await asyncio.get_event_loop().run_in_executor(self._executor, _list)

        except NotFound:
            raise StorageError(f"Bucket not found: {bucket_name}")
        except Forbidden:
            raise StoragePermissionError(
                f"Permission denied listing files with prefix {prefix}"
            )
        except Exception as e:
            raise StorageError(f"Failed to list files: {str(e)}")
</file>

<file path=".ai_docs/templates/api_examples/application/interfaces/storage.py">
"""
Storage interface for the video processing pipeline.

This module defines the abstract interface for storage operations required by the
application. It follows the dependency inversion principle by having core business
logic depend on abstractions rather than concrete implementations.
"""

from abc import ABC, abstractmethod
from pathlib import Path
from typing import Optional, Union


class StorageError(Exception):
    """Base exception for storage-related errors."""

    pass


class FileNotFoundError(StorageError):
    """Raised when a file cannot be found in storage."""

    pass


class StoragePermissionError(StorageError):
    """Raised when there's a permission issue with storage operations."""

    pass


class StorageInterface(ABC):
    """
    Abstract interface for storage operations.

    This interface defines the contract that any storage implementation
    must fulfill, regardless of whether it's cloud storage, local filesystem,
    or another storage mechanism.
    """

    @abstractmethod
    async def upload_file(
        self,
        source_path: Union[str, Path],
        destination_path: str,
        content_type: Optional[str] = None,
    ) -> str:
        """
        Upload a file from local filesystem to storage.

        Args:
            source_path: Local path to file to upload
            destination_path: Path in storage where file should be saved
            content_type: Optional MIME type of the file

        Returns:
            The URL or path to the uploaded file

        Raises:
            StorageError: If upload fails
        """
        pass

    @abstractmethod
    async def upload_from_string(
        self, content: str, destination_path: str, content_type: Optional[str] = None
    ) -> str:
        """
        Upload string content to storage.

        Args:
            content: String content to upload
            destination_path: Path in storage where content should be saved
            content_type: Optional MIME type of the content

        Returns:
            The URL or path to the uploaded content

        Raises:
            StorageError: If upload fails
        """
        pass

    @abstractmethod
    async def download_to_file(
        self, source_path: str, destination_path: Union[str, Path]
    ) -> Path:
        """
        Download a file from storage to local filesystem.

        Args:
            source_path: Path in storage to the file to download
            destination_path: Local path where file should be saved

        Returns:
            Path object pointing to the downloaded file

        Raises:
            FileNotFoundError: If source file doesn't exist
            StorageError: If download fails
        """
        pass

    @abstractmethod
    async def download_as_string(self, source_path: str) -> str:
        """
        Download a file from storage as a string.

        Args:
            source_path: Path in storage to the file to download

        Returns:
            Content of the file as a string

        Raises:
            FileNotFoundError: If source file doesn't exist
            StorageError: If download fails
        """
        pass

    @abstractmethod
    async def download_as_bytes(self, source_path: str) -> bytes:
        """
        Download a file from storage as bytes.

        Args:
            source_path: Path in storage to the file to download

        Returns:
            Content of the file as bytes

        Raises:
            FileNotFoundError: If source file doesn't exist
            StorageError: If download fails
        """
        pass

    @abstractmethod
    async def get_signed_url(
        self, path: str, expires_after_seconds: int = 3600, method: str = "GET"
    ) -> str:
        """
        Generate a signed URL for a file in storage.

        Args:
            path: Path in storage to the file
            expires_after_seconds: Number of seconds until URL expires
            method: HTTP method the URL should support (GET, PUT, etc.)

        Returns:
            A signed URL that can be used to access the file

        Raises:
            FileNotFoundError: If file doesn't exist
            StorageError: If URL generation fails
        """
        pass

    @abstractmethod
    async def file_exists(self, path: str) -> bool:
        """
        Check if a file exists in storage.

        Args:
            path: Path in storage to check

        Returns:
            True if file exists, False otherwise
        """
        pass

    @abstractmethod
    async def delete_file(self, path: str) -> bool:
        """
        Delete a file from storage.

        Args:
            path: Path in storage to the file to delete

        Returns:
            True if file was deleted, False if file didn't exist

        Raises:
            StoragePermissionError: If deletion fails due to permissions
            StorageError: If deletion fails for other reasons
        """
        pass

    @abstractmethod
    async def move_file(self, source_path: str, destination_path: str) -> str:
        """
        Move a file within storage.

        Args:
            source_path: Current path of the file in storage
            destination_path: New path for the file in storage

        Returns:
            The new path or URL to the moved file

        Raises:
            FileNotFoundError: If source file doesn't exist
            StorageError: If move fails
        """
        pass

    @abstractmethod
    async def copy_file(self, source_path: str, destination_path: str) -> str:
        """
        Copy a file within storage.

        Args:
            source_path: Path of the file to copy
            destination_path: Path where the copy should be saved

        Returns:
            The path or URL to the copied file

        Raises:
            FileNotFoundError: If source file doesn't exist
            StorageError: If copy fails
        """
        pass

    @abstractmethod
    async def list_files(self, prefix: str) -> list[str]:
        """
        List files in storage with a given prefix.

        Args:
            prefix: Prefix to filter files by

        Returns:
            List of paths matching the prefix
        """
        pass
</file>

<file path=".ai_docs/templates/api_examples/infrastructure/config/container.py">
"""
Dependency injection container.

This module provides a dependency injection container that manages the
creation and lifecycle of service objects in the application. It helps to
decouple object creation from object usage, enabling better testability
and flexibility.
"""

import os
from concurrent.futures import ThreadPoolExecutor
from typing import Any, Dict, Type, TypeVar

from google.cloud import storage
from vertexai.preview.generative_models import GenerativeModel

from video_processor.adapters.ai.gemini import GeminiAIAdapter
from video_processor.adapters.publishing.youtube import YouTubeAdapter
from video_processor.adapters.storage.gcs import GCSStorageAdapter
from video_processor.adapters.storage.local import LocalStorageAdapter
from video_processor.application.interfaces.ai import AIServiceInterface
from video_processor.application.interfaces.publishing import PublishingInterface
from video_processor.application.interfaces.storage import StorageInterface
from video_processor.application.services.metadata import MetadataService
from video_processor.application.services.subtitle import SubtitleService
from video_processor.application.services.transcription import TranscriptionService
from video_processor.application.services.video_processor import VideoProcessorService
from video_processor.infrastructure.repositories.job_repository import JobRepository
from video_processor.infrastructure.repositories.video_repository import VideoRepository

# Type variable for generics
T = TypeVar("T")


class Container:
    """
    Dependency injection container.

    This container manages the creation, configuration, and lifetime of
    service objects, providing a central registry for application components.
    """

    def __init__(self):
        """Initialize an empty container."""
        self._services: Dict[Type, Any] = {}
        self._factories: Dict[Type, callable] = {}
        self._singletons: Dict[Type, bool] = {}

    def register(
        self, service_type: Type[T], factory: callable, singleton: bool = True
    ) -> None:
        """
        Register a service with the container.

        Args:
            service_type: The type/interface for the service
            factory: A factory function that creates an instance of the service
            singleton: Whether the service should be a singleton
        """
        self._factories[service_type] = factory
        self._singletons[service_type] = singleton

        # Clear existing instance if re-registering
        if service_type in self._services:
            del self._services[service_type]

    def get(self, service_type: Type[T]) -> T:
        """
        Get a service instance from the container.

        Args:
            service_type: The type/interface of the service to retrieve

        Returns:
            An instance of the requested service

        Raises:
            KeyError: If the service type is not registered
        """
        # Return existing instance for singletons
        if service_type in self._services and self._singletons.get(service_type, True):
            return self._services[service_type]

        # Create new instance using factory
        if service_type not in self._factories:
            raise KeyError(f"Service {service_type.__name__} not registered")

        factory = self._factories[service_type]
        instance = factory()

        # Cache singleton instances
        if self._singletons.get(service_type, True):
            self._services[service_type] = instance

        return instance

    def register_instance(self, service_type: Type[T], instance: T) -> None:
        """
        Register an existing instance with the container.

        Args:
            service_type: The type/interface for the service
            instance: An existing instance to use
        """
        self._services[service_type] = instance
        self._singletons[service_type] = True


def create_container(testing: bool = False, local_storage: bool = False) -> Container:
    """
    Create and configure a dependency injection container.

    Args:
        testing: Whether the container is for testing
        local_storage: Whether to use local storage instead of GCS

    Returns:
        A configured Container instance
    """
    container = Container()
    thread_pool = ThreadPoolExecutor(max_workers=10)

    # Register infrastructure services
    if testing:
        # Use mock implementations for testing
        from unittest.mock import MagicMock

        container.register_instance(storage.Client, MagicMock())
    else:
        # Use real implementations for production
        container.register(storage.Client, lambda: storage.Client())

    # Register storage adapter
    if local_storage or testing:
        local_dir = os.environ.get("LOCAL_OUTPUT_DIR", "./output")
        container.register(
            StorageInterface,
            lambda: LocalStorageAdapter(base_dir=local_dir, executor=thread_pool),
        )
    else:
        container.register(
            StorageInterface,
            lambda: GCSStorageAdapter(
                client=container.get(storage.Client), executor=thread_pool
            ),
        )

    # Register AI service adapter
    if testing:
        from unittest.mock import MagicMock

        mock_ai = MagicMock(spec=AIServiceInterface)
        mock_ai.generate_content.return_value = "Mock AI response"
        container.register_instance(AIServiceInterface, mock_ai)
    else:
        # Configure Vertex AI
        project_id = os.environ.get("GOOGLE_CLOUD_PROJECT", "default-project")
        region = os.environ.get("REGION", "us-east1")
        model_name = os.environ.get("MODEL", "gemini-2.0-flash-001")

        import vertexai

        vertexai.init(project=project_id, location=region)

        container.register(GenerativeModel, lambda: GenerativeModel(model_name))

        container.register(
            AIServiceInterface,
            lambda: GeminiAIAdapter(model=container.get(GenerativeModel)),
        )

    # Register YouTube adapter
    container.register(
        PublishingInterface,
        lambda: YouTubeAdapter(
            storage=container.get(StorageInterface),
            credentials_path=os.environ.get(
                "YOUTUBE_CREDENTIALS", "credentials/youtube_credentials.json"
            ),
        ),
    )

    # Register repositories
    container.register(
        JobRepository, lambda: JobRepository(storage=container.get(StorageInterface))
    )

    container.register(
        VideoRepository,
        lambda: VideoRepository(storage=container.get(StorageInterface)),
    )

    # Register application services
    container.register(
        TranscriptionService,
        lambda: TranscriptionService(ai_service=container.get(AIServiceInterface)),
    )

    container.register(
        SubtitleService,
        lambda: SubtitleService(ai_service=container.get(AIServiceInterface)),
    )

    container.register(
        MetadataService,
        lambda: MetadataService(ai_service=container.get(AIServiceInterface)),
    )

    # Register the main processor service
    container.register(
        VideoProcessorService,
        lambda: VideoProcessorService(
            storage=container.get(StorageInterface),
            transcription_service=container.get(TranscriptionService),
            subtitle_service=container.get(SubtitleService),
            metadata_service=container.get(MetadataService),
            publishing_service=container.get(PublishingInterface),
            job_repository=container.get(JobRepository),
            video_repository=container.get(VideoRepository),
        ),
    )

    return container
</file>

<file path=".ai_docs/templates/api_examples/main.py">
"""
Main application entry point.

This module serves as the entry point for the video processing application.
It sets up the dependency injection container, configures services, and
provides the main Cloud Function handler for processing video events.
"""

import logging
import os
from typing import Any, Dict

import functions_framework
from video_processor.application.services.video_processor import VideoProcessorService
from video_processor.domain.models.enums import ProcessingStatus
from video_processor.infrastructure.config.container import create_container

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# Configure testing mode
TESTING_MODE = os.environ.get("TESTING_MODE", "false").lower() == "true"
LOCAL_OUTPUT = os.environ.get("LOCAL_OUTPUT", "false").lower() == "true" or TESTING_MODE

# Create and configure the dependency injection container
container = create_container(testing=TESTING_MODE, local_storage=LOCAL_OUTPUT)

# Get the video processor service from the container
video_processor_service = container.get(VideoProcessorService)


@functions_framework.cloud_event
async def process_video_event(cloud_event: Dict[str, Any]) -> Dict[str, Any]:
    """
    Cloud Function handler for processing video events.

    This function is triggered by GCS events when a new video is uploaded.
    It extracts metadata from the event and delegates to the video processor
    service for actual processing.

    Args:
        cloud_event: GCS event data

    Returns:
        A dictionary with processing results
    """
    try:
        logger.info(f"Received event: {cloud_event.id}")

        # Extract bucket and file information from the event
        bucket = cloud_event.data["bucket"]
        file_name = cloud_event.data["name"]

        logger.info(f"Processing video: gs://{bucket}/{file_name}")

        # Check if we should process this file
        if not await video_processor_service.should_process(bucket, file_name):
            logger.info(f"Skipping file {file_name} (not a video or already processed)")
            return {"status": "skipped", "file": file_name}

        # Start processing job
        job = await video_processor_service.create_job(bucket, file_name)
        job_id = job.id

        logger.info(f"Created processing job {job_id} for {file_name}")

        # Process the video asynchronously
        # In a production environment, this would typically be offloaded to a task queue
        # or background worker to avoid Cloud Function timeout limits
        processing_result = await video_processor_service.process(job_id)

        if processing_result.status == ProcessingStatus.COMPLETED:
            logger.info(f"Successfully processed {file_name}")
            return {
                "status": "success",
                "job_id": job_id,
                "file": file_name,
                "outputs": processing_result.output_paths,
            }
        else:
            logger.error(f"Failed to process {file_name}: {processing_result.error}")
            return {
                "status": "error",
                "job_id": job_id,
                "file": file_name,
                "error": processing_result.error,
            }

    except Exception as e:
        logger.exception(f"Error processing event: {e}")
        return {"status": "error", "error": str(e)}


@functions_framework.http
async def process_video_http(request):
    """
    HTTP endpoint for triggering video processing.

    This function allows manual triggering of video processing via HTTP
    requests, primarily used for testing or administrative purposes.

    Args:
        request: Flask request object

    Returns:
        Flask response with processing results
    """
    try:
        # Extract bucket and file from request parameters
        request_json = request.get_json(silent=True)

        if not request_json:
            return {"error": "No JSON data provided"}, 400

        bucket = request_json.get("bucket")
        file_name = request_json.get("file")

        if not bucket or not file_name:
            return {"error": "Missing required parameters (bucket, file)"}, 400

        logger.info(f"HTTP trigger for processing: gs://{bucket}/{file_name}")

        # Create a synthetic cloud event and process it
        cloud_event = {
            "id": "manual-trigger",
            "data": {"bucket": bucket, "name": file_name},
        }

        result = await process_video_event(cloud_event)
        return result, 200

    except Exception as e:
        logger.exception(f"Error in HTTP handler: {e}")
        return {"status": "error", "error": str(e)}, 500


if __name__ == "__main__":
    """
    Local development entry point.

    This block is executed when the script is run directly (not as a Cloud Function).
    It provides a way to test the processing logic locally.
    """
    import asyncio
    import sys

    if len(sys.argv) < 3:
        print("Usage: python main.py <bucket_name> <file_name>")
        sys.exit(1)

    bucket = sys.argv[1]
    file_name = sys.argv[2]

    # Create synthetic event for local testing
    test_event = {"id": "local-test", "data": {"bucket": bucket, "name": file_name}}

    # Configure more verbose logging for local testing
    logging.basicConfig(level=logging.DEBUG)

    # Run the event handler
    result = asyncio.run(process_video_event(test_event))
    print(f"Processing result: {result}")
</file>

<file path=".ai_docs/templates/2-prd-template.md">
---
> The purpose of this file is to show an example of the PRD we want to generate for a given pitch.
---

# VIDEO PROCESSING PIPELINE - PRODUCT REQUIREMENTS DOCUMENT

## 1. Overview

This Product Requirements Document (PRD) outlines the specifications for developing a modern, scalable video processing pipeline system that automates the conversion of raw video content into publishable assets complete with AI-generated metadata. The system utilizes Google Cloud Platform services, particularly Cloud Run and Vertex AI, to process videos, extract audio, and generate high-quality transcripts, subtitles, chapters, titles, and other metadata to streamline the content publishing workflow.

## 2. Project goals

### 2.1 Primary goals

- Create a scalable, modular backend architecture for processing video files
- Implement AI-powered content generation for video metadata
- Develop a flexible system that supports multiple content channels and platforms
- Establish a maintainable codebase with modern Python design patterns
- Provide a robust API for frontend integration
- Enable automated deployment to Google Cloud Platform

### 2.2 Success metrics

- Processing time < 10 minutes for videos up to 30 minutes in length
- 95% accuracy in AI-generated transcripts (compared to manual transcription)
- 99.5% system availability
- 100% of videos successfully processed with complete metadata
- < 5% error rate in automated metadata generation
- Ability to process at least 50 concurrent video jobs

## 3. Current architecture

The project currently follows a partially modular architecture with several key components already implemented:

```
backend/
 video_processor/               # Main application module
    core/                      # Core application logic
       models/                # Domain models
          video_job.py       # VideoJob and ProcessingStage models
       processors/            # Processing components
           audio.py           # Audio extraction and processing
           chapters.py        # Chapter generation
           transcript.py      # Transcript generation
           video.py           # Video processing
    services/                  # External service integrations
       ai/                    # AI service integrations
       storage/               # Cloud storage services
       youtube/               # YouTube API integration
    api/                       # API definitions
       controllers.py         # Request handlers
       routes.py              # API route definitions
       schemas.py             # API data schemas
    utils/                     # Utility functions
    config/                    # Configuration management
    process_uploaded_video.py  # Main processing logic (monolithic)
    youtube_uploader.py        # YouTube uploading functionality
    main.py                    # Entry point for cloud functions
```

The current implementation primarily uses a monolithic approach in `process_uploaded_video.py` (711 lines), which handles multiple responsibilities:
- Video and audio processing (`extract_audio()` function)
- AI content generation (`generate_transcript()`, `generate_vtt()`, etc.)
- GCS bucket management (`write_blob()`, `move_processed_file()`)
- Process orchestration (`process_video_event()`)

This monolithic approach lacks clear separation of concerns, making it difficult to maintain, test, and extend the codebase.

## 4. Proposed architecture

We propose refactoring the existing codebase to follow a more modern Python architecture using a proper hexagonal/clean architecture approach:

```
backend/
 video_processor/
    domain/                      # Domain models and business logic
       models/                  # Domain entities
          video.py             # Video entity
          job.py               # Processing job entity
          metadata.py          # Video metadata entity
       exceptions.py            # Domain-specific exceptions
       value_objects.py         # Value objects for domain
    application/                 # Application services and use cases
       services/                # Application services
          video_processor.py   # Video processing orchestration
          transcription.py     # Transcript generation service
          subtitle.py          # Subtitle generation service
          metadata.py          # Metadata generation service
       interfaces/              # Service interfaces
          storage.py           # Storage service interface
          ai.py                # AI service interface
          publishing.py        # Publishing service interface
       dtos/                    # Data Transfer Objects
           video_job_dto.py     # DTOs for API communication
    adapters/                    # External service adapters
       ai/                      # AI service adapters
          vertex_ai.py         # Vertex AI implementation
          gemini.py            # Gemini API implementation
       storage/                 # Storage adapters
          gcs.py               # Google Cloud Storage implementation
          local.py             # Local filesystem implementation
       publishing/              # Publishing adapters
           youtube.py           # YouTube API implementation
    infrastructure/              # Framework-specific code
       config/                  # Configuration management
          settings.py          # Application settings
          container.py         # Dependency injection container
       api/                     # API framework implementation
          server.py            # FastAPI server definition
          routes/              # API route handlers
             videos.py        # Video-related endpoints
             health.py        # Health and status endpoints
          schemas/             # API schemas using Pydantic
             video.py         # Video-related schemas
          dependencies.py      # FastAPI dependencies
       repositories/            # Data repositories
          job_repository.py    # Job persistence
          video_repository.py  # Video metadata persistence
       messaging/               # Messaging infrastructure
           pubsub.py            # Google Pub/Sub integration
    utils/                       # Utility functions and helpers
       logging.py               # Logging configuration
       ffmpeg.py                # FFmpeg wrapper (from current audio.py)
       profiling.py             # Performance profiling tools
    main.py                      # Application entry point
 tests/                           # Test suite
    unit/                        # Unit tests
    integration/                 # Integration tests
    e2e/                         # End-to-end tests
 scripts/                         # Utility scripts for development/deployment
```

## 5. Key features & requirements

### 5.1 Video processing pipeline

**REQ-VP-001: Video upload and storage**
- System must accept video uploads from GCS bucket triggers
- Reuse existing `bucket_name` and `file_name` handling from `process_uploaded_video.py`
- Refactor `should_process_file()` and `setup_output_paths()` into domain services

**REQ-VP-002: Audio extraction**
- Extract audio from video files for AI analysis
- Maintain existing functionality from `extract_audio()` in `process_uploaded_video.py`
- Refactor to follow adapter pattern in `utils/ffmpeg.py`

**REQ-VP-003: Transcript generation**
- Generate high-quality transcripts from audio
- Migrate logic from `generate_transcript()` to dedicated service
- Implement as adapter in `adapters/ai/gemini.py`

**REQ-VP-004: Subtitle generation**
- Create WebVTT format subtitles for videos
- Migrate logic from `generate_vtt()` to dedicated service
- Implement in `application/services/subtitle.py`

**REQ-VP-005: Content summarization**
- Generate show notes based on video content
- Migrate logic from `generate_shownotes()` to dedicated service
- Use dependency injection to allow different AI providers

**REQ-VP-006: Chapter generation**
- Create timestamped chapters for video content
- Migrate logic from `generate_chapters()` to dedicated service
- Use existing domain model structure from `core/processors/chapters.py`

**REQ-VP-007: Title suggestion**
- Generate optimized title variations for video
- Migrate logic from `generate_titles()` to dedicated service
- Implement in `application/services/metadata.py`

**REQ-VP-008: YouTube integration**
- Support publishing processed videos to YouTube
- Maintain functionality from `youtube_uploader.py`
- Refactor to adapter pattern in `adapters/publishing/youtube.py`

**REQ-VP-009: Processing job management**
- Track status of video processing through workflow
- Expand current `VideoJob` model from `core/models/video_job.py`
- Implement repository pattern for job persistence

### 5.2 API requirements

**REQ-API-001: Video processing API**
- API endpoint for triggering video processing
- Refactor existing API implementation in `api/routes.py` and `api/controllers.py`
- Migrate to FastAPI framework in `infrastructure/api/server.py`

**REQ-API-002: Job status API**
- Endpoint for checking video processing status
- Create new endpoint based on current job tracking
- Implement in `infrastructure/api/routes/videos.py`

**REQ-API-003: API documentation**
- OpenAPI documentation for all endpoints
- Automatic generation via FastAPI
- Example requests and responses

**REQ-API-004: Authentication & authorization**
- Secure API endpoints with authentication
- Token-based authentication for frontend access
- Role-based access control for administrative functions

**REQ-API-005: Health check endpoints**
- Service health monitoring endpoints
- Connectivity checks for dependent services
- Version information endpoint

### 5.3 Technical requirements

**REQ-TECH-001: Clean architecture**
- Refactor existing code following clean architecture principles
- Separate domain logic from infrastructure concerns
- Implement proper dependency injection

**REQ-TECH-002: Testing strategy**
- Comprehensive unit test coverage for domain and application layers
- Integration tests for adapters and infrastructure
- End-to-end tests for critical flows
- Maintain test fixtures in line with current `conftest.py`

**REQ-TECH-003: Containerization**
- Docker container configuration for local testing
- Utilize and expand existing `Dockerfile` and `docker-compose.yml`
- Multi-stage build for efficient deployment

**REQ-TECH-004: Configuration management**
- Environment-based configuration
- Secrets management using GCP Secret Manager
- Feature flags for progressive rollout

**REQ-TECH-005: Performance optimization**
- Benchmark processing stages
- Implement performance profiling
- Optimize AI service usage

**REQ-TECH-006: Monitoring & observability**
- Structured logging throughout application
- Performance metric collection
- Error reporting and alerting

**REQ-TECH-007: Continuous integration/deployment**
- Automated testing pipeline
- Deployment to GCP Cloud Run
- Infrastructure as code for deployment

### 5.4 Deployment requirements

**REQ-DEPLOY-001: Google Cloud Run deployment**
- Deploy as containerized service on Cloud Run
- Optimize existing `deploy.sh` script (268 lines)
- Configure service accounts and permissions

**REQ-DEPLOY-002: Cloud functions integration**
- Maintain Cloud Function triggers for GCS events
- Refactor `main.py` event handlers
- Optimize cold start performance

**REQ-DEPLOY-003: Scaling configuration**
- Configure auto-scaling parameters
- Resource allocation optimization
- Concurrency limits

**REQ-DEPLOY-004: Monitoring setup**
- Configure Cloud Monitoring
- Set up logging and alerting
- Performance dashboards

**REQ-DEPLOY-005: Cost optimization**
- Optimize resource usage
- Implement caching where appropriate
- Configure budget alerts

## 6. User stories

### 6.1 Content creator stories

**US-001: Video upload processing**
- As a content creator, I want to upload a video and have it automatically processed, so I can quickly publish it with minimal manual effort.
- *Acceptance criteria:*
  - Video is processed within 10 minutes of upload
  - Creator receives notification when processing is complete
  - All metadata (transcript, subtitles, etc.) is generated automatically
  - Implementation should refactor `process_video_event()` from `process_uploaded_video.py`

**US-002: Processing status tracking**
- As a content creator, I want to see the current status of my video processing, so I know when it will be ready.
- *Acceptance criteria:*
  - Status updates for each processing stage
  - Estimated completion time
  - Error notifications if processing fails
  - Progress indication for long-running processes
  - Leverage `ProcessingStage` and `ProcessingStatus` from `core/models/video_job.py`

**US-003: Metadata review and editing**
- As a content creator, I want to review and edit generated metadata before publishing, so I can ensure quality and accuracy.
- *Acceptance criteria:*
  - Interface to view all generated metadata
  - Editing capabilities for transcript, title, description
  - Preview of how metadata will appear on platforms
  - Changes are saved and used for publishing

**US-004: Platform-specific publishing**
- As a content creator, I want to publish my processed video to multiple platforms with optimized metadata, so I can maximize my audience reach.
- *Acceptance criteria:*
  - One-click publishing to YouTube
  - Platform-specific metadata optimization
  - Scheduling options for delayed publishing
  - Publishing status and confirmation
  - Extend existing YouTube integration in `youtube_uploader.py`

**US-005: Processing customization**
- As a content creator, I want to customize which processing steps are applied to my video, so I can control the output based on my needs.
- *Acceptance criteria:*
  - Options to enable/disable specific processing steps
  - Presets for common combinations of processing steps
  - Custom parameters for specific processors (e.g., subtitle style)
  - Settings are remembered for future uploads

### 6.2 Administrator stories

**US-006: System monitoring**
- As an administrator, I want to monitor the overall system performance and processing queue, so I can ensure the service is running optimally.
- *Acceptance criteria:*
  - Dashboard showing current processing queue
  - Resource utilization metrics
  - Error rate and common failure points
  - Throughput and processing time statistics

**US-007: Service configuration**
- As an administrator, I want to configure system parameters and resource allocation, so I can optimize performance and costs.
- *Acceptance criteria:*
  - Configuration interface for system parameters
  - Ability to adjust resource limits
  - Cost estimation based on configuration
  - Changes take effect without service restart

**US-008: User management**
- As an administrator, I want to manage user accounts and permissions, so I can control access to the system.
- *Acceptance criteria:*
  - User creation, deletion, and editing
  - Role assignment
  - Access control for sensitive operations
  - Audit log of administrative actions

**US-009: Error investigation**
- As an administrator, I want to investigate processing errors, so I can resolve issues and improve system reliability.
- *Acceptance criteria:*
  - Detailed error logs with context
  - Ability to reprocess failed videos
  - Trend analysis of common errors
  - Integration with monitoring tools

### 6.3 Developer stories

**US-010: API integration**
- As a developer, I want to integrate with the video processing API, so I can use its capabilities in other applications.
- *Acceptance criteria:*
  - Comprehensive API documentation
  - Authentication mechanism for API access
  - Examples and SDKs for common languages
  - Rate limiting and usage metrics

**US-011: Custom processor development**
- As a developer, I want to create custom processors for the pipeline, so I can extend its functionality for specific needs.
- *Acceptance criteria:*
  - Documentation for processor interface
  - Example processors to use as templates
  - Testing framework for custom processors
  - Deployment process for new processors

**US-012: Webhook integration**
- As a developer, I want to configure webhooks for processing events, so I can trigger actions in external systems.
- *Acceptance criteria:*
  - Configurable webhook endpoints for different event types
  - Event payload documentation
  - Retry mechanism for failed webhook deliveries
  - Webhook delivery logs

## 7. Technical architecture details

### 7.1 Domain model

**Video Job Entity**
- Currently implemented in `core/models/video_job.py`
- Will be refactored to `domain/models/job.py`
- Core attributes:
  - Job ID (unique identifier)
  - Status (pending, in-progress, completed, failed)
  - Current processing stage
  - Completed stages
  - Created/updated timestamps
  - Source video reference
  - Output references
  - Error information

**Video Entity**
- Currently partially implemented in processing logic
- Will be defined in `domain/models/video.py`
- Core attributes:
  - Video ID
  - File information (name, size, format)
  - Duration
  - Resolution
  - Source location
  - Processed state

**Video Metadata Entity**
- Currently implemented as `VideoMetadata` in `core/models/video_job.py`
- Will be refactored to `domain/models/metadata.py`
- Core attributes:
  - Title
  - Description
  - Keywords/tags
  - Chapters
  - Transcript
  - Subtitles
  - Thumbnails
  - Platform-specific metadata

### 7.2 Use cases & application services

**Video Processing Service**
- Orchestrates the processing workflow
- Manages state transitions for video jobs
- Coordinates between different processing services
- Will expand `process_video_event()` from `process_uploaded_video.py`

**Transcription Service**
- Handles audio analysis and transcript generation
- Refactors `generate_transcript()` from `process_uploaded_video.py`
- Interfaces with AI services

**Subtitle Generation Service**
- Creates timestamped subtitles from audio
- Refactors `generate_vtt()` from `process_uploaded_video.py`
- Formats output for different platforms

**Metadata Generation Service**
- Creates various metadata artifacts (titles, descriptions, etc.)
- Refactors `generate_titles()` and other generation functions
- Optimizes metadata for different platforms

**Publishing Service**
- Handles video publishing to platforms
- Will refactor logic from `youtube_uploader.py`
- Manages platform-specific requirements

### 7.3 Adapters & infrastructure

**AI Service Adapters**
- Abstract interface in `application/interfaces/ai.py`
- Vertex AI implementation in `adapters/ai/vertex_ai.py`
- Gemini implementation in `adapters/ai/gemini.py`
- Mock implementation for testing

**Storage Adapters**
- Abstract interface in `application/interfaces/storage.py`
- GCS implementation in `adapters/storage/gcs.py`
- Local filesystem implementation for testing
- Refactors storage interactions from `process_uploaded_video.py`

**Publishing Adapters**
- Abstract interface in `application/interfaces/publishing.py`
- YouTube implementation in `adapters/publishing/youtube.py`
- Refactors from existing `youtube_uploader.py`

**API Infrastructure**
- FastAPI server configuration
- Route handlers for all endpoints
- Request/response schemas using Pydantic
- Dependency injection setup

### 7.4 Dependency injection

**Container Configuration**
- Will be implemented in `infrastructure/config/container.py`
- Manages application service instantiation
- Configures service dependencies
- Allows swapping implementations based on environment

**Service Locator Pattern**
- Provides access to services throughout application
- Simplifies testing through mock injection
- Enforces proper separation of concerns

### 7.5 Testing strategy

**Unit Testing**
- Domain models and business logic
- Application services with mocked dependencies
- Expand existing tests in `test_process_video.py` and `test_audio_processing.py`

**Integration Testing**
- Storage adapters against emulators/test environments
- AI service adapters with simplified models
- API endpoints with test client

**End-to-End Testing**
- Complete processing workflow
- Cloud Function triggers
- Video processing and publishing

### 7.6 Benchmarking and profiling

**Performance Metrics**
- Processing time per stage
- Memory utilization
- API response times
- AI service latency

**Profiling Tools**
- Code instrumentation for timing
- Resource usage tracking
- Bottleneck identification

### 7.7 Deployment architecture

**GCP Services**
- Cloud Run for API and processing service
- Cloud Storage for video files and artifacts
- Cloud Functions for event triggers
- Pub/Sub for asynchronous communication
- Secret Manager for credentials
- Logging and Monitoring

## 8. Implementation phases

### 8.1 Phase 1: Refactor core architecture

**Sprint 1: Domain model and structure setup**
- Establish new project structure
- Refactor domain models from existing code
- Set up dependency injection container
- Implement basic testing framework

**Sprint 2: Service layer implementation**
- Refactor processing services from monolithic code
- Implement storage adapters
- Create AI service adapters
- Unit test coverage for core services

### 8.2 Phase 2: API and infrastructure

**Sprint 3: API implementation**
- Implement FastAPI server
- Create API schemas and routes
- Add authentication and authorization
- Develop API documentation

**Sprint 4: Deployment and monitoring**
- Configure Docker and Cloud Run deployment
- Set up monitoring and logging
- Implement health checks
- Create deployment automation

### 8.3 Phase 3: Enhanced functionality

**Sprint 5: Advanced metadata generation**
- Enhance AI-powered metadata generation
- Implement feedback loop for quality improvement
- Add customization options
- Optimize AI prompt engineering

**Sprint 6: Multi-platform publishing**
- Extend YouTube integration
- Add support for additional platforms
- Implement platform-specific optimizations
- Create publishing queue and scheduling

### 8.4 Phase 4: Frontend integration and polish

**Sprint 7: Frontend API integration**
- Finalize API contracts with frontend
- Implement real-time status updates
- Create comprehensive API examples
- Develop client SDK

**Sprint 8: Performance optimization and scaling**
- Optimize processing performance
- Configure auto-scaling
- Implement caching strategies
- Conduct load testing

## 9. Metrics and analytics

### 9.1 Key performance indicators (KPIs)

- Video processing throughput (videos/hour)
- Average processing time per video
- AI generation quality scores
- API response times
- Error rates by processing stage
- System availability percentage

### 9.2 User analytics

- Processing volume by user
- Feature usage statistics
- Platform publishing distribution
- User retention and engagement

### 9.3 Cost analytics

- Processing cost per video
- AI service usage and cost
- Storage utilization and cost
- Infrastructure cost breakdown

## 10. Migration plan

### 10.1 Current code migration

- The existing code in `backend/video_processor/process_uploaded_video.py` (711 lines) will be refactored into:
  - Domain models in `domain/models/`
  - Application services in `application/services/`
  - Adapters in `adapters/`
  
- The YouTube uploader code in `backend/video_processor/youtube_uploader.py` (535 lines) will be refactored into:
  - YouTube adapter in `adapters/publishing/youtube.py`
  - Publishing service in `application/services/publishing.py`
  
- The API code in `backend/video_processor/api/` will be migrated to FastAPI in `infrastructure/api/`

### 10.2 Database migration

- Any existing data will be migrated to the new structure
- Backward compatibility maintained during transition
- Data validation and cleanup during migration

### 10.3 Testing and verification

- Parallel running of old and new systems
- Comparison of outputs for validation
- Performance benchmarking comparison
- Gradual traffic migration

## 11. Risks and mitigations

**Risk: AI service reliability**
- *Impact:* Processing failures, poor quality metadata
- *Mitigation:* Fallback providers, retry mechanisms, quality monitoring

**Risk: Processing performance issues**
- *Impact:* Long processing times, capacity limitations
- *Mitigation:* Performance profiling, optimization, scaling configuration

**Risk: GCP service limits**
- *Impact:* Processing throttling, increased costs
- *Mitigation:* Quotas monitoring, graceful degradation, cost alerts

**Risk: API compatibility issues**
- *Impact:* Frontend integration failures
- *Mitigation:* Versioned API, comprehensive testing, backward compatibility

**Risk: Security vulnerabilities**
- *Impact:* Unauthorized access, data breaches
- *Mitigation:* Security review, access controls, regular updates

## 12. Glossary

**AI service:** Cloud-based artificial intelligence services used for content analysis and generation.

**Adapter:** A software component that translates between core business logic and external services.

**Clean architecture:** A software design philosophy that separates concerns into layers with dependencies pointing inward.

**Dependency injection:** A technique where object dependencies are provided from outside rather than created internally.

**Domain model:** Core business objects and logic independent of external concerns.

**FastAPI:** A modern, high-performance web framework for building APIs with Python based on standard type hints.

**GCP:** Google Cloud Platform, the cloud infrastructure provider for the system.

**Hexagonal architecture:** An architectural pattern that allows an application to be driven by users, programs, or tests equally.

**Processing stage:** A discrete step in the video processing pipeline.

**Repository pattern:** A design pattern that mediates between the domain and data mapping layers.

**Vertex AI:** Google Cloud's unified machine learning platform.

**WebVTT:** Web Video Text Tracks format, a subtitle format for HTML5 video.
</file>

<file path=".ai_docs/templates/3-architecture-template.md">
--- 
> This is a template of the expected output when generating an architecture document

Be sure to read the following files to understand the context of the project:

`pitch.md`
`prd.md`
`architecture.md`
`system-patterns.md`

## Run the following command

eza . --tree --git-ignore

# Pass in the files you want included in the tree.
> Read the files below and nothing else.

{apps/web/}
{supabase/}



---




# File Structure Comparison: Current vs. Proposed

## Current Backend Structure

The current backend follows a partially modular architecture but has significant monolithic components:

```
backend/
 video_processor/                           # Main application module
    core/                                  # Core application logic
       models/                            # Domain models
          __init__.py                    # (8 lines) Module exports
          video_job.py                   # (167 lines) VideoJob and ProcessingStage models
       processors/                        # Processing components
          __init__.py                    # (16 lines) Module exports
          audio.py                       # (196 lines) Audio extraction and processing
          chapters.py                    # (154 lines) Chapter generation
          transcript.py                  # (132 lines) Transcript generation
          video.py                       # (252 lines) Video processing
       __init__.py                        # (4 lines) Module exports
    services/                              # External service integrations
       ai/                                # AI service integrations
       storage/                           # Cloud storage services
       youtube/                           # YouTube API integration
       __init__.py                        # (4 lines) Module exports
    api/                                   # API definitions
       __init__.py                        # (8 lines) Module exports
       controllers.py                     # (139 lines) Request handlers
       routes.py                          # (99 lines) API route definitions
       schemas.py                         # (64 lines) API data schemas
    utils/                                 # Utility functions
    config/                                # Configuration management
    tests/                                 # Test directory (nested)
    __init__.py                            # (2 lines) Package initialization
    process_uploaded_video.py              # (711 lines) Main processing logic (monolithic)
    youtube_uploader.py                    # (535 lines) YouTube uploading functionality
    generate_youtube_token.py              # (151 lines) Token generation for YouTube API
    firestore_trigger_listener.py          # (69 lines) Firestore event processing
    setup_youtube_secrets.py               # (205 lines) YouTube credential setup
    app.py                                 # (27 lines) Flask application entry point
    main.py                                # (103 lines) Entry point for cloud functions
    test_process_video.py                  # (72 lines) Tests for video processing
    test_audio_processing.py               # (95 lines) Tests for audio processing
    conftest.py                            # (59 lines) Pytest fixtures
    pytest.ini                             # (11 lines) Pytest configuration
    setup.py                               # (22 lines) Package setup
    README.md                              # (190 lines) Module documentation
    youtube_uploader_README.md             # (114 lines) YouTube uploader documentation
    .gcloudignore                          # (9 lines) GCP deployment ignore patterns
 tests/                                     # Test suite (root level)
    unit/                                  # Unit tests
    integration/                           # Integration tests
    e2e/                                   # End-to-end tests
 scripts/                                   # Utility scripts
 docs/                                      # Documentation
 requirements.txt                           # (31 lines) Dependencies
 Dockerfile                                 # (27 lines) Container definition
 Dockerfile.mock                            # (19 lines) Mock container for testing
 deploy.sh                                  # (268 lines) Deployment script
```

## Key Issues with Current Structure

1. **Monolithic Files:**
   - `process_uploaded_video.py` (711 lines) handles multiple concerns:
     - Video and audio processing
     - AI content generation
     - Cloud storage management
     - Process orchestration
   - `youtube_uploader.py` (535 lines) combines too many YouTube API operations

2. **Mixed Responsibilities:**
   - Tests are split between the module-level and project-level directories
   - Configuration is spread across multiple files
   - No clear separation between domain logic and infrastructure

3. **Limited Modularity:**
   - Difficult to replace components with alternative implementations
   - Testing requires many mock objects
   - Heavy coupling between processing stages

## Proposed Backend Structure

The proposed architecture follows clean architecture principles with well-defined layers:

```
backend/
 video_processor/                           # Main package
    domain/                                # Domain models and business logic
       models/                            # Domain entities
          __init__.py                    # Package initialization
          video.py                       # Video entity
          job.py                         # Processing job entity (from video_job.py)
          metadata.py                    # Video metadata entity (from video_job.py)
       __init__.py                        # Package initialization
       exceptions.py                      # Domain-specific exceptions
       value_objects.py                   # Value objects for domain
    application/                           # Application services and use cases
       services/                          # Application services
          __init__.py                    # Package initialization 
          video_processor.py             # Video processing orchestration (from process_uploaded_video.py)
          transcription.py               # Transcript generation service (from generate_transcript())
          subtitle.py                    # Subtitle generation service (from generate_vtt())
          metadata.py                    # Metadata generation service (from existing generators)
       interfaces/                        # Service interfaces
          __init__.py                    # Package initialization
          storage.py                     # Storage service interface
          ai.py                          # AI service interface
          publishing.py                  # Publishing service interface
       __init__.py                        # Package initialization
       dtos/                              # Data Transfer Objects
           __init__.py                    # Package initialization
           video_job_dto.py               # DTOs for API communication
    adapters/                              # External service adapters
       ai/                                # AI service adapters
          __init__.py                    # Package initialization
          vertex_ai.py                   # Vertex AI implementation 
          gemini.py                      # Gemini API implementation (from process_uploaded_video.py)
       storage/                           # Storage adapters
          __init__.py                    # Package initialization
          gcs.py                         # Google Cloud Storage implementation (from process_uploaded_video.py)
          local.py                       # Local filesystem implementation
       __init__.py                        # Package initialization
       publishing/                        # Publishing adapters
           __init__.py                    # Package initialization
           youtube.py                     # YouTube API implementation (from youtube_uploader.py)
    infrastructure/                        # Framework-specific code
       config/                            # Configuration management
          __init__.py                    # Package initialization
          settings.py                    # Application settings
          container.py                   # Dependency injection container
       api/                               # API framework implementation
          __init__.py                    # Package initialization
          server.py                      # FastAPI server definition (replacing Flask app.py)
          routes/                        # API route handlers
             __init__.py                # Package initialization
             videos.py                  # Video-related endpoints (from api/routes.py)
             health.py                  # Health and status endpoints
          schemas/                       # API schemas using Pydantic
             __init__.py                # Package initialization
             video.py                   # Video-related schemas (from api/schemas.py)
          dependencies.py                # FastAPI dependencies
       repositories/                      # Data repositories
          __init__.py                    # Package initialization
          job_repository.py              # Job persistence
          video_repository.py            # Video metadata persistence
       __init__.py                        # Package initialization
       messaging/                         # Messaging infrastructure
           __init__.py                    # Package initialization
           pubsub.py                      # Google Pub/Sub integration
    utils/                                 # Utility functions and helpers
       __init__.py                        # Package initialization
       logging.py                         # Logging configuration
       ffmpeg.py                          # FFmpeg wrapper (from core/processors/audio.py)
       profiling.py                       # Performance profiling tools
    __init__.py                            # Package initialization
    main.py                                # Application entry point (from main.py)
 tests/                                     # Test suite
    unit/                                  # Unit tests
       domain/                            # Tests for domain models
       application/                       # Tests for application services 
       adapters/                          # Tests for adapters
    integration/                           # Integration tests
       api/                               # API integration tests
       storage/                           # Storage integration tests
       ai/                                # AI services integration tests
    e2e/                                   # End-to-end tests
    conftest.py                            # Test fixtures (from video_processor/conftest.py)
 scripts/                                   # Utility scripts for development/deployment
    deploy.sh                              # Deployment script (improved from root deploy.sh)
    generate_youtube_token.py              # Token generation script (from video_processor)
 docs/                                      # Documentation
    be-prd.txt                             # Product Requirements Document
    file-structure-comparison.md           # This file
 requirements.txt                           # Dependencies
 Dockerfile                                 # Container definition
 pyproject.toml                             # Project configuration (replacing setup.py)
 README.md                                  # Project documentation
```

## Key Benefits of Proposed Structure

1. **Clean Architecture:**
   - Clear separation of concerns between layers
   - Domain logic isolated from infrastructure details
   - Easier to reason about and maintain

2. **Improved Testability:**
   - Domain logic is free from external dependencies
   - Interfaces allow easy mocking of external services
   - Dedicated test directory structure aligns with application structure

3. **Enhanced Modularity:**
   - Dependency injection enables swapping implementations
   - Adapters isolate external service interactions
   - Repository pattern separates persistence concerns

4. **Better Maintainability:**
   - Smaller, focused files with single responsibilities
   - Clear boundaries between application components
   - Standardized structure that's easier to navigate

5. **Scalability:**
   - New features can be added without affecting existing components
   - Processing stages can be evolved independently
   - Multiple developers can work on different areas simultaneously
</file>

<file path=".ai_docs/templates/4-system-patterns-template.md">
---

> This is a template of the expected output when generating a list of system patterns.

Be sure to read the following files to understand the context of the project:

`pitch.md`
`prd.md`
`architecture.md`
---

## Backend Architecture Rule Enforcement

**All backend code must strictly adhere to the architectural rules defined in `.cursor/rules/architecture.mdc`. This file is the canonical source for backend architecture, code organization, and quality standards. Code reviews and all development work must enforce these rules. Any deviations must be explicitly justified and documented in the Memory Bank.**

## Overall Architecture Philosophy

The Echo platform is designed as a distributed system with a clear separation between the backend API (responsible for processing, AI integration, and core logic) and the frontend web application (responsible for user interaction and presentation). Communication primarily occurs via RESTful APIs and WebSockets for real-time updates.

```mermaid
graph TD
    User[User] -- HTTPS --> Frontend[Frontend Web App (React/Next.js)]
    Frontend -- REST API / WebSockets --> BackendAPI[Backend API (FastAPI)]

    BackendAPI -- Integrates with --> AIServices[AI Services (Gemini, OpenAI, etc.)]
    BackendAPI -- Integrates with --> CloudStorage[Cloud Storage (GCS)]
    BackendAPI -- Integrates with --> Database[Database (Supabase/Postgres)]
    BackendAPI -- Integrates with --> Publishing[Publishing Platforms (YouTube)]

    Frontend -- Direct Upload --> CloudStorage
    Frontend -- Auth/Data --> Database
```

---

## Backend (API) System Patterns

### Recent Infrastructure & Utility Patterns

- **Router Aggregation Pattern**: All API routers are aggregated and re-exported in a single `api/endpoints/__init__.py` file as `router`. This enables scalable API structure, simplifies router registration in `main.py`, and supports future expansion with minimal changes to the app entrypoint.
- **Async Redis Caching**: Use of an async `RedisCache` class (`lib/cache/redis_cache.py`) for non-blocking cache operations, settings-driven, with simple get/set interface. Used for AI adapter caching and other backend caching needs.
- **Utility Classes**: All infrastructure utilities (FFmpeg, file handling, subtitle generation) are implemented as stateless classes in `lib/utils/`, with only static methods and no side effects. This ensures testability and reusability.
- **Authentication Utilities**: Supabase JWT validation and user extraction are handled by a dedicated module (`lib/auth/supabase_auth.py`) using Pydantic models and FastAPI dependencies.
- **Custom Exception Hierarchy**: All service and utility errors are handled via a custom exception hierarchy in `core/exceptions.py`, with base and specialized error types for video processing, AI, and FFmpeg.
- **Repository Pattern**: All data access for core models is handled by repository classes in `operations/`, each with static CRUD and update methods, accepting a SQLAlchemy Session as the first argument. No business logic is present in repositories.

### Core Architecture: Clean Architecture
The backend adheres to Clean Architecture principles, ensuring a separation of concerns and promoting maintainability, testability, and scalability. The layers are:

1.  **Domain Layer**: Contains core business entities (e.g., `Video`, `Job`, `Metadata`), value objects, domain events, and business rules. It has no dependencies on outer layers.
    *   *Key Patterns*: Rich Domain Models, Value Objects, Domain Events.
2.  **Application Layer**: Orchestrates use cases and defines service interfaces (e.g., `VideoProcessorService`, `StorageInterface`, `AIInterface`). It depends only on the Domain layer.
    *   *Key Patterns*: Application Services, Port (Interface) definitions.
3.  **Adapters Layer**: Implements the interfaces defined by the Application layer, connecting to external systems or providing specific implementations. Examples include GCS storage adapter, Gemini AI adapter.
    *   *Key Patterns*: Adapter Pattern, concrete implementations of Application Ports.
4.  **Infrastructure Layer**: Provides technical capabilities, framework-specific code (FastAPI setup, routing, schemas), configuration, dependency injection container, and actual external system integrations (database connections, API clients for external services).
    *   *Key Patterns*: Dependency Injection Container, Repository Implementations, API Route Handlers, Configuration Management.

```mermaid
graph LR
    subgraph BackendAPI
        direction LR
        Infrastructure[Infrastructure Layer (FastAPI, GCS Client, DB Client)] -- Implements/Uses --> Adapters
        Adapters[Adapters Layer (GCSStorageAdapter, GeminiAIAdapter)] -- Implements --> Application
        Application[Application Layer (VideoService, IStorage)] -- Uses --> Domain
        Domain[Domain Layer (Video, Job, Metadata)]
    end
```

### Key Backend Design Patterns:
*   **Repository Pattern**: Abstracts data persistence operations. Interfaces are defined in the Application (or Domain) layer, with implementations in the Infrastructure layer (e.g., `SupabaseJobRepository`).
*   **Dependency Injection**: Used extensively to decouple components. A DI container (e.g., `punq`) is configured in the Infrastructure layer to manage service lifecycles and inject dependencies into Application services and API controllers.
*   **Adapter Pattern**: For all external service integrations (storage, AI, publishing), isolating the core application from specific third-party library changes. Examples include:
    * `FileStorageService`: Provides a unified interface for both local filesystem and Google Cloud Storage operations.
    * `AIAdapterInterface` and `GeminiAdapter`: Abstract AI service operations behind a consistent interface.
    * `PublishingInterface` and `YouTubeAdapter`: Isolate publishing operations from specific platforms.
*   **Factory Pattern**: Used to create appropriate service instances based on configuration. Examples:
    * `get_ai_adapter()`: Creates the appropriate AI adapter based on available API keys.
    * `get_file_storage_service()`: Provides a singleton FileStorageService instance.
*   **Domain-Driven Design (DDD)**: Emphasis on rich domain models, value objects, and clearly defined bounded contexts (though the current system is largely a single primary context).
*   **SQLAlchemy ORM**: Leveraging SQLAlchemy for database model definition and querying, using `Base` declarative models, relationship mappings, and type-annotated columns. Models follow a convention of suffixing with `Model` (e.g., `VideoModel`, `VideoJobModel`) to distinguish them from domain entities.
*   **Centralized Configuration**: Using a Pydantic `Settings` class to load and validate environment variables, with a singleton instance accessible throughout the application via imports.
*   **Asynchronous Operations with Thread Pooling**: For I/O-bound operations (file storage, AI API calls), using async/await patterns with ThreadPoolExecutor to avoid blocking the event loop.
*   **Task Queuing (Conceptual - e.g., Celery/RabbitMQ/Cloud Tasks)**: For long-running video processing tasks, a task queue will be essential to handle background processing, retries, and scalability. (Specific choice to be finalized).

### Backend Component Relationships & Flows:

*   **API Request Flow (FastAPI)**:
    `HTTP Request` -> `FastAPI Router` -> `Authentication/Authorization Middleware` -> `Dependency Injection (resolves services)` -> `Application Service Method` -> `(Optional) Repository/Adapters` -> `Domain Logic` -> `Response Mapping (Pydantic Schemas)` -> `HTTP Response`.
*   **Video Processing Flow (Simplified)**:
    1.  Client (Frontend) requests a signed URL from Backend API.
    2.  Client uploads video directly to Cloud Storage (GCS).
    3.  Client notifies Backend API of successful upload, triggering a processing job.
    4.  Backend API (potentially via a task queue) initiates `VideoProcessorService`.
    5.  `VideoProcessorService` uses:
        *   `FileStorageService` to access the video, supporting both local and GCS backends.
        *   `AIAdapterInterface` implementations (selected via factory) for text generation, content analysis, and transcription.
        *   `MetadataService` to generate and store metadata.
    6.  Job status is updated in the Database (`JobRepository`).
    7.  Real-time updates sent to client via WebSockets.
    8.  (Optional) `PublishingAdapter` distributes content to platforms like YouTube.

### Backend Module Structure (Illustrative - New Structure):
```
apps/core/
 models/                 # SQLAlchemy ORM models
    enums.py            # Enum definitions (e.g., ProcessingStatus)
    video_model.py      # Video entity model
    video_job_model.py  # Job processing model
    video_metadata_model.py # Video metadata model
 lib/                    # Core libraries and utilities
    ai/                 # AI service adapters
    database/           # Database connection, session management
    storage/            # Storage service adapters
    utils/              # Utility functions and helpers
    cache/              # Caching mechanisms
 operations/             # Repository implementations
 services/               # Service layer orchestration
 api/                    # FastAPI routes and schemas
    endpoints/          # API route handlers
    schemas/            # Pydantic schemas for API
 core/                   # Core application setup
    config.py           # Configuration management
    exceptions.py       # Application exceptions
 alembic/                # Database migrations
 main.py                 # FastAPI app instantiation
```

---

## Frontend (Web Application) System Patterns

### Core Architecture: Modern React SPA
The frontend is a Single-Page Application (SPA) built with React (likely Next.js or Vite with TanStack Router, choice to be confirmed and standardized). Focus is on a responsive user experience, efficient state management, and real-time communication with the backend.

```mermaid
graph TD
    subgraph FrontendWebApp
        direction TB
        Browser[Browser]
        Browser -- Interacts --> UILayer[UI Layer (React Components - shadcn/ui, Custom)]
        UILayer -- Uses --> Routing[Routing (TanStack Router)]
        UILayer -- Manages/Uses --> StateManagement[State Management (TanStack Query, Zustand/Context)]
        StateManagement -- Interacts --> APIServices[API Services Layer]
        APIServices -- HTTP/REST --> BackendAPIClient[Backend API Client (axios/fetch)]
        APIServices -- WebSocket --> WebSocketClient[WebSocket Client]
        APIServices -- Auth/Data --> SupabaseClient[Supabase Client]
    end
```

### Key Frontend Technical Decisions & Patterns:
1.  **Component-Based Architecture**: UI built using reusable React components, potentially leveraging a UI library like `shadcn/ui` for styling and pre-built elements.
2.  **Routing**: Client-side routing managed by a library like `TanStack Router` for navigation without full page reloads.
3.  **State Management**:
    *   **Server State**: `TanStack Query` for managing data fetched from the API, including caching, background updates, and optimistic updates.
    *   **Client/UI State**: Local component state (`useState`, `useReducer`) or a global client state solution (e.g., `Zustand`, `Context API`, or `Redux Toolkit` if complexity warrants) for UI-specific state not tied to the server.
4.  **Authentication**: Integration with Supabase for Google OAuth. JWT tokens obtained from Supabase/backend will be managed by the API client (e.g., stored securely and attached to requests).
5.  **Real-time Updates**: A dedicated WebSocket client (`use-app-websocket` hook or similar) to connect to the FastAPI backend, receive real-time messages, and update relevant state (likely by invalidating or directly updating `TanStack Query` cache).
6.  **Direct-to-Cloud Uploads**: For video uploads, the frontend will request a signed URL from the backend and then upload the file directly to GCS, reducing backend server load.
7.  **Custom Hooks (`/hooks`)**: Encapsulating reusable logic, such as WebSocket connection management, API call patterns, or device detection (`useMobile`).
8.  **TypeScript**: For static typing, improving code quality and maintainability.

### Frontend Component Relationships & Structure (Illustrative):
*   **`src/routes/`**: Components defining application pages/views, responsible for fetching data (often via `TanStack Query` loaders) and composing feature-specific components.
*   **`src/components/features/`** (or `src/components/video/`, `src/components/auth/` etc.): Components that implement specific application features (e.g., `VideoProcessingDashboard`, `MetadataEditor`). They combine UI elements and business logic relevant to that feature.
*   **`src/components/ui/`**: Reusable, presentational UI components (e.g., buttons, modals, cards from `shadcn/ui` or custom-built).
*   **`src/components/shared/`**: Common layout components like `Navbar`, `Footer`, `Container` used across multiple routes.
*   **`src/lib/` or `src/services/`**: API client setup, Supabase client instance, utility functions.
*   **`src/hooks/`**: Custom React hooks for shared logic.
*   **`src/store/`** (if using global state manager): State management configuration.

### Critical Frontend Implementation Paths:
1.  **Authentication Flow**:
    `LoginPage` -> `Supabase Google OAuth Redirect` -> `Callback Handling` -> `Store JWT/Session` -> `Authenticated API Client` -> `Protected Routes`.
2.  **Video Upload Flow**:
    `Upload UI (e.g., Dropzone)` -> `Frontend requests Signed URL (API call)` -> `Backend returns Signed URL` -> `Frontend uploads directly to GCS` -> `Frontend notifies Backend of success (API call)` -> `Backend initiates processing` -> `WebSocket status updates to Frontend`.
3.  **Real-time Dashboard Update Flow**:
    `Dashboard Mounts` -> `Establish WebSocket Connection (useAppWebSocket)` -> `Backend sends processing update message` -> `WebSocket client receives message` -> `Update TanStack Query Cache (invalidate or setData)` -> `Components re-render with new data`.
4.  **Metadata Editing Flow**:
    `VideoDetailPage loads (fetches metadata via TanStack Query)` -> `Display metadata in EditorForm` -> `User edits form` -> `Form submission (API call to save metadata)` -> `On success, invalidate TanStack Query cache for metadata` or `Optimistically update local cache`.

---

## Cross-Cutting Concerns & Platform-Wide Patterns

*   **Error Handling**: Consistent error handling strategies for both backend (domain-specific exceptions, FastAPI error handlers, structured JSON error responses) and frontend (displaying user-friendly messages, logging errors).
*   **Logging**: Structured logging on the backend. Frontend error reporting to a service like Sentry (optional).
*   **Validation**: Pydantic for backend request/response validation. Form validation libraries (e.g., `react-hook-form`, `Zod`) on the frontend.
*   **Security**: HTTPS, secure JWT handling, input sanitization, protection against common web vulnerabilities (OWASP Top 10), proper RLS policies in Supabase.
*   **Configuration Management**: Environment variables for backend (using Pydantic settings) and frontend (e.g., `VITE_` prefixed env vars).
*   **Package Management**: Backend uses `uv` for Python dependency management, virtual environment creation, and dependency installation. Frontend uses `pnpm` for Node.js package management.
*   **API Design**: Adherence to RESTful principles, clear and consistent endpoint naming, use of HTTP status codes. OpenAPI/Swagger for backend API documentation.
</file>

<file path=".ai_docs/templates/5-tasks-template.md">
--- 
> This is a template of the expected output when generating a list of atomic tasks to complete the PRD.

Be sure to read the following files to understand the context of the project:

`pitch.md`
`prd.md`
`architecture.md`
`system-patterns.md`
---

# Video Processor Refactoring Plan

**Project Goal:** Refactor the video processing logic from the old `video_processor` directory into the new `apps/core` architecture, leveraging Supabase for Auth and PostgreSQL, and ensuring a good developer experience with local development parity.

---

**Phase 0: Project Setup & Essential Configuration**

1.   **Directory Structure & Dependency Setup:**
    * [x]  Task 0.1: **Verify/Create Core Directories:** Ensure the following directories exist within `apps/core/`. If not, create them:
        *   `lib/ai/`
        *   `lib/auth/`
        *   `lib/database/`
        *   `lib/messaging/` (if not already present)
        *   `lib/publishing/`
        *   `lib/storage/`
        *   `lib/utils/`
        *   `api/schemas/`
        *   `api/endpoints/`
        *   `models/`
        *   `operations/`
        *   `services/`
        *   `core/` (for shared core logic like exceptions, config)
        *   `tests/unit/`
        *   `tests/integration/`
    * [x]  Task 0.2: **Initialize Python Dependencies:**
        *   Open `apps/core/pyproject.toml`.
        *   Add necessary dependencies:
            *   `fastapi`, `uvicorn`
            *   `sqlalchemy` (for ORM)
            *   `psycopg2-binary` (PostgreSQL adapter)
            *   `pydantic` (with email validation, settings management)
            *   `pydantic-settings` (for loading from .env)
            *   `python-jose[cryptography]` (for JWT handling)
            *   `python-multipart` (for file uploads)
            *   `google-cloud-storage` (if using GCS)
            *   Relevant AI SDKs (e.g., `google-generativeai`, `google-cloud-aiplatform`)
            *   `alembic` (for database migrations)
            *   `supabase` (Python client, if direct interaction is needed beyond auth)
            *   Testing libraries: `pytest`, `pytest-cov`, `httpx` (for TestClient)
        *   Ensure your project uses a tool like `uv`, `poetry`, or `pdm` to manage dependencies based on `pyproject.toml`. Example: `uv pip install fastapi sqlalchemy psycopg2-binary pydantic pydantic-settings python-jose[cryptography] python-multipart google-cloud-storage google-generativeai alembic supabase pytest pytest-cov httpx uvicorn`

2.  **Supabase Local & Cloud Setup:**
    * [x]   Task 0.2.1: **Cloud Supabase Project:** Set up a Supabase project in the cloud (supabase.com). Enable Google Auth in the Auth providers section.
    * [x]   Task 0.2.2: **Obtain Cloud Credentials:** From your cloud Supabase project settings (Project Settings > API), note down:
        *   Project URL
        *   Anon key (public)
        *   Service role key (secret - keep this very secure)
        *   JWT Secret (Project Settings > API > JWT Settings)
    * [x]   Task 0.2.3: **Local Supabase Setup (Supabase CLI):**
        *   Install Supabase CLI: `npm install supabase --save-dev` (or global install if preferred).
        *   Navigate to your workspace root (or create a `supabase/` directory if you prefer to manage it there) and run `supabase init`. This creates a `supabase` configuration folder.
        *   Run `supabase start`. This command will download necessary Docker images and start local Supabase services. Note the local API URL, anon key, service_role key, and default JWT secret provided in the output.
    * [x]   Task 0.2.4: **Environment Variables (`.env`):**
        *   Create `apps/core/.env.example` with placeholders for all required variables.
        *   Create `apps/core/.env` (this file should be in `.gitignore`). Populate it with your *local Supabase* credentials for development, and any dev API keys for AI/GCS. For production, these will be set as environment variables in your deployment environment.
        ```dotenv
        # apps/core/.env.example
        ENVIRONMENT="development" # "production" or "test"

        # Supabase (Replace with your actual Local Dev Defaults from 'supabase start' output)
        SUPABASE_URL="http://localhost:54321"
        SUPABASE_ANON_KEY="your_local_anon_key"
        SUPABASE_SERVICE_ROLE_KEY="your_local_service_role_key"
        SUPABASE_JWT_SECRET="your_local_default_jwt_secret_at_least_32_characters_long"
        DATABASE_URL="postgresql://postgres:postgres@localhost:54322/postgres" # Default local Supabase DB URL

        # AI Service (Example Gemini)
        GEMINI_API_KEY="your_dev_gemini_api_key"

        # Storage (Local dev uses local file system, GCS for prod)
        STORAGE_BACKEND="local" # "gcs" in production
        LOCAL_STORAGE_PATH="./output_files" # Directory for local file storage (relative to apps/core)
        # GCS_BUCKET_NAME="your_gcs_bucket_name" # For production
        # GOOGLE_APPLICATION_CREDENTIALS_PATH="/path/to/your/gcs_service_account.json" # For production

        # Redis (if used for caching)
        REDIS_HOST="localhost"
        REDIS_PORT="6379"
        ```
    * [x]   Task 0.2.5: **Configuration Loading (`apps/core/core/config.py`):**
        *   Create/Update `apps/core/core/config.py`.
        *   Implement a Pydantic `Settings` class inheriting from `BaseSettings` (from `pydantic_settings`) to load configurations from the `.env` file.
        ```python
        # apps/core/core/config.py
        from typing import Optional
        from pydantic_settings import BaseSettings
        from pathlib import Path

        class Settings(BaseSettings):
            ENVIRONMENT: str = "development"

            # Supabase
            SUPABASE_URL: str
            SUPABASE_ANON_KEY: str
            SUPABASE_SERVICE_ROLE_KEY: Optional[str] = None  # Secret, use with care
            SUPABASE_JWT_SECRET: str
            DATABASE_URL: str

            # AI Services
            GEMINI_API_KEY: Optional[str] = None
            OPENAI_API_KEY: Optional[str] = None

            # Storage
            STORAGE_BACKEND: str = "local"  # 'local' or 'gcs'
            LOCAL_STORAGE_PATH: str = "./output_files"  # Ensure this path is valid
            GCS_BUCKET_NAME: Optional[str] = None
            GOOGLE_APPLICATION_CREDENTIALS_PATH: Optional[str] = None  # For GCS service account
            
            # Redis
            REDIS_HOST: str = "localhost"
            REDIS_PORT: int = 6379
            REDIS_DB: int = 0
            REDIS_PASSWORD: str = ""

            # API settings
            PROJECT_NAME: str = "AI-Driven Backend Service"
            API_PREFIX: str = "/api/v1"
            DEBUG: bool = False

            # JWT settings
            SECRET_KEY: str = "secret_key_for_development_only"
            ALGORITHM: str = "HS256"
            ACCESS_TOKEN_EXPIRE_MINUTES: int = 30

            # Email settings
            SMTP_SERVER: str = "smtp.gmail.com"
            SMTP_PORT: int = 587
            SMTP_USERNAME: str = ""
            SMTP_PASSWORD: str = ""
            EMAIL_FROM_ADDRESS: str = "noreply@example.com"
            EMAIL_TEMPLATES_DIR: str = "templates/emails"

            # File Upload
            UPLOAD_DIR: str = "uploads"

            # Paths
            BASE_DIR: Path = Path(__file__).resolve().parent.parent

            class Config:
                env_file = ".env"
                env_file_encoding = "utf-8"
                extra = "ignore"  # Ignore extra fields from .env

        settings = Settings()
        ```
    * [x]   Task 0.2.6: **Database Connection (`apps/core/lib/database/connection.py`):**
        *   Confirmed: `apps/core/lib/database/connection.py` already provides a FastAPI-compatible SQLAlchemy session generator (`get_db_session`), exports `Base`, and is ready for use in dependency injection and Alembic migrations. No changes needed.
    * [x]   Task 0.2.7: **Database Migrations (Alembic):**
        *   Alembic initialized in `apps/core/` with `alembic init alembic`.
        *   `apps/core/alembic/env.py` configured to:
            *   Import `Base` from `apps.core.lib.database.connection` and set `target_metadata = Base.metadata`.
            *   Import `settings` from `apps.core.core.config` and use `settings.DATABASE_URL` for the connection.
        *   `apps/core/alembic.ini` updated:
            *   `sqlalchemy.url` is blank and a comment explains it is set dynamically from `settings.DATABASE_URL` in env.py.
        *   (After defining models in Phase 1) Create an initial migration: `alembic revision -m "initial_setup_video_processing_tables"` then `alembic upgrade head`. Adhere to `sb-create-migration` guidelines if available.

**Phase 1: Foundation - Models and Core Libraries (Libs)**

3.  **Define Core Data Models (`apps/core/models/`)**:
    *   (All ORM models should inherit from `Base` defined in `apps.core.lib.database.connection`)
    * [ ]   Task 3.1: **Video Model (`apps/core/models/video_model.py`):**
        *   Define `VideoModel(Base)`: `id` (PK, Integer, autoincrement), `uploader_user_id` (String, index=True, corresponds to Supabase Auth user ID), `original_filename` (String), `storage_path` (String, unique path in GCS or local filesystem), `content_type` (String), `size_bytes` (Integer), `created_at` (DateTime, default=func.now()), `updated_at` (DateTime, default=func.now(), onupdate=func.now()).
    * [ ]   Task 3.2: **Enums (`apps/core/models/enums.py`):**
        *   Define `ProcessingStatus(str, Enum)`: `PENDING = "PENDING"`, `PROCESSING = "PROCESSING"`, `COMPLETED = "COMPLETED"`, `FAILED = "FAILED"`.
    * [ ]   Task 3.3: **Video Job Model (`apps/core/models/video_job_model.py`):**
        *   Define `VideoJobModel(Base)`: `id` (PK), `video_id` (Integer, ForeignKey("videos.id")), `status` (SQLAlchemy `EnumType(ProcessingStatus)`), `processing_stages` (JSON or Text, nullable), `error_message` (Text, nullable), `created_at`, `updated_at`. Relationship: `video = relationship("VideoModel")`.
    * [ ]   Task 3.4: **Video Metadata Model (`apps/core/models/video_metadata_model.py`):**
        *   Define `VideoMetadataModel(Base)`: `id` (PK), `job_id` (Integer, ForeignKey("video_jobs.id"), unique=True), `title` (String, nullable), `description` (Text, nullable), `tags` (JSON or `sqlalchemy.dialects.postgresql.ARRAY(String)` for PostgreSQL, nullable), `transcript_text` (Text, nullable), `transcript_file_url` (String, nullable), `subtitle_files_urls` (JSON, nullable, e.g., `{"vtt": "url", "srt": "url"}`), `thumbnail_file_url` (String, nullable), `extracted_video_duration_seconds` (Float, nullable), `extracted_video_resolution` (String, nullable), `extracted_video_format` (String, nullable), `show_notes_text` (Text, nullable), `created_at`, `updated_at`. Relationship: `job = relationship("VideoJobModel", back_populates="metadata")`. Add `metadata = relationship("VideoMetadataModel", back_populates="job", uselist=False)` to `VideoJobModel`.
    * [ ]   Task 3.5: **Model Imports & Alembic:**
        *   Ensure `apps/core/models/__init__.py` imports all model classes (e.g., `from .video_model import VideoModel`) and `Base`.
        *   Update `apps/core/alembic/env.py` `target_metadata` to `Base.metadata`.
    * [ ]   Task 3.6: **Generate and Apply Migrations:**
        *   Run `cd apps/core && alembic revision --autogenerate -m "create_video_processing_tables"`.
        *   Inspect the generated migration script in `apps/core/alembic/versions/`.
        *   Run `cd apps/core && alembic upgrade head` to apply to your local Supabase DB.

4.  **Develop/Enhance Core Libraries (`apps/core/lib/`)**:
    *   **Storage (`apps/core/lib/storage/file_storage.py`):**
        * [ ]   Task 4.1: Define `FileStorageService` class. Constructor `__init__(self, settings: Settings)`.
        * [ ]   Task 4.2: Method `async save_file(self, file_content: bytes, filename: str, subdir: Optional[str] = "uploads") -> str`:
            *   If `self.settings.STORAGE_BACKEND == "local"`: Save to `{self.settings.LOCAL_STORAGE_PATH}/{subdir}/{filename}`. Ensure directory exists. Return the local path.
            *   If `self.settings.STORAGE_BACKEND == "gcs"`: Use `google-cloud-storage` client to upload to `self.settings.GCS_BUCKET_NAME` under `{subdir}/{filename}`. Return GCS URI `gs://{bucket_name}/{subdir}/{filename}`.
        * [ ]   Task 4.3: Method `async download_file(self, storage_path: str, destination_local_path: str) -> str`: Downloads from GCS or copies from local.
        * [ ]   Task 4.4: Method `async get_public_url(self, storage_path: str) -> Optional[str]`: Returns HTTP URL for GCS objects or a placeholder for local files.
    *   **AI (`apps/core/lib/ai/`)**:
        * [ ]   Task 4.5: **Base Adapter (`base_adapter.py`):** `AIAdapterInterface(ABC)` with `async generate_text(self, prompt: str, context: Optional[str] = None) -> str`, `async transcribe_audio(self, audio_file_path: str) -> str` (returns structured transcript if possible).
        * [ ]   Task 4.6: **Gemini Adapter (`gemini_adapter.py`):** `GeminiAdapter(AIAdapterInterface)` using `settings.GEMINI_API_KEY`.
        * [ ]   Task 4.7: **AI Client Factory (`ai_client_factory.py`):** `def get_ai_adapter(settings: Settings) -> AIAdapterInterface: return GeminiAdapter(settings)`.
        * [ ]   Task 4.8: **AI Caching (`apps/core/lib/cache/redis_cache.py`):**
            *   Create `RedisCache` class using `redis-py` (sync or async version). Connect using `settings.REDIS_HOST`, `settings.REDIS_PORT`.
            *   Methods: `get(key)`, `set(key, value, ttl_seconds)`.
            *   Modify AI adapters to use this cache for relevant API calls.
    *   **Utilities (`apps/core/lib/utils/`)**:
        * [ ]   Task 4.9: **FFmpeg (`ffmpeg_utils.py`):** `FfmpegUtils` class. Methods: `extract_audio_sync(video_path, output_audio_path)`, `extract_frame_sync(video_path, timestamp_seconds, output_image_path)`, `get_video_metadata_sync(video_path) -> dict`. Use `subprocess.run`. Ensure `ffmpeg` is an accessible command.
        * [ ]   Task 4.10: **File Handling (`file_utils.py`):** `FileUtils` class. Methods for temp dir creation/cleanup: `create_temp_dir()`, `cleanup_temp_dir(dir_path)`.
        * [ ]   Task 4.11: **Subtitles (`subtitle_utils.py`):** `SubtitleUtils` class. Methods: `generate_vtt(transcript_segments: list) -> str`, `generate_srt(transcript_segments: list) -> str`. Define `transcript_segments` structure (e.g., `[{'text': '...', 'start_time': 0.0, 'end_time': 1.5}, ...]`).
    *   **Authentication Utilities (`apps/core/lib/auth/supabase_auth.py`):**
        * [ ]   Task 4.12: Create `AuthenticatedUser(BaseModel)`: `id: str`, `email: Optional[str] = None`, `aud: Optional[str] = None`.
        * [ ]   Task 4.13: `async def get_current_user(token: str = Depends(OAuth2PasswordBearer(tokenUrl="token"))) -> AuthenticatedUser:`
            *   Use `python-jose.jwt.decode` with `settings.SUPABASE_JWT_SECRET` and audience `authenticated`.
            *   Extract `sub` (user_id), `email`, `aud`. Return `AuthenticatedUser`. Handle `JWTError` with `HTTPException`.
            *   (Note: `tokenUrl` is a dummy here as Supabase handles token issuance.)

5.  **Error Handling (`apps/core/core/exceptions.py`):**
    * [ ]   Task 5.1: Define custom exceptions: `VideoProcessingError(Exception)`, `AINoResponseError(VideoProcessingError)`, `FFmpegError(VideoProcessingError)`.

**Phase 2: Data Access - Operations Layer**

6.  **Implement Repositories (`apps/core/operations/`)**:
    *   (All repo methods accept `db: Session` as first arg. Typically called from services.)
    * [ ]   Task 6.1: **Video Repo (`video_repository.py`):** `VideoRepository` class. Methods: `create(...) -> VideoModel`, `get_by_id(...) -> Optional[VideoModel]`.
    * [ ]   Task 6.2: **Video Job Repo (`video_job_repository.py`):** `VideoJobRepository` class. Methods: `create(...) -> VideoJobModel`, `get_by_id(...) -> Optional[VideoJobModel]`, `update_status(...) -> VideoJobModel`, `add_processing_stage(...) -> VideoJobModel`.
    * [ ]   Task 6.3: **Video Metadata Repo (`video_metadata_repository.py`):** `VideoMetadataRepository` class. Methods: `create_or_update(db: Session, job_id: int, **kwargs) -> VideoMetadataModel`, `get_by_job_id(...) -> Optional[VideoMetadataModel]`.

**Phase 3: Business Logic - Service Layer**

7.  **Develop Core Video Processing Service (`apps/core/services/video_processing_service.py`):**
    * [ ]   Task 7.1: Define `VideoProcessingService` class.
        *   `__init__`: Inject instances of `VideoRepository`, `VideoJobRepository`, `VideoMetadataRepository`, `FileStorageService`, `AIAdapterInterface`, `FfmpegUtils`, `SubtitleUtils`, `FileUtils`.
    * [ ]   Task 7.2: Method `async initiate_video_processing(self, db: Session, original_filename: str, video_content: bytes, content_type: str, uploader_user_id: str, background_tasks: BackgroundTasks) -> VideoJobModel`:
        *   Save video via `FileStorageService` (e.g., `uploads/{uploader_user_id}/{uuid_filename}`).
        *   Create `VideoModel` via repo.
        *   Create `VideoJobModel` (status PENDING) via repo.
        *   Add `self._execute_processing_pipeline(job_id=new_job.id, video_storage_path=stored_video_path)` to `background_tasks`.
        *   `db.commit()` for initial records. Return `new_job`.
    * [ ]   Task 7.3: Method `async _execute_processing_pipeline(self, job_id: int, video_storage_path: str)`:
        *   Use a `with get_db_session() as db_bg:` context for the background task's DB operations.
        *   Fetch job, update status to PROCESSING.
        *   Use `FileUtils.create_temp_dir()`.
        *   **Try/Catch/Finally (for cleanup):**
            *   Download video to temp dir using `FileStorageService`.
            *   **Step 1: Basic Metadata:** `FfmpegUtils.get_video_metadata_sync`. Update `VideoMetadataModel`.
            *   **Step 2: Extract Audio:** `FfmpegUtils.extract_audio_sync` to temp audio file.
            *   **Step 3: Transcript:** `AIAdapter.transcribe_audio`. Store text. Upload `.txt` via `FileStorageService`. Update `VideoMetadataModel`.
            *   **Step 4: Content Metadata:** `AIAdapter.generate_text` for title, desc, tags, show_notes. Update `VideoMetadataModel`.
            *   **Step 5: Subtitles:** `SubtitleUtils` using transcript. Upload `.vtt`, `.srt`. Update `VideoMetadataModel`.
            *   **Step 6: Thumbnail:** `FfmpegUtils.extract_frame_sync`. Upload `.jpg`. Update `VideoMetadataModel`.
            *   Update job to COMPLETED. `db_bg.commit()`.
        *   **Catch:** Log error. Update job to FAILED with error message. `db_bg.commit()`.
        *   **Finally:** `FileUtils.cleanup_temp_dir()`.
    * [ ]   Task 7.4: Method `async get_job_details(self, db: Session, job_id: int, user_id: str) -> Optional[VideoJobModel]`: Fetch job, verify ownership (`job.video.uploader_user_id == user_id`), return job with related video and metadata.

8.  **Supporting Services:**
    * [ ]   Task 8.1: `apps/core/services/user_service.py`: (If needed) `async def get_or_create_user_profile(db: Session, auth_user: AuthenticatedUser) -> UserModel`. Could be called after successful auth to ensure a local user profile exists.

**Phase 4: API Layer - Endpoints and Schemas**

9.  **Define API Schemas (`apps/core/api/schemas/`)**:
    * [ ]   Task 9.1: Create `video_processing_schemas.py` (and `user_schemas.py` if `UserService` is used).
    * [ ]   Task 9.2: Pydantic `VideoUploadResponseSchema(BaseModel)`: `job_id: int`, `status: ProcessingStatus`.
    * [ ]   Task 9.3: Pydantic `VideoSchema(BaseModel)`: from `VideoModel`. `model_config = ConfigDict(from_attributes=True)`.
    * [ ]   Task 9.4: Pydantic `VideoMetadataSchema(BaseModel)`: from `VideoMetadataModel`. `model_config = ConfigDict(from_attributes=True)`.
    * [ ]   Task 9.5: Pydantic `VideoJobSchema(BaseModel)`: from `VideoJobModel`, plus `video: Optional[VideoSchema] = None`, `metadata: Optional[VideoMetadataSchema] = None`. `model_config = ConfigDict(from_attributes=True)`.

10. **Create API Endpoints (`apps/core/api/endpoints/video_processing_endpoints.py`):**
    * [ ]   Task 10.1: Create `router = APIRouter()`.
    * [ ]   Task 10.2: `POST /upload`, response_model=`VideoUploadResponseSchema`:
        *   Dependencies: `current_user: AuthenticatedUser = Depends(get_current_user)`, `db: Session = Depends(get_db_session)`, `background_tasks: BackgroundTasks`.
        *   Inject `VideoProcessingService`. Call `initiate_video_processing`.
    * [ ]   Task 10.3: `GET /jobs/{job_id}`, response_model=`VideoJobSchema`:
        *   Dependencies: `current_user: AuthenticatedUser = Depends(get_current_user)`, `db: Session = Depends(get_db_session)`.
        *   Inject `VideoProcessingService`. Call `get_job_details`. Raise 404 or 403 if not found/not owner.
    * [ ]   Task 10.4: **Register Router in `apps/core/main.py`:**
        *   `app.include_router(video_processing_router, prefix="/api/v1/videos", tags=["Video Processing"])`.

**Phase 5: Testing, Documentation, and Cleanup**

11. **Testing (`apps/core/tests/`)**:
    * [ ]   Task 11.1: **Unit Tests (`tests/unit/`)**: For models, libs (mock external IO), operations (mock DB session methods), services (mock repo/lib dependencies).
    * [ ]   Task 11.2: **Integration Tests (`tests/integration/api/`)**:
        *   Use FastAPI `TestClient`.
        *   Override `get_db_session` to use a test DB (local Supabase test DB).
        *   Override `get_current_user` to return a mock `AuthenticatedUser`.
        *   Test `POST /upload` with actual files. For background tasks, either mock the service method called by it, or check DB for results after a short delay (if feasible for tests).
        *   Test `GET /jobs/{job_id}`.
        *   Configure AI/Storage services to use local/mocked backends for integration tests if possible, or carefully managed dev instances.

12. **Documentation and Cleanup**:
    * [ ]   Task 12.1: Update `apps/core/README.md` (API endpoints, local setup with Supabase CLI, env vars).
    * [ ]   Task 12.2: Add Python docstrings (module, class, function levels).
    * [ ]   Task 12.3: (LATER) Securely remove old `apps/core/video_processor` directory after full verification.
    * [ ]   Task 12.4: Update this `.ai_docs/progress.md` as tasks are completed.

---
</file>

<file path=".ai_docs/web/error-handling.md">
# Error Handling Standards

This document outlines the standardized error handling patterns to be implemented across the frontend application. It defines error types, user-facing feedback mechanisms, and recovery strategies.

## Error Types

The application defines several error types to categorize different failure scenarios:

```typescript
// api.ts or errors.ts
export class AppError extends Error {
  constructor(message: string) {
    super(message);
    this.name = 'AppError';
  }
}

export class AuthenticationError extends AppError {
  constructor(message = 'Authentication failed. Please log in again.') {
    super(message);
    this.name = 'AuthenticationError';
  }
}

export class NetworkError extends AppError {
  constructor(message = 'Network connection issue. Please check your internet connection.') {
    super(message);
    this.name = 'NetworkError';
  }
}

export class ValidationError extends AppError {
  constructor(message = 'Invalid input data. Please check your entries.') {
    super(message);
    this.name = 'ValidationError';
  }
}

export class ServerError extends AppError {
  constructor(message = 'Server error. Our team has been notified.') {
    super(message);
    this.name = 'ServerError';
  }
}

export class WebSocketError extends AppError {
  constructor(message = 'Real-time connection error. Updates may be delayed.') {
    super(message);
    this.name = 'WebSocketError';
  }
}
```

## API Error Handling

All API requests should use a standardized error handling approach:

```typescript
// Example API client function with error handling
async function fetchVideos() {
  try {
    const response = await fetch('/api/videos', {
      headers: {
        'Authorization': `Bearer ${getSupabaseToken()}`
      }
    });
    
    if (!response.ok) {
      // Handle different HTTP status codes
      if (response.status === 401 || response.status === 403) {
        throw new AuthenticationError();
      } else if (response.status === 400) {
        throw new ValidationError();
      } else if (response.status >= 500) {
        throw new ServerError();
      } else {
        throw new AppError(`Request failed with status: ${response.status}`);
      }
    }
    
    return await response.json();
  } catch (error) {
    // Categorize uncaught errors
    if (error instanceof AppError) {
      // Rethrow our custom errors
      throw error;
    } else if (error instanceof TypeError && error.message.includes('fetch')) {
      // Network errors
      throw new NetworkError();
    } else {
      // Unknown errors
      console.error('Unexpected error:', error);
      throw new AppError('An unexpected error occurred');
    }
  }
}
```

## User Feedback Mechanisms

### Toast Notifications

Use toast notifications for transient errors:

```typescript
// Example usage with shadcn/ui toast
import { useToast } from '@/components/ui/use-toast';

function ExampleComponent() {
  const { toast } = useToast();
  
  const handleAction = async () => {
    try {
      await someApiCall();
    } catch (error) {
      toast({
        variant: error instanceof ValidationError ? 'warning' : 'destructive',
        title: error.name,
        description: error.message,
        action: error instanceof NetworkError ? (
          <ToastAction altText="Retry" onClick={handleAction}>Retry</ToastAction>
        ) : undefined
      });
    }
  };
}
```

### Form Validation Errors

Display form validation errors inline:

```tsx
function FormExample() {
  const [errors, setErrors] = useState<Record<string, string>>({});
  
  const handleSubmit = async (data) => {
    try {
      setErrors({});
      await submitForm(data);
    } catch (error) {
      if (error instanceof ValidationError && error.fieldErrors) {
        setErrors(error.fieldErrors);
      } else {
        // Handle other errors with toast
      }
    }
  };
  
  return (
    <form onSubmit={handleSubmit}>
      <div>
        <Label htmlFor="title">Title</Label>
        <Input id="title" />
        {errors.title && (
          <p className="text-sm text-red-500">{errors.title}</p>
        )}
      </div>
      {/* Other form fields */}
    </form>
  );
}
```

### Error States

Implement error states for failed data loading:

```tsx
function DataComponent() {
  const { data, isLoading, error, refetch } = useQuery({
    queryKey: ['data'],
    queryFn: fetchData
  });
  
  if (isLoading) {
    return <LoadingSpinner />;
  }
  
  if (error) {
    return (
      <div className="error-container">
        <p>Failed to load data: {error.message}</p>
        <Button onClick={() => refetch()}>Retry</Button>
      </div>
    );
  }
  
  return <DataDisplay data={data} />;
}
```

## Retry Strategies

### Automatic Retries

Implement automatic retries for network errors:

```typescript
// Example retry logic for API calls
async function fetchWithRetry(url, options, maxRetries = 3) {
  let retries = 0;
  
  while (retries < maxRetries) {
    try {
      const response = await fetch(url, options);
      if (response.ok) return response.json();
      
      // Don't retry for client errors (except 429 too many requests)
      if (response.status >= 400 && response.status < 500 && response.status !== 429) {
        handleErrorByStatus(response);
      }
      
      // For 5xx and 429 errors, retry
      throw new NetworkError(`Request failed with status: ${response.status}`);
    } catch (error) {
      retries += 1;
      
      if (retries >= maxRetries) {
        throw error;
      }
      
      // Exponential backoff
      const delay = Math.min(1000 * (2 ** retries) + Math.random() * 1000, 10000);
      await new Promise(resolve => setTimeout(resolve, delay));
    }
  }
}
```

### TanStack Query Configuration

Configure TanStack Query for automatic retries:

```typescript
// Configure in QueryClient setup
const queryClient = new QueryClient({
  defaultOptions: {
    queries: {
      retry: (failureCount, error) => {
        // Don't retry for authentication or validation errors
        if (error instanceof AuthenticationError || error instanceof ValidationError) {
          return false;
        }
        // Retry network errors up to 3 times
        if (error instanceof NetworkError) {
          return failureCount < 3;
        }
        // Default retry logic
        return failureCount < 2;
      },
      retryDelay: attemptIndex => Math.min(1000 * (2 ** attemptIndex), 30000),
    },
  },
});
```

## Error Handling for WebSockets

The WebSocket hook should include specific error handling:

```typescript
function useAppWebSocket() {
  const [connectionStatus, setConnectionStatus] = useState('disconnected');
  const [error, setError] = useState<Error | null>(null);
  
  // Connection logic with error handling
  useEffect(() => {
    let ws: WebSocket | null = null;
    let reconnectAttempts = 0;
    let reconnectTimeout: NodeJS.Timeout | null = null;
    
    const connect = () => {
      try {
        ws = new WebSocket(`${WEBSOCKET_URL}?token=${getSupabaseToken()}`);
        
        ws.onopen = () => {
          setConnectionStatus('connected');
          setError(null);
          reconnectAttempts = 0;
        };
        
        ws.onclose = (event) => {
          setConnectionStatus('disconnected');
          
          // Don't reconnect if closed cleanly (code 1000)
          if (event.code !== 1000) {
            reconnect();
          }
        };
        
        ws.onerror = (event) => {
          setError(new WebSocketError('Connection error occurred'));
          ws?.close();
        };
        
        // Message handling logic
      } catch (err) {
        setError(new WebSocketError('Failed to establish connection'));
        reconnect();
      }
    };
    
    const reconnect = () => {
      if (reconnectTimeout) {
        clearTimeout(reconnectTimeout);
      }
      
      reconnectAttempts += 1;
      setConnectionStatus('reconnecting');
      
      // Exponential backoff with max delay
      const delay = Math.min(1000 * (2 ** reconnectAttempts), 30000);
      
      reconnectTimeout = setTimeout(() => {
        connect();
      }, delay);
    };
    
    connect();
    
    return () => {
      if (reconnectTimeout) {
        clearTimeout(reconnectTimeout);
      }
      if (ws) {
        ws.close(1000, 'Component unmounting');
      }
    };
  }, []);
  
  // Rest of the hook implementation
}
```

## Authentication Error Handling

For authentication errors, implement automatic redirection to login:

```typescript
// Global error handler for authentication errors
function AuthErrorHandler({ children }) {
  const navigate = useNavigate();
  
  // Set up a global error boundary or listener
  useEffect(() => {
    const handleUnauthorized = (error) => {
      if (error instanceof AuthenticationError) {
        // Clear local auth state
        // Redirect to login
        navigate('/login', { 
          state: { 
            from: window.location.pathname,
            reason: 'session_expired' 
          } 
        });
      }
    };
    
    // Listen to custom error events
    window.addEventListener('app:error', (e) => handleUnauthorized(e.detail));
    
    return () => {
      window.removeEventListener('app:error', (e) => handleUnauthorized(e.detail));
    };
  }, [navigate]);
  
  return children;
}
```

## Error Logging

Implement consistent error logging:

```typescript
// Error logger utility
export function logError(error: Error, context?: Record<string, any>) {
  console.error('Application error:', {
    name: error.name,
    message: error.message,
    stack: error.stack,
    context,
    timestamp: new Date().toISOString(),
  });
  
  // In production, could send to error monitoring service
  if (import.meta.env.PROD) {
    // Send to monitoring service
  }
}
```
</file>

<file path=".ai_docs/web/websocket-messages.md">
# WebSocket Message Format Documentation

This document outlines the expected WebSocket message formats for communication between the frontend and the FastAPI backend. These messages are used to provide real-time updates on video processing status, metadata generation, and error conditions.

## Message Structure

All WebSocket messages follow a consistent JSON structure:

```typescript
interface BaseWebSocketMessage {
  type: string;          // Message type identifier
  job_id: string;        // Unique identifier for the video job
  data: unknown;         // Type-specific payload
  timestamp: string;     // ISO-8601 timestamp
}
```

## Message Types

### 1. JOB_UPDATE

Provides updates on the overall status of a video processing job.

```typescript
interface JobUpdateMessage extends BaseWebSocketMessage {
  type: "JOB_UPDATE";
  data: {
    status: "PENDING" | "PROCESSING" | "COMPLETED" | "FAILED";
    progress_percent: number;  // 0-100
    error_message?: string;    // Present only if status is FAILED
  };
}
```

**Usage Example:**
- Dashboard UI updates job status card
- Video detail view updates overall progress indicator
- If status is "FAILED", display error message

### 2. STAGE_UPDATE

Provides updates on a specific processing stage within a job.

```typescript
interface StageUpdateMessage extends BaseWebSocketMessage {
  type: "STAGE_UPDATE";
  data: {
    stage_id: string;    // E.g., "THUMBNAIL_GENERATION", "TRANSCRIPTION", etc.
    status: "PENDING" | "PROCESSING" | "COMPLETED" | "FAILED";
    progress_percent: number;  // 0-100
    error_message?: string;    // Present only if status is FAILED
  };
}
```

**Usage Example:**
- Video detail view updates specific stage progress
- If stage fails, display specific error for that stage

### 3. METADATA_UPDATE

Notifies when new or updated metadata is available for a video.

```typescript
interface MetadataUpdateMessage extends BaseWebSocketMessage {
  type: "METADATA_UPDATE";
  data: {
    metadata_type: "TITLE" | "DESCRIPTION" | "TAGS" | "TRANSCRIPT" | "CHAPTERS";
    content: string | string[] | object;  // Depends on metadata_type
  };
}
```

**Usage Example:**
- Video detail view updates specific metadata field
- Enable editing if processing of that metadata type is complete

### 4. THUMBNAIL_GENERATED

Notifies when a new thumbnail has been generated.

```typescript
interface ThumbnailGeneratedMessage extends BaseWebSocketMessage {
  type: "THUMBNAIL_GENERATED";
  data: {
    thumbnail_url: string;    // Full GCS URL to the thumbnail
    thumbnail_id: string;     // Unique identifier for the thumbnail
    width: number;            // Width in pixels
    height: number;           // Height in pixels
    timestamp_seconds?: number; // Optional: Video timestamp this thumbnail represents
  };
}
```

**Usage Example:**
- Thumbnail gallery adds new thumbnail to the display
- If first thumbnail, update job card with thumbnail preview

### 5. ERROR

Provides information about errors that occurred during processing.

```typescript
interface ErrorMessage extends BaseWebSocketMessage {
  type: "ERROR";
  data: {
    error_code: string;        // Machine-readable error code
    error_message: string;     // Human-readable error message
    recoverable: boolean;      // Whether the error allows processing to continue
    affected_stage?: string;   // Optional: The specific stage affected by this error
  };
}
```

**Usage Example:**
- Display error message to user
- If non-recoverable, show job as failed
- If recoverable, show warning but allow continued interaction

## WebSocket Connection Management

### Connection Lifecycle

1. **Initial Connection**:
   - Connect to `wss://[backend-url]/ws` with Supabase JWT for authentication
   - Upon successful connection, send a message to subscribe to specific job updates (if applicable)

2. **Reconnection Strategy**:
   - Implement exponential backoff for reconnection attempts
   - Start with 1 second delay, double until reaching a maximum of 30 seconds
   - Reset backoff counter after successful reconnection

3. **Heartbeat**:
   - Expect backend to send periodic heartbeat messages (every 30 seconds)
   - If no heartbeat received for 60 seconds, attempt reconnection
   - Send heartbeat responses to keep connection alive

### Subscription Message

To subscribe to updates for specific jobs:

```typescript
interface SubscriptionMessage {
  action: "subscribe";
  job_ids: string[];  // Array of job IDs to subscribe to
}
```

## Integration with TanStack Query

The WebSocket hook should integrate with TanStack Query in the following ways:

1. **Direct Cache Updates**:
   For incremental updates like new thumbnails or metadata changes, directly modify the cache:

   ```typescript
   queryClient.setQueryData(['video', jobId], (oldData) => {
     // Update only the relevant portion of the data
     return {
       ...oldData,
       thumbnails: [...oldData.thumbnails, newThumbnailData]
     };
   });
   ```

2. **Cache Invalidation**:
   For major status changes that affect multiple aspects of the data:

   ```typescript
   // For significant changes, invalidate the query to trigger a refetch
   queryClient.invalidateQueries(['video', jobId]);
   ```

3. **Optimistic Updates**:
   For user actions with immediate feedback needs:

   ```typescript
   // When saving metadata
   queryClient.setQueryData(['video', jobId], (oldData) => {
     return {
       ...oldData,
       metadata: {
         ...oldData.metadata,
         title: newTitle
       }
     };
   });
   ```

## Error Handling

WebSocket-specific error handling should include:

1. **Connection Errors**:
   - Display connection status indicator
   - Attempt reconnection with backoff
   - Fall back to polling API if connection fails repeatedly

2. **Message Parsing Errors**:
   - Log invalid messages for debugging
   - Implement defensive parsing with fallbacks
   - Continue processing valid messages

3. **Authentication Errors**:
   - Redirect to login if authentication fails
   - Attempt to refresh token if expired
   - Provide clear feedback about connection status
</file>

<file path=".clinerules/cline-continuous-improvement-protocol.md">
---
description: Defines Cline's mandatory protocol for self-reflection, persistent knowledge capture using dedicated logs, and continuous improvement of its operational knowledge before task completion.
author: https://github.com/jeanibarz
version: 1.0
tags: ["protocol", "meta", "learning", "reflection", "knowledge-management", "core-behavior"]
globs: ["*"] # This core protocol is always active and applies to all Cline operations.
---
# Cline Continuous Improvement Protocol

**Objective:** Ensure Cline proactively learns from tasks, captures knowledge in a structured way, **distills fundamental insights,** refines understanding, and improves efficiency and reliability. This protocol maintains two key files: `memory-bank/raw_reflection_log.md` for initial detailed logging, and `memory-bank/consolidated_learnings.md` for pruned, actionable, long-term knowledge. This is vital for optimal performance and avoiding redundant effort.

**Core Principle:** Continuous learning and adaptation are **mandatory**. This protocol **must be executed before `attempt_completion`** for tasks with new learning, problem-solving, user feedback, or multiple steps. Trivial mechanical tasks *may* be exempt per higher-level rules; otherwise, execution is default.

**Migration/Refactor Best Practice:**  
For any migration or modularization task involving legacy code:
- Create a migration verification checklist at the start of the process.
- Port and adapt all relevant unit/integration tests to the new structure before deleting legacy code.
- Ensure all ported tests pass before marking the migration as complete.
- Document migration steps, learnings, and patterns in the Memory Bank and raw reflection log.

**Key Knowledge Files:**
*   **`memory-bank/raw_reflection_log.md`**: Contains detailed, timestamped, and task-referenced raw entries from the "Task Review & Analysis" phase. This is the initial dump of all observations.
*   **`memory-bank/consolidated_learnings.md`**: Contains curated, summarized, and actionable insights derived from `raw_reflection_log.md`. This is the primary, refined knowledge base for long-term use. It should be kept concise and highly relevant.

---

## 1. Mandatory Pre-Completion Reflection & Raw Knowledge Capture

Before signaling task completion (e.g., via `attempt_completion`), Cline **must** perform the following internal steps:

### 1.1. Task Review & Analysis:
* Review the completed task (conversation, logs, artifacts).
* **Identify Learnings:** What new information, techniques, **underlying patterns,** API behaviors, project-specific commands (e.g., test, build, run flags), environment variables, setup quirks, or successful outcomes were discovered? **What core principles can be extracted?**
* **Identify Difficulties & Mistakes (as Learning Opportunities):** What challenges were faced? Were there any errors, misunderstandings, or inefficiencies? **How can these experiences refine future approaches (resilience & adaptation)?** Did user feedback indicate a misstep?
* **Identify Successes:** What went particularly well? What strategies or tools were notably effective? **What were the key contributing factors?**

### 1.2. Logging to `.ai_docs/memory-bank/raw_reflection_log.md`:
* Based on Task Review & Analysis (1.1), create a timestamped, task-referenced entry in `.ai_docs/memory-bank/raw_reflection_log.md` detailing all learnings, difficulties (and their resolutions/learnings), and successes (and contributing factors).
* This file serves as the initial, detailed record. Its entries are candidates for later consolidation.
* *Example Entry in `.ai_docs/memory-bank/raw_reflection_log.md`:*
    ```markdown
    ---
    Date: {{CURRENT_DATE_YYYY_MM_DD}}
    TaskRef: "Implement JWT refresh logic for Project Alpha"

    Learnings:
    - Discovered `jose` library's `createRemoteJWKSet` is highly effective for dynamic key fetching for Project Alpha's auth.
    - Confirmed that a 401 error with `X-Reason: token-signature-invalid` from the auth provider requires re-fetching JWKS.
    - Project Alpha's integration tests: `cd services/auth && poetry run pytest -m integration --maxfail=1`
    - Required ENV for local testing of Project Alpha auth: `AUTH_API_KEY="test_key_alpha"`

    Difficulties:
    - Initial confusion about JWKS caching led to intermittent validation failures. Resolved by implementing a 5-minute cache.

    Successes:
    - The 5-minute JWKS cache with explicit bust mechanism proved effective.

    Improvements_Identified_For_Consolidation:
    - General pattern: JWKS caching strategy (5-min cache, explicit bust).
    - Project Alpha: Specific commands and ENV vars.
    ---
    ```

---

## 2. Knowledge Consolidation & Refinement Process (Periodic)

This outlines refining knowledge from `.ai_docs/memory-bank/raw_reflection_log.md` into `.ai_docs/memory-bank/consolidated_learnings.md`. This occurs periodically or when `raw_reflection_log.md` grows significantly, not necessarily after each task.

### 2.1. Review and Identify for Consolidation:
* Periodically, or when prompted by the user or significant new raw entries, review `.ai_docs/memory-bank/raw_reflection_log.md`.
* Identify entries/parts representing durable, actionable, or broadly applicable knowledge (e.g., reusable patterns, critical configurations, effective strategies, resolved errors).

### 2.2. Synthesize and Transfer to `.ai_docs/memory-bank/consolidated_learnings.md`:
* For identified insights:
    * Concisely synthesize, summarize, and **distill into generalizable principles or actionable patterns.**
    * Add refined knowledge to `memory-bank/consolidated_learnings.md`, organizing logically (by topic, project, tech) for easy retrieval.
    * Ensure `consolidated_learnings.md` content is actionable, **generalizable,** and non-redundant.
* *Example Entry in `.ai_docs/memory-bank/consolidated_learnings.md` (derived from above raw log example):*
    ```markdown
    ## JWT Handling & JWKS
    **Pattern: JWKS Caching Strategy**
    - For systems using JWKS for token validation, implement a short-lived cache (e.g., 5 minutes) for fetched JWKS.
    - Include an explicit cache-bust mechanism if immediate key rotation needs to be handled.
    - *Rationale:* Balances performance by reducing frequent JWKS re-fetching against timely key updates. Mitigates intermittent validation failures due to stale keys.

    ## Project Alpha - Specifics
    **Auth Module:**
    - **Integration Tests:** `cd services/auth && poetry run pytest -m integration --maxfail=1`
    - **Local Testing ENV:** `AUTH_API_KEY="test_key_alpha"`
    ```

### 2.3. Prune `.ai_docs/memory-bank/raw_reflection_log.md`:
* **Crucially, once information has been successfully transferred and consolidated into `.ai_docs/memory-bank/consolidated_learnings.md`, the corresponding original entries or processed parts **must be removed** from `.ai_docs/memory-bank/raw_reflection_log.md`.**
* This keeps `raw_reflection_log.md` focused on recent, unprocessed reflections and prevents it from growing indefinitely with redundant information.

### 2.4. Proposing `.clinerule` Enhancements (Exceptional):
* The primary focus of this protocol is the maintenance of `raw_reflection_log.md` and `consolidated_learnings.md`.
* If a significant, broadly applicable insight in `consolidated_learnings.md` strongly suggests modifying *another active `.clinerule`* (e.g., core workflow, tech guidance), Cline MAY propose this change after user confirmation. This is exceptional.

---

## 3. Guidelines for Knowledge Content

These guidelines apply to entries in `.ai_docs/memory-bank/raw_reflection_log.md` (initial capture) and especially to `.ai_docs/memory-bank/consolidated_learnings.md` (refined, long-term knowledge).

* **Prioritize High-Value Insights:** Focus on lessons that significantly impact future performance, **lead to more robust or generalizable understanding,** or detail critical errors and their resolutions, major time-saving discoveries, fundamental shifts in understanding, and essential project-specific configurations.
* **Be Concise & Actionable (especially for `.ai_docs/memory-bank/consolidated_learnings.md`):** Information should be clear, to the point, and useful when revisited. What can be *done* differently or leveraged next time?
* **Strive for Clarity and Future Usability:** Document insights in a way that is clear and easily understandable for future review, facilitating effective knowledge retrieval and application (akin to self-explainability).
* **Document Persistently, Refine & Prune Continuously:** Capture raw insights immediately. Systematically refine, consolidate, and prune this knowledge as per Section 2.
* **Organize for Retrieval:** Structure `.ai_docs/memory-bank/consolidated_learnings.md` logically. Use clear headings and Markdown formatting.
* **Avoid Low-Utility Information in `consolidated_learnings.md`:** This file should not contain trivial statements. Raw, verbose thoughts belong in `.ai_docs/memory-bank/raw_reflection_log.md` before pruning.
* **Support Continuous Improvement:** The ultimate goal is to avoid repeating mistakes, accelerate future tasks, and make Cline's operations more robust and reliable. Frame all knowledge with this in mind.
* **Manage Information Density:** Actively work to keep `consolidated_learnings.md` dense with high-value information and free of outdated or overly verbose content. The pruning of `.ai_docs/memory-bank/raw_reflection_log.md` is key to this.
</file>

<file path=".clinerules/uv-python-usage-guide.md">
---
description: A comprehensive guide to using UV for Python project management, covering installation, environment management, package handling, and best practices.
author: Cline
version: 1.0
tags: ["uv", "python", "package-manager", "venv", "guide"]
globs: ["**/*.py", "pyproject.toml"]
---

# UV Python Project Management Guide

## Table of Contents
- [Introduction](#introduction)
- [Installation](#installation)
- [Managing Python Versions](#managing-python-versions)
- [Project Management](#project-management)
- [Virtual Environment Management](#virtual-environment-management)
- [Package Management](#package-management)
- [Advanced Configuration](#advanced-configuration)
- [Development Workflows](#development-workflows)
- [Best Practices](#best-practices)
- [Security Considerations](#security-considerations)
- [Performance Optimization](#performance-optimization)
- [Troubleshooting](#troubleshooting)
- [Environment Variables](#environment-variables)
- [Tool Integration](#tool-integration)
- [Common Commands Reference](#common-commands-reference)

## Introduction

UV is a modern Python package manager and virtual environment tool that offers significant performance improvements over traditional tools like pip and venv. This guide covers how to effectively use UV for Python project management.

## Installation

### macOS
```bash
# Using Homebrew
brew install uv

# Using the installer script
curl -LsSf https://astral.sh/uv/install.sh | sh
```

### Linux
```bash
# Using the installer script
curl -LsSf https://astral.sh/uv/install.sh | sh
```

### Windows
```powershell
# Using winget
winget install --id=astral-sh.uv -e

# Using scoop
scoop install main/uv
```

## Managing Python Versions

UV can manage Python installations for you. Here's how to work with Python versions:

### Installing Python
```bash
# Install latest Python version
uv python install

# Install specific Python version
uv python install 3.12

# Install multiple versions
uv python install 3.11 3.12

# Install PyPy
uv python install pypy@3.10
```

### Listing Python Versions
```bash
# List available and installed versions
uv python list

# Show all versions including other platforms
uv python list --all-versions

# Only show installed versions
uv python list --only-installed
```

### Finding Python Executables
```bash
# Find default Python
uv python find

# Find specific version
uv python find >=3.11
```

## Project Management

UV provides robust project management capabilities through its project system.

### Creating a New Project
```bash
# Create a new project
uv init my-project
cd my-project

# Or initialize in current directory
mkdir my-project
cd my-project
uv init
```

This creates:
- `pyproject.toml` - Project configuration and dependencies
- `.python-version` - Python version specification
- `README.md` - Project documentation
- `main.py` - Initial Python file

### Project Structure
```
my-project/
 .venv/               # Virtual environment (created on first use)
 .python-version      # Python version specification
 pyproject.toml       # Project configuration
 uv.lock             # Dependency lock file
 README.md           # Project documentation
 main.py             # Main Python file
```

### Managing Dependencies

```bash
# Add dependencies
uv add requests
uv add 'flask>=2.0.0'
uv add 'pytest[testing]'

# Remove dependencies
uv remove requests

# Update dependencies
uv lock --upgrade-package requests

# Install all dependencies
uv sync
```

## Virtual Environment Management

UV automatically manages virtual environments for projects and can work with existing environments.

### Creating Virtual Environments
```bash
# Create venv in default location (.venv)
uv venv

# Create venv with specific name
uv venv my-env

# Create venv with specific Python version
uv venv --python 3.12
```

### Working with Virtual Environments

```bash
# Activate virtual environment
# On Unix/macOS:
source .venv/bin/activate

# On Windows:
.venv\Scripts\activate

# On Fish shell:
source .venv/bin/activate.fish

# Deactivate virtual environment
deactivate
```

### Using Existing Environments

UV automatically detects and uses virtual environments in the following order:
1. Active virtual environment (VIRTUAL_ENV)
2. Active Conda environment (CONDA_PREFIX)
3. `.venv` in current or parent directories

## Package Management

UV provides both high-level project commands and pip-compatible commands for package management.

### Project-Based Package Management
```bash
# Add package to project
uv add package-name

# Remove package from project
uv remove package-name

# Sync project dependencies
uv sync

# Update lockfile
uv lock
```

### Pip-Compatible Commands
```bash
# Install packages
uv pip install package-name
uv pip install -r requirements.txt

# Install in editable mode
uv pip install -e .

# Uninstall packages
uv pip uninstall package-name

# List installed packages
uv pip list

# Show package info
uv pip show package-name

# Generate requirements.txt
uv pip freeze > requirements.txt
```

## Advanced Configuration

### pyproject.toml Configuration
```toml
[project]
name = "my-project"
version = "0.1.0"
description = "Project description"
readme = "README.md"
requires-python = ">=3.8"
license = { text = "MIT" }
authors = [
    { name = "Your Name", email = "your.email@example.com" }
]

dependencies = [
    "requests>=2.28.0",
    "flask[async]>=2.0.0",
    "sqlalchemy",
]

[project.optional-dependencies]
test = ["pytest>=7.0", "pytest-cov"]
dev = ["black", "mypy", "ruff"]

[tool.uv]
python-version = "3.12"
```

### UV Configuration Options
```toml
[tool.uv]
# Package index configuration
[[tool.uv.index]]
url = "https://pypi.org/simple"
default = true

[[tool.uv.index]]
url = "https://test.pypi.org/simple"
secondary = true

# Build settings
no-binary = ["cryptography", "numpy"]
build-isolation = true

# Cache settings
cache-dir = "~/.cache/uv"
```

### Environment-Specific Settings
```toml
[tool.uv.env]
development = { extras = ["dev", "test"] }
production = { extras = [] }
```

## Development Workflows

### Local Development Setup
```bash
# Initialize new project
uv init my-project
cd my-project

# Set up development environment
uv add --dev black ruff mypy pytest
uv add --dev 'pre-commit>=3.0.0'

# Create pre-commit config
cat > .pre-commit-config.yaml << EOF
repos:
-   repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.3.0
    hooks:
    -   id: ruff
        args: [--fix]
-   repo: https://github.com/psf/black
    rev: 24.2.0
    hooks:
    -   id: black
EOF

# Install pre-commit hooks
uv run pre-commit install
```

### CI/CD Integration
```yaml
# .github/workflows/python-ci.yml
name: Python CI

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Install UV
      run: curl -LsSf https://astral.sh/uv/install.sh | sh

    - name: Setup Python
      run: uv python install 3.12

    - name: Install dependencies
      run: |
        uv pip install -e ".[test]"
        
    - name: Run tests
      run: uv run pytest
```

### Working with Multiple Python Versions
```bash
# Create test environments
for version in 3.8 3.9 3.10 3.11 3.12; do
    uv venv "venv-$version" --python "$version"
    source "venv-$version/bin/activate"
    uv pip install -e ".[test]"
    uv run pytest
    deactivate
done
```

## Best Practices

### 1. Project Structure
```
my-project/
 .git/
 .gitignore
 .pre-commit-config.yaml
 .python-version
 .venv/
 src/
    my_project/
        __init__.py
        core.py
        utils/
 tests/
    __init__.py
    test_core.py
 docs/
 pyproject.toml
 uv.lock
 README.md
```

### 2. Version Control Best Practices
```gitignore
# .gitignore
.venv/
__pycache__/
*.py[cod]
*$py.class
.pytest_cache/
.coverage
htmlcov/
dist/
build/
*.egg-info/
```

### 3. Dependency Management
- Use semantic versioning for dependencies:
  ```toml
  dependencies = [
      "requests~=2.28.0",     # Compatible releases (>=2.28.0, <2.29.0)
      "flask>=2.0.0,<3.0.0",  # Specific version range
      "sqlalchemy==2.0.0",    # Exact version
  ]
  ```

- Lock dependencies for reproducibility:
  ```bash
  # Update lockfile
  uv lock
  
  # Sync environment with lockfile
  uv sync
  ```

### 4. Testing and Quality Assurance
```bash
# Install test dependencies
uv add --dev pytest pytest-cov black mypy ruff

# Run tests with coverage
uv run pytest --cov=src/my_project

# Run type checking
uv run mypy src/my_project

# Run linting
uv run ruff check src/my_project
```

### 5. Documentation
- Use docstrings for all public APIs
- Maintain up-to-date README.md
- Document environment setup requirements
- Include example usage

### 6. Security Best Practices
- Keep UV and Python updated
- Use UV's hash verification
- Audit dependencies regularly
- Use private package indexes securely

### 7. Performance Optimization
- Use UV's concurrent downloads
- Leverage caching effectively
- Optimize dependency resolution
- Use prebuilt wheels when possible

## Security Considerations

### Package Verification
```bash
# Enable hash verification
uv pip install --require-hashes -r requirements.txt

# Generate requirements with hashes
uv pip freeze --all --require-hashes > requirements.txt
```

### Private Package Indexes
```toml
[tool.uv]
[[tool.uv.index]]
url = "https://private.pypi.org/simple"
username = "${PYPI_USERNAME}"
password = "${PYPI_PASSWORD}"
```

### Dependency Auditing
```bash
# Install safety checker
uv tool install safety

# Check for known vulnerabilities
safety check
```

## Performance Optimization

### Caching Configuration
```bash
# Set custom cache directory
export UV_CACHE_DIR="/path/to/cache"

# Clear cache
uv cache clean

# Prune old cache entries
uv cache prune
```

### Build Optimization
```bash
# Set concurrent build limit
export UV_CONCURRENT_BUILDS=4

# Disable build isolation for speed
uv pip install --no-build-isolation package-name
```

## Troubleshooting

### Common Issues and Solutions

1. **Package Installation Failures**
   ```bash
   # Try with --verbose for more information
   uv pip install --verbose package-name
   
   # Force reinstall
   uv pip install --force-reinstall package-name
   ```

2. **Virtual Environment Issues**
   ```bash
   # Recreate virtual environment
   rm -rf .venv
   uv venv
   uv sync
   ```

3. **Dependency Conflicts**
   ```bash
   # Check for conflicts
   uv pip check
   
   # Show dependency tree
   uv pip tree
   ```

### Debug Mode
```bash
# Enable debug logging
export RUST_LOG=debug
uv pip install package-name
```

## Environment Variables

### Common Environment Variables
```bash
# Cache configuration
export UV_CACHE_DIR="/path/to/cache"
export UV_NO_CACHE=1

# Python configuration
export UV_PYTHON_INSTALL_DIR="/path/to/pythons"
export UV_PYTHON_PREFERENCE="managed"

# Network configuration
export UV_HTTP_TIMEOUT=30
export UV_OFFLINE=1

# Build configuration
export UV_CONCURRENT_BUILDS=4
export UV_NO_BUILD_ISOLATION=1
```

## Tool Integration

### Editor Integration (VSCode)
```json
{
    "python.defaultInterpreterPath": "${workspaceFolder}/.venv/bin/python",
    "python.analysis.typeCheckingMode": "basic",
    "python.formatting.provider": "black",
    "python.linting.enabled": true,
    "python.linting.lintOnSave": true
}
```

### Pre-commit Integration
```yaml
# .pre-commit-config.yaml
repos:
-   repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.3.0
    hooks:
    -   id: ruff
        args: [--fix]
-   repo: https://github.com/psf/black
    rev: 24.2.0
    hooks:
    -   id: black
-   repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.8.0
    hooks:
    -   id: mypy
        additional_dependencies: [types-all]
```

### Docker Integration
```dockerfile
FROM python:3.12-slim

# Install UV
RUN curl -LsSf https://astral.sh/uv/install.sh | sh

WORKDIR /app
COPY . .

# Install dependencies
RUN uv pip install -e ".[prod]"

CMD ["uv", "run", "python", "-m", "my_project"]
```

## Advanced Package Management

### Monorepo Support
```
monorepo/
 .git/
 pyproject.toml      # Workspace configuration
 project1/
    pyproject.toml  # Project 1 configuration
    src/
    tests/
 project2/
    pyproject.toml  # Project 2 configuration
    src/
    tests/
 shared/
     pyproject.toml  # Shared library configuration
     src/
```

Root pyproject.toml for monorepo:
```toml
[workspace]
members = [
    "project1",
    "project2",
    "shared"
]

[tool.uv.workspace]
python-version = "3.12"
```

### Complex Dependency Scenarios

1. **Git Dependencies with Specific References**
   ```toml
   dependencies = [
       "mypackage @ git+https://github.com/user/repo.git@main",
       "otherpackage @ git+https://github.com/user/repo.git@v1.0.0",
       "debugtools @ git+ssh://git@github.com/user/repo.git@d34db33f"
   ]
   ```

2. **Local Development Dependencies**
   ```toml
   dependencies = [
       "mypackage @ file:///path/to/package",
       "devtool @ file:///${PROJECT_ROOT}/tools/devtool"
   ]
   ```

3. **Complex Version Constraints**
   ```toml
   dependencies = [
       "requests>=2.28.0,<3.0.0,!=2.29.0",  # Exclude specific version
       "flask~=2.0.0",                       # Compatible release
       "sqlalchemy>2.0.0",                   # Greater than
       "pandas==2.0.*",                      # Wildcard matching
   ]
   ```

### Advanced Installation Scenarios

1. **Installing with Extras**
   ```bash
   # Install multiple extras
   uv add 'flask[async,dotenv]'
   
   # Install all extras
   uv add 'flask[all]'
   ```

2. **Platform-Specific Dependencies**
   ```toml
   [project.dependencies]
   pywin32 = { version = ">=305", markers = "sys_platform == 'win32'" }
   pyobjc-framework-Cocoa = { version = ">=9.0", markers = "sys_platform == 'darwin'" }
   ```

3. **Development Dependencies with Groups**
   ```toml
   [project.optional-dependencies]
   test = [
       "pytest>=7.0",
       "pytest-cov>=4.0",
       "pytest-asyncio>=0.21.0"
   ]
   lint = [
       "black>=23.0",
       "ruff>=0.1.0",
       "mypy>=1.0"
   ]
   docs = [
       "sphinx>=7.0",
       "sphinx-rtd-theme>=1.0"
   ]
   dev = [
       "ipython>=8.0",
       "debugpy>=1.6"
   ]
   ```

### Package Publishing Workflow
```bash
# Build distribution
uv build

# Check distribution
uv run twine check dist/*

# Upload to TestPyPI
uv publish --index testpypi

# Upload to PyPI
uv publish

# Upload with trusted publishing (GitHub Actions)
uv publish --oidc
```

### Advanced Cache Management
```bash
# View cache information
uv cache info

# Clean specific cache types
uv cache clean --wheels    # Clean wheel cache
uv cache clean --sources   # Clean source cache
uv cache clean --http      # Clean HTTP cache

# Set cache retention period
uv cache prune --older-than 30d
```

### Custom Index Configuration
```toml
[tool.uv]
# Configure multiple package indexes
[[tool.uv.index]]
url = "https://pypi.org/simple"
default = true

[[tool.uv.index]]
url = "https://test.pypi.org/simple"
secondary = true

[[tool.uv.index]]
url = "https://private.pypi.org/simple"
username = "${PYPI_USERNAME}"
password = "${PYPI_PASSWORD}"

# Index-specific settings
[tool.uv.index-settings]
timeout = 30
verify-ssl = true
retries = 3
```

## Common Commands Reference

### Project Commands
```bash
uv init              # Create new project
uv add              # Add dependency
uv remove           # Remove dependency
uv sync             # Install dependencies
uv lock             # Update lockfile
uv run              # Run command in project environment
```

### Virtual Environment Commands
```bash
uv venv             # Create virtual environment
uv pip install      # Install packages
uv pip uninstall    # Remove packages
uv pip list         # List installed packages
uv pip freeze       # Generate requirements.txt
```

### Python Management Commands
```bash
uv python install   # Install Python
uv python list      # List Python versions
uv python find      # Find Python executable
uv python pin       # Pin Python version
```

### Tool Commands
```bash
uvx                 # Run tool without installing
uv tool install     # Install tool globally
uv tool uninstall   # Remove tool
uv tool list        # List installed tools
</file>

<file path=".cursor/rules/api-client-usage.mdc">
---
description: 
globs: 
alwaysApply: false
---
---
description: 
globs: 
alwaysApply: true
---
# Cursor Rules for API Client Usage

## API Call Organization

1. **Client Import**
   - Import the API client from the centralized location
   ```typescript
   import { api } from "~/api";
   ```

2. **Query Structure**
   - Use the appropriate method for the HTTP verb
   - First parameter is the HTTP method, second is the endpoint path
   ```typescript
   // GET request
   const { data } = api.useQuery("get", "/api/v1/resource");
   
   // POST request
   const mutation = api.useMutation("post", "/api/v1/resource");
   mutation.mutate(payload);
   ```

3. **Endpoint Patterns**
   - All endpoints should use `/api/v1/` prefix
   - Use resource-focused naming: `/api/v1/users`, `/api/v1/products`

## Data Handling

1. **Type Safety**
   - Define response and request types for all API calls
   ```typescript
   type HelloResponse = { message: string };
   const { data } = api.useQuery<HelloResponse>("get", "/api/v1/hello");
   ```

2. **Loading States**
   - Always handle loading states in UI components
   ```typescript
   const { data, isLoading, error } = api.useQuery("get", "/api/v1/resource");
   if (isLoading) return <LoadingSpinner />;
   if (error) return <ErrorMessage error={error} />;
   ```

3. **Error Handling**
   - Check for errors in all API calls
   - Use appropriate error boundaries

## Query Options

1. **Caching and Refetching**
   - Configure cache time and stale time based on data volatility
   ```typescript
   const { data } = api.useQuery("get", "/api/v1/resource", {
     staleTime: 60000, // 1 minute
     cacheTime: 300000, // 5 minutes
   });
   ```

2. **Query Dependencies**
   - Use the enabled option for dependent queries
   ```typescript
   const { data: user } = api.useQuery("get", "/api/v1/user");
   const { data: userPosts } = api.useQuery("get", `/api/v1/posts?userId=${user?.id}`, {
     enabled: !!user?.id,
   });
   ```

## Component Integration

1. **Data Fetching Location**
   - Fetch data at the route component level when possible
   - Pass data down to child components as props

2. **Mutation Patterns**
   - Handle optimistic updates when appropriate
   - Invalidate related queries after successful mutations
   ```typescript
   const mutation = api.useMutation("post", "/api/v1/resource", {
     onSuccess: () => {
       api.queryClient.invalidateQueries(["get", "/api/v1/resource"]);
     },
   });
   ```
</file>

<file path=".cursor/rules/architecture.mdc">
---
description: 
globs: 
alwaysApply: false
---
---
description: 
globs: 
alwaysApply: true
---
# Cursor Rules for AI-Driven Backend Service

This document outlines the rules and guidelines for maintaining code quality and architectural integrity in this project.

## Code Organization Rules

1. **Layered Architecture Adherence**

   - Keep code within its appropriate layer:
     - `api/`  HTTP interfaces only
     - `services/`  Business logic only
     - `operations/`  Data access only
     - `models/`  Data models only
     - `lib/`  Infrastructure components only

2. **Domain Separation**

   - Organize code by domain within each layer (`auth`, `user`, etc.)
   - File naming should reflect both domain and purpose (e.g., `auth_service.py`)

3. **Import Order**

   ```python
   # 1. Standard library imports
   import os
   from typing import Optional, List

   # 2. Third-party imports
   from fastapi import Depends, HTTPException
   from sqlalchemy.orm import Session

   # 3. Internal imports - by layer, alphabetical
   from api.schemas import UserResponse
   from lib.database import get_db_session
   from models.user import User
   from operations.user import UserRepository
   ```

## Coding Style Rules

1. **Type Annotations**

   - All function parameters and return values must have type hints
   - Complex types should use `typing` module (List, Dict, Optional, etc.)

2. **Function Structure**

   - Single responsibility principle for all functions
   - Document complex functions with docstrings

3. **Error Handling**
   - API layer: Return proper HTTP exceptions
   - Service layer: Business-specific exceptions
   - Operations layer: Database-specific errors
   - Use try/except blocks at appropriate layer boundaries

## Layer-Specific Rules

1. **API Layer**

   - Routes should be minimal and delegate to services
   - All endpoint parameters should be validated with Pydantic
   - Response models must be defined for all endpoints
   - Authentication/authorization handled via FastAPI dependencies

2. **Service Layer**

   - No direct database access; use repositories
   - Implement domain-specific business rules
   - Transaction boundaries should be defined here
   - Return domain objects, not ORM models

3. **Operations Layer**

   - Implement repository pattern
   - Methods should focus on CRUD operations
   - No business logic
   - Handle query optimization

4. **Models Layer**

   - ORM models should be minimal and reflect database schema
   - Avoid business logic in models
   - Use appropriate SQLAlchemy types and constraints

5. **Infrastructure/Lib Layer**
   - Components should be stateless where possible
   - Use dependency injection pattern
   - Implement singleton pattern for expensive resources
   - Provide clear interfaces

## Dependency Injection Rules

1. **Dependency Chain**

   - Follow dependency direction: API  Service  Operations  Models
   - Never reference a higher layer from a lower layer

2. **Dependency Functions**
   - Name dependency functions as `get_*`
   - Implement proper error handling
   - Follow FastAPI's dependency pattern

## Testing Rules

1. **Test Coverage**

   - Minimum 80% code coverage
   - Unit tests for all service functions
   - Integration tests for API endpoints

2. **Test Structure**
   - Use fixtures for test setup
   - Use mocks for external dependencies
   - Test both success and error paths

## Documentation Rules

1. **Code Documentation**

   - Every module should have a module-level docstring
   - Complex functions require docstrings
   - Public functions must have parameter and return documentation

2. **API Documentation**
   - All endpoints must have descriptions
   - Use FastAPI's response_model attribute
   - Include example requests/responses

## Security Rules

1. **Input Validation**

   - All user inputs must be validated with Pydantic schemas
   - Implement additional validation for security-sensitive fields

2. **Authentication**

   - All non-public endpoints must require authentication
   - JWT token validation required for authenticated endpoints

3. **Error Messages**
   - No sensitive data in error messages
   - Consistent error format across API

## Performance Rules

1. **Database Access**

   - Use query optimization techniques
   - Implement caching for frequently accessed data
   - Set appropriate indexes on frequently queried columns

2. **Async Operations**
   - Use async endpoints for I/O-bound operations
</file>

<file path=".cursor/rules/fastapi.mdc">
---
description: 
globs: 
alwaysApply: false
---
You are an expert in Python, FastAPI, and scalable API development.
  
Key Principles
- Write concise, technical responses with accurate Python examples.
- Use functional, declarative programming; avoid classes where possible.
- Prefer iteration and modularization over code duplication.
- Use descriptive variable names with auxiliary verbs (e.g., is_active, has_permission).
- Use lowercase with underscores for directories and files (e.g., routers/user_routes.py).
- Favor named exports for routes and utility functions.
- Use the Receive an Object, Return an Object (RORO) pattern.
  
Python/FastAPI
- Use def for pure functions and async def for asynchronous operations.
- Use type hints for all function signatures. Prefer Pydantic models over raw dictionaries for input validation.
- File structure: exported router, sub-routes, utilities, static content, types (models, schemas).
- Avoid unnecessary curly braces in conditional statements.
- For single-line statements in conditionals, omit curly braces.
- Use concise, one-line syntax for simple conditional statements (e.g., if condition: do_something()).
  
Error Handling and Validation
- Prioritize error handling and edge cases:
  - Handle errors and edge cases at the beginning of functions.
  - Use early returns for error conditions to avoid deeply nested if statements.
  - Place the happy path last in the function for improved readability.
  - Avoid unnecessary else statements; use the if-return pattern instead.
  - Use guard clauses to handle preconditions and invalid states early.
  - Implement proper error logging and user-friendly error messages.
  - Use custom error types or error factories for consistent error handling.
  
Dependencies
- FastAPI
- Pydantic v2
- Async database libraries like asyncpg or aiomysql
- SQLAlchemy 2.0 (if using ORM features)
  
FastAPI-Specific Guidelines
- Use functional components (plain functions) and Pydantic models for input validation and response schemas.
- Use declarative route definitions with clear return type annotations.
- Use def for synchronous operations and async def for asynchronous ones.
- Minimize @app.on_event("startup") and @app.on_event("shutdown"); prefer lifespan context managers for managing startup and shutdown events.
- Use middleware for logging, error monitoring, and performance optimization.
- Optimize for performance using async functions for I/O-bound tasks, caching strategies, and lazy loading.
- Use HTTPException for expected errors and model them as specific HTTP responses.
- Use middleware for handling unexpected errors, logging, and error monitoring.
- Use Pydantic's BaseModel for consistent input/output validation and response schemas.
  
Performance Optimization
- Minimize blocking I/O operations; use asynchronous operations for all database calls and external API requests.
- Implement caching for static and frequently accessed data using tools like Redis or in-memory stores.
- Optimize data serialization and deserialization with Pydantic.
- Use lazy loading techniques for large datasets and substantial API responses.
  
Key Conventions
1. Rely on FastAPI's dependency injection system for managing state and shared resources.
2. Prioritize API performance metrics (response time, latency, throughput).
3. Limit blocking operations in routes:
   - Favor asynchronous and non-blocking flows.
   - Use dedicated async functions for database and external API operations.
   - Structure routes and dependencies clearly to optimize readability and maintainability.
  
Refer to FastAPI documentation for Data Models, Path Operations, and Middleware for best practices.
</file>

<file path=".cursor/rules/sb-create-database-functions.mdc">
---
description: Guidelines for writing Supabase database functions
globs: 
alwaysApply: false
---

# Database: Create functions

You're a Supabase Postgres expert in writing database functions. Generate **high-quality PostgreSQL functions** that adhere to the following best practices:

## General Guidelines

1. **Default to `SECURITY INVOKER`:**

   - Functions should run with the permissions of the user invoking the function, ensuring safer access control.
   - Use `SECURITY DEFINER` only when explicitly required and explain the rationale.

2. **Set the `search_path` Configuration Parameter:**

   - Always set `search_path` to an empty string (`set search_path = '';`).
   - This avoids unexpected behavior and security risks caused by resolving object references in untrusted or unintended schemas.
   - Use fully qualified names (e.g., `schema_name.table_name`) for all database objects referenced within the function.

3. **Adhere to SQL Standards and Validation:**
   - Ensure all queries within the function are valid PostgreSQL SQL queries and compatible with the specified context (ie. Supabase).

## Best Practices

1. **Minimize Side Effects:**

   - Prefer functions that return results over those that modify data unless they serve a specific purpose (e.g., triggers).

2. **Use Explicit Typing:**

   - Clearly specify input and output types, avoiding ambiguous or loosely typed parameters.

3. **Default to Immutable or Stable Functions:**

   - Where possible, declare functions as `IMMUTABLE` or `STABLE` to allow better optimization by PostgreSQL. Use `VOLATILE` only if the function modifies data or has side effects.

4. **Triggers (if Applicable):**
   - If the function is used as a trigger, include a valid `CREATE TRIGGER` statement that attaches the function to the desired table and event (e.g., `BEFORE INSERT`).

## Example Templates

### Simple Function with `SECURITY INVOKER`

```sql
create or replace function my_schema.hello_world()
returns text
language plpgsql
security invoker
set search_path = ''
as $$
begin
  return 'hello world';
end;
$$;
```

### Function with Parameters and Fully Qualified Object Names

```sql
create or replace function public.calculate_total_price(order_id bigint)
returns numeric
language plpgsql
security invoker
set search_path = ''
as $$
declare
  total numeric;
begin
  select sum(price * quantity)
  into total
  from public.order_items
  where order_id = calculate_total_price.order_id;

  return total;
end;
$$;
```

### Function as a Trigger

```sql
create or replace function my_schema.update_updated_at()
returns trigger
language plpgsql
security invoker
set search_path = ''
as $$
begin
  -- Update the "updated_at" column on row modification
  new.updated_at := now();
  return new;
end;
$$;

create trigger update_updated_at_trigger
before update on my_schema.my_table
for each row
execute function my_schema.update_updated_at();
```

### Function with Error Handling

```sql
create or replace function my_schema.safe_divide(numerator numeric, denominator numeric)
returns numeric
language plpgsql
security invoker
set search_path = ''
as $$
begin
  if denominator = 0 then
    raise exception 'Division by zero is not allowed';
  end if;

  return numerator / denominator;
end;
$$;
```

### Immutable Function for Better Optimization

```sql
create or replace function my_schema.full_name(first_name text, last_name text)
returns text
language sql
security invoker
set search_path = ''
immutable
as $$
  select first_name || ' ' || last_name;
$$;
```
</file>

<file path=".cursor/rules/sb-create-migration.mdc">
---
description: Guidelines for writing Postgres migrations
globs: 
alwaysApply: false
---

# Database: Create migration

You are a Postgres Expert who loves creating secure database schemas.

This project uses the migrations provided by the Supabase CLI.

## Creating a migration file

Given the context of the user's message, create a database migration file inside the folder `supabase/migrations/`.

The file MUST following this naming convention:

The file MUST be named in the format `YYYYMMDDHHmmss_short_description.sql` with proper casing for months, minutes, and seconds in UTC time:

1. `YYYY` - Four digits for the year (e.g., `2024`).
2. `MM` - Two digits for the month (01 to 12).
3. `DD` - Two digits for the day of the month (01 to 31).
4. `HH` - Two digits for the hour in 24-hour format (00 to 23).
5. `mm` - Two digits for the minute (00 to 59).
6. `ss` - Two digits for the second (00 to 59).
7. Add an appropriate description for the migration.

For example:

```
20240906123045_create_profiles.sql
```

## SQL Guidelines

Write Postgres-compatible SQL code for Supabase migration files that:

- Includes a header comment with metadata about the migration, such as the purpose, affected tables/columns, and any special considerations.
- Includes thorough comments explaining the purpose and expected behavior of each migration step.
- Write all SQL in lowercase.
- Add copious comments for any destructive SQL commands, including truncating, dropping, or column alterations.
- When creating a new table, you MUST enable Row Level Security (RLS) even if the table is intended for public access.
- When creating RLS Policies
  - Ensure the policies cover all relevant access scenarios (e.g. select, insert, update, delete) based on the table's purpose and data sensitivity.
  - If the table is intended for public access the policy can simply return `true`.
  - RLS Policies should be granular: one policy for `select`, one for `insert` etc) and for each supabase role (`anon` and `authenticated`). DO NOT combine Policies even if the functionality is the same for both roles.
  - Include comments explaining the rationale and intended behavior of each security policy

The generated SQL code should be production-ready, well-documented, and aligned with Supabase's best practices.
</file>

<file path=".cursor/rules/sb-create-rls-policies.mdc">
---
description: Guidelines for writing Postgres Row Level Security policies
globs: 
alwaysApply: false
---

# Database: Create RLS policies

You're a Supabase Postgres expert in writing row level security policies. Your purpose is to generate a policy with the constraints given by the user. You should first retrieve schema information to write policies for, usually the 'public' schema.

The output should use the following instructions:

- The generated SQL must be valid SQL.
- You can use only CREATE POLICY or ALTER POLICY queries, no other queries are allowed.
- Always use double apostrophe in SQL strings (eg. 'Night''s watch')
- You can add short explanations to your messages.
- The result should be a valid markdown. The SQL code should be wrapped in ``` (including sql language tag).
- Always use "auth.uid()" instead of "current_user".
- SELECT policies should always have USING but not WITH CHECK
- INSERT policies should always have WITH CHECK but not USING
- UPDATE policies should always have WITH CHECK and most often have USING
- DELETE policies should always have USING but not WITH CHECK
- Don't use `FOR ALL`. Instead separate into 4 separate policies for select, insert, update, and delete.
- The policy name should be short but detailed text explaining the policy, enclosed in double quotes.
- Always put explanations as separate text. Never use inline SQL comments.
- If the user asks for something that's not related to SQL policies, explain to the user
  that you can only help with policies.
- Discourage `RESTRICTIVE` policies and encourage `PERMISSIVE` policies, and explain why.

The output should look like this:

```sql
CREATE POLICY "My descriptive policy." ON books FOR INSERT to authenticated USING ( (select auth.uid()) = author_id ) WITH ( true );
```

Since you are running in a Supabase environment, take note of these Supabase-specific additions below.

## Authenticated and unauthenticated roles

Supabase maps every request to one of the roles:

- `anon`: an unauthenticated request (the user is not logged in)
- `authenticated`: an authenticated request (the user is logged in)

These are actually [Postgres Roles](mdc:docs/guides/database/postgres/roles). You can use these roles within your Policies using the `TO` clause:

```sql
create policy "Profiles are viewable by everyone"
on profiles
for select
to authenticated, anon
using ( true );

-- OR

create policy "Public profiles are viewable only by authenticated users"
on profiles
for select
to authenticated
using ( true );
```

Note that `for ...` must be added after the table but before the roles. `to ...` must be added after `for ...`:

### Incorrect

```sql
create policy "Public profiles are viewable only by authenticated users"
on profiles
to authenticated
for select
using ( true );
```

### Correct

```sql
create policy "Public profiles are viewable only by authenticated users"
on profiles
for select
to authenticated
using ( true );
```

## Multiple operations

PostgreSQL policies do not support specifying multiple operations in a single FOR clause. You need to create separate policies for each operation.

### Incorrect

```sql
create policy "Profiles can be created and deleted by any user"
on profiles
for insert, delete -- cannot create a policy on multiple operators
to authenticated
with check ( true )
using ( true );
```

### Correct

```sql
create policy "Profiles can be created by any user"
on profiles
for insert
to authenticated
with check ( true );

create policy "Profiles can be deleted by any user"
on profiles
for delete
to authenticated
using ( true );
```

## Helper functions

Supabase provides some helper functions that make it easier to write Policies.

### `auth.uid()`

Returns the ID of the user making the request.

### `auth.jwt()`

Returns the JWT of the user making the request. Anything that you store in the user's `raw_app_meta_data` column or the `raw_user_meta_data` column will be accessible using this function. It's important to know the distinction between these two:

- `raw_user_meta_data` - can be updated by the authenticated user using the `supabase.auth.update()` function. It is not a good place to store authorization data.
- `raw_app_meta_data` - cannot be updated by the user, so it's a good place to store authorization data.

The `auth.jwt()` function is extremely versatile. For example, if you store some team data inside `app_metadata`, you can use it to determine whether a particular user belongs to a team. For example, if this was an array of IDs:

```sql
create policy "User is in team"
on my_table
to authenticated
using ( team_id in (select auth.jwt() -> 'app_metadata' -> 'teams'));
```

### MFA

The `auth.jwt()` function can be used to check for [Multi-Factor Authentication](mdc:docs/guides/auth/auth-mfa#enforce-rules-for-mfa-logins). For example, you could restrict a user from updating their profile unless they have at least 2 levels of authentication (Assurance Level 2):

```sql
create policy "Restrict updates."
on profiles
as restrictive
for update
to authenticated using (
  (select auth.jwt()->>'aal') = 'aal2'
);
```

## RLS performance recommendations

Every authorization system has an impact on performance. While row level security is powerful, the performance impact is important to keep in mind. This is especially true for queries that scan every row in a table - like many `select` operations, including those using limit, offset, and ordering.

Based on a series of [tests](mdc:https:/github.com/GaryAustin1/RLS-Performance), we have a few recommendations for RLS:

### Add indexes

Make sure you've added [indexes](mdc:docs/guides/database/postgres/indexes) on any columns used within the Policies which are not already indexed (or primary keys). For a Policy like this:

```sql
create policy "Users can access their own records" on test_table
to authenticated
using ( (select auth.uid()) = user_id );
```

You can add an index like:

```sql
create index userid
on test_table
using btree (user_id);
```

### Call functions with `select`

You can use `select` statement to improve policies that use functions. For example, instead of this:

```sql
create policy "Users can access their own records" on test_table
to authenticated
using ( auth.uid() = user_id );
```

You can do:

```sql
create policy "Users can access their own records" on test_table
to authenticated
using ( (select auth.uid()) = user_id );
```

This method works well for JWT functions like `auth.uid()` and `auth.jwt()` as well as `security definer` Functions. Wrapping the function causes an `initPlan` to be run by the Postgres optimizer, which allows it to "cache" the results per-statement, rather than calling the function on each row.

Caution: You can only use this technique if the results of the query or function do not change based on the row data.

### Minimize joins

You can often rewrite your Policies to avoid joins between the source and the target table. Instead, try to organize your policy to fetch all the relevant data from the target table into an array or set, then you can use an `IN` or `ANY` operation in your filter.

For example, this is an example of a slow policy which joins the source `test_table` to the target `team_user`:

```sql
create policy "Users can access records belonging to their teams" on test_table
to authenticated
using (
  (select auth.uid()) in (
    select user_id
    from team_user
    where team_user.team_id = team_id -- joins to the source "test_table.team_id"
  )
);
```

We can rewrite this to avoid this join, and instead select the filter criteria into a set:

```sql
create policy "Users can access records belonging to their teams" on test_table
to authenticated
using (
  team_id in (
    select team_id
    from team_user
    where user_id = (select auth.uid()) -- no join
  )
);
```

### Specify roles in your policies

Always use the Role of inside your policies, specified by the `TO` operator. For example, instead of this query:

```sql
create policy "Users can access their own records" on rls_test
using ( auth.uid() = user_id );
```

Use:

```sql
create policy "Users can access their own records" on rls_test
to authenticated
using ( (select auth.uid()) = user_id );
```

This prevents the policy `( (select auth.uid()) = user_id )` from running for any `anon` users, since the execution stops at the `to authenticated` step.
</file>

<file path=".github/DISCUSSION_TEMPLATE/questions.yml">
labels: [question]
body:
  - type: markdown
    attributes:
      value: |
        Thanks for your interest in this project! 

        Please follow these instructions, fill every question, and do every step. 

        I'm asking this because answering questions and solving problems in GitHub is what consumes most of the time.

        I end up not being able to add new features, fix bugs, review pull requests, etc. as fast as I wish because I have to spend too much time handling questions.

        All that, on top of all the incredible help provided by a bunch of community members, that give a lot of their time to come here and help others.

        That's a lot of work, but if more users came to help others like them just a little bit more, it would be much less effort for them (and you and me ).

        By asking questions in a structured way (following this) it will be much easier to help you.

        And there's a high chance that you will find the solution along the way and you won't even have to submit it and wait for an answer. 

        As there are too many questions, I'll have to discard and close the incomplete ones. That will allow me (and others) to focus on helping people like you that follow the whole process and help us help you. 
  - type: checkboxes
    id: checks
    attributes:
      label: First Check
      description: Please confirm and check all the following options.
      options:
        - label: I added a very descriptive title here.
          required: true
        - label: I used the GitHub search to find a similar question and didn't find it.
          required: true
        - label: I searched in the documentation/README.
          required: true
        - label: I already searched in Google "How to do X" and didn't find any information.
          required: true
        - label: I already read and followed all the tutorial in the docs/README and didn't find an answer.
          required: true
  - type: checkboxes
    id: help
    attributes:
      label: Commit to Help
      description: |
        After submitting this, I commit to one of:

          * Read open questions until I find 2 where I can help someone and add a comment to help there.
          * I already hit the "watch" button in this repository to receive notifications and I commit to help at least 2 people that ask questions in the future.

      options:
        - label: I commit to help with one of those options 
          required: true
  - type: textarea
    id: example
    attributes:
      label: Example Code
      description: |
        Please add a self-contained, [minimal, reproducible, example](https://stackoverflow.com/help/minimal-reproducible-example) with your use case.

        If I (or someone) can copy it, run it, and see it right away, there's a much higher chance I (or someone) will be able to help you.

      placeholder: |
        Write your example code here.
      render: Text
    validations:
      required: true
  - type: textarea
    id: description
    attributes:
      label: Description
      description: |
        What is the problem, question, or error?

        Write a short description telling me what you are doing, what you expect to happen, and what is currently happening.
      placeholder: |
        * Open the browser and call the endpoint `/`.
        * It returns a JSON with `{"message": "Hello World"}`.
        * But I expected it to return `{"message": "Hello Morty"}`.
    validations:
      required: true
  - type: dropdown
    id: os
    attributes:
      label: Operating System
      description: What operating system are you on?
      multiple: true
      options:
        - Linux
        - Windows
        - macOS
        - Other
    validations:
      required: true
  - type: textarea
    id: os-details
    attributes:
      label: Operating System Details
      description: You can add more details about your operating system here, in particular if you chose "Other".
    validations:
      required: true
  - type: input
    id: python-version
    attributes:
      label: Python Version
      description: |
        What Python version are you using?

        You can find the Python version with:

        ```bash
        python --version
        ```
    validations:
      required: true
  - type: textarea
    id: context
    attributes:
      label: Additional Context
      description: Add any additional context information or screenshots you think are useful.
</file>

<file path=".github/workflows/require-pr-checklist.yml">
name: Require PR Checklist

on:
  pull_request:
    types: [opened, edited, synchronize]

jobs:
  check:
    name: Check PR Checklist Completion
    runs-on: ubuntu-latest
    steps:
      # Although this action doesn't strictly need the code, 
      # it's good practice to checkout if other steps were added later.
      - name: Checkout code
        uses: actions/checkout@v4 

      - name: Require Checklist Completion
        uses: mheap/require-checklist-action@v2
        with:
          # Fail if no checklist is found or if any item is unchecked.
          requireChecklist: true
</file>

<file path=".github/pull_request_template.md">
<!-- Provide a general summary of your changes in the Title above -->

## Description
<!--- Describe your changes in detail -->


**Required Pre-Merge Check:**
- [ ] Synced Railway environment with Dev/Prod as needed.
</file>

<file path="apps/core/.ai_docs/migration_verification_checklist.md">
# Migration Verification Checklist: Legacy `video_processor/` to Modular Backend

## 1. Core Logic & Adapters

| Legacy Module/Utility                                           | New Location/Status      | Ported? | Notes                                                                                       |
| --------------------------------------------------------------- | ------------------------ | ------- | ------------------------------------------------------------------------------------------- |
| Adapters: AI (Gemini, Vertex, Cache)                            | `lib/ai/`, `lib/cache/`  | [x]     | Vertex AI not present; Gemini and cache ported.                                             |
| Adapters: Storage (GCS, Local)                                  | `lib/storage/`           | [x]     | Unified in file_storage.py.                                                                 |
| Adapters: Publishing (YouTube)                                  | `lib/publishing/`        | [x]     | Ported: All major features migrated to `youtube_adapter.py`.                                |
| Application Services (Video, Metadata, Subtitle, Transcription) | `services/`              | [x]     | All core services ported; subtitle/transcription logic handled in utils or service methods. |
| Domain Models (Video, Job, Metadata)                            | `models/`                | [x]     | All models present and in use. Circular import between models resolved with string refs.    |
| Infrastructure: API, Config, DI                                 | `api/`, `core/config.py` | [x]     | Endpoints, config, and DI via FastAPI present.                                              |
| Utilities: FFmpeg, File, Logging                                | `lib/utils/`             | [x]     | FFmpeg, file, and subtitle utils ported; logging via stdlib.                                |
| Exception Hierarchy                                             | `core/exceptions.py`     | [x]     | All required exceptions now defined and used.                                               |

## 2. Tests

| Legacy Test/Type                 | New Location/Status  | Ported? | Notes                                                                     |
| -------------------------------- | -------------------- | ------- | ------------------------------------------------------------------------- |
| Unit Tests (Adapters, Services)  | `tests/unit/`        | [x]     | YouTube adapter tests ported to `lib/publishing/test_youtube_adapter.py`. |
| Integration Tests (API, Storage) | `tests/integration/` | [x]     | Video processing API integration/E2E tests fully functional.              |
| E2E/Functional Tests             | `tests/`             | [x]     | Integrated tests for the API endpoints pass successfully.                 |

## 3. Documentation & Scripts

| Legacy Doc/Script               | New Location/Status      | Ported? | Notes                                |
| ------------------------------- | ------------------------ | ------- | ------------------------------------ |
| README, Architecture Docs       | `.ai_docs/`, `README.md` | [x]     | Migration documentation up to date.  |
| Utility Scripts (YouTube, etc.) | `bin/` or removed        | [x]     | All utility scripts present in bin/. |

## 4. Removal Readiness

- [x] No imports or dependencies on `video_processor/` from new codebase
- [x] All required features and logic present in new structure
- [x] All essential tests ported or intentionally dropped
- [x] Documentation and scripts archived or ported as needed
- [ ] Team sign-off (if required)

---

**Instructions:**  
- For each row, mark as `[x]` when verified/ported.
- Add notes for any special cases or intentional drops.
- Once all boxes are checked, it is safe to delete `apps/core/video_processor/` and its tests.

**Recent Progress:**
- Fixed circular import dependency between VideoJobModel and VideoMetadataModel by using string-based references in relationships instead of direct imports
- Updated Pydantic schemas to properly handle datetime fields and nullable values
- Fixed integration tests to properly handle authentication checks
- All tests are now passing, indicating successful migration!
</file>

<file path="apps/core/.ai_docs/progress.md">
* [x]   Task 3.5: **Model Imports & Alembic:**
    *   Ensure `apps/core/models/__init__.py` imports all model classes (e.g., `from .video_model import VideoModel`) and `Base`.
    *   Update `apps/core/alembic/env.py` `target_metadata` to `Base.metadata`.
* [x]   Task 3.6: **Generate and Apply Migrations:**
    *   Run `cd apps/core && alembic revision --autogenerate -m "create_video_processing_tables"`.
    *   Inspect the generated migration script in `apps/core/alembic/versions/`.
    *   Run `cd apps/core && alembic upgrade head` to apply to your local Supabase DB.
</file>

<file path="apps/core/alembic/versions/deb5e8c9c1fd_create_video_processing_tables.py">
"""create_video_processing_tables

Revision ID: deb5e8c9c1fd
Revises: 
Create Date: 2025-05-16 20:36:31.640462

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = 'deb5e8c9c1fd'
down_revision: Union[str, None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('chats',
    sa.Column('id', sa.UUID(), nullable=False),
    sa.Column('title', sa.String(length=255), nullable=True),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=True),
    sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=True),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index('ix_chats_created_at', 'chats', [sa.literal_column('created_at DESC')], unique=False)
    op.create_index(op.f('ix_chats_id'), 'chats', ['id'], unique=False)
    op.create_table('users',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('username', sa.String(), nullable=True),
    sa.Column('email', sa.String(), nullable=True),
    sa.Column('full_name', sa.String(), nullable=True),
    sa.Column('hashed_password', sa.String(), nullable=True),
    sa.Column('is_active', sa.Boolean(), nullable=True),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=True),
    sa.Column('updated_at', sa.DateTime(timezone=True), nullable=True),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_users_email'), 'users', ['email'], unique=True)
    op.create_index(op.f('ix_users_id'), 'users', ['id'], unique=False)
    op.create_index(op.f('ix_users_username'), 'users', ['username'], unique=True)
    op.create_table('videos',
    sa.Column('id', sa.Integer(), autoincrement=True, nullable=False),
    sa.Column('uploader_user_id', sa.String(), nullable=False),
    sa.Column('original_filename', sa.String(), nullable=False),
    sa.Column('storage_path', sa.String(), nullable=False),
    sa.Column('content_type', sa.String(), nullable=False),
    sa.Column('size_bytes', sa.Integer(), nullable=False),
    sa.Column('created_at', sa.DateTime(), nullable=False),
    sa.Column('updated_at', sa.DateTime(), nullable=False),
    sa.PrimaryKeyConstraint('id'),
    sa.UniqueConstraint('storage_path')
    )
    op.create_index(op.f('ix_videos_uploader_user_id'), 'videos', ['uploader_user_id'], unique=False)
    op.create_table('messages',
    sa.Column('id', sa.UUID(), nullable=False),
    sa.Column('chat_id', sa.UUID(), nullable=False),
    sa.Column('content', sa.Text(), nullable=False),
    sa.Column('is_from_ai', sa.Integer(), nullable=False),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=True),
    sa.ForeignKeyConstraint(['chat_id'], ['chats.id'], ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index('ix_messages_chat_id', 'messages', ['chat_id'], unique=False)
    op.create_index('ix_messages_created_at', 'messages', ['created_at'], unique=False)
    op.create_index(op.f('ix_messages_id'), 'messages', ['id'], unique=False)
    op.create_table('video_jobs',
    sa.Column('id', sa.Integer(), autoincrement=True, nullable=False),
    sa.Column('video_id', sa.Integer(), nullable=False),
    sa.Column('status', sa.Enum('PENDING', 'PROCESSING', 'COMPLETED', 'FAILED', name='processingstatus'), nullable=False),
    sa.Column('processing_stages', sa.JSON(), nullable=True),
    sa.Column('error_message', sa.Text(), nullable=True),
    sa.Column('created_at', sa.DateTime(), nullable=False),
    sa.Column('updated_at', sa.DateTime(), nullable=False),
    sa.ForeignKeyConstraint(['video_id'], ['videos.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_table('video_metadata',
    sa.Column('id', sa.Integer(), autoincrement=True, nullable=False),
    sa.Column('job_id', sa.Integer(), nullable=False),
    sa.Column('title', sa.String(), nullable=True),
    sa.Column('description', sa.Text(), nullable=True),
    sa.Column('tags', sa.JSON(), nullable=True),
    sa.Column('transcript_text', sa.Text(), nullable=True),
    sa.Column('transcript_file_url', sa.String(), nullable=True),
    sa.Column('subtitle_files_urls', sa.JSON(), nullable=True),
    sa.Column('thumbnail_file_url', sa.String(), nullable=True),
    sa.Column('extracted_video_duration_seconds', sa.Float(), nullable=True),
    sa.Column('extracted_video_resolution', sa.String(), nullable=True),
    sa.Column('extracted_video_format', sa.String(), nullable=True),
    sa.Column('show_notes_text', sa.Text(), nullable=True),
    sa.Column('created_at', sa.DateTime(), nullable=False),
    sa.Column('updated_at', sa.DateTime(), nullable=False),
    sa.ForeignKeyConstraint(['job_id'], ['video_jobs.id'], ),
    sa.PrimaryKeyConstraint('id'),
    sa.UniqueConstraint('job_id')
    )
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_table('video_metadata')
    op.drop_table('video_jobs')
    op.drop_index(op.f('ix_messages_id'), table_name='messages')
    op.drop_index('ix_messages_created_at', table_name='messages')
    op.drop_index('ix_messages_chat_id', table_name='messages')
    op.drop_table('messages')
    op.drop_index(op.f('ix_videos_uploader_user_id'), table_name='videos')
    op.drop_table('videos')
    op.drop_index(op.f('ix_users_username'), table_name='users')
    op.drop_index(op.f('ix_users_id'), table_name='users')
    op.drop_index(op.f('ix_users_email'), table_name='users')
    op.drop_table('users')
    op.drop_index(op.f('ix_chats_id'), table_name='chats')
    op.drop_index('ix_chats_created_at', table_name='chats')
    op.drop_table('chats')
    # ### end Alembic commands ###
</file>

<file path="apps/core/alembic/README">
Generic single-database configuration.
</file>

<file path="apps/core/alembic/script.py.mako">
"""${message}

Revision ID: ${up_revision}
Revises: ${down_revision | comma,n}
Create Date: ${create_date}

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
${imports if imports else ""}

# revision identifiers, used by Alembic.
revision: str = ${repr(up_revision)}
down_revision: Union[str, None] = ${repr(down_revision)}
branch_labels: Union[str, Sequence[str], None] = ${repr(branch_labels)}
depends_on: Union[str, Sequence[str], None] = ${repr(depends_on)}


def upgrade() -> None:
    """Upgrade schema."""
    ${upgrades if upgrades else "pass"}


def downgrade() -> None:
    """Downgrade schema."""
    ${downgrades if downgrades else "pass"}
</file>

<file path="apps/core/api/endpoints/__init__.py">
from apps.core.api.endpoints.video_processing_endpoints import router as router
</file>

<file path="apps/core/api/schemas/user_schemas.py">
from datetime import datetime
from typing import Any, Dict, List, Optional
from uuid import UUID

from pydantic import BaseModel, ConfigDict, EmailStr


# User schemas
class UserBase(BaseModel):
    username: str
    email: EmailStr
    full_name: Optional[str] = None


class UserCreate(UserBase):
    password: str


class UserUpdate(BaseModel):
    username: Optional[str] = None
    email: Optional[EmailStr] = None
    full_name: Optional[str] = None
    password: Optional[str] = None


class UserResponse(UserBase):
    id: int
    is_active: bool

    model_config = ConfigDict(from_attributes=True)


# Auth schemas
class Token(BaseModel):
    access_token: str
    token_type: str


class TokenData(BaseModel):
    username: Optional[str] = None


class LoginRequest(BaseModel):
    username: str
    password: str


class HelloResponse(BaseModel):
    message: str


# Chat schemas
class MessageBase(BaseModel):
    content: str
    is_from_ai: bool = False


class MessageCreate(MessageBase):
    chat_id: int


class MessageResponse(MessageBase):
    id: int
    chat_id: int
    created_at: datetime

    model_config = ConfigDict(from_attributes=True)


class ChatBase(BaseModel):
    title: Optional[str] = None


class ChatCreate(ChatBase):
    pass


class ChatUpdate(ChatBase):
    title: Optional[str] = None


class ChatResponse(ChatBase):
    id: int
    created_at: datetime
    updated_at: datetime

    model_config = ConfigDict(from_attributes=True)


class ChatDetailResponse(ChatResponse):
    messages: List[MessageResponse] = []

    model_config = ConfigDict(from_attributes=True)


# Streaming Chat schemas
class ClientAttachment(BaseModel):
    name: str
    contentType: str
    url: str


class ToolInvocation(BaseModel):
    toolCallId: str
    toolName: str
    args: Dict[str, Any]
    result: Dict[str, Any]


class ClientMessage(BaseModel):
    role: str
    content: str
    experimental_attachments: Optional[List[ClientAttachment]] = None
    toolInvocations: Optional[List[ToolInvocation]] = None


class ChatRequest(BaseModel):
    messages: List[ClientMessage]
    chat_id: UUID


class StreamingMessageCreate(BaseModel):
    content: str
    chat_id: UUID
    protocol: str = "data"
</file>

<file path="apps/core/api/endpoints.py">
from typing import List
from uuid import UUID

from fastapi import APIRouter, Depends, HTTPException, Query, status
from fastapi.responses import StreamingResponse
from services.ai_service import AIService, get_ai_service
from services.auth_service import AuthService, get_auth_service
from services.chat_service import ChatService, get_chat_service
from services.user_service import UserService, get_user_service

from api.schemas import (
    ChatCreate,
    ChatDetailResponse,
    ChatRequest,
    ChatResponse,
    ChatUpdate,
    HelloResponse,
    LoginRequest,
    MessageCreate,
    MessageResponse,
    StreamingMessageCreate,
    Token,
    UserCreate,
    UserResponse,
)

router = APIRouter()


@router.post(
    "/users/", response_model=UserResponse, status_code=status.HTTP_201_CREATED
)
def create_user(
    user_data: UserCreate,
    auth_service: AuthService = Depends(get_auth_service),
):
    """Create a new user"""
    return auth_service.register_user(user_data.model_dump())


@router.get("/users/{user_id}", response_model=UserResponse)
def get_user_by_id(user_id: int, user_service: UserService = Depends(get_user_service)):
    """Get user by ID"""
    return user_service.get_user_profile(user_id)


@router.post("/login/", response_model=Token)
def login(
    login_data: LoginRequest,
    auth_service: AuthService = Depends(get_auth_service),
):
    """Login and get access token"""
    user = auth_service.authenticate_user(login_data.username, login_data.password)
    if not user:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Incorrect username or password",
            headers={"WWW-Authenticate": "Bearer"},
        )

    # In a real app, generate JWT token here
    access_token = f"dummy_token_for_{user['username']}"

    return {"access_token": access_token, "token_type": "bearer"}


@router.get("/hello", response_model=HelloResponse)
def hello() -> HelloResponse:
    return HelloResponse(message="Hello, World!")


# Chat endpoints
@router.get("/chats/", response_model=List[ChatResponse])
def get_chats(
    skip: int = 0,
    limit: int = 20,
    chat_service: ChatService = Depends(get_chat_service),
):
    """Get all chats"""
    return chat_service.get_chats(skip=skip, limit=limit)


@router.post(
    "/chats/", response_model=ChatDetailResponse, status_code=status.HTTP_201_CREATED
)
def create_chat(
    chat_data: ChatCreate, chat_service: ChatService = Depends(get_chat_service)
):
    """Create a new chat"""
    return chat_service.create_chat(chat_data.model_dump())


@router.get("/chats/{chat_id}", response_model=ChatDetailResponse)
def get_chat(chat_id: UUID, chat_service: ChatService = Depends(get_chat_service)):
    """Get chat by ID with messages"""
    return chat_service.get_chat(chat_id)


@router.put("/chats/{chat_id}", response_model=ChatResponse)
def update_chat(
    chat_id: UUID,
    chat_data: ChatUpdate,
    chat_service: ChatService = Depends(get_chat_service),
):
    """Update chat title"""
    return chat_service.update_chat(chat_id, chat_data.model_dump())


@router.delete("/chats/{chat_id}", status_code=status.HTTP_200_OK)
def delete_chat(chat_id: UUID, chat_service: ChatService = Depends(get_chat_service)):
    """Delete a chat"""
    return chat_service.delete_chat(chat_id)


# Message endpoints
@router.post(
    "/messages/", response_model=MessageResponse, status_code=status.HTTP_201_CREATED
)
def create_message(
    message_data: MessageCreate, chat_service: ChatService = Depends(get_chat_service)
):
    """Create a new message"""
    return chat_service.create_message(message_data.model_dump())


@router.get("/chats/{chat_id}/messages/", response_model=List[MessageResponse])
def get_chat_messages(
    chat_id: UUID,
    skip: int = 0,
    limit: int = 50,
    chat_service: ChatService = Depends(get_chat_service),
):
    """Get all messages for a chat"""
    return chat_service.get_chat_messages(chat_id, skip=skip, limit=limit)


# Streaming Chat endpoints
@router.post("/chat")
async def stream_chat(
    request: ChatRequest,
    protocol: str = Query("data"),
    ai_service: AIService = Depends(get_ai_service),
):
    """
    Stream a chat interaction using Vercel AI protocol
    This endpoint is designed to be compatible with the Vercel AI SDK
    """
    # Return an error if there are no messages or the last message is not from the user
    if not request.messages or request.messages[-1].role != "user":
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Last message must be from the user",
        )

    # Extract messages in dict format for processing
    client_messages = [msg.model_dump() for msg in request.messages]

    # For demonstration purposes, use chat ID 1
    # In a real app, you would maintain the chat ID in a session or create a new chat
    chat_id = request.chat_id

    # Create a streaming response using the AI service
    response = StreamingResponse(
        ai_service.process_chat_stream(client_messages, chat_id, protocol)
    )

    # Add the Vercel AI protocol header
    response.headers["x-vercel-ai-data-stream"] = "v1"
    return response


@router.post("/stream-message")
async def stream_message(
    message_data: StreamingMessageCreate,
    ai_service: AIService = Depends(get_ai_service),
):
    """Send a message and get a streaming AI response"""

    # Create a streaming response
    response = StreamingResponse(
        ai_service.stream_ai_response(
            message_content=message_data.content,
            chat_id=message_data.chat_id,
            protocol=message_data.protocol,
        )
    )

    # Add Vercel AI protocol header
    response.headers["x-vercel-ai-data-stream"] = "v1"
    return response
</file>

<file path="apps/core/bin/clean_test_files.sh">
#!/bin/bash

# Get the app directory
APP_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
cd "$APP_DIR"

# Define test directories to clean
TEST_UPLOADS_DIR="$APP_DIR/output_files/uploads/test"
TEST_USER_UPLOADS_DIR="$APP_DIR/output_files/uploads/test-user-id"

# Also check for and clean the incorrect path that might have been created
INCORRECT_PATH="$(cd "$APP_DIR/.." && pwd)/output_files/uploads/test"

echo "Cleaning test files..."

# Remove files in test directory
if [ -d "$TEST_UPLOADS_DIR" ]; then
    echo "Removing files from $TEST_UPLOADS_DIR"
    rm -rf "$TEST_UPLOADS_DIR"/*
    echo " Test directory cleaned"
fi

# Remove mp4 files in test-user-id directory
if [ -d "$TEST_USER_UPLOADS_DIR" ]; then
    echo "Removing mp4 files from $TEST_USER_UPLOADS_DIR"
    rm -rf "$TEST_USER_UPLOADS_DIR"/*.mp4
    echo " Test user files cleaned"
fi

# Clean incorrect path if it exists
if [ -d "$INCORRECT_PATH" ]; then
    echo "Cleaning incorrect path: $INCORRECT_PATH"
    rm -rf "$INCORRECT_PATH"
    echo " Incorrect path removed"
fi

# Recreate the test directory structure
mkdir -p "$TEST_UPLOADS_DIR"

echo "Test files cleanup complete!"
</file>

<file path="apps/core/bin/format.sh">
#!/bin/bash

uv run ruff format
</file>

<file path="apps/core/bin/lint.sh">
#!/bin/bash

uv run ruff check --fix
</file>

<file path="apps/core/bin/setup.sh">
#!/bin/bash

uv python install

uv sync
</file>

<file path="apps/core/bin/test.sh">
#!/bin/bash

# Get the app directory
APP_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
cd "$APP_DIR"

# Set PYTHONPATH to include the app directory
export PYTHONPATH="$APP_DIR"

# Debug output to show PATH and PYTHONPATH
echo "Running tests with PYTHONPATH=$PYTHONPATH"

# Run pytest with uv instead of direct Python call
uv run pytest "$@"
</file>

<file path="apps/core/bin/typecheck.sh">
#!/bin/bash

# Run mypy
echo "Running mypy..."
uv run mypy . --exclude .venv --exclude venv --exclude __pycache__ --exclude .pytest_cache

# Return success/failure status
if [ $? -eq 0 ]; then
  echo " Type checking passed!"
  exit 0
else
  echo " Type checking failed."
  exit 1
fi
</file>

<file path="apps/core/core/exceptions.py">
"""
Custom exception classes for the Echo Core application.

This module defines various exception types used throughout the application
to provide more specific error handling for different failure scenarios.
These exceptions help with debugging and proper error reporting to clients.

Usage:
    from apps.core.core.exceptions import VideoProcessingError, FFmpegError

    try:
        # Code that might fail with FFmpeg
        process_video_with_ffmpeg(...)
    except FFmpegError as e:
        # Handle FFmpeg-specific errors
        logger.error(f"FFmpeg processing failed: {str(e)}")
        raise VideoProcessingError("Video processing failed due to FFmpeg error") from e
"""


class PublishingError(Exception):
    """Exception raised for errors during publishing operations (e.g., YouTube upload failures)."""

    pass


class MetadataGenerationError(Exception):
    """Exception raised for errors during metadata generation operations."""

    pass


class VideoProcessingError(Exception):
    """Exception raised for errors during video processing operations."""

    pass


class FFmpegError(VideoProcessingError):
    """Exception raised for errors during FFmpeg operations."""

    pass


class AINoResponseError(VideoProcessingError):
    """Exception raised when AI services fail to provide a response."""

    pass
</file>

<file path="apps/core/lib/ai/__init__.py">
from lib.ai.ai_client_factory import get_ai_adapter
from lib.ai.base_adapter import AIAdapterInterface
from lib.ai.gemini_adapter import AINoResponseError, GeminiAdapter

__all__ = ["AIAdapterInterface", "AINoResponseError", "GeminiAdapter", "get_ai_adapter"]
</file>

<file path="apps/core/lib/ai/ai_client_factory.py">
"""
Factory for creating AI service adapters.

This module provides a factory function to get the appropriate AI adapter instance
based on the configuration settings.
"""

from typing import Optional

from lib.ai.base_adapter import AIAdapterInterface
from lib.ai.gemini_adapter import GeminiAdapter

from core.config import settings


def get_ai_adapter(settings_instance=None) -> AIAdapterInterface:
    """
    Factory function to get the appropriate AI adapter based on settings.

    Args:
        settings_instance: Optional settings instance. If not provided, uses the global settings.

    Returns:
        An instance of a class implementing AIAdapterInterface

    Raises:
        ImportError: If the required AI service library is not installed
        ValueError: If the configuration is invalid or missing
    """
    settings_obj = settings_instance or settings

    # Determine which AI provider to use based on available API keys
    # If multiple API keys are available, prioritize in this order:
    # 1. Gemini (default)
    # 2. OpenAI
    # 3. Fall back to default (Gemini)

    if settings_obj.GEMINI_API_KEY:
        return GeminiAdapter(settings_obj)

    # Future implementations can add more AI adapters here
    # elif settings_obj.OPENAI_API_KEY:
    #     return OpenAIAdapter(settings_obj)

    # Default to Gemini (will raise an error if no API key is set)
    return GeminiAdapter(settings_obj)
</file>

<file path="apps/core/lib/ai/base_adapter.py">
"""
Base adapter interface for AI service integrations.

This module defines the abstract base class that all AI service adapters must implement.
"""

from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional, Union


class AIAdapterInterface(ABC):
    """
    Abstract base class defining the interface for AI service adapters.

    All AI service implementations (e.g., Gemini, OpenAI) must inherit from this
    class and implement its methods to provide a consistent interface for the application.
    """

    @abstractmethod
    async def generate_text(self, prompt: str, context: Optional[str] = None) -> str:
        """Generate text based on a prompt and optional context."""
        pass

    @abstractmethod
    async def transcribe_audio(self, audio_file_path: str) -> str:
        """Transcribe speech from an audio file to text."""
        pass

    @abstractmethod
    async def analyze_content(self, content: str) -> Dict[str, Any]:
        """Analyze content to extract meaningful information."""
        pass

    @abstractmethod
    async def segment_transcript(
        self, transcript: str
    ) -> List[Dict[str, Union[str, float]]]:
        """Segment a transcript into meaningful chunks with timestamps."""
        pass

    @abstractmethod
    async def summarize_text(self, text: str, max_length: Optional[int] = None) -> str:
        """Generate a concise summary of the provided text."""
        pass

    # --- Additional methods required by MetadataService ---

    def generate_metadata(self, transcript: str) -> Dict[str, Any]:
        """
        Generate all metadata components from a transcript.
        Should return a dict with keys: title, description, tags, show_notes, chapters.
        """
        raise NotImplementedError

    def summarize_content(self, transcript: str, max_length: int = 500) -> str:
        """
        Summarize transcript content to a description.
        """
        raise NotImplementedError

    def generate_thumbnail_description(
        self, transcript: str, timestamp: float = 30.0
    ) -> str:
        """
        Generate a description for a thumbnail at a specific timestamp.
        """
        raise NotImplementedError

    def _generate_title_tags(self, transcript: str) -> Dict[str, str]:
        """
        Generate title and tags as a dictionary from transcript.
        Should return a dict with keys: Description, Keywords.
        """
        raise NotImplementedError
</file>

<file path="apps/core/lib/ai/gemini_adapter.py">
"""
Gemini AI service adapter implementation.

This module implements the AIAdapterInterface for Google's Gemini AI models.
"""

import asyncio
import json
import os
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

# Import Google's Generative AI library with error handling
try:
    import google.generativeai as genai
    from google.generativeai.types import content_types

    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False

from lib.ai.base_adapter import AIAdapterInterface

from core.config import settings


class GeminiAdapter(AIAdapterInterface):
    """
    Adapter for Google's Gemini AI models.
    """

    def __init__(self, settings_instance=None):
        """
        Initialize the Gemini adapter with API key from settings.

        Args:
            settings_instance: Optional settings instance. If not provided, uses the global settings.
        """
        self.settings = settings_instance or settings

        if not GEMINI_AVAILABLE:
            raise ImportError(
                "Google Generative AI library is not installed. "
                "Please install with: pip install google-generativeai"
            )

        if not self.settings.GEMINI_API_KEY:
            raise ValueError(
                "GEMINI_API_KEY must be set in settings to use GeminiAdapter"
            )

        # Configure the Gemini API
        genai.configure(api_key=self.settings.GEMINI_API_KEY)

        # Set up the default model
        self.text_model = genai.GenerativeModel("gemini-pro")
        self.vision_model = genai.GenerativeModel("gemini-pro-vision")

    async def generate_text(self, prompt: str, context: Optional[str] = None) -> str:
        """
        Generate text based on a prompt and optional context using Gemini.

        Args:
            prompt: The text prompt to generate content from
            context: Optional context to provide additional information to the AI

        Returns:
            The generated text as a string

        Raises:
            AINoResponseError: If Gemini fails to generate a response
        """
        try:
            # Combine prompt and context if context is provided
            full_prompt = f"{context}\n\n{prompt}" if context else prompt

            # Run in a thread pool to avoid blocking the event loop
            loop = asyncio.get_event_loop()
            with ThreadPoolExecutor() as pool:
                response = await loop.run_in_executor(
                    pool, lambda: self.text_model.generate_content(full_prompt)
                )

            # Check response validity
            if not response or not hasattr(response, "text"):
                raise AINoResponseError("Gemini failed to generate a response")

            return response.text

        except Exception as e:
            # Re-raise as AINoResponseError for consistent error handling
            raise AINoResponseError(
                f"Error generating text with Gemini: {str(e)}"
            ) from e

    async def transcribe_audio(self, audio_file_path: str) -> str:
        """
        Transcribe speech from an audio file to text.

        Note: As of current implementation, Gemini does not directly support audio transcription.
        This method relies on a custom implementation or may use other Google services.

        Args:
            audio_file_path: Path to the audio file to transcribe

        Returns:
            The transcribed text as a string

        Raises:
            AINoResponseError: If transcription fails
            FileNotFoundError: If the audio file does not exist
        """
        # Check if file exists
        if not os.path.exists(audio_file_path):
            raise FileNotFoundError(f"Audio file not found: {audio_file_path}")

        # Note: This is a placeholder implementation.
        # In a real implementation, this would typically use:
        # 1. Google Speech-to-Text API
        # 2. Google Cloud Speech API
        # 3. A separate transcription service

        try:
            # Placeholder for actual transcription implementation
            # In a real implementation, would call the appropriate API
            raise NotImplementedError(
                "Direct audio transcription is not supported by Gemini. "
                "Consider using Google Speech-to-Text API instead."
            )

        except Exception as e:
            raise AINoResponseError(f"Error transcribing audio: {str(e)}") from e

    async def analyze_content(self, content: str) -> Dict[str, Any]:
        """
        Analyze content to extract meaningful information using Gemini.

        Args:
            content: The text content to analyze

        Returns:
            A dictionary containing analysis results

        Raises:
            AINoResponseError: If Gemini fails to analyze the content
        """
        try:
            # Create a structured prompt for content analysis
            analysis_prompt = """
            Analyze the following content and extract meaningful information.
            Return your analysis as a valid JSON object with the following structure:
            {
                "title": "A concise, attention-grabbing title",
                "description": "A clear description summarizing the main points (150-200 words)",
                "tags": ["relevant", "tags", "for", "categorization"],
                "key_points": ["Main point 1", "Main point 2", "Main point 3"],
                "sentiment": "positive/negative/neutral"
            }
            
            Only provide the JSON object without any additional text or explanation.
            
            Content to analyze:
            
            """

            full_prompt = f"{analysis_prompt}\n{content}"

            # Run in a thread pool to avoid blocking the event loop
            loop = asyncio.get_event_loop()
            with ThreadPoolExecutor() as pool:
                response = await loop.run_in_executor(
                    pool, lambda: self.text_model.generate_content(full_prompt)
                )

            # Parse the JSON response
            result = json.loads(response.text)

            # Validate essential fields
            required_fields = ["title", "description", "tags", "key_points"]
            for field in required_fields:
                if field not in result:
                    result[field] = None

            return result

        except json.JSONDecodeError as e:
            raise AINoResponseError(f"Gemini returned malformed JSON: {str(e)}") from e
        except Exception as e:
            raise AINoResponseError(
                f"Error analyzing content with Gemini: {str(e)}"
            ) from e

    async def segment_transcript(
        self, transcript: str
    ) -> List[Dict[str, Union[str, float]]]:
        """
        Segment a transcript into meaningful chunks with timestamps.

        Args:
            transcript: The transcript text to segment

        Returns:
            A list of segment dictionaries

        Raises:
            AINoResponseError: If Gemini fails to segment the transcript
        """
        try:
            # Create a structured prompt for transcript segmentation
            segmentation_prompt = """
            Segment the following transcript into meaningful chunks with estimated timestamps.
            Return your segmentation as a valid JSON array with the following structure:
            [
                {
                    "text": "Segment text",
                    "start_time": 0.0,
                    "end_time": 10.5
                },
                {
                    "text": "Next segment text",
                    "start_time": 10.5,
                    "end_time": 20.0
                }
            ]
            
            Only provide the JSON array without any additional text or explanation.
            Assume the transcript starts at 0.0 seconds.
            Estimate reasonable timestamps based on the length of text.
            
            Transcript to segment:
            
            """

            full_prompt = f"{segmentation_prompt}\n{transcript}"

            # Run in a thread pool to avoid blocking the event loop
            loop = asyncio.get_event_loop()
            with ThreadPoolExecutor() as pool:
                response = await loop.run_in_executor(
                    pool, lambda: self.text_model.generate_content(full_prompt)
                )

            # Parse the JSON response
            segments = json.loads(response.text)

            # Validate segment structure
            for segment in segments:
                if not all(
                    key in segment for key in ["text", "start_time", "end_time"]
                ):
                    raise ValueError("Segment is missing required fields")

            return segments

        except json.JSONDecodeError as e:
            raise AINoResponseError(f"Gemini returned malformed JSON: {str(e)}") from e
        except Exception as e:
            raise AINoResponseError(
                f"Error segmenting transcript with Gemini: {str(e)}"
            ) from e

    async def summarize_text(self, text: str, max_length: Optional[int] = None) -> str:
        """
        Generate a concise summary of the provided text using Gemini.

        Args:
            text: The text to summarize
            max_length: Optional maximum length of the summary in characters

        Returns:
            The summarized text as a string

        Raises:
            AINoResponseError: If Gemini fails to summarize the text
        """
        try:
            # Create a structured prompt for summarization
            summary_prompt = "Summarize the following text in a concise manner"

            if max_length:
                summary_prompt += f" in {max_length} characters or less"

            full_prompt = f"{summary_prompt}:\n\n{text}"

            # Run in a thread pool to avoid blocking the event loop
            loop = asyncio.get_event_loop()
            with ThreadPoolExecutor() as pool:
                response = await loop.run_in_executor(
                    pool, lambda: self.text_model.generate_content(full_prompt)
                )

            # Check response validity
            if not response or not hasattr(response, "text"):
                raise AINoResponseError("Gemini failed to generate a summary")

            summary = response.text

            # Trim if necessary
            if max_length and len(summary) > max_length:
                summary = summary[:max_length]

            return summary

        except Exception as e:
            raise AINoResponseError(
                f"Error summarizing text with Gemini: {str(e)}"
            ) from e


# Custom exception for AI services
class AINoResponseError(Exception):
    """Exception raised when an AI service fails to generate a response."""

    pass
</file>

<file path="apps/core/lib/auth/supabase_auth.py">
"""
supabase_auth.py: Supabase authentication utilities for Echo backend.

- Provides AuthenticatedUser Pydantic model.
- Provides get_current_user FastAPI dependency for JWT validation.
- Designed for use in the infrastructure/lib layer.

Directory: apps/core/lib/auth/supabase_auth.py
Layer: Infrastructure/Lib
"""

from typing import Optional

from fastapi import Depends, HTTPException, status
from fastapi.security import OAuth2PasswordBearer
from jose import JWTError, jwt
from pydantic import BaseModel

from apps.core.core.config import settings

oauth2_scheme = OAuth2PasswordBearer(tokenUrl="token")


class AuthenticatedUser(BaseModel):
    id: str
    email: Optional[str] = None
    aud: Optional[str] = None


async def get_current_user(token: str = Depends(oauth2_scheme)) -> AuthenticatedUser:
    """
    FastAPI dependency to extract and validate the current user from a Supabase JWT.

    Args:
        token (str): JWT token from the Authorization header.

    Returns:
        AuthenticatedUser: The authenticated user.

    Raises:
        HTTPException: If the token is invalid or missing required claims.
    """
    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Could not validate Supabase credentials",
        headers={"WWW-Authenticate": "Bearer"},
    )
    try:
        payload = jwt.decode(
            token,
            settings.SUPABASE_JWT_SECRET,
            algorithms=[settings.ALGORITHM],
            audience="authenticated",
        )
        user_id = payload.get("sub")
        email: Optional[str] = payload.get("email")
        aud: Optional[str] = payload.get("aud")
        if user_id is None:
            raise credentials_exception
        return AuthenticatedUser(id=str(user_id), email=email, aud=aud)
    except JWTError:
        raise credentials_exception
</file>

<file path="apps/core/lib/cache/__init__.py">
from lib.cache.redis import RedisClient, get_redis_client

__all__ = ["RedisClient", "get_redis_client"]
</file>

<file path="apps/core/lib/cache/redis_cache.py">
"""
RedisCache: Async Redis cache utility for Echo backend.

- Uses redis-py (asyncio) for non-blocking cache operations.
- Connects using settings from the central Settings object.
- Provides async get/set methods with TTL support.
- Designed for dependency injection and stateless usage.

Directory: apps/core/lib/cache/redis_cache.py
Layer: Infrastructure/Lib
"""

from typing import Any, Optional

import redis.asyncio as aioredis

from apps.core.core.config import settings


class RedisCache:
    """
    Async Redis cache utility for storing and retrieving values with TTL.
    """

    def __init__(self):
        self._client = aioredis.Redis(
            host=settings.REDIS_HOST,
            port=settings.REDIS_PORT,
            db=getattr(settings, "REDIS_DB", 0),
            password=getattr(settings, "REDIS_PASSWORD", None),
            decode_responses=True,
        )

    async def get(self, key: str) -> Optional[str]:
        """
        Retrieve a value from Redis by key.

        Args:
            key (str): The cache key.

        Returns:
            Optional[str]: The cached value, or None if not found.
        """
        return await self._client.get(key)

    async def set(self, key: str, value: Any, ttl_seconds: int = 3600) -> None:
        """
        Set a value in Redis with an optional TTL.

        Args:
            key (str): The cache key.
            value (Any): The value to cache (will be stringified).
            ttl_seconds (int): Time-to-live in seconds (default: 1 hour).
        """
        await self._client.set(key, str(value), ex=ttl_seconds)

    async def close(self):
        """
        Close the Redis connection.
        """
        await self._client.close()
</file>

<file path="apps/core/lib/cache/redis.py">
import json
from typing import Any, Optional, cast

import redis

from core.config import settings


class RedisClient:
    def __init__(self):
        self.redis_client = redis.Redis(
            host=settings.REDIS_HOST,
            port=settings.REDIS_PORT,
            db=settings.REDIS_DB,
            password=settings.REDIS_PASSWORD,
            decode_responses=True,
        )

    def get(self, key: str) -> Optional[Any]:
        """Get a value from Redis"""
        value = self.redis_client.get(key)
        if value:
            try:
                return json.loads(cast(str, value))
            except json.JSONDecodeError:
                return value
        return None

    def set(self, key: str, value: Any, expiry: Optional[int] = None) -> bool:
        """Set a value in Redis with optional expiry in seconds"""
        if not isinstance(value, str):
            value = json.dumps(value)

        if expiry:
            return bool(self.redis_client.setex(key, expiry, value))
        return bool(self.redis_client.set(key, value))

    def delete(self, key: str) -> bool:
        """Delete a key from Redis"""
        return bool(self.redis_client.delete(key))

    def exists(self, key: str) -> bool:
        """Check if a key exists in Redis"""
        return bool(self.redis_client.exists(key))

    def hash_get(self, hash_key: str, field: str) -> Optional[Any]:
        """Get a field from a hash in Redis"""
        value = self.redis_client.hget(hash_key, field)
        if value:
            try:
                return json.loads(cast(str, value))
            except json.JSONDecodeError:
                return value
        return None

    def hash_set(self, hash_key: str, field: str, value: Any) -> bool:
        """Set a field in a hash in Redis"""
        if not isinstance(value, str):
            value = json.dumps(value)
        return bool(self.redis_client.hset(hash_key, field, value))

    def flush(self) -> bool:
        """Clear the entire Redis database"""
        return bool(self.redis_client.flushdb())


# Singleton instance
_redis_client = None


def get_redis_client() -> RedisClient:
    """Dependency to get Redis client instance"""
    global _redis_client
    if _redis_client is None:
        _redis_client = RedisClient()
    return _redis_client
</file>

<file path="apps/core/lib/database/__init__.py">
from lib.database.connection import (
    Base,
    SessionLocal,
    create_session,
    engine,
    get_db_session,
)

__all__ = [
    "engine",
    "Base",
    "SessionLocal",
    "get_db_session",
    "create_session",
]
</file>

<file path="apps/core/lib/messaging/__init__.py">
from lib.messaging.email import EmailService, get_email_service

__all__ = ["EmailService", "get_email_service"]
</file>

<file path="apps/core/lib/messaging/email.py">
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from pathlib import Path
from typing import Any, Dict, List, Optional

import jinja2

from core.config import settings


class EmailService:
    def __init__(self):
        self.server = settings.SMTP_SERVER
        self.port = settings.SMTP_PORT
        self.username = settings.SMTP_USERNAME
        self.password = settings.SMTP_PASSWORD
        self.from_email = settings.EMAIL_FROM_ADDRESS

        # Template environment
        template_dir = Path(settings.EMAIL_TEMPLATES_DIR)
        self.template_env = jinja2.Environment(
            loader=jinja2.FileSystemLoader(template_dir),
            autoescape=jinja2.select_autoescape(["html", "xml"]),
        )

    def _get_connection(self):
        """Get SMTP connection"""
        connection = smtplib.SMTP(self.server, self.port)
        connection.starttls()
        if self.username and self.password:
            connection.login(self.username, self.password)
        return connection

    def send_email(
        self,
        to_email: List[str],
        subject: str,
        body: str,
        cc: Optional[List[str]] = None,
        bcc: Optional[List[str]] = None,
        is_html: bool = False,
    ) -> bool:
        """Send a simple email"""
        if not to_email:
            return False

        msg = MIMEMultipart()
        msg["From"] = self.from_email
        msg["To"] = ", ".join(to_email)
        msg["Subject"] = subject

        if cc:
            msg["Cc"] = ", ".join(cc)
        if bcc:
            msg["Bcc"] = ", ".join(bcc)

        msg.attach(MIMEText(body, "html" if is_html else "plain"))

        try:
            with self._get_connection() as server:
                all_recipients = to_email + (cc or []) + (bcc or [])
                server.send_message(msg, self.from_email, all_recipients)
            return True
        except Exception as e:
            print(f"Failed to send email: {e}")
            return False

    def send_template_email(
        self,
        to_email: List[str],
        subject: str,
        template_name: str,
        template_vars: Dict[str, Any],
        cc: Optional[List[str]] = None,
        bcc: Optional[List[str]] = None,
    ) -> bool:
        """Send an email using a template"""
        try:
            template = self.template_env.get_template(template_name)
            html_content = template.render(**template_vars)
            return self.send_email(
                to_email, subject, html_content, cc, bcc, is_html=True
            )
        except Exception as e:
            print(f"Failed to render or send template email: {e}")
            return False


# Singleton instance
_email_service = None


def get_email_service() -> EmailService:
    """Dependency to get Email service instance"""
    global _email_service
    if _email_service is None:
        _email_service = EmailService()
    return _email_service
</file>

<file path="apps/core/lib/publishing/publishing_interface.py">
"""
Publishing interface for modular video processing backend.

Defines the contract for publishing operations independent of any specific
platform implementation (YouTube, Vimeo, etc.).
"""

from abc import ABC, abstractmethod
from typing import Dict, Optional


class PublishingInterface(ABC):
    """
    Interface for video publishing operations.

    This interface defines the contract for all publishing adapter implementations,
    ensuring they provide the necessary methods for publishing videos to platforms.
    """

    @abstractmethod
    def upload_video(self, video_file: str, metadata: Dict) -> str:
        """
        Upload a video to the publishing platform.

        Args:
            video_file: Path to the video file
            metadata: Dictionary containing video metadata:
                      {
                          "title": str,
                          "description": str,
                          "tags": List[str],
                          "category_id": str,
                          ...
                      }

        Returns:
            The video ID on the platform

        Raises:
            PublishingError: If the upload fails
        """
        pass

    @abstractmethod
    def update_metadata(self, video_id: str, metadata: Dict) -> bool:
        """
        Update metadata for a video on the publishing platform.

        Args:
            video_id: ID of the video on the platform
            metadata: Dictionary containing video metadata to update

        Returns:
            True if the update succeeded, False otherwise

        Raises:
            PublishingError: If the update fails
        """
        pass

    @abstractmethod
    def get_upload_status(self, video_id: str) -> str:
        """
        Get the status of a video upload.

        Args:
            video_id: ID of the video on the platform

        Returns:
            Status of the upload (e.g., "uploading", "processing", "ready", "failed")

        Raises:
            PublishingError: If the status check fails
        """
        pass

    @abstractmethod
    def delete_video(self, video_id: str) -> bool:
        """
        Delete a video from the publishing platform.

        Args:
            video_id: ID of the video on the platform

        Returns:
            True if the deletion succeeded, False otherwise

        Raises:
            PublishingError: If the deletion fails
        """
        pass

    @abstractmethod
    def get_video_url(self, video_id: str) -> str:
        """
        Get the public URL for a video on the publishing platform.

        Args:
            video_id: ID of the video on the platform

        Returns:
            Public URL for the video

        Raises:
            PublishingError: If the URL retrieval fails
        """
        pass

    @abstractmethod
    def upload_caption(
        self, video_id: str, caption_file: str, language: str = "en"
    ) -> bool:
        """
        Upload a caption file for a video on the publishing platform.

        Args:
            video_id: ID of the video on the platform
            caption_file: Path to the caption file (VTT, SRT, etc.)
            language: Language code for the captions

        Returns:
            True if the upload succeeded, False otherwise

        Raises:
            PublishingError: If the caption upload fails
        """
        pass

    @abstractmethod
    def set_publishing_time(
        self, video_id: str, publish_at: Optional[str] = None
    ) -> bool:
        """
        Set the publishing time for a video on the platform.

        Args:
            video_id: ID of the video on the platform
            publish_at: ISO 8601 format datetime string when the video should be published,
                        or None to publish immediately

        Returns:
            True if the publishing time was set successfully, False otherwise

        Raises:
            PublishingError: If setting the publishing time fails
        """
        pass
</file>

<file path="apps/core/lib/publishing/youtube_adapter.py">
"""
YouTubeAdapter: Modular publishing adapter for YouTube integration.

Implements the publishing interface for uploading, updating, and managing videos on YouTube.
Ported from legacy video_processor/adapters/publishing/youtube.py.
"""

import http.client
import json
import logging
import os
import time
from typing import Any, Dict, List, Optional

# Google API client imports
import google.oauth2.credentials
import httplib2
from core.exceptions import PublishingError
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from googleapiclient.http import MediaFileUpload

from apps.core.lib.publishing.publishing_interface import PublishingInterface

logger = logging.getLogger(__name__)

YOUTUBE_API_SERVICE_NAME = "youtube"
YOUTUBE_API_VERSION = "v3"
RETRIABLE_EXCEPTIONS = (
    httplib2.HttpLib2Error,
    IOError,
    http.client.NotConnected,
    http.client.IncompleteRead,
    http.client.ImproperConnectionState,
    http.client.CannotSendRequest,
    http.client.CannotSendHeader,
    http.client.ResponseNotReady,
    http.client.BadStatusLine,
)
RETRIABLE_STATUS_CODES = [500, 502, 503, 504]
MAX_RETRIES = 10


class YouTubeAdapter(PublishingInterface):
    """
    YouTube adapter for publishing videos and managing metadata.
    """

    def __init__(
        self,
        client_secrets_file: str,
        oauth_token_file: str,
        scopes: Optional[List[str]] = None,
        api_service_name: str = "youtube",
        api_version: str = "v3",
    ):
        """
        Initialize the YouTube adapter.

        Args:
            client_secrets_file: Path to the client secrets file
            oauth_token_file: Path to the OAuth token file
            scopes: List of OAuth scopes (default: upload and read)
            api_service_name: YouTube API service name
            api_version: YouTube API version
        """
        self._client_secrets_file = client_secrets_file
        self._oauth_token_file = oauth_token_file
        self._scopes = scopes or [
            "https://www.googleapis.com/auth/youtube.upload",
            "https://www.googleapis.com/auth/youtube.force-ssl",
        ]
        self._api_service_name = api_service_name
        self._api_version = api_version
        self._youtube = None  # Will be set by tests or real API client in production
        self._initialize_youtube_client()

    def upload_video(self, video_file: str, metadata: Dict[str, Any]) -> str:
        """
        Upload a video to YouTube with metadata.
        """
        if not self._youtube:
            self._initialize_youtube_client()

        if not video_file or not os.path.isfile(video_file):
            raise PublishingError(f"Video file does not exist: {video_file}")

        try:
            body = {
                "snippet": {
                    "title": metadata.get("title", "Untitled Video"),
                    "description": metadata.get("description", ""),
                    "tags": metadata.get("tags", []),
                    "categoryId": metadata.get("category_id", "22"),
                },
                "status": {
                    "privacyStatus": metadata.get("privacy_status", "private"),
                    "selfDeclaredMadeForKids": metadata.get("made_for_kids", False),
                },
            }

            media = MediaFileUpload(
                video_file,
                mimetype="video/*",
                resumable=True,
                chunksize=1024 * 1024 * 5,
            )

            insert_request = self._youtube.videos().insert(
                part=",".join(body.keys()), body=body, media_body=media
            )

            video_id = self._execute_upload_with_retries(insert_request)
            logger.info(f"Video uploaded successfully with ID: {video_id}")
            return video_id

        except HttpError as e:
            error_content = json.loads(e.content.decode("utf-8"))
            error_message = error_content.get("error", {}).get("message", str(e))
            logger.error(f"HTTP error during upload: {error_message}")
            raise PublishingError(f"YouTube upload failed: {error_message}")

        except Exception as e:
            logger.error(f"Failed to upload video: {str(e)}")
            raise PublishingError(f"YouTube upload failed: {str(e)}")

    def _initialize_youtube_client(self) -> None:
        """
        Initialize the YouTube API client.
        """
        try:
            if not os.path.exists(self._oauth_token_file):
                raise PublishingError(
                    f"OAuth token file not found: {self._oauth_token_file}. "
                    "Please run the authentication flow first."
                )

            with open(self._oauth_token_file, "r") as token_file:
                token_data = json.load(token_file)

            credentials = google.oauth2.credentials.Credentials(
                token=token_data.get("token"),
                refresh_token=token_data.get("refresh_token"),
                token_uri=token_data.get(
                    "token_uri", "https://oauth2.googleapis.com/token"
                ),
                client_id=token_data.get("client_id"),
                client_secret=token_data.get("client_secret"),
                scopes=self._scopes,
            )

            self._youtube = build(
                self._api_service_name,
                self._api_version,
                credentials=credentials,
                cache_discovery=False,
            )

            logger.info("YouTube API client initialized successfully")

        except Exception as e:
            logger.error(f"Failed to initialize YouTube API client: {str(e)}")
            raise PublishingError(f"Failed to initialize YouTube API client: {str(e)}")

    def _execute_upload_with_retries(self, request):
        """
        Execute an upload request with retries for transient errors.
        """
        response = None
        error = None
        retry = 0
        video_id = None

        while response is None:
            try:
                logger.info(f"Uploading video (attempt {retry + 1}/{MAX_RETRIES})")
                status, response = request.next_chunk()

                if response is not None:
                    if "id" in response:
                        video_id = response["id"]
                        return video_id
                    else:
                        raise PublishingError(
                            f"YouTube upload failed, no video ID in response: {response}"
                        )

                if status:
                    progress = int(status.progress() * 100)
                    logger.info(f"Upload progress: {progress}%")

            except HttpError as e:
                if e.resp.status in RETRIABLE_STATUS_CODES:
                    error = (
                        f"A retriable HTTP error {e.resp.status} occurred: {e.content}"
                    )
                else:
                    raise

            except RETRIABLE_EXCEPTIONS as e:
                error = f"A retriable error occurred: {e}"

            if error is not None:
                logger.warning(error)
                retry += 1

                if retry > MAX_RETRIES:
                    logger.error("Maximum retries exceeded")
                    raise PublishingError(
                        f"YouTube upload failed after {MAX_RETRIES} retries: {error}"
                    )

                max_sleep = 2**retry
                sleep_seconds = min(max_sleep, 60)
                logger.info(f"Sleeping {sleep_seconds} seconds before retrying...")
                time.sleep(sleep_seconds)

    def update_metadata(self, video_id: str, metadata: Dict[str, Any]) -> bool:
        """
        Update metadata for an existing YouTube video.
        """
        if not self._youtube:
            self._initialize_youtube_client()

        try:
            video_response = (
                self._youtube.videos()
                .list(part="snippet,status", id=video_id)
                .execute()
            )

            if not video_response.get("items"):
                raise PublishingError(f"Video with ID {video_id} not found")

            video_item = video_response["items"][0]
            snippet = video_item["snippet"]
            status = video_item["status"]

            # Update snippet fields if provided
            if "title" in metadata:
                snippet["title"] = metadata["title"]

            if "description" in metadata:
                snippet["description"] = metadata["description"]

            if "tags" in metadata:
                snippet["tags"] = metadata["tags"]

            if "category_id" in metadata:
                snippet["categoryId"] = metadata["category_id"]

            # Update status fields if provided
            if "privacy_status" in metadata:
                status["privacyStatus"] = metadata["privacy_status"]

            if "made_for_kids" in metadata:
                status["selfDeclaredMadeForKids"] = metadata["made_for_kids"]

            update_request = self._youtube.videos().update(
                part="snippet,status",
                body={"id": video_id, "snippet": snippet, "status": status},
            )

            update_response = update_request.execute()

            logger.info(f"Video metadata updated successfully for ID: {video_id}")
            return True

        except HttpError as e:
            error_content = json.loads(e.content.decode("utf-8"))
            error_message = error_content.get("error", {}).get("message", str(e))
            logger.error(f"HTTP error during metadata update: {error_message}")
            raise PublishingError(f"YouTube metadata update failed: {error_message}")

        except Exception as e:
            logger.error(f"Failed to update video metadata: {str(e)}")
            raise PublishingError(f"YouTube metadata update failed: {str(e)}")

    def get_upload_status(self, video_id: str) -> str:
        """
        Get the upload/processing status of a YouTube video.
        """
        if not self._youtube:
            self._initialize_youtube_client()

        try:
            processing_response = (
                self._youtube.videos()
                .list(part="processingDetails,status", id=video_id)
                .execute()
            )

            if not processing_response.get("items"):
                raise PublishingError(f"Video with ID {video_id} not found")

            video_item = processing_response["items"][0]
            processing_details = video_item.get("processingDetails", {})
            status = video_item.get("status", {})

            # Check if video is being processed
            if processing_details.get("processingStatus") == "processing":
                return "processing"

            # Check if video processing failed
            if processing_details.get("processingStatus") == "failed":
                return "failed"

            # Check upload status
            upload_status = status.get("uploadStatus")
            if upload_status == "uploaded" or upload_status == "processed":
                return "ready"
            elif upload_status == "failed":
                return "failed"

            # If we can't determine status, return the raw processing status
            return processing_details.get("processingStatus", "unknown")

        except HttpError as e:
            error_content = json.loads(e.content.decode("utf-8"))
            error_message = error_content.get("error", {}).get("message", str(e))
            logger.error(f"HTTP error during status check: {error_message}")
            raise PublishingError(f"YouTube status check failed: {error_message}")

        except Exception as e:
            logger.error(f"Failed to get video status: {str(e)}")
            raise PublishingError(f"YouTube status check failed: {str(e)}")

    def delete_video(self, video_id: str) -> bool:
        """
        Delete a YouTube video.
        """
        if not self._youtube:
            self._initialize_youtube_client()

        try:
            self._youtube.videos().delete(id=video_id).execute()
            logger.info(f"Video with ID {video_id} deleted successfully")
            return True

        except HttpError as e:
            if e.resp.status == 404:
                logger.warning(
                    f"Video with ID {video_id} not found, considering deletion successful"
                )
                return True

            error_content = json.loads(e.content.decode("utf-8"))
            error_message = error_content.get("error", {}).get("message", str(e))
            logger.error(f"HTTP error during video deletion: {error_message}")
            raise PublishingError(f"YouTube video deletion failed: {error_message}")

        except Exception as e:
            logger.error(f"Failed to delete video: {str(e)}")
            raise PublishingError(f"YouTube video deletion failed: {str(e)}")

    def get_video_url(self, video_id: str) -> str:
        """
        Get the public URL for a video on the publishing platform.
        """
        if not video_id:
            raise PublishingError("Invalid video_id for YouTube URL.")
        return f"https://www.youtube.com/watch?v={video_id}"

    def upload_caption(
        self, video_id: str, caption_file: str, language: str = "en"
    ) -> bool:
        """
        Upload a caption file for a video on the publishing platform.
        """
        if not self._youtube:
            self._initialize_youtube_client()

        if not os.path.exists(caption_file):
            raise PublishingError(f"Caption file not found: {caption_file}")

        try:
            name, extension = os.path.splitext(caption_file)
            extension = extension.lower().strip(".")

            format_mapping = {
                "srt": "srt",
                "vtt": "webvtt",
                "sbv": "sbv",
                "sub": "sub",
                "ttml": "ttml",
            }

            if extension not in format_mapping:
                raise PublishingError(
                    f"Unsupported caption format: {extension}. "
                    f"Supported formats: {', '.join(format_mapping.keys())}"
                )

            caption_format = format_mapping[extension]

            caption_insert = self._youtube.captions().insert(
                part="snippet",
                body={
                    "snippet": {
                        "videoId": video_id,
                        "language": language,
                        "name": f"{language.upper()} - {os.path.basename(name)}",
                    }
                },
                media_body=MediaFileUpload(
                    caption_file, mimetype=f"text/{extension}", resumable=True
                ),
            )

            caption_response = caption_insert.execute()
            logger.info(
                f"Caption uploaded successfully for video ID {video_id} in language {language}"
            )
            return True

        except HttpError as e:
            error_content = json.loads(e.content.decode("utf-8"))
            error_message = error_content.get("error", {}).get("message", str(e))
            logger.error(f"HTTP error during caption upload: {error_message}")
            raise PublishingError(f"YouTube caption upload failed: {error_message}")

        except Exception as e:
            logger.error(f"Failed to upload caption: {str(e)}")
            raise PublishingError(f"YouTube caption upload failed: {str(e)}")

    def set_publishing_time(
        self, video_id: str, publish_at: Optional[str] = None
    ) -> bool:
        """
        Set the publishing time for a video on the platform.
        """
        if not self._youtube:
            self._initialize_youtube_client()

        try:
            video_response = (
                self._youtube.videos().list(part="status", id=video_id).execute()
            )

            if not video_response.get("items"):
                raise PublishingError(f"Video with ID {video_id} not found")

            video_item = video_response["items"][0]
            status = video_item["status"]

            if publish_at is None:
                if "publishAt" in status:
                    del status["publishAt"]
                status["privacyStatus"] = "public"
            else:
                status["privacyStatus"] = "private"
                status["publishAt"] = publish_at

            update_request = self._youtube.videos().update(
                part="status", body={"id": video_id, "status": status}
            )

            update_response = update_request.execute()

            logger.info(
                f"Publishing time for video ID {video_id} set to: {publish_at or 'immediate'}"
            )
            return True

        except HttpError as e:
            error_content = json.loads(e.content.decode("utf-8"))
            error_message = error_content.get("error", {}).get("message", str(e))
            logger.error(f"HTTP error during publishing time update: {error_message}")
            raise PublishingError(
                f"YouTube publishing time update failed: {error_message}"
            )

        except Exception as e:
            logger.error(f"Failed to set publishing time: {str(e)}")
            raise PublishingError(f"YouTube publishing time update failed: {str(e)}")


# TODO: Add dependency injection registration and configuration as needed for modular backend.
</file>

<file path="apps/core/lib/storage/__init__.py">
from lib.storage.file_storage import FileStorageService, get_file_storage_service


# Temporary stub for legacy imports
class FileStorage:
    pass


# Temporary alias for legacy import
get_file_storage = get_file_storage_service

__all__ = [
    "FileStorageService",
    "get_file_storage_service",
    "FileStorage",
    "get_file_storage",
]
</file>

<file path="apps/core/lib/storage/file_storage.py">
import asyncio
import os
import shutil
import uuid
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
from typing import Any, BinaryIO, List, Optional, Tuple, Union

from fastapi import UploadFile

# Import Google Cloud Storage dependencies with error handling
# to allow the module to load even if these aren't installed
try:
    from google.cloud import storage
    from google.oauth2 import service_account

    GCS_AVAILABLE = True
except ImportError:
    GCS_AVAILABLE = False

    # Dummy classes for type hints
    class storage:
        class Client:
            pass

    class service_account:
        class Credentials:
            @staticmethod
            def from_service_account_file(path: str) -> Any:
                pass


from core.config import settings


class FileStorageService:
    """
    A service for handling file storage operations with support for both local filesystem
    and Google Cloud Storage (GCS).
    """

    def upload_from_string(
        self,
        content: str,
        storage_path: str,
        content_type: str = "application/octet-stream",
    ) -> None:
        """
        Upload a string as a file to the configured storage backend.

        Args:
            content: The string content to upload
            storage_path: The path (relative or absolute) where the file should be stored
            content_type: The MIME type of the content (default: application/octet-stream)
        """
        if self.settings.STORAGE_BACKEND == "local":
            # Save to local filesystem
            full_path = self.local_storage_path / storage_path
            os.makedirs(os.path.dirname(full_path), exist_ok=True)
            with open(full_path, "w", encoding="utf-8") as f:
                f.write(content)
        elif self.settings.STORAGE_BACKEND == "gcs":
            if self.gcs_client is None:
                raise RuntimeError("GCS client not initialized")
            bucket = self.gcs_client.bucket(self.settings.GCS_BUCKET_NAME)
            blob = bucket.blob(storage_path)
            blob.upload_from_string(content, content_type=content_type)
        else:
            raise ValueError(
                f"Unsupported storage backend: {self.settings.STORAGE_BACKEND}"
            )

    def __init__(self, settings_instance=None):
        """
        Initialize the storage service based on configuration.

        Args:
            settings_instance: An optional Settings instance. If not provided, uses the global settings.
        """
        self.settings = settings_instance or settings

        # Set up local storage configuration
        self.local_storage_path = (
            Path(self.settings.BASE_DIR) / self.settings.LOCAL_STORAGE_PATH
        )
        os.makedirs(self.local_storage_path, exist_ok=True)

        # Set up GCS client if needed
        self.gcs_client = None
        if self.settings.STORAGE_BACKEND == "gcs":
            if not GCS_AVAILABLE:
                raise ImportError(
                    "Google Cloud Storage dependencies are not installed. "
                    "Please install with: pip install google-cloud-storage"
                )

            if self.settings.GOOGLE_APPLICATION_CREDENTIALS_PATH:
                credentials = service_account.Credentials.from_service_account_file(
                    self.settings.GOOGLE_APPLICATION_CREDENTIALS_PATH
                )
                self.gcs_client = storage.Client(
                    credentials=credentials, project=credentials.project_id
                )
            else:
                # Use default credentials from environment
                self.gcs_client = storage.Client()

            # Verify bucket exists
            if not self.settings.GCS_BUCKET_NAME:
                raise ValueError(
                    "GCS_BUCKET_NAME must be set when using GCS storage backend"
                )

    async def save_file(
        self, file_content: bytes, filename: str, subdir: Optional[str] = "uploads"
    ) -> str:
        """
        Save a file to the configured storage backend.

        Args:
            file_content: The binary content of the file
            filename: The name of the file
            subdir: Optional subdirectory for organizing storage

        Returns:
            A string representing the storage path of the saved file
        """
        # Generate a unique filename to prevent collisions
        file_extension = os.path.splitext(filename)[1]
        unique_filename = f"{uuid.uuid4()}{file_extension}"

        # Define the full path including subdirectory
        relative_path = f"{subdir}/{unique_filename}" if subdir else unique_filename

        if self.settings.STORAGE_BACKEND == "local":
            # Ensure subdirectory exists
            if subdir:
                target_dir = self.local_storage_path / subdir
                os.makedirs(target_dir, exist_ok=True)

            # Save to local filesystem
            file_path = self.local_storage_path / relative_path

            # Use async to avoid blocking on file I/O
            loop = asyncio.get_event_loop()
            with ThreadPoolExecutor() as pool:
                await loop.run_in_executor(
                    pool, lambda: self._save_local_file(file_path, file_content)
                )

            return relative_path

        elif self.settings.STORAGE_BACKEND == "gcs":
            # Upload to GCS using a thread pool to avoid blocking
            loop = asyncio.get_event_loop()
            with ThreadPoolExecutor() as pool:
                await loop.run_in_executor(
                    pool, lambda: self._upload_to_gcs(relative_path, file_content)
                )

            return f"gs://{self.settings.GCS_BUCKET_NAME}/{relative_path}"

        else:
            raise ValueError(
                f"Unsupported storage backend: {self.settings.STORAGE_BACKEND}"
            )

    async def download_file(
        self, storage_path: str, destination_local_path: str
    ) -> str:
        """
        Download a file from storage to a local path.

        Args:
            storage_path: The path where the file is stored
            destination_local_path: The local path where the file should be saved

        Returns:
            The local path where the file is downloaded
        """
        # Ensure destination directory exists
        os.makedirs(os.path.dirname(destination_local_path), exist_ok=True)

        if self.settings.STORAGE_BACKEND == "local":
            # For local storage, just copy the file
            source_path = self._get_local_path(storage_path)

            loop = asyncio.get_event_loop()
            with ThreadPoolExecutor() as pool:
                await loop.run_in_executor(
                    pool, lambda: shutil.copy2(source_path, destination_local_path)
                )

        elif self.settings.STORAGE_BACKEND == "gcs":
            # Handle GCS URI format (gs://bucket-name/path/to/file)
            bucket_name: str
            blob_name: str

            if storage_path.startswith("gs://"):
                parts = self._parse_gcs_uri(storage_path)
                bucket_name, blob_name = parts
            else:
                # Assume it's just the blob name
                bucket_name = self.settings.GCS_BUCKET_NAME or ""
                blob_name = storage_path

            loop = asyncio.get_event_loop()
            with ThreadPoolExecutor() as pool:
                await loop.run_in_executor(
                    pool,
                    lambda: self._download_from_gcs(
                        bucket_name, blob_name, destination_local_path
                    ),
                )

        else:
            raise ValueError(
                f"Unsupported storage backend: {self.settings.STORAGE_BACKEND}"
            )

        return destination_local_path

    async def get_public_url(self, storage_path: str) -> Optional[str]:
        """
        Get a public URL for accessing the file.

        Args:
            storage_path: The path where the file is stored

        Returns:
            A public URL string or None if not available
        """
        if self.settings.STORAGE_BACKEND == "local":
            # For local files, we typically don't have a public URL
            # Return a placeholder or None
            return None

        elif self.settings.STORAGE_BACKEND == "gcs":
            # Handle GCS URI format
            bucket_name: str
            blob_name: str

            if storage_path.startswith("gs://"):
                parts = self._parse_gcs_uri(storage_path)
                bucket_name, blob_name = parts
            else:
                # Assume it's just the blob name
                bucket_name = self.settings.GCS_BUCKET_NAME or ""
                blob_name = storage_path

            # Generate a public URL for GCS objects
            return f"https://storage.googleapis.com/{bucket_name}/{blob_name}"

        else:
            raise ValueError(
                f"Unsupported storage backend: {self.settings.STORAGE_BACKEND}"
            )

    def _save_local_file(self, file_path: Path, content: bytes) -> None:
        """Helper method to save content to a local file"""
        with open(file_path, "wb") as f:
            f.write(content)

    def _upload_to_gcs(self, blob_name: str, content: bytes) -> None:
        """Helper method to upload content to GCS"""
        if self.gcs_client is None:
            raise RuntimeError("GCS client not initialized")

        bucket = self.gcs_client.bucket(self.settings.GCS_BUCKET_NAME)
        blob = bucket.blob(blob_name)
        blob.upload_from_string(content)

    def _download_from_gcs(
        self, bucket_name: str, blob_name: str, destination_path: str
    ) -> None:
        """Helper method to download a file from GCS"""
        if self.gcs_client is None:
            raise RuntimeError("GCS client not initialized")

        bucket = self.gcs_client.bucket(bucket_name)
        blob = bucket.blob(blob_name)
        blob.download_to_filename(destination_path)

    def _get_local_path(self, relative_path: str) -> Path:
        """Convert a relative storage path to a full local path"""
        # Handle absolute paths that might have been stored
        if os.path.isabs(relative_path):
            return Path(relative_path)
        return self.local_storage_path / relative_path

    def _parse_gcs_uri(self, gcs_uri: str) -> Tuple[str, str]:
        """Parse a GCS URI (gs://bucket-name/path/to/file) into bucket and blob names"""
        if not gcs_uri.startswith("gs://"):
            raise ValueError(f"Invalid GCS URI: {gcs_uri}")

        # Remove 'gs://' prefix
        path = gcs_uri[5:]

        # Split into bucket and blob name
        parts = path.split("/", 1)
        if len(parts) != 2:
            raise ValueError(f"Invalid GCS URI format: {gcs_uri}")

        bucket_name, blob_name = parts
        return bucket_name, blob_name


# Singleton factory
_file_storage_service = None


def get_file_storage_service() -> FileStorageService:
    """Factory function to get the FileStorageService singleton instance"""
    global _file_storage_service
    if _file_storage_service is None:
        _file_storage_service = FileStorageService()
    return _file_storage_service
</file>

<file path="apps/core/lib/utils/ffmpeg_utils.py">
"""
FfmpegUtils: Synchronous FFmpeg utility functions for video processing.

- Provides methods for audio extraction, frame extraction, and metadata retrieval.
- Uses subprocess.run to call the ffmpeg CLI.
- Designed for use in the infrastructure/lib layer.

Directory: apps/core/lib/utils/ffmpeg_utils.py
Layer: Infrastructure/Lib
"""

import json
import subprocess
from typing import Any, Dict


class FfmpegUtils:
    """
    Utility class for common FFmpeg operations.
    All methods are synchronous and use subprocess.run.
    """

    @staticmethod
    def extract_audio_sync(video_path: str, output_audio_path: str) -> None:
        """
        Extracts audio from a video file and saves it to output_audio_path.

        Args:
            video_path (str): Path to the input video file.
            output_audio_path (str): Path to save the extracted audio file.

        Raises:
            RuntimeError: If ffmpeg fails.
        """
        cmd = [
            "ffmpeg",
            "-y",  # Overwrite output files without asking
            "-i",
            video_path,
            "-vn",  # No video
            "-acodec",
            "pcm_s16le",
            "-ar",
            "44100",
            "-ac",
            "2",
            output_audio_path,
        ]
        result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        if result.returncode != 0:
            raise RuntimeError(
                f"FFmpeg audio extraction failed: {result.stderr.decode('utf-8')}"
            )

    @staticmethod
    def extract_frame_sync(
        video_path: str, timestamp_seconds: float, output_image_path: str
    ) -> None:
        """
        Extracts a single frame from a video at the specified timestamp.

        Args:
            video_path (str): Path to the input video file.
            timestamp_seconds (float): Timestamp in seconds to extract the frame.
            output_image_path (str): Path to save the extracted image.

        Raises:
            RuntimeError: If ffmpeg fails.
        """
        cmd = [
            "ffmpeg",
            "-y",
            "-ss",
            str(timestamp_seconds),
            "-i",
            video_path,
            "-frames:v",
            "1",
            output_image_path,
        ]
        result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        if result.returncode != 0:
            raise RuntimeError(
                f"FFmpeg frame extraction failed: {result.stderr.decode('utf-8')}"
            )

    @staticmethod
    def get_video_metadata_sync(video_path: str) -> Dict[str, Any]:
        """
        Retrieves metadata from a video file using ffprobe.

        Args:
            video_path (str): Path to the input video file.

        Returns:
            Dict[str, Any]: Parsed metadata dictionary.

        Raises:
            RuntimeError: If ffprobe fails.
        """
        cmd = [
            "ffprobe",
            "-v",
            "error",
            "-show_entries",
            "format:stream",
            "-print_format",
            "json",
            video_path,
        ]
        result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        if result.returncode != 0:
            raise RuntimeError(
                f"ffprobe metadata extraction failed: {result.stderr.decode('utf-8')}"
            )
        return json.loads(result.stdout.decode("utf-8"))
</file>

<file path="apps/core/lib/utils/file_utils.py">
"""
FileUtils: Utility functions for temporary directory management.

- Provides methods for creating and cleaning up temporary directories.
- Designed for use in the infrastructure/lib layer.

Directory: apps/core/lib/utils/file_utils.py
Layer: Infrastructure/Lib
"""

import os
import shutil
import tempfile
from typing import Optional


class FileUtils:
    """
    Utility class for temporary directory creation and cleanup.
    """

    @staticmethod
    def create_temp_dir(prefix: Optional[str] = "echo_tmp_") -> str:
        """
        Creates a temporary directory for processing.

        Args:
            prefix (Optional[str]): Prefix for the temp directory name.

        Returns:
            str: Path to the created temporary directory.
        """
        return tempfile.mkdtemp(prefix=prefix)

    @staticmethod
    def cleanup_temp_dir(dir_path: str) -> None:
        """
        Removes a temporary directory and all its contents.

        Args:
            dir_path (str): Path to the directory to remove.
        """
        if os.path.exists(dir_path):
            shutil.rmtree(dir_path)
</file>

<file path="apps/core/lib/utils/subtitle_utils.py">
"""
SubtitleUtils: Utility functions for generating VTT and SRT subtitle files.

- Provides methods to generate VTT and SRT content from transcript segments.
- Designed for use in the infrastructure/lib layer.

Directory: apps/core/lib/utils/subtitle_utils.py
Layer: Infrastructure/Lib
"""

from typing import Dict, List


class SubtitleUtils:
    """
    Utility class for generating VTT and SRT subtitle content.
    """

    @staticmethod
    def generate_vtt(transcript_segments: List[Dict]) -> str:
        """
        Generates VTT subtitle content from transcript segments.

        Args:
            transcript_segments (List[Dict]): List of segments, each with 'text', 'start_time', 'end_time'.

        Returns:
            str: VTT formatted subtitle content.
        """

        def format_timestamp(seconds: float) -> str:
            h = int(seconds // 3600)
            m = int((seconds % 3600) // 60)
            s = seconds % 60
            ms = int((s - int(s)) * 1000)
            return f"{h:02}:{m:02}:{int(s):02}.{ms:03}"

        lines = ["WEBVTT\n"]
        for seg in transcript_segments:
            start = format_timestamp(seg["start_time"])
            end = format_timestamp(seg["end_time"])
            lines.append(f"{start} --> {end}")
            lines.append(seg["text"])
            lines.append("")
        return "\n".join(lines)

    @staticmethod
    def generate_srt(transcript_segments: List[Dict]) -> str:
        """
        Generates SRT subtitle content from transcript segments.

        Args:
            transcript_segments (List[Dict]): List of segments, each with 'text', 'start_time', 'end_time'.

        Returns:
            str: SRT formatted subtitle content.
        """

        def format_timestamp(seconds: float) -> str:
            h = int(seconds // 3600)
            m = int((seconds % 3600) // 60)
            s = int(seconds % 60)
            ms = int((seconds - int(seconds)) * 1000)
            return f"{h:02}:{m:02}:{s:02},{ms:03}"

        lines = []
        for idx, seg in enumerate(transcript_segments, 1):
            start = format_timestamp(seg["start_time"])
            end = format_timestamp(seg["end_time"])
            lines.append(f"{idx}")
            lines.append(f"{start} --> {end}")
            lines.append(seg["text"])
            lines.append("")
        return "\n".join(lines)
</file>

<file path="apps/core/lib/__init__.py">
"""
Common libraries and utilities for the application.
"""

# Import and expose key components for easier access
from lib.cache import RedisClient, get_redis_client
from lib.database import Base, create_session, engine, get_db_session
from lib.messaging import EmailService, get_email_service
from lib.storage import FileStorage, get_file_storage

__all__ = [
    # Cache
    "RedisClient",
    "get_redis_client",
    # Database
    "engine",
    "Base",
    "get_db_session",
    "create_session",
    # Messaging
    "EmailService",
    "get_email_service",
    # Storage
    "FileStorage",
    "get_file_storage",
]
</file>

<file path="apps/core/models/__init__.py">
from apps.core.models.chat_model import Chat, Message
from apps.core.models.enums import ProcessingStatus
from apps.core.models.user_model import User
from apps.core.models.video_job_model import VideoJobModel
from apps.core.models.video_metadata_model import VideoMetadataModel
from apps.core.models.video_model import VideoModel

__all__ = [
    "User",
    "Chat",
    "Message",
    "VideoModel",
    "VideoJobModel",
    "VideoMetadataModel",
    "ProcessingStatus",
]
</file>

<file path="apps/core/models/chat_model.py">
import uuid

from lib.database import Base
from sqlalchemy import Column, DateTime, ForeignKey, Index, Integer, String, Text
from sqlalchemy.dialects.postgresql import UUID
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func


class Chat(Base):
    __tablename__ = "chats"

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4, index=True)
    title = Column(String(255), nullable=True)
    # Anonymous chats don't need a user_id, but we can add it later if needed
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(
        DateTime(timezone=True), onupdate=func.now(), server_default=func.now()
    )

    # Relationships
    messages = relationship(
        "Message", back_populates="chat", cascade="all, delete-orphan"
    )

    # Add index on created_at for sorting chats by date
    __table_args__ = (Index("ix_chats_created_at", created_at.desc()),)


class Message(Base):
    __tablename__ = "messages"

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4, index=True)
    chat_id = Column(
        UUID(as_uuid=True), ForeignKey("chats.id", ondelete="CASCADE"), nullable=False
    )
    content = Column(Text, nullable=False)
    # is_from_ai: True if message is from AI, False if from user
    is_from_ai = Column(Integer, default=0, nullable=False)
    created_at = Column(DateTime(timezone=True), server_default=func.now())

    # Relationship
    chat = relationship("Chat", back_populates="messages")

    # Add indexes for query optimization
    __table_args__ = (
        Index("ix_messages_chat_id", chat_id),
        Index("ix_messages_created_at", created_at),
    )
</file>

<file path="apps/core/models/enums.py">
"""
Enumeration types used throughout the video processing system.

This module defines enumeration classes used to represent fixed sets of values
in a type-safe manner, such as processing status states. Using enums instead of
string literals improves type safety and code readability.

Usage:
    from apps.core.models.enums import ProcessingStatus

    # Create a new job with pending status
    job = VideoJobModel(status=ProcessingStatus.PENDING)

    # Check job status
    if job.status == ProcessingStatus.COMPLETED:
        print("Processing is complete!")
"""

from enum import Enum, auto


class ProcessingStatus(str, Enum):
    """
    Enumeration of possible video processing job statuses.

    This enum inherits from str to allow for easy serialization to/from databases
    and JSON, while still providing type safety and enumeration benefits.

    Attributes:
        PENDING: Job has been created but processing has not started.
        PROCESSING: Job is currently being processed.
        COMPLETED: Job has completed successfully.
        FAILED: Job processing failed with an error.
    """

    PENDING = "PENDING"
    PROCESSING = "PROCESSING"
    COMPLETED = "COMPLETED"
    FAILED = "FAILED"
</file>

<file path="apps/core/models/user_model.py">
from lib.database import Base
from sqlalchemy import Boolean, Column, DateTime, Integer, String
from sqlalchemy.sql import func


class User(Base):
    __tablename__ = "users"

    id = Column(Integer, primary_key=True, index=True)
    username = Column(String, unique=True, index=True)
    email = Column(String, unique=True, index=True)
    full_name = Column(String)
    hashed_password = Column(String)
    is_active = Column(Boolean, default=True)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), onupdate=func.now())
</file>

<file path="apps/core/models/video_job_model.py">
"""
SQLAlchemy model for video processing jobs.

This module defines the VideoJobModel class, which represents a video processing job
in the database. Each job is associated with a specific video and tracks the
processing status, stages, and any error information.

Usage:
    from apps.core.models.video_job_model import VideoJobModel
    from apps.core.models.enums import ProcessingStatus

    # Create a new video processing job
    new_job = VideoJobModel(
        video_id=1,
        status=ProcessingStatus.PENDING,
        processing_stages={"transcription": False, "metadata": False}
    )

    # Add to database
    db.add(new_job)
    db.commit()

    # Update job status
    new_job.status = ProcessingStatus.PROCESSING
    new_job.processing_stages["transcription"] = True
    db.commit()
"""

from sqlalchemy import (
    JSON,
    Column,
    DateTime,
    Enum,
    ForeignKey,
    Integer,
    String,
    Text,
    func,
)
from sqlalchemy.orm import relationship

from apps.core.lib.database.connection import Base
from apps.core.models.enums import ProcessingStatus

# Remove direct import causing circular dependency
# from apps.core.models.video_metadata_model import VideoMetadataModel


class VideoJobModel(Base):
    """
    SQLAlchemy model representing a video processing job.

    This model tracks the status and progress of a video processing task, including
    which processing stages have been completed and any errors that occurred.
    It maintains relationships with both the source video and the generated metadata.

    Attributes:
        id (int): Primary key, auto-incrementing identifier.
        video_id (int): Foreign key referencing the associated video.
        status (ProcessingStatus): Current status of the job (PENDING, PROCESSING, COMPLETED, FAILED).
        processing_stages (JSON): Dictionary tracking completion of various processing steps.
        error_message (str): Error message if processing failed, otherwise None.
        created_at (datetime): Timestamp when the job was created.
        updated_at (datetime): Timestamp when the job was last updated.
        video (relationship): Many-to-one relationship with VideoModel.
        video_metadata (relationship): One-to-one relationship with VideoMetadataModel.
    """

    __tablename__ = "video_jobs"

    id = Column(Integer, primary_key=True, autoincrement=True)
    video_id = Column(Integer, ForeignKey("videos.id"), nullable=False)
    status = Column(
        Enum(ProcessingStatus), default=ProcessingStatus.PENDING, nullable=False
    )
    processing_stages = Column(JSON, nullable=True)
    error_message = Column(Text, nullable=True)
    created_at = Column(DateTime, default=func.now(), nullable=False)
    updated_at = Column(
        DateTime, default=func.now(), onupdate=func.now(), nullable=False
    )

    # Define relationships
    video = relationship("VideoModel", back_populates="jobs")
    # Use string reference instead of direct class reference
    video_metadata = relationship(
        "VideoMetadataModel",
        back_populates="job",
        uselist=False,
        cascade="all, delete-orphan",
    )

    def __repr__(self):
        return f"<VideoJob(id={self.id}, status={self.status})>"
</file>

<file path="apps/core/models/video_metadata_model.py">
"""
SQLAlchemy model for storing video metadata information.

This module defines the VideoMetadataModel class which stores metadata extracted from
videos during processing, such as title, description, transcript, and technical details.
It also provides a ValueObject class (VideoMetadata) for simpler in-memory representation.

Usage:
    from apps.core.models.video_metadata_model import VideoMetadataModel, VideoMetadata

    # Create new video metadata
    metadata = VideoMetadataModel(
        job_id=123,
        title="My Awesome Video",
        description="This is a video about...",
        tags=["tutorial", "programming"],
        transcript_text="Full transcript text..."
    )

    # Add to database
    db.add(metadata)
    db.commit()

    # Using the value object for in-memory operations
    metadata_vo = VideoMetadata(
        title="My Video",
        description="Description",
        tags=["tag1", "tag2"]
    )
    metadata_json = metadata_vo.to_json()
"""

import json
from dataclasses import asdict, dataclass, field
from typing import Any, Dict, List

from sqlalchemy import (
    JSON,
    Column,
    DateTime,
    Float,
    ForeignKey,
    Integer,
    String,
    Text,
    func,
)
from sqlalchemy.orm import relationship

from apps.core.lib.database.connection import Base

# Remove direct import causing circular dependency
# from apps.core.models.video_job_model import VideoJobModel


class VideoMetadataModel(Base):
    """
    SQLAlchemy model representing metadata extracted from a processed video.

    This model stores all metadata extracted or generated during video processing,
    including AI-generated title and description, transcripts, subtitles, technical
    information about the video, and generated assets like thumbnails.

    Attributes:
        id (int): Primary key, auto-incrementing identifier.
        job_id (int): Foreign key referencing the processing job that created this metadata.
        title (str): AI-generated or user-provided title for the video.
        description (str): AI-generated or user-provided description.
        tags (JSON): List of keywords/tags related to the video content.
        transcript_text (str): Full text transcript of the video's audio content.
        transcript_file_url (str): URL to the stored transcript file.
        subtitle_files_urls (JSON): Dictionary mapping subtitle format to file URLs.
        thumbnail_file_url (str): URL to the generated thumbnail image.
        extracted_video_duration_seconds (float): Duration of the video in seconds.
        extracted_video_resolution (str): Resolution of the video (e.g., "1920x1080").
        extracted_video_format (str): Format/codec of the video file.
        show_notes_text (str): AI-generated or user-provided show notes/summary.
        created_at (datetime): Timestamp when the metadata was created.
        updated_at (datetime): Timestamp when the metadata was last updated.
        job (relationship): Many-to-one relationship with VideoJobModel.
    """

    __tablename__ = "video_metadata"

    id = Column(Integer, primary_key=True, autoincrement=True)
    job_id = Column(Integer, ForeignKey("video_jobs.id"), unique=True, nullable=False)
    title = Column(String, nullable=True)
    description = Column(Text, nullable=True)
    tags = Column(JSON, nullable=True)
    transcript_text = Column(Text, nullable=True)
    transcript_file_url = Column(String, nullable=True)
    subtitle_files_urls = Column(JSON, nullable=True)
    thumbnail_file_url = Column(String, nullable=True)
    extracted_video_duration_seconds = Column(Float, nullable=True)
    extracted_video_resolution = Column(String, nullable=True)
    extracted_video_format = Column(String, nullable=True)
    show_notes_text = Column(Text, nullable=True)
    created_at = Column(DateTime, default=func.now(), nullable=False)
    updated_at = Column(
        DateTime, default=func.now(), onupdate=func.now(), nullable=False
    )

    # Define relationship back to job - use string reference instead of direct class reference
    job = relationship("VideoJobModel", back_populates="video_metadata")

    def __repr__(self):
        return f"<VideoMetadata(id={self.id}, job_id={self.job_id})>"


@dataclass
class VideoMetadata:
    """
    Value object representing video metadata for in-memory operations.

    This dataclass provides a lightweight representation of video metadata
    without the ORM overhead, useful for business logic operations and
    serialization/deserialization.

    Attributes:
        title (str): Title of the video, defaults to "Untitled Video".
        description (str): Description of the video content.
        tags (List[str]): List of keywords/tags related to the video.
        show_notes (str): Notes or summary about the video content.
        chapters (List[Dict[str, Any]]): Chapter markers with timestamps.
    """

    title: str = "Untitled Video"
    description: str = ""
    tags: List[str] = field(default_factory=list)
    show_notes: str = ""
    chapters: List[Dict[str, Any]] = field(default_factory=list)

    def to_dict(self) -> Dict[str, Any]:
        """
        Convert the metadata to a dictionary.

        Returns:
            Dict[str, Any]: Dictionary representation of the metadata.
        """
        return asdict(self)

    def to_json(self) -> str:
        """
        Convert the metadata to a JSON string.

        Returns:
            str: JSON string representation of the metadata.
        """
        return json.dumps(self.to_dict(), ensure_ascii=False, indent=2)
</file>

<file path="apps/core/models/video_model.py">
"""
SQLAlchemy model for storing video file information.

This module defines the VideoModel class, which represents uploaded video files
in the database. Each video is associated with an uploader user and can have
multiple related processing jobs.

Usage:
    from apps.core.models.video_model import VideoModel

    # Create a new video entry
    new_video = VideoModel(
        uploader_user_id="user123",
        original_filename="my_video.mp4",
        storage_path="uploads/user123/my_video.mp4",
        content_type="video/mp4",
        size_bytes=1024000
    )

    # Add to database
    db.add(new_video)
    db.commit()
"""

from sqlalchemy import Column, DateTime, ForeignKey, Integer, String, func
from sqlalchemy.orm import relationship

from apps.core.lib.database.connection import Base


class VideoModel(Base):
    """
    SQLAlchemy model representing an uploaded video file.

    This model stores information about the original video file, including its
    storage location, file metadata, and the user who uploaded it. It maintains
    relationships with processing jobs and tracks creation/update timestamps.

    Attributes:
        id (int): Primary key, auto-incrementing identifier.
        uploader_user_id (str): User ID of the person who uploaded the video.
        original_filename (str): Original filename of the uploaded video.
        storage_path (str): Path where the video is stored (GCS or local filesystem).
        content_type (str): MIME type of the video file (e.g., 'video/mp4').
        size_bytes (int): Size of the video file in bytes.
        created_at (datetime): Timestamp when the entry was created.
        updated_at (datetime): Timestamp when the entry was last updated.
        jobs (relationship): One-to-many relationship with VideoJobModel.
    """

    __tablename__ = "videos"

    id = Column(Integer, primary_key=True, autoincrement=True)
    uploader_user_id = Column(String, index=True, nullable=False)
    original_filename = Column(String, nullable=False)
    storage_path = Column(String, unique=True, nullable=False)
    content_type = Column(String, nullable=False)
    size_bytes = Column(Integer, nullable=False)
    created_at = Column(DateTime, default=func.now(), nullable=False)
    updated_at = Column(
        DateTime, default=func.now(), onupdate=func.now(), nullable=False
    )

    # Define relationship to video jobs
    jobs = relationship(
        "VideoJobModel", back_populates="video", cascade="all, delete-orphan"
    )

    def __repr__(self):
        return f"<Video(id={self.id}, filename={self.original_filename})>"
</file>

<file path="apps/core/operations/__init__.py">
from apps.core.operations.chat_repository import ChatRepository, get_chat_repository
from apps.core.operations.user_repository import UserRepository, get_user_repository

__all__ = [
    "UserRepository",
    "get_user_repository",
    "ChatRepository",
    "get_chat_repository",
]
</file>

<file path="apps/core/services/__init__.py">
from services.ai_service import AIService, get_ai_service
from services.auth_service import AuthService, get_auth_service
from services.chat_service import ChatService, get_chat_service
from services.user_service import UserService, get_user_service

__all__ = [
    "UserService",
    "get_user_service",
    "AuthService",
    "get_auth_service",
    "ChatService",
    "get_chat_service",
    "AIService",
    "get_ai_service",
]
</file>

<file path="apps/core/services/metadata_service.py">
"""
Metadata service for generating and managing video metadata.

This module provides services for generating video metadata components including
titles, descriptions, tags, and thumbnails.
"""

import logging
import os
from typing import Dict, List

from apps.core.core.exceptions import MetadataGenerationError
from apps.core.lib.ai.base_adapter import AIAdapterInterface
from apps.core.lib.storage.file_storage import FileStorageService
from apps.core.models.video_metadata_model import VideoMetadata


class MetadataService:
    """
    Service for generating video metadata.

    This service handles the generation of various metadata components for videos
    using AI services, including titles, descriptions, tags, and thumbnails.
    """

    def __init__(
        self,
        ai_adapter: AIAdapterInterface,
        storage_adapter: FileStorageService,
        output_dir: str = "metadata",
    ):
        """
        Initialize the MetadataService with required dependencies.

        Args:
            ai_adapter: AI adapter for content generation
            storage_adapter: Storage adapter for file operations
            output_dir: Directory for generated metadata files
        """
        self._ai = ai_adapter
        self._storage = storage_adapter
        self._output_dir = output_dir

        # Ensure output directory exists
        os.makedirs(output_dir, exist_ok=True)

        logging.info(f"Initialized MetadataService with output_dir={output_dir}")

    def generate_metadata(self, transcript: str) -> VideoMetadata:
        """
        Generate complete metadata from a transcript.

        Args:
            transcript: Transcript text

        Returns:
            VideoMetadata object with all metadata components

        Raises:
            MetadataGenerationError: If metadata generation fails
        """
        try:
            logging.info("Generating complete metadata from transcript")

            # Delegate to AI service to generate all metadata components at once
            metadata_dict = self._ai.generate_metadata(transcript)

            # Create VideoMetadata object
            metadata = VideoMetadata(
                title=metadata_dict.get("title", "Untitled Video"),
                description=metadata_dict.get("description", ""),
                tags=metadata_dict.get("tags", []),
                show_notes=metadata_dict.get("show_notes", ""),
                chapters=metadata_dict.get("chapters", []),
            )

            logging.info(
                f"Successfully generated metadata with title: {metadata.title}"
            )
            return metadata

        except Exception as e:
            error_msg = f"Failed to generate metadata: {str(e)}"
            logging.error(error_msg)
            raise MetadataGenerationError(error_msg) from e

    def generate_title(self, transcript: str) -> str:
        """
        Generate a title from a transcript.

        Args:
            transcript: Transcript text

        Returns:
            Generated title

        Raises:
            MetadataGenerationError: If title generation fails
        """
        try:
            logging.info("Generating title from transcript")

            # Use AI to generate title and keywords
            title_tags = self._generate_title_tags(transcript)

            title = title_tags.get("Description", "Untitled Video")
            logging.info(f"Generated title: {title}")
            return title

        except Exception as e:
            error_msg = f"Failed to generate title: {str(e)}"
            logging.error(error_msg)
            raise MetadataGenerationError(error_msg) from e

    def generate_description(self, transcript: str, max_length: int = 500) -> str:
        """
        Generate a description from a transcript.

        Args:
            transcript: Transcript text
            max_length: Maximum length of the description

        Returns:
            Generated description

        Raises:
            MetadataGenerationError: If description generation fails
        """
        try:
            logging.info(
                f"Generating description from transcript (max {max_length} chars)"
            )

            # Use AI to summarize content
            description = self._ai.summarize_content(transcript, max_length)

            logging.info(f"Generated description ({len(description)} chars)")
            return description

        except Exception as e:
            error_msg = f"Failed to generate description: {str(e)}"
            logging.error(error_msg)
            raise MetadataGenerationError(error_msg) from e

    def generate_tags(self, transcript: str, max_tags: int = 10) -> List[str]:
        """
        Generate tags from a transcript.

        Args:
            transcript: Transcript text
            max_tags: Maximum number of tags to generate

        Returns:
            List of generated tags

        Raises:
            MetadataGenerationError: If tag generation fails
        """
        try:
            logging.info(f"Generating tags from transcript (max {max_tags} tags)")

            # Use AI to generate title and keywords
            title_tags = self._generate_title_tags(transcript)

            # Convert comma-separated keywords to list and limit to max_tags
            keywords = title_tags.get("Keywords", "video,content")
            tags = [tag.strip() for tag in keywords.split(",") if tag.strip()][
                :max_tags
            ]

            logging.info(f"Generated {len(tags)} tags")
            return tags

        except Exception as e:
            error_msg = f"Failed to generate tags: {str(e)}"
            logging.error(error_msg)
            raise MetadataGenerationError(error_msg) from e

    def generate_thumbnail_description(
        self, transcript: str, timestamp: float = 30.0
    ) -> str:
        """
        Generate a description for a thumbnail at a specific timestamp.

        Args:
            transcript: Transcript text
            timestamp: Time in seconds for the thumbnail

        Returns:
            Description for the thumbnail

        Raises:
            MetadataGenerationError: If description generation fails
        """
        try:
            logging.info(f"Generating thumbnail description at timestamp {timestamp}s")

            # Delegate to AI adapter
            description = self._ai.generate_thumbnail_description(transcript, timestamp)

            logging.info(f"Generated thumbnail description: {description}")
            return description

        except Exception as e:
            error_msg = f"Failed to generate thumbnail description: {str(e)}"
            logging.error(error_msg)
            raise MetadataGenerationError(error_msg) from e

    def save_metadata_to_json(self, metadata: VideoMetadata, filename: str) -> str:
        """
        Save metadata to a JSON file.

        Args:
            metadata: VideoMetadata object
            filename: Base filename without extension

        Returns:
            Path to the saved JSON file

        Raises:
            MetadataGenerationError: If saving fails
        """
        try:
            # Ensure filename has .json extension
            if not filename.endswith(".json"):
                filename = f"{filename}.json"

            output_path = os.path.join(self._output_dir, filename)

            # Convert metadata to dictionary
            metadata_dict = metadata.to_dict()

            # Save to storage
            json_content = metadata.to_json()
            self._storage.upload_from_string(
                json_content, output_path, content_type="application/json"
            )

            logging.info(f"Saved metadata to {output_path}")
            return output_path

        except Exception as e:
            error_msg = f"Failed to save metadata to JSON: {str(e)}"
            logging.error(error_msg)
            raise MetadataGenerationError(error_msg) from e

    def _generate_title_tags(self, transcript: str) -> Dict[str, str]:
        """
        Generate title and tags as a dictionary from transcript.

        Args:
            transcript: Transcript text

        Returns:
            Dictionary with "Description" (title) and "Keywords" (comma-separated tags)

        Raises:
            MetadataGenerationError: If generation fails
        """
        try:
            # This is a wrapper around the AI adapter method to simplify testing
            # and to provide consistent error handling
            result = self._ai._generate_title_tags(transcript)

            # Ensure required keys are present
            if "Description" not in result:
                result["Description"] = "Untitled Video"

            if "Keywords" not in result:
                result["Keywords"] = "video,content"

            return result

        except Exception as e:
            error_msg = f"Failed to generate title and tags: {str(e)}"
            logging.error(error_msg)
            # Return default values instead of raising to make this more robust
            return {"Description": "Untitled Video", "Keywords": "video,content"}
</file>

<file path="apps/core/tests/api/__init__.py">

</file>

<file path="apps/core/tests/integration/conftest.py">
import os
import tempfile
from typing import Any, Dict, Generator

import pytest
from fastapi.testclient import TestClient
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

from apps.core.lib.auth.supabase_auth import AuthenticatedUser, get_current_user
from apps.core.lib.database.connection import Base, get_db_session
from apps.core.main import app
from apps.core.models.enums import ProcessingStatus
from apps.core.models.video_job_model import VideoJobModel
from apps.core.models.video_model import VideoModel

# Create test database engine and session
TEST_DB_URL = "sqlite:///./test.db"
engine = create_engine(TEST_DB_URL)
TestingSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)


# Override the get_db dependency
def override_get_db() -> Generator:
    try:
        db = TestingSessionLocal()
        yield db
    finally:
        db.close()


# Override the auth dependency to return test user
def override_get_current_user() -> AuthenticatedUser:
    return AuthenticatedUser(
        id="test-user-id", email="test@example.com", aud="authenticated"
    )


# Apply test dependency overrides
app.dependency_overrides[get_db_session] = override_get_db
app.dependency_overrides[get_current_user] = override_get_current_user


@pytest.fixture
def client() -> TestClient:
    """
    Test client for FastAPI application.
    """
    return TestClient(app)


@pytest.fixture(scope="function")
def db_session() -> Generator:
    """
    Create a fresh database session for each test.
    """
    # Create all tables
    Base.metadata.create_all(bind=engine)

    # Use the session
    try:
        db = TestingSessionLocal()
        yield db
    finally:
        db.close()
        # Drop all tables for a clean slate
        Base.metadata.drop_all(bind=engine)


@pytest.fixture
def authenticated_user() -> AuthenticatedUser:
    """
    Return a mock authenticated user.
    """
    return override_get_current_user()


@pytest.fixture
def test_video_file() -> Generator:
    """
    Create a temporary test video file (mock).
    """
    # Create a temporary file
    fd, path = tempfile.mkstemp(suffix=".mp4")

    # Write some dummy data
    with os.fdopen(fd, "wb") as f:
        f.write(b"mock video data")

    # Return the file path
    yield path

    # Clean up after the test
    if os.path.exists(path):
        os.unlink(path)


@pytest.fixture
def populated_db(db_session) -> Dict[str, Any]:
    """
    Populate the database with test data and return the created objects.
    """
    # Create a test video
    test_video = VideoModel(
        uploader_user_id="test-user-id",
        original_filename="test_video.mp4",
        storage_path="uploads/test-user-id/test_video.mp4",
        content_type="video/mp4",
        size_bytes=1024,
    )
    db_session.add(test_video)
    db_session.flush()

    # Create a test job
    test_job = VideoJobModel(
        video_id=test_video.id,
        status=ProcessingStatus.COMPLETED,
        processing_stages={"transcription": True, "metadata": True},
    )
    db_session.add(test_job)
    db_session.commit()

    return {"video": test_video, "job": test_job}
</file>

<file path="apps/core/tests/operations/__init__.py">

</file>

<file path="apps/core/tests/services/__init__.py">

</file>

<file path="apps/core/tests/unit/lib/ai/__init__.py">
"""
Unit tests for AI adapters and services.
"""
</file>

<file path="apps/core/tests/unit/lib/ai/test_ai_client_factory.py">
"""
Unit tests for the AI client factory.
"""

from unittest.mock import patch

import pytest
from core.config import Settings

from apps.core.lib.ai.ai_client_factory import get_ai_adapter
from apps.core.lib.ai.base_adapter import AIAdapterInterface
from apps.core.lib.ai.gemini_adapter import GeminiAdapter


@pytest.fixture
def mock_settings_gemini():
    """Create mock settings with Gemini API key."""
    settings = Settings()
    settings.GEMINI_API_KEY = "fake-gemini-api-key"
    return settings


@pytest.fixture
def mock_settings_no_keys():
    """Create mock settings without any API keys."""
    settings = Settings()
    settings.GEMINI_API_KEY = None
    return settings


class TestAIClientFactory:
    """Test cases for the AI client factory."""

    def test_get_ai_adapter_with_gemini_key(self, mock_settings_gemini):
        """Test get_ai_adapter returns GeminiAdapter when Gemini API key is available."""
        with patch("apps.core.lib.ai.gemini_adapter.GeminiAdapter") as mock_gemini:
            # Configure the mock to return itself when called
            mock_gemini.return_value = mock_gemini

            # Call the factory function
            adapter = get_ai_adapter(mock_settings_gemini)

            # Verify GeminiAdapter was created with the settings
            mock_gemini.assert_called_once_with(mock_settings_gemini)

            # Verify correct adapter was returned
            assert adapter == mock_gemini

    def test_get_ai_adapter_with_no_keys(self, mock_settings_no_keys):
        """Test get_ai_adapter falls back to GeminiAdapter when no keys are available."""
        # This will raise an error since the GeminiAdapter requires an API key
        with pytest.raises(ValueError) as excinfo:
            get_ai_adapter(mock_settings_no_keys)

        assert "GEMINI_API_KEY must be set" in str(excinfo.value)

    @patch("core.config.settings")
    def test_get_ai_adapter_uses_global_settings(self, mock_global_settings):
        """Test get_ai_adapter uses global settings when none are provided."""
        # Configure the global settings mock
        mock_global_settings.GEMINI_API_KEY = "global-gemini-key"

        with patch("apps.core.lib.ai.gemini_adapter.GeminiAdapter") as mock_gemini:
            # Configure the mock to return itself when called
            mock_gemini.return_value = mock_gemini

            # Call the factory function without settings
            adapter = get_ai_adapter()

            # Verify GeminiAdapter was created with the global settings
            mock_gemini.assert_called_once_with(mock_global_settings)

            # Verify correct adapter was returned
            assert adapter == mock_gemini

    def test_get_ai_adapter_fallback_behavior(self):
        """Test the fallback behavior of get_ai_adapter."""
        custom_settings = Settings()
        custom_settings.GEMINI_API_KEY = "test-key"

        # Create a patch for GeminiAdapter that verifies it was called with our settings
        with patch("apps.core.lib.ai.gemini_adapter.GeminiAdapter") as mock_gemini:
            adapter = get_ai_adapter(custom_settings)
            mock_gemini.assert_called_once_with(custom_settings)
</file>

<file path="apps/core/tests/unit/lib/ai/test_gemini_adapter.py">
"""
Unit tests for the Gemini AI adapter.
"""

import asyncio
import json
import os
from unittest.mock import AsyncMock, MagicMock, patch

import pytest
from core.config import Settings
from core.exceptions import VideoProcessingError

from apps.core.lib.ai.gemini_adapter import AINoResponseError, GeminiAdapter


@pytest.fixture
def mock_settings():
    """Create mock settings with a valid Gemini API key."""
    settings = Settings()
    settings.GEMINI_API_KEY = "fake-api-key"
    return settings


@pytest.fixture
def mock_settings_no_api_key():
    """Create mock settings without a Gemini API key."""
    settings = Settings()
    settings.GEMINI_API_KEY = None
    return settings


@pytest.fixture
def mock_audio_file():
    """Create a temporary audio file for testing."""
    import tempfile

    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as temp_file:
        temp_file.write(b"fake audio data")
        temp_file_path = temp_file.name

    yield temp_file_path

    # Clean up
    if os.path.exists(temp_file_path):
        os.unlink(temp_file_path)


@pytest.fixture
def mock_genai():
    """Mock the Google Generative AI library."""
    with patch("lib.ai.gemini_adapter.genai") as mock_genai:
        # Mock the GenerativeModel class
        mock_text_model = MagicMock()
        mock_vision_model = MagicMock()

        # Mock the text response
        mock_response = MagicMock()
        mock_response.text = "Generated text from Gemini"
        mock_text_model.generate_content.return_value = mock_response

        # Set up the GenerativeModel constructor to return our mocks
        mock_genai.GenerativeModel.side_effect = (
            lambda model_name: mock_text_model
            if model_name == "gemini-pro"
            else mock_vision_model
        )

        yield mock_genai


class TestGeminiAdapter:
    """Test cases for the GeminiAdapter class."""

    def test_initialization_with_valid_api_key(self, mock_settings, mock_genai):
        """Test adapter initialization with a valid API key."""
        adapter = GeminiAdapter(mock_settings)

        # Verify genai was configured with the API key
        mock_genai.configure.assert_called_once_with(api_key="fake-api-key")

        # Verify models were initialized
        assert adapter.text_model is not None
        assert adapter.vision_model is not None

    def test_initialization_without_api_key(self, mock_settings_no_api_key):
        """Test adapter initialization fails without an API key."""
        with pytest.raises(ValueError) as excinfo:
            GeminiAdapter(mock_settings_no_api_key)

        assert "GEMINI_API_KEY must be set" in str(excinfo.value)

    @pytest.mark.asyncio
    async def test_generate_text_with_prompt_only(self, mock_settings, mock_genai):
        """Test generate_text with a prompt but no context."""
        adapter = GeminiAdapter(mock_settings)

        result = await adapter.generate_text("Generate a poem about AI")

        # Verify the model was called with the correct prompt
        adapter.text_model.generate_content.assert_called_once_with(
            "Generate a poem about AI"
        )

        # Verify the result
        assert result == "Generated text from Gemini"

    @pytest.mark.asyncio
    async def test_generate_text_with_prompt_and_context(
        self, mock_settings, mock_genai
    ):
        """Test generate_text with both prompt and context."""
        adapter = GeminiAdapter(mock_settings)

        result = await adapter.generate_text(
            prompt="Generate a poem about AI",
            context="Make it sound hopeful and optimistic",
        )

        # Verify the model was called with the combined prompt and context
        adapter.text_model.generate_content.assert_called_once_with(
            "Make it sound hopeful and optimistic\n\nGenerate a poem about AI"
        )

        # Verify the result
        assert result == "Generated text from Gemini"

    @pytest.mark.asyncio
    async def test_generate_text_handles_errors(self, mock_settings, mock_genai):
        """Test generate_text handles errors properly."""
        adapter = GeminiAdapter(mock_settings)

        # Set up the mock to raise an exception
        adapter.text_model.generate_content.side_effect = Exception("API error")

        # Test that our method wraps the exception properly
        with pytest.raises(AINoResponseError) as excinfo:
            await adapter.generate_text("Generate a poem about AI")

        assert "Error generating text with Gemini" in str(excinfo.value)

    @pytest.mark.asyncio
    async def test_generate_text_handles_empty_response(
        self, mock_settings, mock_genai
    ):
        """Test generate_text handles empty response."""
        adapter = GeminiAdapter(mock_settings)

        # Set up the mock to return None
        adapter.text_model.generate_content.return_value = None

        # Test that our method detects the empty response
        with pytest.raises(AINoResponseError) as excinfo:
            await adapter.generate_text("Generate a poem about AI")

        assert "Gemini failed to generate a response" in str(excinfo.value)

    @pytest.mark.asyncio
    async def test_transcribe_audio(self, mock_settings, mock_genai, mock_audio_file):
        """Test transcribe_audio method."""
        adapter = GeminiAdapter(mock_settings)

        # Since Gemini doesn't natively support audio transcription (per the implementation),
        # verify it raises NotImplementedError wrapped in our AINoResponseError
        with pytest.raises(AINoResponseError) as excinfo:
            await adapter.transcribe_audio(mock_audio_file)

        assert "not supported by Gemini" in str(excinfo.value)

    @pytest.mark.asyncio
    async def test_transcribe_audio_file_not_found(self, mock_settings, mock_genai):
        """Test transcribe_audio with a non-existent file."""
        adapter = GeminiAdapter(mock_settings)

        with pytest.raises(FileNotFoundError):
            await adapter.transcribe_audio("/nonexistent/audio.mp3")

    @pytest.mark.asyncio
    async def test_analyze_content(self, mock_settings, mock_genai):
        """Test analyze_content method."""
        adapter = GeminiAdapter(mock_settings)

        # Mock the JSON response
        adapter.text_model.generate_content.return_value.text = json.dumps(
            {
                "title": "Test Title",
                "description": "Test description",
                "tags": ["test", "ai", "gemini"],
                "key_points": ["Point 1", "Point 2", "Point 3"],
                "sentiment": "positive",
            }
        )

        result = await adapter.analyze_content("This is content to analyze")

        # Verify the model was called with the structured prompt
        assert adapter.text_model.generate_content.called

        # Verify the result contains expected keys
        assert "title" in result
        assert "description" in result
        assert "tags" in result
        assert "key_points" in result
        assert "sentiment" in result

        # Verify specific values
        assert result["title"] == "Test Title"
        assert result["tags"] == ["test", "ai", "gemini"]

    @pytest.mark.asyncio
    async def test_analyze_content_handles_malformed_json(
        self, mock_settings, mock_genai
    ):
        """Test analyze_content handles malformed JSON."""
        adapter = GeminiAdapter(mock_settings)

        # Mock malformed JSON response
        adapter.text_model.generate_content.return_value.text = "Not a valid JSON"

        # Test that our method wraps the JSON decode exception
        with pytest.raises(AINoResponseError) as excinfo:
            await adapter.analyze_content("This is content to analyze")

        assert "Gemini returned malformed JSON" in str(excinfo.value)

    @pytest.mark.asyncio
    async def test_segment_transcript(self, mock_settings, mock_genai):
        """Test segment_transcript method."""
        adapter = GeminiAdapter(mock_settings)

        # Mock the JSON response for transcript segmentation
        adapter.text_model.generate_content.return_value.text = json.dumps(
            [
                {"text": "First segment", "start_time": 0.0, "end_time": 5.0},
                {"text": "Second segment", "start_time": 5.0, "end_time": 10.0},
            ]
        )

        result = await adapter.segment_transcript("This is a transcript to segment")

        # Verify the model was called with the structured prompt
        assert adapter.text_model.generate_content.called

        # Verify the result is a list of segments
        assert isinstance(result, list)
        assert len(result) == 2

        # Verify each segment has required fields
        for segment in result:
            assert "text" in segment
            assert "start_time" in segment
            assert "end_time" in segment

    @pytest.mark.asyncio
    async def test_summarize_text(self, mock_settings, mock_genai):
        """Test summarize_text method."""
        adapter = GeminiAdapter(mock_settings)

        # Test without max_length
        result1 = await adapter.summarize_text("This is text to summarize")

        # Verify the model was called with the right prompt
        adapter.text_model.generate_content.assert_called_with(
            "Summarize the following text in a concise manner:\n\nThis is text to summarize"
        )

        # Test with max_length
        result2 = await adapter.summarize_text(
            "This is text to summarize", max_length=100
        )

        # Verify the model was called with the right prompt including max_length
        adapter.text_model.generate_content.assert_called_with(
            "Summarize the following text in a concise manner in 100 characters or less:\n\nThis is text to summarize"
        )
</file>

<file path="apps/core/tests/unit/lib/auth/__init__.py">
"""
Unit tests for authentication utilities.
"""
</file>

<file path="apps/core/tests/unit/lib/auth/test_supabase_auth.py">
"""
Unit tests for Supabase authentication utilities.
"""

from datetime import datetime, timedelta
from unittest.mock import AsyncMock, MagicMock, patch

import pytest
from fastapi import HTTPException
from jose import jwt

from apps.core.core.config import Settings
from apps.core.lib.auth.supabase_auth import (
    AuthenticatedUser,
    get_current_user,
    oauth2_scheme,
)


@pytest.fixture
def mock_settings():
    """Create a mock settings object with JWT configuration."""
    settings = Settings()
    settings.SUPABASE_JWT_SECRET = "test_secret_key"
    settings.ALGORITHM = "HS256"
    return settings


@pytest.fixture
def valid_token_payload():
    """Create a valid token payload."""
    return {
        "sub": "user-id-123",
        "email": "test@example.com",
        "aud": "authenticated",
        "exp": datetime.utcnow() + timedelta(minutes=15),
    }


@pytest.fixture
def valid_jwt(valid_token_payload, mock_settings):
    """Create a valid JWT token."""
    with patch("apps.core.lib.auth.supabase_auth.settings", mock_settings):
        return jwt.encode(
            valid_token_payload,
            mock_settings.SUPABASE_JWT_SECRET,
            algorithm=mock_settings.ALGORITHM,
        )


class TestAuthenticatedUser:
    """Test cases for the AuthenticatedUser model."""

    def test_authenticated_user_model(self):
        """Test creating an AuthenticatedUser instance."""
        user = AuthenticatedUser(
            id="user-id-123", email="test@example.com", aud="authenticated"
        )

        assert user.id == "user-id-123"
        assert user.email == "test@example.com"
        assert user.aud == "authenticated"

    def test_authenticated_user_optional_fields(self):
        """Test creating an AuthenticatedUser with only required fields."""
        user = AuthenticatedUser(id="user-id-456")

        assert user.id == "user-id-456"
        assert user.email is None
        assert user.aud is None


class TestGetCurrentUser:
    """Test cases for the get_current_user function."""

    @pytest.mark.asyncio
    async def test_get_current_user_valid_token(
        self, valid_jwt, mock_settings, valid_token_payload
    ):
        """Test getting the current user with a valid token."""
        with patch("apps.core.lib.auth.supabase_auth.settings", mock_settings):
            user = await get_current_user(valid_jwt)

            assert user.id == valid_token_payload["sub"]
            assert user.email == valid_token_payload["email"]
            assert user.aud == valid_token_payload["aud"]

    @pytest.mark.asyncio
    async def test_get_current_user_invalid_token(self, mock_settings):
        """Test getting the current user with an invalid token."""
        with patch("apps.core.lib.auth.supabase_auth.settings", mock_settings):
            with pytest.raises(HTTPException) as excinfo:
                await get_current_user("invalid_token")

            assert excinfo.value.status_code == 401
            assert "Could not validate Supabase credentials" in excinfo.value.detail

    @pytest.mark.asyncio
    async def test_get_current_user_missing_sub(self, mock_settings):
        """Test getting the current user with a token missing the 'sub' claim."""
        # Create a token without the 'sub' claim
        payload = {
            "email": "test@example.com",
            "aud": "authenticated",
            "exp": datetime.utcnow() + timedelta(minutes=15),
        }
        token = jwt.encode(
            payload,
            mock_settings.SUPABASE_JWT_SECRET,
            algorithm=mock_settings.ALGORITHM,
        )

        with patch("apps.core.lib.auth.supabase_auth.settings", mock_settings):
            with pytest.raises(HTTPException) as excinfo:
                await get_current_user(token)

            assert excinfo.value.status_code == 401

    @pytest.mark.asyncio
    async def test_get_current_user_expired_token(self, mock_settings):
        """Test getting the current user with an expired token."""
        # Create an expired token
        payload = {
            "sub": "user-id-123",
            "email": "test@example.com",
            "aud": "authenticated",
            "exp": datetime.utcnow() - timedelta(minutes=15),  # Expired 15 minutes ago
        }
        token = jwt.encode(
            payload,
            mock_settings.SUPABASE_JWT_SECRET,
            algorithm=mock_settings.ALGORITHM,
        )

        with patch("apps.core.lib.auth.supabase_auth.settings", mock_settings):
            with pytest.raises(HTTPException) as excinfo:
                await get_current_user(token)

            assert excinfo.value.status_code == 401

    @pytest.mark.asyncio
    async def test_get_current_user_wrong_audience(self, mock_settings):
        """Test getting the current user with a token having the wrong audience."""
        # Create a token with a wrong audience
        payload = {
            "sub": "user-id-123",
            "email": "test@example.com",
            "aud": "wrong-audience",
            "exp": datetime.utcnow() + timedelta(minutes=15),
        }
        token = jwt.encode(
            payload,
            mock_settings.SUPABASE_JWT_SECRET,
            algorithm=mock_settings.ALGORITHM,
        )

        with patch("apps.core.lib.auth.supabase_auth.settings", mock_settings):
            with pytest.raises(HTTPException) as excinfo:
                await get_current_user(token)

            assert excinfo.value.status_code == 401

    @pytest.mark.asyncio
    async def test_oauth2_scheme_dependency(self, valid_jwt):
        """Test that the oauth2_scheme dependency is used correctly."""
        # Mock the OAuth2PasswordBearer to return our valid JWT
        mock_oauth2 = AsyncMock(return_value=valid_jwt)

        # Test the get_current_user function with the mocked dependency
        with patch("apps.core.lib.auth.supabase_auth.oauth2_scheme", mock_oauth2):
            # This will pass our valid JWT to get_current_user
            user = await get_current_user()

            # We expect oauth2_scheme to be called
            mock_oauth2.assert_called_once()
</file>

<file path="apps/core/tests/unit/lib/cache/__init__.py">
"""
Unit tests for cache implementations.
"""
</file>

<file path="apps/core/tests/unit/lib/cache/test_redis_cache.py">
"""
Unit tests for the RedisCache class.
"""

from unittest.mock import AsyncMock, MagicMock, patch

import pytest

from apps.core.lib.cache.redis_cache import RedisCache


@pytest.fixture
def mock_redis_client():
    """Create a mock Redis async client."""
    with patch("redis.asyncio.Redis") as mock_redis:
        # Create AsyncMock instances for the Redis methods we'll use
        mock_client = AsyncMock()
        mock_client.get = AsyncMock()
        mock_client.set = AsyncMock()
        mock_client.close = AsyncMock()

        # Configure the Redis constructor to return our mock client
        mock_redis.return_value = mock_client

        # Return both the Redis constructor mock and the client mock for verification
        yield mock_redis, mock_client


class TestRedisCache:
    """Test cases for the RedisCache class."""

    def test_initialization(self, mock_redis_client):
        """Test initializing the RedisCache with settings."""
        mock_redis, mock_client = mock_redis_client

        # Initialize the cache
        cache = RedisCache()

        # Verify Redis client was initialized with the correct parameters
        mock_redis.assert_called_once()

        # Check the parameters (note: we can't check the exact values as they come from settings)
        args, kwargs = mock_redis.call_args
        assert "host" in kwargs
        assert "port" in kwargs
        assert "db" in kwargs
        assert "password" in kwargs
        assert kwargs["decode_responses"] is True

    @pytest.mark.asyncio
    async def test_get_existing_value(self, mock_redis_client):
        """Test retrieving an existing value from the cache."""
        mock_redis, mock_client = mock_redis_client

        # Configure the mock to return a specific value
        mock_client.get.return_value = "cached_value"

        # Initialize the cache and call get
        cache = RedisCache()
        result = await cache.get("test_key")

        # Verify get was called with the correct key
        mock_client.get.assert_called_once_with("test_key")

        # Verify the result
        assert result == "cached_value"

    @pytest.mark.asyncio
    async def test_get_nonexistent_value(self, mock_redis_client):
        """Test retrieving a non-existent value from the cache."""
        mock_redis, mock_client = mock_redis_client

        # Configure the mock to return None (key not found)
        mock_client.get.return_value = None

        # Initialize the cache and call get
        cache = RedisCache()
        result = await cache.get("nonexistent_key")

        # Verify get was called with the correct key
        mock_client.get.assert_called_once_with("nonexistent_key")

        # Verify the result is None
        assert result is None

    @pytest.mark.asyncio
    async def test_set_with_default_ttl(self, mock_redis_client):
        """Test setting a value in the cache with the default TTL."""
        mock_redis, mock_client = mock_redis_client

        # Initialize the cache and call set
        cache = RedisCache()
        await cache.set("test_key", "test_value")

        # Verify set was called with the correct parameters
        mock_client.set.assert_called_once_with("test_key", "test_value", ex=3600)

    @pytest.mark.asyncio
    async def test_set_with_custom_ttl(self, mock_redis_client):
        """Test setting a value in the cache with a custom TTL."""
        mock_redis, mock_client = mock_redis_client

        # Initialize the cache and call set with a custom TTL
        cache = RedisCache()
        await cache.set("test_key", "test_value", ttl_seconds=600)

        # Verify set was called with the correct parameters
        mock_client.set.assert_called_once_with("test_key", "test_value", ex=600)

    @pytest.mark.asyncio
    async def test_set_non_string_value(self, mock_redis_client):
        """Test setting a non-string value in the cache (should be stringified)."""
        mock_redis, mock_client = mock_redis_client

        # Initialize the cache and call set with a non-string value
        cache = RedisCache()
        await cache.set("test_key", 123)

        # Verify set was called with the stringified value
        mock_client.set.assert_called_once_with("test_key", "123", ex=3600)

    @pytest.mark.asyncio
    async def test_close(self, mock_redis_client):
        """Test closing the Redis connection."""
        mock_redis, mock_client = mock_redis_client

        # Initialize the cache and call close
        cache = RedisCache()
        await cache.close()

        # Verify close was called
        mock_client.close.assert_called_once()
</file>

<file path="apps/core/tests/unit/lib/publishing/test_youtube_adapter.py">
"""
Unit tests for the YouTube adapter.
"""

import json
import os
import tempfile
from unittest.mock import MagicMock, patch

import pytest
from core.exceptions import PublishingError
from googleapiclient.http import MediaFileUpload

from apps.core.lib.publishing.youtube_adapter import YouTubeAdapter


@pytest.fixture
def mock_credentials():
    """Create a mock OAuth credentials file for testing."""
    # Create a temporary file with mock credentials
    with tempfile.NamedTemporaryFile(delete=False, mode="w") as temp_file:
        credentials = {
            "token": "mock_token",
            "refresh_token": "mock_refresh_token",
            "token_uri": "https://oauth2.googleapis.com/token",
            "client_id": "mock_client_id",
            "client_secret": "mock_client_secret",
        }
        json.dump(credentials, temp_file)
        temp_file_path = temp_file.name

    yield temp_file_path

    # Clean up the temporary file
    os.unlink(temp_file_path)


@pytest.fixture
def mock_video_file():
    """Create a mock video file for testing."""
    # Create a temporary file as a mock video
    with tempfile.NamedTemporaryFile(delete=False, mode="wb") as temp_file:
        temp_file.write(b"mock video content")
        temp_file_path = temp_file.name

    yield temp_file_path

    # Clean up the temporary file
    os.unlink(temp_file_path)


@pytest.fixture
def mock_youtube_api():
    """Mock the YouTube API client."""
    with patch("googleapiclient.discovery.build") as mock_build:
        # Create mock YouTube API
        mock_youtube = MagicMock()
        mock_build.return_value = mock_youtube

        # Set up video responses
        mock_videos = MagicMock()
        mock_youtube.videos.return_value = mock_videos

        # Set up insert operation
        mock_insert = MagicMock()
        mock_videos.insert.return_value = mock_insert

        # Set up list operation
        mock_list = MagicMock()
        mock_videos.list.return_value = mock_list
        mock_list.execute.return_value = {
            "items": [
                {
                    "id": "test_video_id",
                    "snippet": {
                        "title": "Test Video",
                        "description": "Test Description",
                        "tags": ["test", "video"],
                        "categoryId": "22",
                    },
                    "status": {
                        "privacyStatus": "private",
                        "selfDeclaredMadeForKids": False,
                    },
                    "processingDetails": {
                        "processingStatus": "processing",
                    },
                }
            ]
        }

        # Set up update operation
        mock_update = MagicMock()
        mock_videos.update.return_value = mock_update
        mock_update.execute.return_value = {"id": "test_video_id"}

        # Set up delete operation
        mock_delete = MagicMock()
        mock_videos.delete.return_value = mock_delete

        # Set up captions
        mock_captions = MagicMock()
        mock_youtube.captions.return_value = mock_captions

        # Set up caption insert
        mock_caption_insert = MagicMock()
        mock_captions.insert.return_value = mock_caption_insert
        mock_caption_insert.execute.return_value = {"id": "test_caption_id"}

        yield mock_youtube


@pytest.fixture
def youtube_adapter(mock_credentials, mock_youtube_api):
    """Create a YouTubeAdapter instance for testing."""
    # Mock the client_secrets_file which isn't used in our tests
    mock_client_secrets = "/path/to/client_secrets.json"

    # Create the adapter
    adapter = YouTubeAdapter(
        client_secrets_file=mock_client_secrets,
        oauth_token_file=mock_credentials,
    )

    # Ensure the YouTube API is initialized
    adapter._youtube = mock_youtube_api

    return adapter


class TestYouTubeAdapter:
    """Test cases for the YouTubeAdapter class."""

    def test_initialization(self, mock_credentials, mock_youtube_api):
        """Test adapter initialization."""
        # Mock the client_secrets_file which isn't used in our tests
        mock_client_secrets = "/path/to/client_secrets.json"

        # Create the adapter
        adapter = YouTubeAdapter(
            client_secrets_file=mock_client_secrets,
            oauth_token_file=mock_credentials,
        )

        # Check that the adapter was initialized correctly
        assert adapter._client_secrets_file == mock_client_secrets
        assert adapter._oauth_token_file == mock_credentials
        assert adapter._youtube is not None

    def test_upload_video(self, youtube_adapter, mock_video_file):
        """Test uploading a video."""
        # Set up the mock response for the video upload
        mock_insert_request = youtube_adapter._youtube.videos().insert()
        mock_insert_request.next_chunk.side_effect = [
            (None, None),  # First chunk (progress)
            (None, {"id": "test_video_id"}),  # Final chunk (completion)
        ]

        # Call the method
        video_id = youtube_adapter.upload_video(
            video_file=mock_video_file,
            metadata={
                "title": "Test Video",
                "description": "Test Description",
                "tags": ["test", "video"],
                "privacy_status": "private",
            },
        )

        # Check the result
        assert video_id == "test_video_id"

        # Verify the insert method was called
        youtube_adapter._youtube.videos().insert.assert_called()

        # Verify part and body parameters were passed as expected
        # This is less strict than assert_called_once_with but still verifies parameters
        args, kwargs = youtube_adapter._youtube.videos().insert.call_args
        assert "part" in kwargs
        assert "body" in kwargs
        assert "media_body" in kwargs

        # Check that the body contains expected metadata
        body = kwargs["body"]
        assert body["snippet"]["title"] == "Test Video"
        assert body["snippet"]["description"] == "Test Description"
        assert body["status"]["privacyStatus"] == "private"

    def test_update_metadata(self, youtube_adapter):
        """Test updating video metadata."""
        # Call the method
        result = youtube_adapter.update_metadata(
            video_id="test_video_id",
            metadata={
                "title": "Updated Title",
                "description": "Updated Description",
                "tags": ["updated", "tags"],
                "privacy_status": "public",
            },
        )

        # Check the result
        assert result is True

        # Verify the right methods were called
        youtube_adapter._youtube.videos().list.assert_called_once_with(
            part="snippet,status", id="test_video_id"
        )
        youtube_adapter._youtube.videos().update.assert_called_once()

    def test_get_upload_status(self, youtube_adapter):
        """Test getting upload status."""
        # Call the method
        status = youtube_adapter.get_upload_status(video_id="test_video_id")

        # Check the result
        assert status == "processing"

        # Verify the right methods were called
        youtube_adapter._youtube.videos().list.assert_called_once_with(
            part="processingDetails,status", id="test_video_id"
        )

    def test_delete_video(self, youtube_adapter):
        """Test deleting a video."""
        # Call the method
        result = youtube_adapter.delete_video(video_id="test_video_id")

        # Check the result
        assert result is True

        # Verify the right methods were called
        youtube_adapter._youtube.videos().delete.assert_called_once_with(
            id="test_video_id"
        )

    def test_get_video_url(self, youtube_adapter):
        """Test getting a video URL."""
        # Call the method
        url = youtube_adapter.get_video_url(video_id="test_video_id")

        # Check the result
        assert url == "https://www.youtube.com/watch?v=test_video_id"

    def test_upload_caption(self, youtube_adapter, mock_video_file):
        """Test uploading captions."""
        # Create a temporary caption file
        with tempfile.NamedTemporaryFile(
            suffix=".vtt", delete=False, mode="w"
        ) as temp_file:
            temp_file.write("WEBVTT\n\n00:00:00.000 --> 00:00:05.000\nTest caption")
            caption_file = temp_file.name

        try:
            # Call the method
            result = youtube_adapter.upload_caption(
                video_id="test_video_id",
                caption_file=caption_file,
                language="en",
            )

            # Check the result
            assert result is True

            # Verify the right methods were called
            youtube_adapter._youtube.captions().insert.assert_called_once()
        finally:
            # Clean up
            os.unlink(caption_file)

    def test_set_publishing_time(self, youtube_adapter):
        """Test setting publishing time."""
        # Test immediate publishing
        result1 = youtube_adapter.set_publishing_time(
            video_id="test_video_id",
            publish_at=None,
        )
        assert result1 is True

        # Test scheduled publishing
        result2 = youtube_adapter.set_publishing_time(
            video_id="test_video_id",
            publish_at="2025-12-31T23:59:59Z",
        )
        assert result2 is True

        # Verify the right methods were called
        assert youtube_adapter._youtube.videos().list.call_count == 2
        assert youtube_adapter._youtube.videos().update.call_count == 2

    def test_error_handling(self, youtube_adapter):
        """Test error handling."""
        # Test missing video ID
        with pytest.raises(PublishingError):
            youtube_adapter.get_video_url(video_id=None)

        # Test file not found error
        with pytest.raises(PublishingError):
            youtube_adapter.upload_video(
                video_file="/nonexistent/file.mp4",
                metadata={},
            )
</file>

<file path="apps/core/tests/unit/lib/storage/__init__.py">
"""
Unit tests for storage adapters and services.
"""
</file>

<file path="apps/core/tests/unit/lib/utils/__init__.py">
"""
Unit tests for utility classes and functions.
"""
</file>

<file path="apps/core/tests/unit/lib/utils/test_file_utils.py">
"""
Unit tests for the FileUtils class.
"""

import os
import shutil
from unittest.mock import patch

import pytest

from apps.core.lib.utils.file_utils import FileUtils


class TestFileUtils:
    """Test cases for the FileUtils class."""

    def test_create_temp_dir_default_prefix(self):
        """Test creating a temporary directory with the default prefix."""
        with patch("tempfile.mkdtemp") as mock_mkdtemp:
            mock_mkdtemp.return_value = "/tmp/echo_tmp_12345"

            # Call the method
            temp_dir = FileUtils.create_temp_dir()

            # Verify tempfile.mkdtemp was called with the default prefix
            mock_mkdtemp.assert_called_once_with(prefix="echo_tmp_")

            # Verify the return value
            assert temp_dir == "/tmp/echo_tmp_12345"

    def test_create_temp_dir_custom_prefix(self):
        """Test creating a temporary directory with a custom prefix."""
        with patch("tempfile.mkdtemp") as mock_mkdtemp:
            mock_mkdtemp.return_value = "/tmp/custom_prefix_12345"

            # Call the method with a custom prefix
            temp_dir = FileUtils.create_temp_dir(prefix="custom_prefix_")

            # Verify tempfile.mkdtemp was called with the custom prefix
            mock_mkdtemp.assert_called_once_with(prefix="custom_prefix_")

            # Verify the return value
            assert temp_dir == "/tmp/custom_prefix_12345"

    def test_cleanup_temp_dir_existing(self):
        """Test cleaning up an existing temporary directory."""
        # Create a real temp directory for testing
        temp_dir = os.path.join(os.path.dirname(__file__), "test_temp_dir")
        os.makedirs(temp_dir, exist_ok=True)

        # Create a test file in the directory
        test_file = os.path.join(temp_dir, "test_file.txt")
        with open(test_file, "w") as f:
            f.write("test content")

        # Verify the directory and file exist
        assert os.path.exists(temp_dir)
        assert os.path.exists(test_file)

        # Call the method
        FileUtils.cleanup_temp_dir(temp_dir)

        # Verify the directory and file are gone
        assert not os.path.exists(temp_dir)
        assert not os.path.exists(test_file)

    def test_cleanup_temp_dir_nonexistent(self):
        """Test cleaning up a non-existent directory doesn't raise an error."""
        non_existent_dir = "/tmp/non_existent_dir_12345"

        # Make sure the directory doesn't exist
        if os.path.exists(non_existent_dir):
            shutil.rmtree(non_existent_dir)

        # Verify the directory doesn't exist
        assert not os.path.exists(non_existent_dir)

        # This should not raise an exception
        FileUtils.cleanup_temp_dir(non_existent_dir)

    def test_actual_directory_creation(self):
        """Test the actual creation of a temporary directory."""
        # Call the method
        temp_dir = FileUtils.create_temp_dir(prefix="test_file_utils_")

        try:
            # Verify the directory exists
            assert os.path.exists(temp_dir)

            # Verify the prefix
            assert os.path.basename(temp_dir).startswith("test_file_utils_")

            # Create a test file in the directory
            test_file = os.path.join(temp_dir, "test_file.txt")
            with open(test_file, "w") as f:
                f.write("test content")

            # Verify the file exists
            assert os.path.exists(test_file)

        finally:
            # Clean up
            FileUtils.cleanup_temp_dir(temp_dir)

            # Verify the cleanup worked
            assert not os.path.exists(temp_dir)
</file>

<file path="apps/core/tests/unit/lib/utils/test_subtitle_utils.py">
"""
Unit tests for the SubtitleUtils class.
"""

import re
from typing import Dict, List

import pytest

from apps.core.lib.utils.subtitle_utils import SubtitleUtils


class TestSubtitleUtils:
    """Test cases for the SubtitleUtils class."""

    @pytest.fixture
    def sample_transcript_segments(self) -> List[Dict]:
        """
        Creates a sample transcript segments list for testing.
        """
        return [
            {"text": "This is the first segment.", "start_time": 0.0, "end_time": 5.5},
            {
                "text": "This is the second segment.",
                "start_time": 5.5,
                "end_time": 10.0,
            },
            {"text": "This spans over a minute.", "start_time": 59.0, "end_time": 65.0},
            {
                "text": "This spans over an hour.",
                "start_time": 3599.0,
                "end_time": 3605.0,
            },
        ]

    def test_generate_vtt_format(self, sample_transcript_segments):
        """Test VTT formatting with sample transcript segments."""
        vtt_content = SubtitleUtils.generate_vtt(sample_transcript_segments)

        # Verify basic structure
        assert vtt_content.startswith("WEBVTT")

        # Split lines and analyze
        lines = vtt_content.strip().split("\n")

        # Verify we have the right number of segments
        # Each segment has: timestamp line, text line, empty line
        # Plus the WEBVTT header and a final empty line
        expected_lines = 1 + (3 * len(sample_transcript_segments))
        assert len(lines) == expected_lines

        # Verify timestamp format (HH:MM:SS.mmm --> HH:MM:SS.mmm)
        timestamp_pattern = r"^\d{2}:\d{2}:\d{2}\.\d{3} --> \d{2}:\d{2}:\d{2}\.\d{3}$"
        for i in range(1, len(lines), 3):
            assert re.match(timestamp_pattern, lines[i]), (
                f"Line {i} is not a valid VTT timestamp: {lines[i]}"
            )

        # Verify the text content for each segment
        for i, segment in enumerate(sample_transcript_segments):
            text_line_index = 2 + (i * 3)
            assert lines[text_line_index] == segment["text"]

    def test_generate_vtt_timestamps(self, sample_transcript_segments):
        """Test VTT timestamp formatting specifically."""
        vtt_content = SubtitleUtils.generate_vtt(sample_transcript_segments)
        lines = vtt_content.strip().split("\n")

        # First segment: 00:00:00.000 --> 00:00:05.500
        assert lines[1] == "00:00:00.000 --> 00:00:05.500"

        # Second segment: 00:00:05.500 --> 00:00:10.000
        assert lines[4] == "00:00:05.500 --> 00:00:10.000"

        # Minute spanning: 00:00:59.000 --> 00:01:05.000
        assert lines[7] == "00:00:59.000 --> 00:01:05.000"

        # Hour spanning: 00:59:59.000 --> 01:00:05.000
        assert lines[10] == "00:59:59.000 --> 01:00:05.000"

    def test_generate_srt_format(self, sample_transcript_segments):
        """Test SRT formatting with sample transcript segments."""
        srt_content = SubtitleUtils.generate_srt(sample_transcript_segments)

        # Split lines and analyze
        lines = srt_content.strip().split("\n")

        # Verify we have the right number of segments
        # Each segment has: index line, timestamp line, text line, empty line
        expected_lines = 4 * len(sample_transcript_segments)
        assert len(lines) == expected_lines

        # Verify sequence numbers
        for i in range(0, len(lines), 4):
            seq_num = i // 4 + 1
            assert lines[i] == str(seq_num)

        # Verify timestamp format (HH:MM:SS,mmm --> HH:MM:SS,mmm)
        timestamp_pattern = r"^\d{2}:\d{2}:\d{2},\d{3} --> \d{2}:\d{2}:\d{2},\d{3}$"
        for i in range(1, len(lines), 4):
            assert re.match(timestamp_pattern, lines[i]), (
                f"Line {i} is not a valid SRT timestamp: {lines[i]}"
            )

        # Verify the text content for each segment
        for i, segment in enumerate(sample_transcript_segments):
            text_line_index = 2 + (i * 4)
            assert lines[text_line_index] == segment["text"]

    def test_generate_srt_timestamps(self, sample_transcript_segments):
        """Test SRT timestamp formatting specifically."""
        srt_content = SubtitleUtils.generate_srt(sample_transcript_segments)
        lines = srt_content.strip().split("\n")

        # First segment: 00:00:00,000 --> 00:00:05,500
        assert lines[1] == "00:00:00,000 --> 00:00:05,500"

        # Second segment: 00:00:05,500 --> 00:00:10,000
        assert lines[5] == "00:00:05,500 --> 00:00:10,000"

        # Minute spanning: 00:00:59,000 --> 00:01:05,000
        assert lines[9] == "00:00:59,000 --> 00:01:05,000"

        # Hour spanning: 00:59:59,000 --> 01:00:05,000
        assert lines[13] == "00:59:59,000 --> 01:00:05,000"

    def test_empty_transcript(self):
        """Test generating subtitles with an empty transcript."""
        empty_segments = []

        # VTT should just have the header
        vtt_content = SubtitleUtils.generate_vtt(empty_segments)
        assert vtt_content == "WEBVTT\n"

        # SRT should be empty
        srt_content = SubtitleUtils.generate_srt(empty_segments)
        assert srt_content == ""

    def test_malformed_segments(self):
        """Test handling of malformed segment data."""
        # Missing start_time
        malformed_segments = [
            {"text": "This segment is missing start_time.", "end_time": 5.0}
        ]

        # This should raise a KeyError
        with pytest.raises(KeyError):
            SubtitleUtils.generate_vtt(malformed_segments)

        with pytest.raises(KeyError):
            SubtitleUtils.generate_srt(malformed_segments)

        # Missing text
        malformed_segments = [{"start_time": 0.0, "end_time": 5.0}]

        # This should raise a KeyError
        with pytest.raises(KeyError):
            SubtitleUtils.generate_vtt(malformed_segments)

        with pytest.raises(KeyError):
            SubtitleUtils.generate_srt(malformed_segments)
</file>

<file path="apps/core/tests/unit/operations/__init__.py">
"""
Unit tests for operations (repositories) layer.
"""
</file>

<file path="apps/core/tests/conftest.py">
import asyncio
from typing import AsyncGenerator, Generator

import pytest
import pytest_asyncio
from core.lib.database.connection import Base, get_async_db_session

# Assuming your FastAPI app instance is in 'main.py' at the root of 'apps/core'
# Adjust the import path if your main app instance is located elsewhere.
from main import app
from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine
from sqlalchemy.orm import sessionmaker
from starlette.testclient import TestClient

# Use a separate in-memory SQLite database for tests
# Using a file-based SQLite for easier inspection if needed: ./test.db
# For true in-memory, use "sqlite+aiosqlite:///:memory:"
# However, :memory: dbs are distinct per connection, which can be tricky with SQLAlchemy engines/sessions.
# A file-based one ensures all connections in a test session hit the same DB.
TEST_DATABASE_URL = "sqlite+aiosqlite:///./test_db_file.db"


@pytest.fixture(scope="session")
def event_loop() -> Generator[asyncio.AbstractEventLoop, None, None]:
    """Create an instance of the default event loop for each test session."""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()


@pytest_asyncio.fixture(scope="session")
async def async_engine_fixture():  # Renamed to avoid conflict if a var is named async_engine
    """Create an async engine for the test database for the entire session."""
    engine = create_async_engine(TEST_DATABASE_URL)
    async with engine.begin() as conn:
        # Drop all tables first to ensure a clean state for each test session
        await conn.run_sync(Base.metadata.drop_all)
        await conn.run_sync(Base.metadata.create_all)
    yield engine
    await engine.dispose()
    # Consider removing the test_db_file.db if created
    # import os
    # if os.path.exists("./test_db_file.db"):
    #     os.remove("./test_db_file.db")


@pytest_asyncio.fixture(scope="function")
async def db_session_fixture(
    async_engine_fixture,
) -> AsyncGenerator[AsyncSession, None]:  # Renamed
    """Yield an AsyncSession for each test function, ensuring transaction rollback."""
    # sessionmaker for test sessions
    AsyncTestSessionLocal = sessionmaker(
        bind=async_engine_fixture,
        class_=AsyncSession,
        expire_on_commit=False,
        autoflush=False,
    )

    async with AsyncTestSessionLocal() as session:
        # Begin a transaction
        await session.begin()
        try:
            yield session
        finally:
            # Rollback the transaction to ensure test isolation
            await session.rollback()
            await session.close()


@pytest.fixture(scope="function")  # Changed to function scope
def test_client_fixture(
    db_session_fixture: AsyncSession,
) -> Generator[TestClient, None, None]:  # Renamed
    """
    Create a TestClient for API tests for each function.
    Overrides the get_async_db_session dependency with the test session.
    """

    async def override_get_async_db_session() -> AsyncGenerator[AsyncSession, None]:
        yield db_session_fixture

    app.dependency_overrides[get_async_db_session] = override_get_async_db_session
    with TestClient(app) as client:
        yield client
    # Clean up the override after the test
    del app.dependency_overrides[get_async_db_session]
</file>

<file path="apps/core/tests/test_architecture.py">
import ast
from pathlib import Path


def get_import_modules(file_path):
    """Extract all import statements from a Python file"""
    with open(file_path, "r") as f:
        file_content = f.read()

    tree = ast.parse(file_content)
    imports = []

    for node in ast.walk(tree):
        # Check for import statements (import x, import x.y)
        if isinstance(node, ast.Import):
            for name in node.names:
                imports.append(name.name)
        # Check for from import statements (from x import y)
        elif isinstance(node, ast.ImportFrom) and node.module is not None:
            imports.append(node.module)

    return imports


def check_imports(directory, forbidden_imports, error_msg_template):
    """Helper function to check imports in a directory against forbidden ones"""
    # Get the project root directory
    project_root = Path(__file__).parent.parent
    target_dir = project_root / directory

    # Get all Python files in the target directory
    py_files = [f for f in target_dir.glob("*.py") if f.is_file()]

    # Verify there are files to check
    assert len(py_files) > 0, f"No Python files found in the {directory} directory"

    violations = []
    for file_path in py_files:
        imports = get_import_modules(file_path)

        for forbidden_prefix in forbidden_imports:
            illegal_imports = [
                imp for imp in imports if imp.startswith(forbidden_prefix)
            ]
            if illegal_imports:
                violation = {
                    "file": file_path.name,
                    "forbidden_prefix": forbidden_prefix,
                    "imports": illegal_imports,
                }
                violations.append(violation)
                print(
                    error_msg_template.format(
                        file=file_path.name,
                        layer=forbidden_prefix,
                        imports=illegal_imports,
                    )
                )

    return violations


def test_api_layer_architecture():
    """
    Test that the API layer follows architectural boundaries by
    not importing directly from operations or models
    """
    violations = check_imports(
        directory="api",
        forbidden_imports=["operations", "models"],
        error_msg_template="File {file} directly imports from {layer} layer: {imports}",
    )

    assert not violations, (
        "API layer should not import directly from operations or models layers"
    )


def test_service_layer_architecture():
    """
    Test that the Service layer follows architectural boundaries by
    not importing directly from models layer (should use operations layer)
    """
    violations = check_imports(
        directory="services",
        forbidden_imports=["models"],
        error_msg_template="File {file} directly imports from {layer} layer: {imports}",
    )

    assert not violations, (
        "Service layer should not import directly from models layer (use operations layer instead)"
    )


def test_all_architectural_boundaries():
    """Comprehensive test of all architectural boundaries in the layered design"""
    # Define the layer boundaries
    layer_boundaries = {
        "api": [
            "operations",
            "models",
        ],  # API should not import from operations or models
        "services": ["models"],  # Services should not import from models
    }

    all_violations = []

    for layer, forbidden in layer_boundaries.items():
        violations = check_imports(
            directory=layer,
            forbidden_imports=forbidden,
            error_msg_template=f"Architectural violation in {layer}: {{file}} imports from {{layer}} layer: {{imports}}",
        )
        all_violations.extend(violations)

    # Detailed report of violations
    if all_violations:
        violation_report = "\n".join(
            [
                f"- {v['file']} imports from {v['forbidden_prefix']}: {v['imports']}"
                for v in all_violations
            ]
        )
        assert False, f"Architectural boundary violations found:\n{violation_report}"
</file>

<file path="apps/core/.gitignore">
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# Distribution / packaging
dist/
build/
*.egg-info/

# Virtual environments
venv/
.venv/
env/
ENV/

# Environment variables
.env.local

# Database
*.db
*.sqlite3

# Testing
.coverage
htmlcov/
.pytest_cache/

# IDE
.idea/

# Logs
*.log 

.mypy_cache/
.ruff_cache/
</file>

<file path="apps/core/.python-version">
3.13
</file>

<file path="apps/core/alembic.ini">
# A generic, single database configuration.

[alembic]
# path to migration scripts
# Use forward slashes (/) also on windows to provide an os agnostic path
script_location = alembic

# template used to generate migration file names; The default value is %%(rev)s_%%(slug)s
# Uncomment the line below if you want the files to be prepended with date and time
# see https://alembic.sqlalchemy.org/en/latest/tutorial.html#editing-the-ini-file
# for all available tokens
# file_template = %%(year)d_%%(month).2d_%%(day).2d_%%(hour).2d%%(minute).2d-%%(rev)s_%%(slug)s

# sys.path path, will be prepended to sys.path if present.
# defaults to the current working directory.
prepend_sys_path = .

# timezone to use when rendering the date within the migration file
# as well as the filename.
# If specified, requires the python>=3.9 or backports.zoneinfo library and tzdata library.
# Any required deps can installed by adding `alembic[tz]` to the pip requirements
# string value is passed to ZoneInfo()
# leave blank for localtime
# timezone =

# max length of characters to apply to the "slug" field
# truncate_slug_length = 40

# set to 'true' to run the environment during
# the 'revision' command, regardless of autogenerate
# revision_environment = false

# set to 'true' to allow .pyc and .pyo files without
# a source .py file to be detected as revisions in the
# versions/ directory
# sourceless = false

# version location specification; This defaults
# to alembic/versions.  When using multiple version
# directories, initial revisions must be specified with --version-path.
# The path separator used here should be the separator specified by "version_path_separator" below.
# version_locations = %(here)s/bar:%(here)s/bat:alembic/versions

# version path separator; As mentioned above, this is the character used to split
# version_locations. The default within new alembic.ini files is "os", which uses os.pathsep.
# If this key is omitted entirely, it falls back to the legacy behavior of splitting on spaces and/or commas.
# Valid values for version_path_separator are:
#
# version_path_separator = :
# version_path_separator = ;
# version_path_separator = space
# version_path_separator = newline
#
# Use os.pathsep. Default configuration used for new projects.
version_path_separator = os

# set to 'true' to search source files recursively
# in each "version_locations" directory
# new in Alembic version 1.10
# recursive_version_locations = false

# the output encoding used when revision files
# are written from script.py.mako
# output_encoding = utf-8

# sqlalchemy.url is set dynamically from apps.core.core.config.settings.DATABASE_URL in env.py
sqlalchemy.url =


[post_write_hooks]
# post_write_hooks defines scripts or Python functions that are run
# on newly generated revision scripts.  See the documentation for further
# detail and examples

# format using "black" - use the console_scripts runner, against the "black" entrypoint
# hooks = black
# black.type = console_scripts
# black.entrypoint = black
# black.options = -l 79 REVISION_SCRIPT_FILENAME

# lint with attempts to fix using "ruff" - use the exec runner, execute a binary
# hooks = ruff
# ruff.type = exec
# ruff.executable = %(here)s/.venv/bin/ruff
# ruff.options = check --fix REVISION_SCRIPT_FILENAME

# Logging configuration
[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARNING
handlers = console
qualname =

[logger_sqlalchemy]
level = WARNING
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S
</file>

<file path="apps/core/package.json">
{
    "name": "@echo/core",
    "version": "1.0.0",
    "description": "Python API Core Service",
    "private": true,
    "scripts": {
      "test": "./bin/test.sh",
      "dev": "./bin/dev.sh",
      "setup-env": "./bin/setup.sh",
      "lint": "./bin/lint.sh",
      "lint:fix": "./bin/lint-fix.sh",
      "format": "./bin/format.sh",
      "typecheck": "./bin/typecheck.sh"
    },
    "keywords": [
      "python",
      "api",
      "fastapi"
    ],
    "author": "",
    "license": "ISC"
  }
</file>

<file path="apps/web/public/vite.svg">
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="iconify iconify--logos" width="31.88" height="32" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 257"><defs><linearGradient id="IconifyId1813088fe1fbc01fb466" x1="-.828%" x2="57.636%" y1="7.652%" y2="78.411%"><stop offset="0%" stop-color="#41D1FF"></stop><stop offset="100%" stop-color="#BD34FE"></stop></linearGradient><linearGradient id="IconifyId1813088fe1fbc01fb467" x1="43.376%" x2="50.316%" y1="2.242%" y2="89.03%"><stop offset="0%" stop-color="#FFEA83"></stop><stop offset="8.333%" stop-color="#FFDD35"></stop><stop offset="100%" stop-color="#FFA800"></stop></linearGradient></defs><path fill="url(#IconifyId1813088fe1fbc01fb466)" d="M255.153 37.938L134.897 252.976c-2.483 4.44-8.862 4.466-11.382.048L.875 37.958c-2.746-4.814 1.371-10.646 6.827-9.67l120.385 21.517a6.537 6.537 0 0 0 2.322-.004l117.867-21.483c5.438-.991 9.574 4.796 6.877 9.62Z"></path><path fill="url(#IconifyId1813088fe1fbc01fb467)" d="M185.432.063L96.44 17.501a3.268 3.268 0 0 0-2.634 3.014l-5.474 92.456a3.268 3.268 0 0 0 3.997 3.378l24.777-5.718c2.318-.535 4.413 1.507 3.936 3.838l-7.361 36.047c-.495 2.426 1.782 4.5 4.151 3.78l15.304-4.649c2.372-.72 4.652 1.36 4.15 3.788l-11.698 56.621c-.732 3.542 3.979 5.473 5.943 2.437l1.313-2.028l72.516-144.72c1.215-2.423-.88-5.186-3.54-4.672l-25.505 4.922c-2.396.462-4.435-1.77-3.759-4.114l16.646-57.705c.677-2.35-1.37-4.583-3.769-4.113Z"></path></svg>
</file>

<file path="apps/web/src/components/home/hero.tsx">
import Container from "../shared/container";
import {
	Tabs,
	TabsContent,
	TabsList,
	TabsTrigger,
} from "../ui/tabs";

// Props interface currently empty, update if needed for future enhancements.
type Props = {};

export default function Hero({}: Props) {
	return (
		<Container spacer={true}>
			{/* Main container using flexbox for vertical stacking */}
			<div className="flex flex-col items-center text-center py-16">
				{/* Title Section */}
				<div className="flex items-center justify-center bg-[var(--accent-blue)] text-white h-24 w-24 rounded-full mb-6">
					<span className="text-2xl font-bold">VIDEO</span>
				</div>

				{/* Main Heading */}
				<h1 className="text-7xl text-[var(--accent-blue)] lg:text-6xl md:text-5xl sm:text-4xl">
					VIDEO AI PIPELINE
				</h1>

				{/* Tagline */}
				<p className="text-xl text-[var(--foreground)] mt-4 md:text-lg sm:text-base max-w-2xl">
					Automate your video publishing workflow with AI-powered metadata
					generation and YouTube integration
				</p>

				{/* Feature Icons as Text */}
				<div className="my-8 flex items-center justify-center gap-8">
					<div className="flex flex-col items-center">
						<div className="bg-[var(--accent-blue)] text-white h-12 w-12 rounded-full flex items-center justify-center">
							<span className="font-bold">AI</span>
						</div>
						<span className="text-sm mt-1">Gemini</span>
					</div>
					<div className="flex flex-col items-center">
						<div className="bg-red-600 text-white h-12 w-12 rounded-full flex items-center justify-center">
							<span className="font-bold">YT</span>
						</div>
						<span className="text-sm mt-1">YouTube</span>
					</div>
					<div className="flex flex-col items-center">
						<div className="bg-gray-700 text-white h-12 w-12 rounded-full flex items-center justify-center">
							<span className="font-bold">GCP</span>
						</div>
						<span className="text-sm mt-1">Cloud</span>
					</div>
				</div>

				{/* Separator */}
				<hr className="my-6 w-1/5 border-[var(--accent-blue)] border-t-2" />

				{/* Feature Tabs Section */}
				<div className="w-full max-w-2xl mt-8">
					<Tabs defaultValue="upload" className="w-full">
						<TabsList className="grid w-full grid-cols-4">
							<TabsTrigger value="upload">Upload</TabsTrigger>
							<TabsTrigger value="process">Process</TabsTrigger>
							<TabsTrigger value="metadata">Metadata</TabsTrigger>
							<TabsTrigger value="publish">Publish</TabsTrigger>
						</TabsList>
						<TabsContent value="upload" className="mt-4 text-left space-y-2">
							<p>
								<strong>Drag & Drop:</strong> Simple video upload interface
							</p>
							<p>
								<strong>Multi-Channel:</strong> Support for both daily and main
								channels
							</p>
							<p>
								<strong>Cloud Storage:</strong> Videos stored securely in Google
								Cloud Storage
							</p>
							<p>
								<strong>Real-time:</strong> Live status updates as your video
								moves through the pipeline
							</p>
						</TabsContent>
						<TabsContent value="process" className="mt-4 text-left space-y-2">
							<p>
								<strong>Automatic:</strong> Hands-off video processing workflow
							</p>
							<p>
								<strong>AI-Powered:</strong> Gemini 2.5 Pro generates
								high-quality metadata
							</p>
							<p>
								<strong>Transcription:</strong> Accurate speech-to-text for your
								entire video
							</p>
							<p>
								<strong>Chapters:</strong> Smart chapter markers generated from
								video content
							</p>
						</TabsContent>
						<TabsContent value="metadata" className="mt-4 text-left space-y-2">
							<p>
								<strong>Title Generation:</strong> AI creates engaging,
								SEO-friendly titles
							</p>
							<p>
								<strong>Description:</strong> Comprehensive video descriptions
								with keywords
							</p>
							<p>
								<strong>Thumbnails:</strong> Generate 10 thumbnails with Imagen3
								+ Pillow
							</p>
							<p>
								<strong>Editable:</strong> Review and customize all metadata
								before publishing
							</p>
						</TabsContent>
						<TabsContent value="publish" className="mt-4 text-left space-y-2">
							<p>
								<strong>YouTube Integration:</strong> Direct upload to your
								YouTube channel
							</p>
							<p>
								<strong>Scheduling:</strong> Set publication dates and times
							</p>
							<p>
								<strong>Status Tracking:</strong> Monitor progress from
								processing to published
							</p>
							<p>
								<strong>Error Handling:</strong> Robust error recovery and
								notification
							</p>
						</TabsContent>
					</Tabs>
				</div>

				{/* Key Benefits Section */}
				<div className="w-full max-w-2xl mt-16">
					<h2 className="font-bold text-3xl text-[var(--accent-blue)] md:text-2xl sm:text-xl mb-6">
						KEY BENEFITS
					</h2>
					<ul className="grid grid-cols-2 gap-4 text-base text-[var(--foreground)]">
						<li className="flex items-center gap-2">
							<span className="text-[var(--accent-blue)]"></span> Save hours on
							video publishing
						</li>
						<li className="flex items-center gap-2">
							<span className="text-[var(--accent-blue)]"></span> Consistent,
							high-quality metadata
						</li>
						<li className="flex items-center gap-2">
							<span className="text-[var(--accent-blue)]"></span> AI-generated
							thumbnails
						</li>
						<li className="flex items-center gap-2">
							<span className="text-[var(--accent-blue)]"></span> Real-time
							status tracking
						</li>
						<li className="flex items-center gap-2">
							<span className="text-[var(--accent-blue)]"></span> Full editing
							control
						</li>
						<li className="flex items-center gap-2">
							<span className="text-[var(--accent-blue)]"></span> Cloud-native
							architecture
						</li>
					</ul>
				</div>

				{/* CTA Button */}
				<div className="mt-12">
					<a
						href="/dashboard"
						className="bg-[var(--accent-blue)] text-white px-8 py-3 rounded-md text-lg hover:bg-opacity-90 transition-all"
					>
						Get Started
					</a>
				</div>
			</div>
		</Container>
	);
}
</file>

<file path="apps/web/src/components/shared/container.tsx">
import { cn } from "@/lib/utils";

type Props = {
	children: React.ReactNode;
	tag?: "div" | "section" | "header" | "footer";
	className?: string;
	id?: string;
	spacer?: boolean;
	padding?: boolean;
};

export default function Container({
	children,
	tag = "div",
	className,
	id,
	spacer = false,
	padding = true,
}: Props) {
	const Tag = tag;
	return (
		<Tag
			className={cn(
				"w-full flex flex-col",
				spacer ? "mt-19" : "",
				padding ? "px-6 md:px-8 lg:px-10" : "",
				className,
			)}
			id={id}
		>
			{children}
		</Tag>
	);
}
</file>

<file path="apps/web/src/components/shared/navbar.tsx">
import { cn } from "@/lib/utils";
import { Link } from "@tanstack/react-router";
import { Home, Menu, Search, X } from "lucide-react";
import { useState } from "react";
import { Button } from "../ui/button";
import Container from "./container";

type Props = {};

export default function Navbar({}: Props) {
	const [isMenuOpen, setIsMenuOpen] = useState(false);

	const navItems = [
		{
			label: "Home",
			href: "/",
		},
		{
			label: "Upload",
			href: "/upload",
		},
		{
			label: "Dashboard",
			href: "/dashboard",
		},
		{
			label: "Settings",
			href: "/settings",
		},
	];

	return (
		<div className="w-full bg-background/90 backdrop-blur-sm border-b border-border/40 sticky top-0 z-50">
			<Container className="flex h-16 items-center justify-between">
				{/* Logo */}
				<div className="flex items-center">
					<Link to="/" className="flex items-center space-x-2">
						<div className="flex h-8 w-8 items-center justify-center rounded-md bg-primary">
							<Home className="h-4 w-4 text-primary-foreground" />
						</div>
						<span className="font-bold text-lg tracking-tight">UrcKe</span>
					</Link>
				</div>

				{/* Desktop navigation */}
				<nav className="hidden md:flex">
					<ul className="flex items-center space-x-8">
						{navItems.map((item) => (
							<li key={item.href}>
								<Link
									to={item.href}
									className={
										"text-sm font-medium transition-colors hover:text-primary"
									}
									activeOptions={{ exact: true }}
									activeProps={{
										className: "text-primary font-semibold",
									}}
								>
									{item.label}
								</Link>
							</li>
						))}
					</ul>
				</nav>

				{/* Actions */}
				<div className="flex items-center space-x-4">
					<Button variant="ghost" size="icon" className="hidden md:flex">
						<Search className="h-4 w-4" />
						<span className="sr-only">Search</span>
					</Button>
					<Button className="hidden md:flex">Get Started</Button>

					{/* Mobile menu button */}
					<Button
						variant="ghost"
						size="icon"
						className="md:hidden"
						onClick={() => setIsMenuOpen(!isMenuOpen)}
					>
						{isMenuOpen ? (
							<X className="h-5 w-5" />
						) : (
							<Menu className="h-5 w-5" />
						)}
						<span className="sr-only">Toggle menu</span>
					</Button>
				</div>
			</Container>

			{/* Mobile navigation */}
			<div
				className={cn(
					"md:hidden border-t border-border/40",
					isMenuOpen ? "block" : "hidden",
				)}
			>
				<Container className="py-4 px-6 space-y-4">
					<nav>
						<ul className="space-y-4">
							{navItems.map((item) => (
								<li key={item.href}>
									<Link
										to={item.href}
										className="block text-sm font-medium transition-colors hover:text-primary"
										activeOptions={{ exact: true }}
										activeProps={{
											className: "text-primary font-semibold",
										}}
										onClick={() => setIsMenuOpen(false)}
									>
										{item.label}
									</Link>
								</li>
							))}
						</ul>
					</nav>
					<div className="flex flex-col space-y-3 pt-3 border-t border-border/40">
						<Button
							variant="outline"
							className="w-full justify-start"
							size="sm"
						>
							<Search className="mr-2 h-4 w-4" />
							Search
						</Button>
						<Button className="w-full" size="sm">
							Get Started
						</Button>
					</div>
				</Container>
			</div>
		</div>
	);
}
</file>

<file path="apps/web/src/components/ui/accordion.tsx">
import * as AccordionPrimitive from "@radix-ui/react-accordion";
import { ChevronDownIcon } from "lucide-react";
import type * as React from "react";

import { cn } from "@/lib/utils";

function Accordion({
	...props
}: React.ComponentProps<typeof AccordionPrimitive.Root>) {
	return <AccordionPrimitive.Root data-slot="accordion" {...props} />;
}

function AccordionItem({
	className,
	...props
}: React.ComponentProps<typeof AccordionPrimitive.Item>) {
	return (
		<AccordionPrimitive.Item
			data-slot="accordion-item"
			className={cn("border-b last:border-b-0", className)}
			{...props}
		/>
	);
}

function AccordionTrigger({
	className,
	children,
	...props
}: React.ComponentProps<typeof AccordionPrimitive.Trigger>) {
	return (
		<AccordionPrimitive.Header className="flex">
			<AccordionPrimitive.Trigger
				data-slot="accordion-trigger"
				className={cn(
					"focus-visible:border-ring focus-visible:ring-ring/50 flex flex-1 items-start justify-between gap-4 rounded-md py-4 text-left text-sm font-medium transition-all outline-none hover:underline focus-visible:ring-[3px] disabled:pointer-events-none disabled:opacity-50 [&[data-state=open]>svg]:rotate-180",
					className,
				)}
				{...props}
			>
				{children}
				<ChevronDownIcon className="text-muted-foreground pointer-events-none size-4 shrink-0 translate-y-0.5 transition-transform duration-200" />
			</AccordionPrimitive.Trigger>
		</AccordionPrimitive.Header>
	);
}

function AccordionContent({
	className,
	children,
	...props
}: React.ComponentProps<typeof AccordionPrimitive.Content>) {
	return (
		<AccordionPrimitive.Content
			data-slot="accordion-content"
			className="data-[state=closed]:animate-accordion-up data-[state=open]:animate-accordion-down overflow-hidden text-sm"
			{...props}
		>
			<div className={cn("pt-0 pb-4", className)}>{children}</div>
		</AccordionPrimitive.Content>
	);
}

export { Accordion, AccordionItem, AccordionTrigger, AccordionContent };
</file>

<file path="apps/web/src/components/ui/alert-dialog.tsx">
"use client";

import * as AlertDialogPrimitive from "@radix-ui/react-alert-dialog";
import type * as React from "react";

import { buttonVariants } from "@/components/ui/button";
import { cn } from "@/lib/utils";

function AlertDialog({
	...props
}: React.ComponentProps<typeof AlertDialogPrimitive.Root>) {
	return <AlertDialogPrimitive.Root data-slot="alert-dialog" {...props} />;
}

function AlertDialogTrigger({
	...props
}: React.ComponentProps<typeof AlertDialogPrimitive.Trigger>) {
	return (
		<AlertDialogPrimitive.Trigger data-slot="alert-dialog-trigger" {...props} />
	);
}

function AlertDialogPortal({
	...props
}: React.ComponentProps<typeof AlertDialogPrimitive.Portal>) {
	return (
		<AlertDialogPrimitive.Portal data-slot="alert-dialog-portal" {...props} />
	);
}

function AlertDialogOverlay({
	className,
	...props
}: React.ComponentProps<typeof AlertDialogPrimitive.Overlay>) {
	return (
		<AlertDialogPrimitive.Overlay
			data-slot="alert-dialog-overlay"
			className={cn(
				"data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 fixed inset-0 z-50 bg-black/50",
				className,
			)}
			{...props}
		/>
	);
}

function AlertDialogContent({
	className,
	...props
}: React.ComponentProps<typeof AlertDialogPrimitive.Content>) {
	return (
		<AlertDialogPortal>
			<AlertDialogOverlay />
			<AlertDialogPrimitive.Content
				data-slot="alert-dialog-content"
				className={cn(
					"bg-background data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 fixed top-[50%] left-[50%] z-50 grid w-full max-w-[calc(100%-2rem)] translate-x-[-50%] translate-y-[-50%] gap-4 rounded-lg border p-6 shadow-lg duration-200 sm:max-w-lg",
					className,
				)}
				{...props}
			/>
		</AlertDialogPortal>
	);
}

function AlertDialogHeader({
	className,
	...props
}: React.ComponentProps<"div">) {
	return (
		<div
			data-slot="alert-dialog-header"
			className={cn("flex flex-col gap-2 text-center sm:text-left", className)}
			{...props}
		/>
	);
}

function AlertDialogFooter({
	className,
	...props
}: React.ComponentProps<"div">) {
	return (
		<div
			data-slot="alert-dialog-footer"
			className={cn(
				"flex flex-col-reverse gap-2 sm:flex-row sm:justify-end",
				className,
			)}
			{...props}
		/>
	);
}

function AlertDialogTitle({
	className,
	...props
}: React.ComponentProps<typeof AlertDialogPrimitive.Title>) {
	return (
		<AlertDialogPrimitive.Title
			data-slot="alert-dialog-title"
			className={cn("text-lg font-semibold", className)}
			{...props}
		/>
	);
}

function AlertDialogDescription({
	className,
	...props
}: React.ComponentProps<typeof AlertDialogPrimitive.Description>) {
	return (
		<AlertDialogPrimitive.Description
			data-slot="alert-dialog-description"
			className={cn("text-muted-foreground text-sm", className)}
			{...props}
		/>
	);
}

function AlertDialogAction({
	className,
	...props
}: React.ComponentProps<typeof AlertDialogPrimitive.Action>) {
	return (
		<AlertDialogPrimitive.Action
			className={cn(buttonVariants(), className)}
			{...props}
		/>
	);
}

function AlertDialogCancel({
	className,
	...props
}: React.ComponentProps<typeof AlertDialogPrimitive.Cancel>) {
	return (
		<AlertDialogPrimitive.Cancel
			className={cn(buttonVariants({ variant: "outline" }), className)}
			{...props}
		/>
	);
}

export {
	AlertDialog,
	AlertDialogPortal,
	AlertDialogOverlay,
	AlertDialogTrigger,
	AlertDialogContent,
	AlertDialogHeader,
	AlertDialogFooter,
	AlertDialogTitle,
	AlertDialogDescription,
	AlertDialogAction,
	AlertDialogCancel,
};
</file>

<file path="apps/web/src/components/ui/alert.tsx">
import { type VariantProps, cva } from "class-variance-authority";
import type * as React from "react";

import { cn } from "@/lib/utils";

const alertVariants = cva(
	"relative w-full rounded-lg border px-4 py-3 text-sm grid has-[>svg]:grid-cols-[calc(var(--spacing)*4)_1fr] grid-cols-[0_1fr] has-[>svg]:gap-x-3 gap-y-0.5 items-start [&>svg]:size-4 [&>svg]:translate-y-0.5 [&>svg]:text-current",
	{
		variants: {
			variant: {
				default: "bg-card text-card-foreground",
				destructive:
					"text-destructive bg-card [&>svg]:text-current *:data-[slot=alert-description]:text-destructive/90",
			},
		},
		defaultVariants: {
			variant: "default",
		},
	},
);

function Alert({
	className,
	variant,
	...props
}: React.ComponentProps<"div"> & VariantProps<typeof alertVariants>) {
	return (
		<div
			data-slot="alert"
			role="alert"
			className={cn(alertVariants({ variant }), className)}
			{...props}
		/>
	);
}

function AlertTitle({ className, ...props }: React.ComponentProps<"div">) {
	return (
		<div
			data-slot="alert-title"
			className={cn(
				"col-start-2 line-clamp-1 min-h-4 font-medium tracking-tight",
				className,
			)}
			{...props}
		/>
	);
}

function AlertDescription({
	className,
	...props
}: React.ComponentProps<"div">) {
	return (
		<div
			data-slot="alert-description"
			className={cn(
				"text-muted-foreground col-start-2 grid justify-items-start gap-1 text-sm [&_p]:leading-relaxed",
				className,
			)}
			{...props}
		/>
	);
}

export { Alert, AlertTitle, AlertDescription };
</file>

<file path="apps/web/src/components/ui/aspect-ratio.tsx">
import * as AspectRatioPrimitive from "@radix-ui/react-aspect-ratio";

function AspectRatio({
	...props
}: React.ComponentProps<typeof AspectRatioPrimitive.Root>) {
	return <AspectRatioPrimitive.Root data-slot="aspect-ratio" {...props} />;
}

export { AspectRatio };
</file>

<file path="apps/web/src/components/ui/avatar.tsx">
"use client";

import * as AvatarPrimitive from "@radix-ui/react-avatar";
import type * as React from "react";

import { cn } from "@/lib/utils";

function Avatar({
	className,
	...props
}: React.ComponentProps<typeof AvatarPrimitive.Root>) {
	return (
		<AvatarPrimitive.Root
			data-slot="avatar"
			className={cn(
				"relative flex size-8 shrink-0 overflow-hidden rounded-full",
				className,
			)}
			{...props}
		/>
	);
}

function AvatarImage({
	className,
	...props
}: React.ComponentProps<typeof AvatarPrimitive.Image>) {
	return (
		<AvatarPrimitive.Image
			data-slot="avatar-image"
			className={cn("aspect-square size-full", className)}
			{...props}
		/>
	);
}

function AvatarFallback({
	className,
	...props
}: React.ComponentProps<typeof AvatarPrimitive.Fallback>) {
	return (
		<AvatarPrimitive.Fallback
			data-slot="avatar-fallback"
			className={cn(
				"bg-muted flex size-full items-center justify-center rounded-full",
				className,
			)}
			{...props}
		/>
	);
}

export { Avatar, AvatarImage, AvatarFallback };
</file>

<file path="apps/web/src/components/ui/badge.tsx">
import { Slot } from "@radix-ui/react-slot";
import { type VariantProps, cva } from "class-variance-authority";
import type * as React from "react";

import { cn } from "@/lib/utils";

const badgeVariants = cva(
	"inline-flex items-center justify-center rounded-md border px-2 py-0.5 text-xs font-medium w-fit whitespace-nowrap shrink-0 [&>svg]:size-3 gap-1 [&>svg]:pointer-events-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive transition-[color,box-shadow] overflow-hidden",
	{
		variants: {
			variant: {
				default:
					"border-transparent bg-primary text-primary-foreground [a&]:hover:bg-primary/90",
				secondary:
					"border-transparent bg-secondary text-secondary-foreground [a&]:hover:bg-secondary/90",
				destructive:
					"border-transparent bg-destructive text-white [a&]:hover:bg-destructive/90 focus-visible:ring-destructive/20 dark:focus-visible:ring-destructive/40 dark:bg-destructive/60",
				outline:
					"text-foreground [a&]:hover:bg-accent [a&]:hover:text-accent-foreground",
			},
		},
		defaultVariants: {
			variant: "default",
		},
	},
);

function Badge({
	className,
	variant,
	asChild = false,
	...props
}: React.ComponentProps<"span"> &
	VariantProps<typeof badgeVariants> & { asChild?: boolean }) {
	const Comp = asChild ? Slot : "span";

	return (
		<Comp
			data-slot="badge"
			className={cn(badgeVariants({ variant }), className)}
			{...props}
		/>
	);
}

export { Badge, badgeVariants };
</file>

<file path="apps/web/src/components/ui/breadcrumb.tsx">
import { Slot } from "@radix-ui/react-slot";
import { ChevronRight, MoreHorizontal } from "lucide-react";
import type * as React from "react";

import { cn } from "@/lib/utils";

function Breadcrumb({ ...props }: React.ComponentProps<"nav">) {
	return <nav aria-label="breadcrumb" data-slot="breadcrumb" {...props} />;
}

function BreadcrumbList({ className, ...props }: React.ComponentProps<"ol">) {
	return (
		<ol
			data-slot="breadcrumb-list"
			className={cn(
				"text-muted-foreground flex flex-wrap items-center gap-1.5 text-sm break-words sm:gap-2.5",
				className,
			)}
			{...props}
		/>
	);
}

function BreadcrumbItem({ className, ...props }: React.ComponentProps<"li">) {
	return (
		<li
			data-slot="breadcrumb-item"
			className={cn("inline-flex items-center gap-1.5", className)}
			{...props}
		/>
	);
}

function BreadcrumbLink({
	asChild,
	className,
	...props
}: React.ComponentProps<"a"> & {
	asChild?: boolean;
}) {
	const Comp = asChild ? Slot : "a";

	return (
		<Comp
			data-slot="breadcrumb-link"
			className={cn("hover:text-foreground transition-colors", className)}
			{...props}
		/>
	);
}

function BreadcrumbPage({ className, ...props }: React.ComponentProps<"span">) {
	return (
		<span
			data-slot="breadcrumb-page"
			role="link"
			aria-disabled="true"
			aria-current="page"
			className={cn("text-foreground font-normal", className)}
			{...props}
		/>
	);
}

function BreadcrumbSeparator({
	children,
	className,
	...props
}: React.ComponentProps<"li">) {
	return (
		<li
			data-slot="breadcrumb-separator"
			role="presentation"
			aria-hidden="true"
			className={cn("[&>svg]:size-3.5", className)}
			{...props}
		>
			{children ?? <ChevronRight />}
		</li>
	);
}

function BreadcrumbEllipsis({
	className,
	...props
}: React.ComponentProps<"span">) {
	return (
		<span
			data-slot="breadcrumb-ellipsis"
			role="presentation"
			aria-hidden="true"
			className={cn("flex size-9 items-center justify-center", className)}
			{...props}
		>
			<MoreHorizontal className="size-4" />
			<span className="sr-only">More</span>
		</span>
	);
}

export {
	Breadcrumb,
	BreadcrumbList,
	BreadcrumbItem,
	BreadcrumbLink,
	BreadcrumbPage,
	BreadcrumbSeparator,
	BreadcrumbEllipsis,
};
</file>

<file path="apps/web/src/components/ui/button.tsx">
import { Slot } from "@radix-ui/react-slot";
import { type VariantProps, cva } from "class-variance-authority";
import type * as React from "react";

import { cn } from "@/lib/utils";

const buttonVariants = cva(
	"inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive",
	{
		variants: {
			variant: {
				default:
					"bg-primary text-primary-foreground shadow-xs hover:bg-primary/90",
				destructive:
					"bg-destructive text-white shadow-xs hover:bg-destructive/90 focus-visible:ring-destructive/20 dark:focus-visible:ring-destructive/40 dark:bg-destructive/60",
				outline:
					"border bg-background shadow-xs hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50",
				secondary:
					"bg-secondary text-secondary-foreground shadow-xs hover:bg-secondary/80",
				ghost:
					"hover:bg-accent hover:text-accent-foreground dark:hover:bg-accent/50",
				link: "text-primary underline-offset-4 hover:underline",
			},
			size: {
				default: "h-9 px-4 py-2 has-[>svg]:px-3",
				sm: "h-8 rounded-md gap-1.5 px-3 has-[>svg]:px-2.5",
				lg: "h-10 rounded-md px-6 has-[>svg]:px-4",
				icon: "size-9",
			},
		},
		defaultVariants: {
			variant: "default",
			size: "default",
		},
	},
);

function Button({
	className,
	variant,
	size,
	asChild = false,
	...props
}: React.ComponentProps<"button"> &
	VariantProps<typeof buttonVariants> & {
		asChild?: boolean;
	}) {
	const Comp = asChild ? Slot : "button";

	return (
		<Comp
			data-slot="button"
			className={cn(buttonVariants({ variant, size, className }))}
			{...props}
		/>
	);
}

export { Button, buttonVariants };
</file>

<file path="apps/web/src/components/ui/calendar.tsx">
import { ChevronLeft, ChevronRight } from "lucide-react";
import type * as React from "react";
import { DayPicker } from "react-day-picker";

import { buttonVariants } from "@/components/ui/button";
import { cn } from "@/lib/utils";

function Calendar({
	className,
	classNames,
	showOutsideDays = true,
	...props
}: React.ComponentProps<typeof DayPicker>) {
	return (
		<DayPicker
			showOutsideDays={showOutsideDays}
			className={cn("p-3", className)}
			classNames={{
				months: "flex flex-col sm:flex-row gap-2",
				month: "flex flex-col gap-4",
				caption: "flex justify-center pt-1 relative items-center w-full",
				caption_label: "text-sm font-medium",
				nav: "flex items-center gap-1",
				nav_button: cn(
					buttonVariants({ variant: "outline" }),
					"size-7 bg-transparent p-0 opacity-50 hover:opacity-100",
				),
				nav_button_previous: "absolute left-1",
				nav_button_next: "absolute right-1",
				table: "w-full border-collapse space-x-1",
				head_row: "flex",
				head_cell:
					"text-muted-foreground rounded-md w-8 font-normal text-[0.8rem]",
				row: "flex w-full mt-2",
				cell: cn(
					"relative p-0 text-center text-sm focus-within:relative focus-within:z-20 [&:has([aria-selected])]:bg-accent [&:has([aria-selected].day-range-end)]:rounded-r-md",
					props.mode === "range"
						? "[&:has(>.day-range-end)]:rounded-r-md [&:has(>.day-range-start)]:rounded-l-md first:[&:has([aria-selected])]:rounded-l-md last:[&:has([aria-selected])]:rounded-r-md"
						: "[&:has([aria-selected])]:rounded-md",
				),
				day: cn(
					buttonVariants({ variant: "ghost" }),
					"size-8 p-0 font-normal aria-selected:opacity-100",
				),
				day_range_start:
					"day-range-start aria-selected:bg-primary aria-selected:text-primary-foreground",
				day_range_end:
					"day-range-end aria-selected:bg-primary aria-selected:text-primary-foreground",
				day_selected:
					"bg-primary text-primary-foreground hover:bg-primary hover:text-primary-foreground focus:bg-primary focus:text-primary-foreground",
				day_today: "bg-accent text-accent-foreground",
				day_outside:
					"day-outside text-muted-foreground aria-selected:text-muted-foreground",
				day_disabled: "text-muted-foreground opacity-50",
				day_range_middle:
					"aria-selected:bg-accent aria-selected:text-accent-foreground",
				day_hidden: "invisible",
				...classNames,
			}}
			components={{
				IconLeft: ({ className, ...props }) => (
					<ChevronLeft className={cn("size-4", className)} {...props} />
				),
				IconRight: ({ className, ...props }) => (
					<ChevronRight className={cn("size-4", className)} {...props} />
				),
			}}
			{...props}
		/>
	);
}

export { Calendar };
</file>

<file path="apps/web/src/components/ui/card.tsx">
import type * as React from "react";

import { cn } from "@/lib/utils";

function Card({ className, ...props }: React.ComponentProps<"div">) {
	return (
		<div
			data-slot="card"
			className={cn(
				"bg-card text-card-foreground flex flex-col gap-6 rounded-xl border py-6 shadow-sm",
				className,
			)}
			{...props}
		/>
	);
}

function CardHeader({ className, ...props }: React.ComponentProps<"div">) {
	return (
		<div
			data-slot="card-header"
			className={cn(
				"@container/card-header grid auto-rows-min grid-rows-[auto_auto] items-start gap-1.5 px-6 has-data-[slot=card-action]:grid-cols-[1fr_auto] [.border-b]:pb-6",
				className,
			)}
			{...props}
		/>
	);
}

function CardTitle({ className, ...props }: React.ComponentProps<"div">) {
	return (
		<div
			data-slot="card-title"
			className={cn("leading-none font-semibold", className)}
			{...props}
		/>
	);
}

function CardDescription({ className, ...props }: React.ComponentProps<"div">) {
	return (
		<div
			data-slot="card-description"
			className={cn("text-muted-foreground text-sm", className)}
			{...props}
		/>
	);
}

function CardAction({ className, ...props }: React.ComponentProps<"div">) {
	return (
		<div
			data-slot="card-action"
			className={cn(
				"col-start-2 row-span-2 row-start-1 self-start justify-self-end",
				className,
			)}
			{...props}
		/>
	);
}

function CardContent({ className, ...props }: React.ComponentProps<"div">) {
	return (
		<div
			data-slot="card-content"
			className={cn("px-6", className)}
			{...props}
		/>
	);
}

function CardFooter({ className, ...props }: React.ComponentProps<"div">) {
	return (
		<div
			data-slot="card-footer"
			className={cn("flex items-center px-6 [.border-t]:pt-6", className)}
			{...props}
		/>
	);
}

export {
	Card,
	CardHeader,
	CardFooter,
	CardTitle,
	CardAction,
	CardDescription,
	CardContent,
};
</file>

<file path="apps/web/src/components/ui/carousel.tsx">
"use client";

import useEmblaCarousel, {
	type UseEmblaCarouselType,
} from "embla-carousel-react";
import { ArrowLeft, ArrowRight } from "lucide-react";
import * as React from "react";

import { Button } from "@/components/ui/button";
import { cn } from "@/lib/utils";

type CarouselApi = UseEmblaCarouselType[1];
type UseCarouselParameters = Parameters<typeof useEmblaCarousel>;
type CarouselOptions = UseCarouselParameters[0];
type CarouselPlugin = UseCarouselParameters[1];

type CarouselProps = {
	opts?: CarouselOptions;
	plugins?: CarouselPlugin;
	orientation?: "horizontal" | "vertical";
	setApi?: (api: CarouselApi) => void;
};

type CarouselContextProps = {
	carouselRef: ReturnType<typeof useEmblaCarousel>[0];
	api: ReturnType<typeof useEmblaCarousel>[1];
	scrollPrev: () => void;
	scrollNext: () => void;
	canScrollPrev: boolean;
	canScrollNext: boolean;
} & CarouselProps;

const CarouselContext = React.createContext<CarouselContextProps | null>(null);

function useCarousel() {
	const context = React.useContext(CarouselContext);

	if (!context) {
		throw new Error("useCarousel must be used within a <Carousel />");
	}

	return context;
}

function Carousel({
	orientation = "horizontal",
	opts,
	setApi,
	plugins,
	className,
	children,
	...props
}: React.ComponentProps<"div"> & CarouselProps) {
	const [carouselRef, api] = useEmblaCarousel(
		{
			...opts,
			axis: orientation === "horizontal" ? "x" : "y",
		},
		plugins,
	);
	const [canScrollPrev, setCanScrollPrev] = React.useState(false);
	const [canScrollNext, setCanScrollNext] = React.useState(false);

	const onSelect = React.useCallback((api: CarouselApi) => {
		if (!api) return;
		setCanScrollPrev(api.canScrollPrev());
		setCanScrollNext(api.canScrollNext());
	}, []);

	const scrollPrev = React.useCallback(() => {
		api?.scrollPrev();
	}, [api]);

	const scrollNext = React.useCallback(() => {
		api?.scrollNext();
	}, [api]);

	const handleKeyDown = React.useCallback(
		(event: React.KeyboardEvent<HTMLDivElement>) => {
			if (event.key === "ArrowLeft") {
				event.preventDefault();
				scrollPrev();
			} else if (event.key === "ArrowRight") {
				event.preventDefault();
				scrollNext();
			}
		},
		[scrollPrev, scrollNext],
	);

	React.useEffect(() => {
		if (!api || !setApi) return;
		setApi(api);
	}, [api, setApi]);

	React.useEffect(() => {
		if (!api) return;
		onSelect(api);
		api.on("reInit", onSelect);
		api.on("select", onSelect);

		return () => {
			api?.off("select", onSelect);
		};
	}, [api, onSelect]);

	return (
		<CarouselContext.Provider
			value={{
				carouselRef,
				api: api,
				opts,
				orientation:
					orientation || (opts?.axis === "y" ? "vertical" : "horizontal"),
				scrollPrev,
				scrollNext,
				canScrollPrev,
				canScrollNext,
			}}
		>
			<div
				onKeyDownCapture={handleKeyDown}
				className={cn("relative", className)}
				role="region"
				aria-roledescription="carousel"
				data-slot="carousel"
				{...props}
			>
				{children}
			</div>
		</CarouselContext.Provider>
	);
}

function CarouselContent({ className, ...props }: React.ComponentProps<"div">) {
	const { carouselRef, orientation } = useCarousel();

	return (
		<div
			ref={carouselRef}
			className="overflow-hidden"
			data-slot="carousel-content"
		>
			<div
				className={cn(
					"flex",
					orientation === "horizontal" ? "-ml-4" : "-mt-4 flex-col",
					className,
				)}
				{...props}
			/>
		</div>
	);
}

function CarouselItem({ className, ...props }: React.ComponentProps<"div">) {
	const { orientation } = useCarousel();

	return (
		<div
			role="group"
			aria-roledescription="slide"
			data-slot="carousel-item"
			className={cn(
				"min-w-0 shrink-0 grow-0 basis-full",
				orientation === "horizontal" ? "pl-4" : "pt-4",
				className,
			)}
			{...props}
		/>
	);
}

function CarouselPrevious({
	className,
	variant = "outline",
	size = "icon",
	...props
}: React.ComponentProps<typeof Button>) {
	const { orientation, scrollPrev, canScrollPrev } = useCarousel();

	return (
		<Button
			data-slot="carousel-previous"
			variant={variant}
			size={size}
			className={cn(
				"absolute size-8 rounded-full",
				orientation === "horizontal"
					? "top-1/2 -left-12 -translate-y-1/2"
					: "-top-12 left-1/2 -translate-x-1/2 rotate-90",
				className,
			)}
			disabled={!canScrollPrev}
			onClick={scrollPrev}
			{...props}
		>
			<ArrowLeft />
			<span className="sr-only">Previous slide</span>
		</Button>
	);
}

function CarouselNext({
	className,
	variant = "outline",
	size = "icon",
	...props
}: React.ComponentProps<typeof Button>) {
	const { orientation, scrollNext, canScrollNext } = useCarousel();

	return (
		<Button
			data-slot="carousel-next"
			variant={variant}
			size={size}
			className={cn(
				"absolute size-8 rounded-full",
				orientation === "horizontal"
					? "top-1/2 -right-12 -translate-y-1/2"
					: "-bottom-12 left-1/2 -translate-x-1/2 rotate-90",
				className,
			)}
			disabled={!canScrollNext}
			onClick={scrollNext}
			{...props}
		>
			<ArrowRight />
			<span className="sr-only">Next slide</span>
		</Button>
	);
}

export {
	type CarouselApi,
	Carousel,
	CarouselContent,
	CarouselItem,
	CarouselPrevious,
	CarouselNext,
};
</file>

<file path="apps/web/src/components/ui/chart.tsx">
import * as React from "react";
import * as RechartsPrimitive from "recharts";

import { cn } from "@/lib/utils";

// Format: { THEME_NAME: CSS_SELECTOR }
const THEMES = { light: "", dark: ".dark" } as const;

export type ChartConfig = {
	[k in string]: {
		label?: React.ReactNode;
		icon?: React.ComponentType;
	} & (
		| { color?: string; theme?: never }
		| { color?: never; theme: Record<keyof typeof THEMES, string> }
	);
};

type ChartContextProps = {
	config: ChartConfig;
};

const ChartContext = React.createContext<ChartContextProps | null>(null);

function useChart() {
	const context = React.useContext(ChartContext);

	if (!context) {
		throw new Error("useChart must be used within a <ChartContainer />");
	}

	return context;
}

function ChartContainer({
	id,
	className,
	children,
	config,
	...props
}: React.ComponentProps<"div"> & {
	config: ChartConfig;
	children: React.ComponentProps<
		typeof RechartsPrimitive.ResponsiveContainer
	>["children"];
}) {
	const uniqueId = React.useId();
	const chartId = `chart-${id || uniqueId.replace(/:/g, "")}`;

	return (
		<ChartContext.Provider value={{ config }}>
			<div
				data-slot="chart"
				data-chart={chartId}
				className={cn(
					"[&_.recharts-cartesian-axis-tick_text]:fill-muted-foreground [&_.recharts-cartesian-grid_line[stroke='#ccc']]:stroke-border/50 [&_.recharts-curve.recharts-tooltip-cursor]:stroke-border [&_.recharts-polar-grid_[stroke='#ccc']]:stroke-border [&_.recharts-radial-bar-background-sector]:fill-muted [&_.recharts-rectangle.recharts-tooltip-cursor]:fill-muted [&_.recharts-reference-line_[stroke='#ccc']]:stroke-border flex aspect-video justify-center text-xs [&_.recharts-dot[stroke='#fff']]:stroke-transparent [&_.recharts-layer]:outline-hidden [&_.recharts-sector]:outline-hidden [&_.recharts-sector[stroke='#fff']]:stroke-transparent [&_.recharts-surface]:outline-hidden",
					className,
				)}
				{...props}
			>
				<ChartStyle id={chartId} config={config} />
				<RechartsPrimitive.ResponsiveContainer>
					{children}
				</RechartsPrimitive.ResponsiveContainer>
			</div>
		</ChartContext.Provider>
	);
}

const ChartStyle = ({ id, config }: { id: string; config: ChartConfig }) => {
	const colorConfig = Object.entries(config).filter(
		([, config]) => config.theme || config.color,
	);

	if (!colorConfig.length) {
		return null;
	}

	return (
		<style
			dangerouslySetInnerHTML={{
				__html: Object.entries(THEMES)
					.map(
						([theme, prefix]) => `
${prefix} [data-chart=${id}] {
${colorConfig
	.map(([key, itemConfig]) => {
		const color =
			itemConfig.theme?.[theme as keyof typeof itemConfig.theme] ||
			itemConfig.color;
		return color ? `  --color-${key}: ${color};` : null;
	})
	.join("\n")}
}
`,
					)
					.join("\n"),
			}}
		/>
	);
};

const ChartTooltip = RechartsPrimitive.Tooltip;

function ChartTooltipContent({
	active,
	payload,
	className,
	indicator = "dot",
	hideLabel = false,
	hideIndicator = false,
	label,
	labelFormatter,
	labelClassName,
	formatter,
	color,
	nameKey,
	labelKey,
}: React.ComponentProps<typeof RechartsPrimitive.Tooltip> &
	React.ComponentProps<"div"> & {
		hideLabel?: boolean;
		hideIndicator?: boolean;
		indicator?: "line" | "dot" | "dashed";
		nameKey?: string;
		labelKey?: string;
	}) {
	const { config } = useChart();

	const tooltipLabel = React.useMemo(() => {
		if (hideLabel || !payload?.length) {
			return null;
		}

		const [item] = payload;
		const key = `${labelKey || item?.dataKey || item?.name || "value"}`;
		const itemConfig = getPayloadConfigFromPayload(config, item, key);
		const value =
			!labelKey && typeof label === "string"
				? config[label as keyof typeof config]?.label || label
				: itemConfig?.label;

		if (labelFormatter) {
			return (
				<div className={cn("font-medium", labelClassName)}>
					{labelFormatter(value, payload)}
				</div>
			);
		}

		if (!value) {
			return null;
		}

		return <div className={cn("font-medium", labelClassName)}>{value}</div>;
	}, [
		label,
		labelFormatter,
		payload,
		hideLabel,
		labelClassName,
		config,
		labelKey,
	]);

	if (!active || !payload?.length) {
		return null;
	}

	const nestLabel = payload.length === 1 && indicator !== "dot";

	return (
		<div
			className={cn(
				"border-border/50 bg-background grid min-w-[8rem] items-start gap-1.5 rounded-lg border px-2.5 py-1.5 text-xs shadow-xl",
				className,
			)}
		>
			{!nestLabel ? tooltipLabel : null}
			<div className="grid gap-1.5">
				{payload.map((item, index) => {
					const key = `${nameKey || item.name || item.dataKey || "value"}`;
					const itemConfig = getPayloadConfigFromPayload(config, item, key);
					const indicatorColor = color || item.payload.fill || item.color;

					return (
						<div
							key={item.dataKey}
							className={cn(
								"[&>svg]:text-muted-foreground flex w-full flex-wrap items-stretch gap-2 [&>svg]:h-2.5 [&>svg]:w-2.5",
								indicator === "dot" && "items-center",
							)}
						>
							{formatter && item?.value !== undefined && item.name ? (
								formatter(item.value, item.name, item, index, item.payload)
							) : (
								<>
									{itemConfig?.icon ? (
										<itemConfig.icon />
									) : (
										!hideIndicator && (
											<div
												className={cn(
													"shrink-0 rounded-[2px] border-(--color-border) bg-(--color-bg)",
													{
														"h-2.5 w-2.5": indicator === "dot",
														"w-1": indicator === "line",
														"w-0 border-[1.5px] border-dashed bg-transparent":
															indicator === "dashed",
														"my-0.5": nestLabel && indicator === "dashed",
													},
												)}
												style={
													{
														"--color-bg": indicatorColor,
														"--color-border": indicatorColor,
													} as React.CSSProperties
												}
											/>
										)
									)}
									<div
										className={cn(
											"flex flex-1 justify-between leading-none",
											nestLabel ? "items-end" : "items-center",
										)}
									>
										<div className="grid gap-1.5">
											{nestLabel ? tooltipLabel : null}
											<span className="text-muted-foreground">
												{itemConfig?.label || item.name}
											</span>
										</div>
										{item.value && (
											<span className="text-foreground font-mono font-medium tabular-nums">
												{item.value.toLocaleString()}
											</span>
										)}
									</div>
								</>
							)}
						</div>
					);
				})}
			</div>
		</div>
	);
}

const ChartLegend = RechartsPrimitive.Legend;

function ChartLegendContent({
	className,
	hideIcon = false,
	payload,
	verticalAlign = "bottom",
	nameKey,
}: React.ComponentProps<"div"> &
	Pick<RechartsPrimitive.LegendProps, "payload" | "verticalAlign"> & {
		hideIcon?: boolean;
		nameKey?: string;
	}) {
	const { config } = useChart();

	if (!payload?.length) {
		return null;
	}

	return (
		<div
			className={cn(
				"flex items-center justify-center gap-4",
				verticalAlign === "top" ? "pb-3" : "pt-3",
				className,
			)}
		>
			{payload.map((item) => {
				const key = `${nameKey || item.dataKey || "value"}`;
				const itemConfig = getPayloadConfigFromPayload(config, item, key);

				return (
					<div
						key={item.value}
						className={cn(
							"[&>svg]:text-muted-foreground flex items-center gap-1.5 [&>svg]:h-3 [&>svg]:w-3",
						)}
					>
						{itemConfig?.icon && !hideIcon ? (
							<itemConfig.icon />
						) : (
							<div
								className="h-2 w-2 shrink-0 rounded-[2px]"
								style={{
									backgroundColor: item.color,
								}}
							/>
						)}
						{itemConfig?.label}
					</div>
				);
			})}
		</div>
	);
}

// Helper to extract item config from a payload.
function getPayloadConfigFromPayload(
	config: ChartConfig,
	payload: unknown,
	key: string,
) {
	if (typeof payload !== "object" || payload === null) {
		return undefined;
	}

	const payloadPayload =
		"payload" in payload &&
		typeof payload.payload === "object" &&
		payload.payload !== null
			? payload.payload
			: undefined;

	let configLabelKey: string = key;

	if (
		key in payload &&
		typeof payload[key as keyof typeof payload] === "string"
	) {
		configLabelKey = payload[key as keyof typeof payload] as string;
	} else if (
		payloadPayload &&
		key in payloadPayload &&
		typeof payloadPayload[key as keyof typeof payloadPayload] === "string"
	) {
		configLabelKey = payloadPayload[
			key as keyof typeof payloadPayload
		] as string;
	}

	return configLabelKey in config
		? config[configLabelKey]
		: config[key as keyof typeof config];
}

export {
	ChartContainer,
	ChartTooltip,
	ChartTooltipContent,
	ChartLegend,
	ChartLegendContent,
	ChartStyle,
};
</file>

<file path="apps/web/src/components/ui/checkbox.tsx">
"use client";

import * as CheckboxPrimitive from "@radix-ui/react-checkbox";
import { CheckIcon } from "lucide-react";
import type * as React from "react";

import { cn } from "@/lib/utils";

function Checkbox({
	className,
	...props
}: React.ComponentProps<typeof CheckboxPrimitive.Root>) {
	return (
		<CheckboxPrimitive.Root
			data-slot="checkbox"
			className={cn(
				"peer border-input dark:bg-input/30 data-[state=checked]:bg-primary data-[state=checked]:text-primary-foreground dark:data-[state=checked]:bg-primary data-[state=checked]:border-primary focus-visible:border-ring focus-visible:ring-ring/50 aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive size-4 shrink-0 rounded-[4px] border shadow-xs transition-shadow outline-none focus-visible:ring-[3px] disabled:cursor-not-allowed disabled:opacity-50",
				className,
			)}
			{...props}
		>
			<CheckboxPrimitive.Indicator
				data-slot="checkbox-indicator"
				className="flex items-center justify-center text-current transition-none"
			>
				<CheckIcon className="size-3.5" />
			</CheckboxPrimitive.Indicator>
		</CheckboxPrimitive.Root>
	);
}

export { Checkbox };
</file>

<file path="apps/web/src/components/ui/collapsible.tsx">
import * as CollapsiblePrimitive from "@radix-ui/react-collapsible";

function Collapsible({
	...props
}: React.ComponentProps<typeof CollapsiblePrimitive.Root>) {
	return <CollapsiblePrimitive.Root data-slot="collapsible" {...props} />;
}

function CollapsibleTrigger({
	...props
}: React.ComponentProps<typeof CollapsiblePrimitive.CollapsibleTrigger>) {
	return (
		<CollapsiblePrimitive.CollapsibleTrigger
			data-slot="collapsible-trigger"
			{...props}
		/>
	);
}

function CollapsibleContent({
	...props
}: React.ComponentProps<typeof CollapsiblePrimitive.CollapsibleContent>) {
	return (
		<CollapsiblePrimitive.CollapsibleContent
			data-slot="collapsible-content"
			{...props}
		/>
	);
}

export { Collapsible, CollapsibleTrigger, CollapsibleContent };
</file>

<file path="apps/web/src/components/ui/combo-box.tsx">
"use client";

import { Check, ChevronsUpDown } from "lucide-react";
import * as React from "react";

import { Button } from "@/components/ui/button";
import {
	Command,
	CommandEmpty,
	CommandGroup,
	CommandInput,
	CommandItem,
	CommandList,
} from "@/components/ui/command";
import {
	Popover,
	PopoverContent,
	PopoverTrigger,
} from "@/components/ui/popover";
import { cn } from "@/lib/utils";

const frameworks = [
	{
		value: "next.js",
		label: "Next.js",
	},
	{
		value: "sveltekit",
		label: "SvelteKit",
	},
	{
		value: "nuxt.js",
		label: "Nuxt.js",
	},
	{
		value: "remix",
		label: "Remix",
	},
	{
		value: "astro",
		label: "Astro",
	},
];

export function ComboboxDemo() {
	const [open, setOpen] = React.useState(false);
	const [value, setValue] = React.useState("");

	return (
		<Popover open={open} onOpenChange={setOpen}>
			<PopoverTrigger asChild>
				<Button
					variant="outline"
					role="combobox"
					aria-expanded={open}
					className="w-[200px] justify-between"
				>
					{value
						? frameworks.find((framework) => framework.value === value)?.label
						: "Select framework..."}
					<ChevronsUpDown className="opacity-50" />
				</Button>
			</PopoverTrigger>
			<PopoverContent className="w-[200px] p-0">
				<Command>
					<CommandInput placeholder="Search framework..." className="h-9" />
					<CommandList>
						<CommandEmpty>No framework found.</CommandEmpty>
						<CommandGroup>
							{frameworks.map((framework) => (
								<CommandItem
									key={framework.value}
									value={framework.value}
									onSelect={(currentValue) => {
										setValue(currentValue === value ? "" : currentValue);
										setOpen(false);
									}}
								>
									{framework.label}
									<Check
										className={cn(
											"ml-auto",
											value === framework.value ? "opacity-100" : "opacity-0",
										)}
									/>
								</CommandItem>
							))}
						</CommandGroup>
					</CommandList>
				</Command>
			</PopoverContent>
		</Popover>
	);
}
</file>

<file path="apps/web/src/components/ui/command.tsx">
"use client";

import { Command as CommandPrimitive } from "cmdk";
import { SearchIcon } from "lucide-react";
import type * as React from "react";

import {
	Dialog,
	DialogContent,
	DialogDescription,
	DialogHeader,
	DialogTitle,
} from "@/components/ui/dialog";
import { cn } from "@/lib/utils";

function Command({
	className,
	...props
}: React.ComponentProps<typeof CommandPrimitive>) {
	return (
		<CommandPrimitive
			data-slot="command"
			className={cn(
				"bg-popover text-popover-foreground flex h-full w-full flex-col overflow-hidden rounded-md",
				className,
			)}
			{...props}
		/>
	);
}

function CommandDialog({
	title = "Command Palette",
	description = "Search for a command to run...",
	children,
	...props
}: React.ComponentProps<typeof Dialog> & {
	title?: string;
	description?: string;
}) {
	return (
		<Dialog {...props}>
			<DialogHeader className="sr-only">
				<DialogTitle>{title}</DialogTitle>
				<DialogDescription>{description}</DialogDescription>
			</DialogHeader>
			<DialogContent className="overflow-hidden p-0">
				<Command className="[&_[cmdk-group-heading]]:text-muted-foreground **:data-[slot=command-input-wrapper]:h-12 [&_[cmdk-group-heading]]:px-2 [&_[cmdk-group-heading]]:font-medium [&_[cmdk-group]]:px-2 [&_[cmdk-group]:not([hidden])_~[cmdk-group]]:pt-0 [&_[cmdk-input-wrapper]_svg]:h-5 [&_[cmdk-input-wrapper]_svg]:w-5 [&_[cmdk-input]]:h-12 [&_[cmdk-item]]:px-2 [&_[cmdk-item]]:py-3 [&_[cmdk-item]_svg]:h-5 [&_[cmdk-item]_svg]:w-5">
					{children}
				</Command>
			</DialogContent>
		</Dialog>
	);
}

function CommandInput({
	className,
	...props
}: React.ComponentProps<typeof CommandPrimitive.Input>) {
	return (
		<div
			data-slot="command-input-wrapper"
			className="flex h-9 items-center gap-2 border-b px-3"
		>
			<SearchIcon className="size-4 shrink-0 opacity-50" />
			<CommandPrimitive.Input
				data-slot="command-input"
				className={cn(
					"placeholder:text-muted-foreground flex h-10 w-full rounded-md bg-transparent py-3 text-sm outline-hidden disabled:cursor-not-allowed disabled:opacity-50",
					className,
				)}
				{...props}
			/>
		</div>
	);
}

function CommandList({
	className,
	...props
}: React.ComponentProps<typeof CommandPrimitive.List>) {
	return (
		<CommandPrimitive.List
			data-slot="command-list"
			className={cn(
				"max-h-[300px] scroll-py-1 overflow-x-hidden overflow-y-auto",
				className,
			)}
			{...props}
		/>
	);
}

function CommandEmpty({
	...props
}: React.ComponentProps<typeof CommandPrimitive.Empty>) {
	return (
		<CommandPrimitive.Empty
			data-slot="command-empty"
			className="py-6 text-center text-sm"
			{...props}
		/>
	);
}

function CommandGroup({
	className,
	...props
}: React.ComponentProps<typeof CommandPrimitive.Group>) {
	return (
		<CommandPrimitive.Group
			data-slot="command-group"
			className={cn(
				"text-foreground [&_[cmdk-group-heading]]:text-muted-foreground overflow-hidden p-1 [&_[cmdk-group-heading]]:px-2 [&_[cmdk-group-heading]]:py-1.5 [&_[cmdk-group-heading]]:text-xs [&_[cmdk-group-heading]]:font-medium",
				className,
			)}
			{...props}
		/>
	);
}

function CommandSeparator({
	className,
	...props
}: React.ComponentProps<typeof CommandPrimitive.Separator>) {
	return (
		<CommandPrimitive.Separator
			data-slot="command-separator"
			className={cn("bg-border -mx-1 h-px", className)}
			{...props}
		/>
	);
}

function CommandItem({
	className,
	...props
}: React.ComponentProps<typeof CommandPrimitive.Item>) {
	return (
		<CommandPrimitive.Item
			data-slot="command-item"
			className={cn(
				"data-[selected=true]:bg-accent data-[selected=true]:text-accent-foreground [&_svg:not([class*='text-'])]:text-muted-foreground relative flex cursor-default items-center gap-2 rounded-sm px-2 py-1.5 text-sm outline-hidden select-none data-[disabled=true]:pointer-events-none data-[disabled=true]:opacity-50 [&_svg]:pointer-events-none [&_svg]:shrink-0 [&_svg:not([class*='size-'])]:size-4",
				className,
			)}
			{...props}
		/>
	);
}

function CommandShortcut({
	className,
	...props
}: React.ComponentProps<"span">) {
	return (
		<span
			data-slot="command-shortcut"
			className={cn(
				"text-muted-foreground ml-auto text-xs tracking-widest",
				className,
			)}
			{...props}
		/>
	);
}

export {
	Command,
	CommandDialog,
	CommandInput,
	CommandList,
	CommandEmpty,
	CommandGroup,
	CommandItem,
	CommandShortcut,
	CommandSeparator,
};
</file>

<file path="apps/web/src/components/ui/context-menu.tsx">
"use client";

import * as ContextMenuPrimitive from "@radix-ui/react-context-menu";
import { CheckIcon, ChevronRightIcon, CircleIcon } from "lucide-react";
import type * as React from "react";

import { cn } from "@/lib/utils";

function ContextMenu({
	...props
}: React.ComponentProps<typeof ContextMenuPrimitive.Root>) {
	return <ContextMenuPrimitive.Root data-slot="context-menu" {...props} />;
}

function ContextMenuTrigger({
	...props
}: React.ComponentProps<typeof ContextMenuPrimitive.Trigger>) {
	return (
		<ContextMenuPrimitive.Trigger data-slot="context-menu-trigger" {...props} />
	);
}

function ContextMenuGroup({
	...props
}: React.ComponentProps<typeof ContextMenuPrimitive.Group>) {
	return (
		<ContextMenuPrimitive.Group data-slot="context-menu-group" {...props} />
	);
}

function ContextMenuPortal({
	...props
}: React.ComponentProps<typeof ContextMenuPrimitive.Portal>) {
	return (
		<ContextMenuPrimitive.Portal data-slot="context-menu-portal" {...props} />
	);
}

function ContextMenuSub({
	...props
}: React.ComponentProps<typeof ContextMenuPrimitive.Sub>) {
	return <ContextMenuPrimitive.Sub data-slot="context-menu-sub" {...props} />;
}

function ContextMenuRadioGroup({
	...props
}: React.ComponentProps<typeof ContextMenuPrimitive.RadioGroup>) {
	return (
		<ContextMenuPrimitive.RadioGroup
			data-slot="context-menu-radio-group"
			{...props}
		/>
	);
}

function ContextMenuSubTrigger({
	className,
	inset,
	children,
	...props
}: React.ComponentProps<typeof ContextMenuPrimitive.SubTrigger> & {
	inset?: boolean;
}) {
	return (
		<ContextMenuPrimitive.SubTrigger
			data-slot="context-menu-sub-trigger"
			data-inset={inset}
			className={cn(
				"focus:bg-accent focus:text-accent-foreground data-[state=open]:bg-accent data-[state=open]:text-accent-foreground flex cursor-default items-center rounded-sm px-2 py-1.5 text-sm outline-hidden select-none data-[inset]:pl-8 [&_svg]:pointer-events-none [&_svg]:shrink-0 [&_svg:not([class*='size-'])]:size-4",
				className,
			)}
			{...props}
		>
			{children}
			<ChevronRightIcon className="ml-auto" />
		</ContextMenuPrimitive.SubTrigger>
	);
}

function ContextMenuSubContent({
	className,
	...props
}: React.ComponentProps<typeof ContextMenuPrimitive.SubContent>) {
	return (
		<ContextMenuPrimitive.SubContent
			data-slot="context-menu-sub-content"
			className={cn(
				"bg-popover text-popover-foreground data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2 z-50 min-w-[8rem] origin-(--radix-context-menu-content-transform-origin) overflow-hidden rounded-md border p-1 shadow-lg",
				className,
			)}
			{...props}
		/>
	);
}

function ContextMenuContent({
	className,
	...props
}: React.ComponentProps<typeof ContextMenuPrimitive.Content>) {
	return (
		<ContextMenuPrimitive.Portal>
			<ContextMenuPrimitive.Content
				data-slot="context-menu-content"
				className={cn(
					"bg-popover text-popover-foreground data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2 z-50 max-h-(--radix-context-menu-content-available-height) min-w-[8rem] origin-(--radix-context-menu-content-transform-origin) overflow-x-hidden overflow-y-auto rounded-md border p-1 shadow-md",
					className,
				)}
				{...props}
			/>
		</ContextMenuPrimitive.Portal>
	);
}

function ContextMenuItem({
	className,
	inset,
	variant = "default",
	...props
}: React.ComponentProps<typeof ContextMenuPrimitive.Item> & {
	inset?: boolean;
	variant?: "default" | "destructive";
}) {
	return (
		<ContextMenuPrimitive.Item
			data-slot="context-menu-item"
			data-inset={inset}
			data-variant={variant}
			className={cn(
				"focus:bg-accent focus:text-accent-foreground data-[variant=destructive]:text-destructive data-[variant=destructive]:focus:bg-destructive/10 dark:data-[variant=destructive]:focus:bg-destructive/20 data-[variant=destructive]:focus:text-destructive data-[variant=destructive]:*:[svg]:!text-destructive [&_svg:not([class*='text-'])]:text-muted-foreground relative flex cursor-default items-center gap-2 rounded-sm px-2 py-1.5 text-sm outline-hidden select-none data-[disabled]:pointer-events-none data-[disabled]:opacity-50 data-[inset]:pl-8 [&_svg]:pointer-events-none [&_svg]:shrink-0 [&_svg:not([class*='size-'])]:size-4",
				className,
			)}
			{...props}
		/>
	);
}

function ContextMenuCheckboxItem({
	className,
	children,
	checked,
	...props
}: React.ComponentProps<typeof ContextMenuPrimitive.CheckboxItem>) {
	return (
		<ContextMenuPrimitive.CheckboxItem
			data-slot="context-menu-checkbox-item"
			className={cn(
				"focus:bg-accent focus:text-accent-foreground relative flex cursor-default items-center gap-2 rounded-sm py-1.5 pr-2 pl-8 text-sm outline-hidden select-none data-[disabled]:pointer-events-none data-[disabled]:opacity-50 [&_svg]:pointer-events-none [&_svg]:shrink-0 [&_svg:not([class*='size-'])]:size-4",
				className,
			)}
			checked={checked}
			{...props}
		>
			<span className="pointer-events-none absolute left-2 flex size-3.5 items-center justify-center">
				<ContextMenuPrimitive.ItemIndicator>
					<CheckIcon className="size-4" />
				</ContextMenuPrimitive.ItemIndicator>
			</span>
			{children}
		</ContextMenuPrimitive.CheckboxItem>
	);
}

function ContextMenuRadioItem({
	className,
	children,
	...props
}: React.ComponentProps<typeof ContextMenuPrimitive.RadioItem>) {
	return (
		<ContextMenuPrimitive.RadioItem
			data-slot="context-menu-radio-item"
			className={cn(
				"focus:bg-accent focus:text-accent-foreground relative flex cursor-default items-center gap-2 rounded-sm py-1.5 pr-2 pl-8 text-sm outline-hidden select-none data-[disabled]:pointer-events-none data-[disabled]:opacity-50 [&_svg]:pointer-events-none [&_svg]:shrink-0 [&_svg:not([class*='size-'])]:size-4",
				className,
			)}
			{...props}
		>
			<span className="pointer-events-none absolute left-2 flex size-3.5 items-center justify-center">
				<ContextMenuPrimitive.ItemIndicator>
					<CircleIcon className="size-2 fill-current" />
				</ContextMenuPrimitive.ItemIndicator>
			</span>
			{children}
		</ContextMenuPrimitive.RadioItem>
	);
}

function ContextMenuLabel({
	className,
	inset,
	...props
}: React.ComponentProps<typeof ContextMenuPrimitive.Label> & {
	inset?: boolean;
}) {
	return (
		<ContextMenuPrimitive.Label
			data-slot="context-menu-label"
			data-inset={inset}
			className={cn(
				"text-foreground px-2 py-1.5 text-sm font-medium data-[inset]:pl-8",
				className,
			)}
			{...props}
		/>
	);
}

function ContextMenuSeparator({
	className,
	...props
}: React.ComponentProps<typeof ContextMenuPrimitive.Separator>) {
	return (
		<ContextMenuPrimitive.Separator
			data-slot="context-menu-separator"
			className={cn("bg-border -mx-1 my-1 h-px", className)}
			{...props}
		/>
	);
}

function ContextMenuShortcut({
	className,
	...props
}: React.ComponentProps<"span">) {
	return (
		<span
			data-slot="context-menu-shortcut"
			className={cn(
				"text-muted-foreground ml-auto text-xs tracking-widest",
				className,
			)}
			{...props}
		/>
	);
}

export {
	ContextMenu,
	ContextMenuTrigger,
	ContextMenuContent,
	ContextMenuItem,
	ContextMenuCheckboxItem,
	ContextMenuRadioItem,
	ContextMenuLabel,
	ContextMenuSeparator,
	ContextMenuShortcut,
	ContextMenuGroup,
	ContextMenuPortal,
	ContextMenuSub,
	ContextMenuSubContent,
	ContextMenuSubTrigger,
	ContextMenuRadioGroup,
};
</file>

<file path="apps/web/src/components/ui/date-picker.tsx">
"use client";

import { format } from "date-fns";
import { Calendar as CalendarIcon } from "lucide-react";
import * as React from "react";

import { Button } from "@/components/ui/button";
import { Calendar } from "@/components/ui/calendar";
import {
	Popover,
	PopoverContent,
	PopoverTrigger,
} from "@/components/ui/popover";
import { cn } from "@/lib/utils";

export function DatePickerDemo() {
	const [date, setDate] = React.useState<Date>();

	return (
		<Popover>
			<PopoverTrigger asChild>
				<Button
					variant={"outline"}
					className={cn(
						"w-[280px] justify-start text-left font-normal",
						!date && "text-muted-foreground",
					)}
				>
					<CalendarIcon className="mr-2 h-4 w-4" />
					{date ? format(date, "PPP") : <span>Pick a date</span>}
				</Button>
			</PopoverTrigger>
			<PopoverContent className="w-auto p-0">
				<Calendar
					mode="single"
					selected={date}
					onSelect={setDate}
					initialFocus
				/>
			</PopoverContent>
		</Popover>
	);
}
</file>

<file path="apps/web/src/components/ui/dialog.tsx">
import * as DialogPrimitive from "@radix-ui/react-dialog";
import { XIcon } from "lucide-react";
import type * as React from "react";

import { cn } from "@/lib/utils";

function Dialog({
	...props
}: React.ComponentProps<typeof DialogPrimitive.Root>) {
	return <DialogPrimitive.Root data-slot="dialog" {...props} />;
}

function DialogTrigger({
	...props
}: React.ComponentProps<typeof DialogPrimitive.Trigger>) {
	return <DialogPrimitive.Trigger data-slot="dialog-trigger" {...props} />;
}

function DialogPortal({
	...props
}: React.ComponentProps<typeof DialogPrimitive.Portal>) {
	return <DialogPrimitive.Portal data-slot="dialog-portal" {...props} />;
}

function DialogClose({
	...props
}: React.ComponentProps<typeof DialogPrimitive.Close>) {
	return <DialogPrimitive.Close data-slot="dialog-close" {...props} />;
}

function DialogOverlay({
	className,
	...props
}: React.ComponentProps<typeof DialogPrimitive.Overlay>) {
	return (
		<DialogPrimitive.Overlay
			data-slot="dialog-overlay"
			className={cn(
				"data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 fixed inset-0 z-50 bg-black/50",
				className,
			)}
			{...props}
		/>
	);
}

function DialogContent({
	className,
	children,
	...props
}: React.ComponentProps<typeof DialogPrimitive.Content>) {
	return (
		<DialogPortal data-slot="dialog-portal">
			<DialogOverlay />
			<DialogPrimitive.Content
				data-slot="dialog-content"
				className={cn(
					"bg-background data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 fixed top-[50%] left-[50%] z-50 grid w-full max-w-[calc(100%-2rem)] translate-x-[-50%] translate-y-[-50%] gap-4 rounded-lg border p-6 shadow-lg duration-200 sm:max-w-lg",
					className,
				)}
				{...props}
			>
				{children}
				<DialogPrimitive.Close className="ring-offset-background focus:ring-ring data-[state=open]:bg-accent data-[state=open]:text-muted-foreground absolute top-4 right-4 rounded-xs opacity-70 transition-opacity hover:opacity-100 focus:ring-2 focus:ring-offset-2 focus:outline-hidden disabled:pointer-events-none [&_svg]:pointer-events-none [&_svg]:shrink-0 [&_svg:not([class*='size-'])]:size-4">
					<XIcon />
					<span className="sr-only">Close</span>
				</DialogPrimitive.Close>
			</DialogPrimitive.Content>
		</DialogPortal>
	);
}

function DialogHeader({ className, ...props }: React.ComponentProps<"div">) {
	return (
		<div
			data-slot="dialog-header"
			className={cn("flex flex-col gap-2 text-center sm:text-left", className)}
			{...props}
		/>
	);
}

function DialogFooter({ className, ...props }: React.ComponentProps<"div">) {
	return (
		<div
			data-slot="dialog-footer"
			className={cn(
				"flex flex-col-reverse gap-2 sm:flex-row sm:justify-end",
				className,
			)}
			{...props}
		/>
	);
}

function DialogTitle({
	className,
	...props
}: React.ComponentProps<typeof DialogPrimitive.Title>) {
	return (
		<DialogPrimitive.Title
			data-slot="dialog-title"
			className={cn("text-lg leading-none font-semibold", className)}
			{...props}
		/>
	);
}

function DialogDescription({
	className,
	...props
}: React.ComponentProps<typeof DialogPrimitive.Description>) {
	return (
		<DialogPrimitive.Description
			data-slot="dialog-description"
			className={cn("text-muted-foreground text-sm", className)}
			{...props}
		/>
	);
}

export {
	Dialog,
	DialogClose,
	DialogContent,
	DialogDescription,
	DialogFooter,
	DialogHeader,
	DialogOverlay,
	DialogPortal,
	DialogTitle,
	DialogTrigger,
};
</file>

<file path="apps/web/src/components/ui/drawer.tsx">
import type * as React from "react";
import { Drawer as DrawerPrimitive } from "vaul";

import { cn } from "@/lib/utils";

function Drawer({
	...props
}: React.ComponentProps<typeof DrawerPrimitive.Root>) {
	return <DrawerPrimitive.Root data-slot="drawer" {...props} />;
}

function DrawerTrigger({
	...props
}: React.ComponentProps<typeof DrawerPrimitive.Trigger>) {
	return <DrawerPrimitive.Trigger data-slot="drawer-trigger" {...props} />;
}

function DrawerPortal({
	...props
}: React.ComponentProps<typeof DrawerPrimitive.Portal>) {
	return <DrawerPrimitive.Portal data-slot="drawer-portal" {...props} />;
}

function DrawerClose({
	...props
}: React.ComponentProps<typeof DrawerPrimitive.Close>) {
	return <DrawerPrimitive.Close data-slot="drawer-close" {...props} />;
}

function DrawerOverlay({
	className,
	...props
}: React.ComponentProps<typeof DrawerPrimitive.Overlay>) {
	return (
		<DrawerPrimitive.Overlay
			data-slot="drawer-overlay"
			className={cn(
				"data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 fixed inset-0 z-50 bg-black/50",
				className,
			)}
			{...props}
		/>
	);
}

function DrawerContent({
	className,
	children,
	...props
}: React.ComponentProps<typeof DrawerPrimitive.Content>) {
	return (
		<DrawerPortal data-slot="drawer-portal">
			<DrawerOverlay />
			<DrawerPrimitive.Content
				data-slot="drawer-content"
				className={cn(
					"group/drawer-content bg-background fixed z-50 flex h-auto flex-col",
					"data-[vaul-drawer-direction=top]:inset-x-0 data-[vaul-drawer-direction=top]:top-0 data-[vaul-drawer-direction=top]:mb-24 data-[vaul-drawer-direction=top]:max-h-[80vh] data-[vaul-drawer-direction=top]:rounded-b-lg data-[vaul-drawer-direction=top]:border-b",
					"data-[vaul-drawer-direction=bottom]:inset-x-0 data-[vaul-drawer-direction=bottom]:bottom-0 data-[vaul-drawer-direction=bottom]:mt-24 data-[vaul-drawer-direction=bottom]:max-h-[80vh] data-[vaul-drawer-direction=bottom]:rounded-t-lg data-[vaul-drawer-direction=bottom]:border-t",
					"data-[vaul-drawer-direction=right]:inset-y-0 data-[vaul-drawer-direction=right]:right-0 data-[vaul-drawer-direction=right]:w-3/4 data-[vaul-drawer-direction=right]:border-l data-[vaul-drawer-direction=right]:sm:max-w-sm",
					"data-[vaul-drawer-direction=left]:inset-y-0 data-[vaul-drawer-direction=left]:left-0 data-[vaul-drawer-direction=left]:w-3/4 data-[vaul-drawer-direction=left]:border-r data-[vaul-drawer-direction=left]:sm:max-w-sm",
					className,
				)}
				{...props}
			>
				<div className="bg-muted mx-auto mt-4 hidden h-2 w-[100px] shrink-0 rounded-full group-data-[vaul-drawer-direction=bottom]/drawer-content:block" />
				{children}
			</DrawerPrimitive.Content>
		</DrawerPortal>
	);
}

function DrawerHeader({ className, ...props }: React.ComponentProps<"div">) {
	return (
		<div
			data-slot="drawer-header"
			className={cn("flex flex-col gap-1.5 p-4", className)}
			{...props}
		/>
	);
}

function DrawerFooter({ className, ...props }: React.ComponentProps<"div">) {
	return (
		<div
			data-slot="drawer-footer"
			className={cn("mt-auto flex flex-col gap-2 p-4", className)}
			{...props}
		/>
	);
}

function DrawerTitle({
	className,
	...props
}: React.ComponentProps<typeof DrawerPrimitive.Title>) {
	return (
		<DrawerPrimitive.Title
			data-slot="drawer-title"
			className={cn("text-foreground font-semibold", className)}
			{...props}
		/>
	);
}

function DrawerDescription({
	className,
	...props
}: React.ComponentProps<typeof DrawerPrimitive.Description>) {
	return (
		<DrawerPrimitive.Description
			data-slot="drawer-description"
			className={cn("text-muted-foreground text-sm", className)}
			{...props}
		/>
	);
}

export {
	Drawer,
	DrawerPortal,
	DrawerOverlay,
	DrawerTrigger,
	DrawerClose,
	DrawerContent,
	DrawerHeader,
	DrawerFooter,
	DrawerTitle,
	DrawerDescription,
};
</file>

<file path="apps/web/src/components/ui/dropdown-menu.tsx">
"use client";

import * as DropdownMenuPrimitive from "@radix-ui/react-dropdown-menu";
import { CheckIcon, ChevronRightIcon, CircleIcon } from "lucide-react";
import type * as React from "react";

import { cn } from "@/lib/utils";

function DropdownMenu({
	...props
}: React.ComponentProps<typeof DropdownMenuPrimitive.Root>) {
	return <DropdownMenuPrimitive.Root data-slot="dropdown-menu" {...props} />;
}

function DropdownMenuPortal({
	...props
}: React.ComponentProps<typeof DropdownMenuPrimitive.Portal>) {
	return (
		<DropdownMenuPrimitive.Portal data-slot="dropdown-menu-portal" {...props} />
	);
}

function DropdownMenuTrigger({
	...props
}: React.ComponentProps<typeof DropdownMenuPrimitive.Trigger>) {
	return (
		<DropdownMenuPrimitive.Trigger
			data-slot="dropdown-menu-trigger"
			{...props}
		/>
	);
}

function DropdownMenuContent({
	className,
	sideOffset = 4,
	...props
}: React.ComponentProps<typeof DropdownMenuPrimitive.Content>) {
	return (
		<DropdownMenuPrimitive.Portal>
			<DropdownMenuPrimitive.Content
				data-slot="dropdown-menu-content"
				sideOffset={sideOffset}
				className={cn(
					"bg-popover text-popover-foreground data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2 z-50 max-h-(--radix-dropdown-menu-content-available-height) min-w-[8rem] origin-(--radix-dropdown-menu-content-transform-origin) overflow-x-hidden overflow-y-auto rounded-md border p-1 shadow-md",
					className,
				)}
				{...props}
			/>
		</DropdownMenuPrimitive.Portal>
	);
}

function DropdownMenuGroup({
	...props
}: React.ComponentProps<typeof DropdownMenuPrimitive.Group>) {
	return (
		<DropdownMenuPrimitive.Group data-slot="dropdown-menu-group" {...props} />
	);
}

function DropdownMenuItem({
	className,
	inset,
	variant = "default",
	...props
}: React.ComponentProps<typeof DropdownMenuPrimitive.Item> & {
	inset?: boolean;
	variant?: "default" | "destructive";
}) {
	return (
		<DropdownMenuPrimitive.Item
			data-slot="dropdown-menu-item"
			data-inset={inset}
			data-variant={variant}
			className={cn(
				"focus:bg-accent focus:text-accent-foreground data-[variant=destructive]:text-destructive data-[variant=destructive]:focus:bg-destructive/10 dark:data-[variant=destructive]:focus:bg-destructive/20 data-[variant=destructive]:focus:text-destructive data-[variant=destructive]:*:[svg]:!text-destructive [&_svg:not([class*='text-'])]:text-muted-foreground relative flex cursor-default items-center gap-2 rounded-sm px-2 py-1.5 text-sm outline-hidden select-none data-[disabled]:pointer-events-none data-[disabled]:opacity-50 data-[inset]:pl-8 [&_svg]:pointer-events-none [&_svg]:shrink-0 [&_svg:not([class*='size-'])]:size-4",
				className,
			)}
			{...props}
		/>
	);
}

function DropdownMenuCheckboxItem({
	className,
	children,
	checked,
	...props
}: React.ComponentProps<typeof DropdownMenuPrimitive.CheckboxItem>) {
	return (
		<DropdownMenuPrimitive.CheckboxItem
			data-slot="dropdown-menu-checkbox-item"
			className={cn(
				"focus:bg-accent focus:text-accent-foreground relative flex cursor-default items-center gap-2 rounded-sm py-1.5 pr-2 pl-8 text-sm outline-hidden select-none data-[disabled]:pointer-events-none data-[disabled]:opacity-50 [&_svg]:pointer-events-none [&_svg]:shrink-0 [&_svg:not([class*='size-'])]:size-4",
				className,
			)}
			checked={checked}
			{...props}
		>
			<span className="pointer-events-none absolute left-2 flex size-3.5 items-center justify-center">
				<DropdownMenuPrimitive.ItemIndicator>
					<CheckIcon className="size-4" />
				</DropdownMenuPrimitive.ItemIndicator>
			</span>
			{children}
		</DropdownMenuPrimitive.CheckboxItem>
	);
}

function DropdownMenuRadioGroup({
	...props
}: React.ComponentProps<typeof DropdownMenuPrimitive.RadioGroup>) {
	return (
		<DropdownMenuPrimitive.RadioGroup
			data-slot="dropdown-menu-radio-group"
			{...props}
		/>
	);
}

function DropdownMenuRadioItem({
	className,
	children,
	...props
}: React.ComponentProps<typeof DropdownMenuPrimitive.RadioItem>) {
	return (
		<DropdownMenuPrimitive.RadioItem
			data-slot="dropdown-menu-radio-item"
			className={cn(
				"focus:bg-accent focus:text-accent-foreground relative flex cursor-default items-center gap-2 rounded-sm py-1.5 pr-2 pl-8 text-sm outline-hidden select-none data-[disabled]:pointer-events-none data-[disabled]:opacity-50 [&_svg]:pointer-events-none [&_svg]:shrink-0 [&_svg:not([class*='size-'])]:size-4",
				className,
			)}
			{...props}
		>
			<span className="pointer-events-none absolute left-2 flex size-3.5 items-center justify-center">
				<DropdownMenuPrimitive.ItemIndicator>
					<CircleIcon className="size-2 fill-current" />
				</DropdownMenuPrimitive.ItemIndicator>
			</span>
			{children}
		</DropdownMenuPrimitive.RadioItem>
	);
}

function DropdownMenuLabel({
	className,
	inset,
	...props
}: React.ComponentProps<typeof DropdownMenuPrimitive.Label> & {
	inset?: boolean;
}) {
	return (
		<DropdownMenuPrimitive.Label
			data-slot="dropdown-menu-label"
			data-inset={inset}
			className={cn(
				"px-2 py-1.5 text-sm font-medium data-[inset]:pl-8",
				className,
			)}
			{...props}
		/>
	);
}

function DropdownMenuSeparator({
	className,
	...props
}: React.ComponentProps<typeof DropdownMenuPrimitive.Separator>) {
	return (
		<DropdownMenuPrimitive.Separator
			data-slot="dropdown-menu-separator"
			className={cn("bg-border -mx-1 my-1 h-px", className)}
			{...props}
		/>
	);
}

function DropdownMenuShortcut({
	className,
	...props
}: React.ComponentProps<"span">) {
	return (
		<span
			data-slot="dropdown-menu-shortcut"
			className={cn(
				"text-muted-foreground ml-auto text-xs tracking-widest",
				className,
			)}
			{...props}
		/>
	);
}

function DropdownMenuSub({
	...props
}: React.ComponentProps<typeof DropdownMenuPrimitive.Sub>) {
	return <DropdownMenuPrimitive.Sub data-slot="dropdown-menu-sub" {...props} />;
}

function DropdownMenuSubTrigger({
	className,
	inset,
	children,
	...props
}: React.ComponentProps<typeof DropdownMenuPrimitive.SubTrigger> & {
	inset?: boolean;
}) {
	return (
		<DropdownMenuPrimitive.SubTrigger
			data-slot="dropdown-menu-sub-trigger"
			data-inset={inset}
			className={cn(
				"focus:bg-accent focus:text-accent-foreground data-[state=open]:bg-accent data-[state=open]:text-accent-foreground flex cursor-default items-center rounded-sm px-2 py-1.5 text-sm outline-hidden select-none data-[inset]:pl-8",
				className,
			)}
			{...props}
		>
			{children}
			<ChevronRightIcon className="ml-auto size-4" />
		</DropdownMenuPrimitive.SubTrigger>
	);
}

function DropdownMenuSubContent({
	className,
	...props
}: React.ComponentProps<typeof DropdownMenuPrimitive.SubContent>) {
	return (
		<DropdownMenuPrimitive.SubContent
			data-slot="dropdown-menu-sub-content"
			className={cn(
				"bg-popover text-popover-foreground data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2 z-50 min-w-[8rem] origin-(--radix-dropdown-menu-content-transform-origin) overflow-hidden rounded-md border p-1 shadow-lg",
				className,
			)}
			{...props}
		/>
	);
}

export {
	DropdownMenu,
	DropdownMenuPortal,
	DropdownMenuTrigger,
	DropdownMenuContent,
	DropdownMenuGroup,
	DropdownMenuLabel,
	DropdownMenuItem,
	DropdownMenuCheckboxItem,
	DropdownMenuRadioGroup,
	DropdownMenuRadioItem,
	DropdownMenuSeparator,
	DropdownMenuShortcut,
	DropdownMenuSub,
	DropdownMenuSubTrigger,
	DropdownMenuSubContent,
};
</file>

<file path="apps/web/src/components/ui/dropzone.test.tsx">
/// <reference types="vitest" />
import { fireEvent, render, screen } from "@testing-library/react";
import { vi } from "vitest";
import { DropZone } from "./dropzone";

describe("DropZone", () => {
	it("renders the drop zone UI", () => {
		render(<DropZone onFilesAccepted={() => {}} />);
		// Use a function matcher to match text across element boundaries
		const dropZone = screen.getByRole("button", {
			name: /upload video files/i,
		});
		expect(dropZone.textContent?.toLowerCase()).toContain(
			"drag & drop your .mp4 file here",
		);
		expect(dropZone.textContent?.toLowerCase()).toContain(
			"or click to select from your computer",
		);
	});

	it("calls onFilesAccepted when a file is dropped", () => {
		const handleFilesAccepted = vi.fn();
		render(<DropZone onFilesAccepted={handleFilesAccepted} />);
		const dropZone = screen.getByRole("button", {
			name: /upload video files/i,
		});

		// Create a mock file
		const file = new File(["dummy content"], "test.mp4", { type: "video/mp4" });
		const dataTransfer = {
			files: [file],
			types: ["Files"],
			getData: vi.fn(),
			setData: vi.fn(),
			clearData: vi.fn(),
		};

		fireEvent.dragOver(dropZone, { dataTransfer });
		fireEvent.drop(dropZone, { dataTransfer });

		expect(handleFilesAccepted).toHaveBeenCalledWith([file]);
	});

	it("calls onFilesAccepted when a file is selected via input", () => {
		const handleFilesAccepted = vi.fn();
		render(<DropZone onFilesAccepted={handleFilesAccepted} />);
		const input = screen
			.getByLabelText(/upload video files/i)
			.querySelector("input[type='file']") as HTMLInputElement;

		// Create a mock file list
		const file = new File(["dummy content"], "test.mp4", { type: "video/mp4" });
		Object.defineProperty(input, "files", {
			value: [file],
			writable: false,
		});

		fireEvent.change(input);

		expect(handleFilesAccepted).toHaveBeenCalledWith([file]);
	});
});
</file>

<file path="apps/web/src/components/ui/dropzone.tsx">
import type React from "react";
import { useRef, useState } from "react";
import { cn } from "../../lib/utils";

interface DropZoneProps {
	onFilesAccepted: (files: FileList | File[]) => void;
	accept?: string;
	multiple?: boolean;
	className?: string;
	disabled?: boolean;
}

export const DropZone: React.FC<DropZoneProps> = ({
	onFilesAccepted,
	accept = ".mp4",
	multiple = false,
	className = "",
	disabled = false,
}) => {
	const [isDragging, setIsDragging] = useState(false);
	const inputRef = useRef<HTMLInputElement>(null);

	const handleDragOver = (e: React.DragEvent<HTMLDivElement>) => {
		if (disabled) return;
		e.preventDefault();
		e.stopPropagation();
		setIsDragging(true);
	};

	const handleDragLeave = (e: React.DragEvent<HTMLDivElement>) => {
		if (disabled) return;
		e.preventDefault();
		e.stopPropagation();
		setIsDragging(false);
	};

	const handleDrop = (e: React.DragEvent<HTMLDivElement>) => {
		if (disabled) return;
		e.preventDefault();
		e.stopPropagation();
		setIsDragging(false);
		if (e.dataTransfer.files && e.dataTransfer.files.length > 0) {
			onFilesAccepted(e.dataTransfer.files);
		}
	};

	const handleClick = () => {
		if (disabled) return;
		inputRef.current?.click();
	};

	const handleInputChange = (e: React.ChangeEvent<HTMLInputElement>) => {
		if (disabled) return;
		if (e.target.files && e.target.files.length > 0) {
			onFilesAccepted(e.target.files);
		}
	};

	return (
		<div
			className={cn(
				"flex flex-col items-center justify-center border-2 border-dashed rounded-lg p-8 cursor-pointer transition-colors",
				isDragging
					? "border-blue-500 bg-blue-50"
					: "border-gray-300 bg-white hover:border-blue-400",
				className,
			)}
			onDragOver={handleDragOver}
			onDragLeave={handleDragLeave}
			onDrop={handleDrop}
			onClick={handleClick}
			tabIndex={0}
			role="button"
			aria-label="Upload video files"
		>
			<input
				ref={inputRef}
				type="file"
				accept={accept}
				multiple={multiple}
				className="hidden"
				onChange={handleInputChange}
				tabIndex={-1}
			/>
			<div className="flex flex-col items-center">
				<svg
					className="w-12 h-12 text-blue-400 mb-2"
					fill="none"
					stroke="currentColor"
					strokeWidth={2}
					viewBox="0 0 48 48"
				>
					<path
						strokeLinecap="round"
						strokeLinejoin="round"
						d="M24 6v24m0 0l-8-8m8 8l8-8M6 36h36"
					/>
				</svg>
				<span className="text-lg font-medium text-gray-700">
					Drag & drop your <span className="text-blue-500">.mp4</span> file here
				</span>
				<span className="text-sm text-gray-500 mt-1">
					or click to select from your computer
				</span>
			</div>
		</div>
	);
};

export default DropZone;
</file>

<file path="apps/web/src/components/ui/form.tsx">
import type * as LabelPrimitive from "@radix-ui/react-label";
import { Slot } from "@radix-ui/react-slot";
import * as React from "react";
import {
	Controller,
	type ControllerProps,
	type FieldPath,
	type FieldValues,
	FormProvider,
	useFormContext,
	useFormState,
} from "react-hook-form";

import { Label } from "@/components/ui/label";
import { cn } from "@/lib/utils";

const Form = FormProvider;

type FormFieldContextValue<
	TFieldValues extends FieldValues = FieldValues,
	TName extends FieldPath<TFieldValues> = FieldPath<TFieldValues>,
> = {
	name: TName;
};

const FormFieldContext = React.createContext<FormFieldContextValue>(
	{} as FormFieldContextValue,
);

const FormField = <
	TFieldValues extends FieldValues = FieldValues,
	TName extends FieldPath<TFieldValues> = FieldPath<TFieldValues>,
>({
	...props
}: ControllerProps<TFieldValues, TName>) => {
	return (
		<FormFieldContext.Provider value={{ name: props.name }}>
			<Controller {...props} />
		</FormFieldContext.Provider>
	);
};

const useFormField = () => {
	const fieldContext = React.useContext(FormFieldContext);
	const itemContext = React.useContext(FormItemContext);
	const { getFieldState } = useFormContext();
	const formState = useFormState({ name: fieldContext.name });
	const fieldState = getFieldState(fieldContext.name, formState);

	if (!fieldContext) {
		throw new Error("useFormField should be used within <FormField>");
	}

	const { id } = itemContext;

	return {
		id,
		name: fieldContext.name,
		formItemId: `${id}-form-item`,
		formDescriptionId: `${id}-form-item-description`,
		formMessageId: `${id}-form-item-message`,
		...fieldState,
	};
};

type FormItemContextValue = {
	id: string;
};

const FormItemContext = React.createContext<FormItemContextValue>(
	{} as FormItemContextValue,
);

function FormItem({ className, ...props }: React.ComponentProps<"div">) {
	const id = React.useId();

	return (
		<FormItemContext.Provider value={{ id }}>
			<div
				data-slot="form-item"
				className={cn("grid gap-2", className)}
				{...props}
			/>
		</FormItemContext.Provider>
	);
}

function FormLabel({
	className,
	...props
}: React.ComponentProps<typeof LabelPrimitive.Root>) {
	const { error, formItemId } = useFormField();

	return (
		<Label
			data-slot="form-label"
			data-error={!!error}
			className={cn("data-[error=true]:text-destructive", className)}
			htmlFor={formItemId}
			{...props}
		/>
	);
}

function FormControl({ ...props }: React.ComponentProps<typeof Slot>) {
	const { error, formItemId, formDescriptionId, formMessageId } =
		useFormField();

	return (
		<Slot
			data-slot="form-control"
			id={formItemId}
			aria-describedby={
				!error
					? `${formDescriptionId}`
					: `${formDescriptionId} ${formMessageId}`
			}
			aria-invalid={!!error}
			{...props}
		/>
	);
}

function FormDescription({ className, ...props }: React.ComponentProps<"p">) {
	const { formDescriptionId } = useFormField();

	return (
		<p
			data-slot="form-description"
			id={formDescriptionId}
			className={cn("text-muted-foreground text-sm", className)}
			{...props}
		/>
	);
}

function FormMessage({ className, ...props }: React.ComponentProps<"p">) {
	const { error, formMessageId } = useFormField();
	const body = error ? String(error?.message ?? "") : props.children;

	if (!body) {
		return null;
	}

	return (
		<p
			data-slot="form-message"
			id={formMessageId}
			className={cn("text-destructive text-sm", className)}
			{...props}
		>
			{body}
		</p>
	);
}

export {
	useFormField,
	Form,
	FormItem,
	FormLabel,
	FormControl,
	FormDescription,
	FormMessage,
	FormField,
};
</file>

<file path="apps/web/src/components/ui/hover-card.tsx">
import * as HoverCardPrimitive from "@radix-ui/react-hover-card";
import type * as React from "react";

import { cn } from "@/lib/utils";

function HoverCard({
	...props
}: React.ComponentProps<typeof HoverCardPrimitive.Root>) {
	return <HoverCardPrimitive.Root data-slot="hover-card" {...props} />;
}

function HoverCardTrigger({
	...props
}: React.ComponentProps<typeof HoverCardPrimitive.Trigger>) {
	return (
		<HoverCardPrimitive.Trigger data-slot="hover-card-trigger" {...props} />
	);
}

function HoverCardContent({
	className,
	align = "center",
	sideOffset = 4,
	...props
}: React.ComponentProps<typeof HoverCardPrimitive.Content>) {
	return (
		<HoverCardPrimitive.Portal data-slot="hover-card-portal">
			<HoverCardPrimitive.Content
				data-slot="hover-card-content"
				align={align}
				sideOffset={sideOffset}
				className={cn(
					"bg-popover text-popover-foreground data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2 z-50 w-64 origin-(--radix-hover-card-content-transform-origin) rounded-md border p-4 shadow-md outline-hidden",
					className,
				)}
				{...props}
			/>
		</HoverCardPrimitive.Portal>
	);
}

export { HoverCard, HoverCardTrigger, HoverCardContent };
</file>

<file path="apps/web/src/components/ui/input-otp.tsx">
"use client";

import { OTPInput, OTPInputContext } from "input-otp";
import { MinusIcon } from "lucide-react";
import * as React from "react";

import { cn } from "@/lib/utils";

function InputOTP({
	className,
	containerClassName,
	...props
}: React.ComponentProps<typeof OTPInput> & {
	containerClassName?: string;
}) {
	return (
		<OTPInput
			data-slot="input-otp"
			containerClassName={cn(
				"flex items-center gap-2 has-disabled:opacity-50",
				containerClassName,
			)}
			className={cn("disabled:cursor-not-allowed", className)}
			{...props}
		/>
	);
}

function InputOTPGroup({ className, ...props }: React.ComponentProps<"div">) {
	return (
		<div
			data-slot="input-otp-group"
			className={cn("flex items-center", className)}
			{...props}
		/>
	);
}

function InputOTPSlot({
	index,
	className,
	...props
}: React.ComponentProps<"div"> & {
	index: number;
}) {
	const inputOTPContext = React.useContext(OTPInputContext);
	const { char, hasFakeCaret, isActive } = inputOTPContext?.slots[index] ?? {};

	return (
		<div
			data-slot="input-otp-slot"
			data-active={isActive}
			className={cn(
				"data-[active=true]:border-ring data-[active=true]:ring-ring/50 data-[active=true]:aria-invalid:ring-destructive/20 dark:data-[active=true]:aria-invalid:ring-destructive/40 aria-invalid:border-destructive data-[active=true]:aria-invalid:border-destructive dark:bg-input/30 border-input relative flex h-9 w-9 items-center justify-center border-y border-r text-sm shadow-xs transition-all outline-none first:rounded-l-md first:border-l last:rounded-r-md data-[active=true]:z-10 data-[active=true]:ring-[3px]",
				className,
			)}
			{...props}
		>
			{char}
			{hasFakeCaret && (
				<div className="pointer-events-none absolute inset-0 flex items-center justify-center">
					<div className="animate-caret-blink bg-foreground h-4 w-px duration-1000" />
				</div>
			)}
		</div>
	);
}

function InputOTPSeparator({ ...props }: React.ComponentProps<"div">) {
	return (
		<div data-slot="input-otp-separator" role="separator" {...props}>
			<MinusIcon />
		</div>
	);
}

export { InputOTP, InputOTPGroup, InputOTPSlot, InputOTPSeparator };
</file>

<file path="apps/web/src/components/ui/input.tsx">
import type * as React from "react";

import { cn } from "@/lib/utils";

function Input({ className, type, ...props }: React.ComponentProps<"input">) {
	return (
		<input
			type={type}
			data-slot="input"
			className={cn(
				"file:text-foreground placeholder:text-muted-foreground selection:bg-primary selection:text-primary-foreground dark:bg-input/30 border-input flex h-9 w-full min-w-0 rounded-md border bg-transparent px-3 py-1 text-base shadow-xs transition-[color,box-shadow] outline-none file:inline-flex file:h-7 file:border-0 file:bg-transparent file:text-sm file:font-medium disabled:pointer-events-none disabled:cursor-not-allowed disabled:opacity-50 md:text-sm",
				"focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px]",
				"aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive",
				className,
			)}
			{...props}
		/>
	);
}

export { Input };
</file>

<file path="apps/web/src/components/ui/label.tsx">
"use client";

import * as LabelPrimitive from "@radix-ui/react-label";
import type * as React from "react";

import { cn } from "@/lib/utils";

function Label({
	className,
	...props
}: React.ComponentProps<typeof LabelPrimitive.Root>) {
	return (
		<LabelPrimitive.Root
			data-slot="label"
			className={cn(
				"flex items-center gap-2 text-sm leading-none font-medium select-none group-data-[disabled=true]:pointer-events-none group-data-[disabled=true]:opacity-50 peer-disabled:cursor-not-allowed peer-disabled:opacity-50",
				className,
			)}
			{...props}
		/>
	);
}

export { Label };
</file>

<file path="apps/web/src/components/ui/menubar.tsx">
import * as MenubarPrimitive from "@radix-ui/react-menubar";
import { CheckIcon, ChevronRightIcon, CircleIcon } from "lucide-react";
import type * as React from "react";

import { cn } from "@/lib/utils";

function Menubar({
	className,
	...props
}: React.ComponentProps<typeof MenubarPrimitive.Root>) {
	return (
		<MenubarPrimitive.Root
			data-slot="menubar"
			className={cn(
				"bg-background flex h-9 items-center gap-1 rounded-md border p-1 shadow-xs",
				className,
			)}
			{...props}
		/>
	);
}

function MenubarMenu({
	...props
}: React.ComponentProps<typeof MenubarPrimitive.Menu>) {
	return <MenubarPrimitive.Menu data-slot="menubar-menu" {...props} />;
}

function MenubarGroup({
	...props
}: React.ComponentProps<typeof MenubarPrimitive.Group>) {
	return <MenubarPrimitive.Group data-slot="menubar-group" {...props} />;
}

function MenubarPortal({
	...props
}: React.ComponentProps<typeof MenubarPrimitive.Portal>) {
	return <MenubarPrimitive.Portal data-slot="menubar-portal" {...props} />;
}

function MenubarRadioGroup({
	...props
}: React.ComponentProps<typeof MenubarPrimitive.RadioGroup>) {
	return (
		<MenubarPrimitive.RadioGroup data-slot="menubar-radio-group" {...props} />
	);
}

function MenubarTrigger({
	className,
	...props
}: React.ComponentProps<typeof MenubarPrimitive.Trigger>) {
	return (
		<MenubarPrimitive.Trigger
			data-slot="menubar-trigger"
			className={cn(
				"focus:bg-accent focus:text-accent-foreground data-[state=open]:bg-accent data-[state=open]:text-accent-foreground flex items-center rounded-sm px-2 py-1 text-sm font-medium outline-hidden select-none",
				className,
			)}
			{...props}
		/>
	);
}

function MenubarContent({
	className,
	align = "start",
	alignOffset = -4,
	sideOffset = 8,
	...props
}: React.ComponentProps<typeof MenubarPrimitive.Content>) {
	return (
		<MenubarPortal>
			<MenubarPrimitive.Content
				data-slot="menubar-content"
				align={align}
				alignOffset={alignOffset}
				sideOffset={sideOffset}
				className={cn(
					"bg-popover text-popover-foreground data-[state=open]:animate-in data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2 z-50 min-w-[12rem] origin-(--radix-menubar-content-transform-origin) overflow-hidden rounded-md border p-1 shadow-md",
					className,
				)}
				{...props}
			/>
		</MenubarPortal>
	);
}

function MenubarItem({
	className,
	inset,
	variant = "default",
	...props
}: React.ComponentProps<typeof MenubarPrimitive.Item> & {
	inset?: boolean;
	variant?: "default" | "destructive";
}) {
	return (
		<MenubarPrimitive.Item
			data-slot="menubar-item"
			data-inset={inset}
			data-variant={variant}
			className={cn(
				"focus:bg-accent focus:text-accent-foreground data-[variant=destructive]:text-destructive data-[variant=destructive]:focus:bg-destructive/10 dark:data-[variant=destructive]:focus:bg-destructive/20 data-[variant=destructive]:focus:text-destructive data-[variant=destructive]:*:[svg]:!text-destructive [&_svg:not([class*='text-'])]:text-muted-foreground relative flex cursor-default items-center gap-2 rounded-sm px-2 py-1.5 text-sm outline-hidden select-none data-[disabled]:pointer-events-none data-[disabled]:opacity-50 data-[inset]:pl-8 [&_svg]:pointer-events-none [&_svg]:shrink-0 [&_svg:not([class*='size-'])]:size-4",
				className,
			)}
			{...props}
		/>
	);
}

function MenubarCheckboxItem({
	className,
	children,
	checked,
	...props
}: React.ComponentProps<typeof MenubarPrimitive.CheckboxItem>) {
	return (
		<MenubarPrimitive.CheckboxItem
			data-slot="menubar-checkbox-item"
			className={cn(
				"focus:bg-accent focus:text-accent-foreground relative flex cursor-default items-center gap-2 rounded-xs py-1.5 pr-2 pl-8 text-sm outline-hidden select-none data-[disabled]:pointer-events-none data-[disabled]:opacity-50 [&_svg]:pointer-events-none [&_svg]:shrink-0 [&_svg:not([class*='size-'])]:size-4",
				className,
			)}
			checked={checked}
			{...props}
		>
			<span className="pointer-events-none absolute left-2 flex size-3.5 items-center justify-center">
				<MenubarPrimitive.ItemIndicator>
					<CheckIcon className="size-4" />
				</MenubarPrimitive.ItemIndicator>
			</span>
			{children}
		</MenubarPrimitive.CheckboxItem>
	);
}

function MenubarRadioItem({
	className,
	children,
	...props
}: React.ComponentProps<typeof MenubarPrimitive.RadioItem>) {
	return (
		<MenubarPrimitive.RadioItem
			data-slot="menubar-radio-item"
			className={cn(
				"focus:bg-accent focus:text-accent-foreground relative flex cursor-default items-center gap-2 rounded-xs py-1.5 pr-2 pl-8 text-sm outline-hidden select-none data-[disabled]:pointer-events-none data-[disabled]:opacity-50 [&_svg]:pointer-events-none [&_svg]:shrink-0 [&_svg:not([class*='size-'])]:size-4",
				className,
			)}
			{...props}
		>
			<span className="pointer-events-none absolute left-2 flex size-3.5 items-center justify-center">
				<MenubarPrimitive.ItemIndicator>
					<CircleIcon className="size-2 fill-current" />
				</MenubarPrimitive.ItemIndicator>
			</span>
			{children}
		</MenubarPrimitive.RadioItem>
	);
}

function MenubarLabel({
	className,
	inset,
	...props
}: React.ComponentProps<typeof MenubarPrimitive.Label> & {
	inset?: boolean;
}) {
	return (
		<MenubarPrimitive.Label
			data-slot="menubar-label"
			data-inset={inset}
			className={cn(
				"px-2 py-1.5 text-sm font-medium data-[inset]:pl-8",
				className,
			)}
			{...props}
		/>
	);
}

function MenubarSeparator({
	className,
	...props
}: React.ComponentProps<typeof MenubarPrimitive.Separator>) {
	return (
		<MenubarPrimitive.Separator
			data-slot="menubar-separator"
			className={cn("bg-border -mx-1 my-1 h-px", className)}
			{...props}
		/>
	);
}

function MenubarShortcut({
	className,
	...props
}: React.ComponentProps<"span">) {
	return (
		<span
			data-slot="menubar-shortcut"
			className={cn(
				"text-muted-foreground ml-auto text-xs tracking-widest",
				className,
			)}
			{...props}
		/>
	);
}

function MenubarSub({
	...props
}: React.ComponentProps<typeof MenubarPrimitive.Sub>) {
	return <MenubarPrimitive.Sub data-slot="menubar-sub" {...props} />;
}

function MenubarSubTrigger({
	className,
	inset,
	children,
	...props
}: React.ComponentProps<typeof MenubarPrimitive.SubTrigger> & {
	inset?: boolean;
}) {
	return (
		<MenubarPrimitive.SubTrigger
			data-slot="menubar-sub-trigger"
			data-inset={inset}
			className={cn(
				"focus:bg-accent focus:text-accent-foreground data-[state=open]:bg-accent data-[state=open]:text-accent-foreground flex cursor-default items-center rounded-sm px-2 py-1.5 text-sm outline-none select-none data-[inset]:pl-8",
				className,
			)}
			{...props}
		>
			{children}
			<ChevronRightIcon className="ml-auto h-4 w-4" />
		</MenubarPrimitive.SubTrigger>
	);
}

function MenubarSubContent({
	className,
	...props
}: React.ComponentProps<typeof MenubarPrimitive.SubContent>) {
	return (
		<MenubarPrimitive.SubContent
			data-slot="menubar-sub-content"
			className={cn(
				"bg-popover text-popover-foreground data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2 z-50 min-w-[8rem] origin-(--radix-menubar-content-transform-origin) overflow-hidden rounded-md border p-1 shadow-lg",
				className,
			)}
			{...props}
		/>
	);
}

export {
	Menubar,
	MenubarPortal,
	MenubarMenu,
	MenubarTrigger,
	MenubarContent,
	MenubarGroup,
	MenubarSeparator,
	MenubarLabel,
	MenubarItem,
	MenubarShortcut,
	MenubarCheckboxItem,
	MenubarRadioGroup,
	MenubarRadioItem,
	MenubarSub,
	MenubarSubTrigger,
	MenubarSubContent,
};
</file>

<file path="apps/web/src/components/ui/navigation-menu.tsx">
import * as NavigationMenuPrimitive from "@radix-ui/react-navigation-menu";
import { cva } from "class-variance-authority";
import { ChevronDownIcon } from "lucide-react";
import type * as React from "react";

import { cn } from "@/lib/utils";

function NavigationMenu({
	className,
	children,
	viewport = true,
	...props
}: React.ComponentProps<typeof NavigationMenuPrimitive.Root> & {
	viewport?: boolean;
}) {
	return (
		<NavigationMenuPrimitive.Root
			data-slot="navigation-menu"
			data-viewport={viewport}
			className={cn(
				"group/navigation-menu relative flex max-w-max flex-1 items-center justify-center",
				className,
			)}
			{...props}
		>
			{children}
			{viewport && <NavigationMenuViewport />}
		</NavigationMenuPrimitive.Root>
	);
}

function NavigationMenuList({
	className,
	...props
}: React.ComponentProps<typeof NavigationMenuPrimitive.List>) {
	return (
		<NavigationMenuPrimitive.List
			data-slot="navigation-menu-list"
			className={cn(
				"group flex flex-1 list-none items-center justify-center gap-1",
				className,
			)}
			{...props}
		/>
	);
}

function NavigationMenuItem({
	className,
	...props
}: React.ComponentProps<typeof NavigationMenuPrimitive.Item>) {
	return (
		<NavigationMenuPrimitive.Item
			data-slot="navigation-menu-item"
			className={cn("relative", className)}
			{...props}
		/>
	);
}

const navigationMenuTriggerStyle = cva(
	"group inline-flex h-9 w-max items-center justify-center rounded-md bg-background px-4 py-2 text-sm font-medium hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground disabled:pointer-events-none disabled:opacity-50 data-[state=open]:hover:bg-accent data-[state=open]:text-accent-foreground data-[state=open]:focus:bg-accent data-[state=open]:bg-accent/50 focus-visible:ring-ring/50 outline-none transition-[color,box-shadow] focus-visible:ring-[3px] focus-visible:outline-1",
);

function NavigationMenuTrigger({
	className,
	children,
	...props
}: React.ComponentProps<typeof NavigationMenuPrimitive.Trigger>) {
	return (
		<NavigationMenuPrimitive.Trigger
			data-slot="navigation-menu-trigger"
			className={cn(navigationMenuTriggerStyle(), "group", className)}
			{...props}
		>
			{children}{" "}
			<ChevronDownIcon
				className="relative top-[1px] ml-1 size-3 transition duration-300 group-data-[state=open]:rotate-180"
				aria-hidden="true"
			/>
		</NavigationMenuPrimitive.Trigger>
	);
}

function NavigationMenuContent({
	className,
	...props
}: React.ComponentProps<typeof NavigationMenuPrimitive.Content>) {
	return (
		<NavigationMenuPrimitive.Content
			data-slot="navigation-menu-content"
			className={cn(
				"data-[motion^=from-]:animate-in data-[motion^=to-]:animate-out data-[motion^=from-]:fade-in data-[motion^=to-]:fade-out data-[motion=from-end]:slide-in-from-right-52 data-[motion=from-start]:slide-in-from-left-52 data-[motion=to-end]:slide-out-to-right-52 data-[motion=to-start]:slide-out-to-left-52 top-0 left-0 w-full p-2 pr-2.5 md:absolute md:w-auto",
				"group-data-[viewport=false]/navigation-menu:bg-popover group-data-[viewport=false]/navigation-menu:text-popover-foreground group-data-[viewport=false]/navigation-menu:data-[state=open]:animate-in group-data-[viewport=false]/navigation-menu:data-[state=closed]:animate-out group-data-[viewport=false]/navigation-menu:data-[state=closed]:zoom-out-95 group-data-[viewport=false]/navigation-menu:data-[state=open]:zoom-in-95 group-data-[viewport=false]/navigation-menu:data-[state=open]:fade-in-0 group-data-[viewport=false]/navigation-menu:data-[state=closed]:fade-out-0 group-data-[viewport=false]/navigation-menu:top-full group-data-[viewport=false]/navigation-menu:mt-1.5 group-data-[viewport=false]/navigation-menu:overflow-hidden group-data-[viewport=false]/navigation-menu:rounded-md group-data-[viewport=false]/navigation-menu:border group-data-[viewport=false]/navigation-menu:shadow group-data-[viewport=false]/navigation-menu:duration-200 **:data-[slot=navigation-menu-link]:focus:ring-0 **:data-[slot=navigation-menu-link]:focus:outline-none",
				className,
			)}
			{...props}
		/>
	);
}

function NavigationMenuViewport({
	className,
	...props
}: React.ComponentProps<typeof NavigationMenuPrimitive.Viewport>) {
	return (
		<div
			className={cn(
				"absolute top-full left-0 isolate z-50 flex justify-center",
			)}
		>
			<NavigationMenuPrimitive.Viewport
				data-slot="navigation-menu-viewport"
				className={cn(
					"origin-top-center bg-popover text-popover-foreground data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-90 relative mt-1.5 h-[var(--radix-navigation-menu-viewport-height)] w-full overflow-hidden rounded-md border shadow md:w-[var(--radix-navigation-menu-viewport-width)]",
					className,
				)}
				{...props}
			/>
		</div>
	);
}

function NavigationMenuLink({
	className,
	...props
}: React.ComponentProps<typeof NavigationMenuPrimitive.Link>) {
	return (
		<NavigationMenuPrimitive.Link
			data-slot="navigation-menu-link"
			className={cn(
				"data-[active=true]:focus:bg-accent data-[active=true]:hover:bg-accent data-[active=true]:bg-accent/50 data-[active=true]:text-accent-foreground hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground focus-visible:ring-ring/50 [&_svg:not([class*='text-'])]:text-muted-foreground flex flex-col gap-1 rounded-sm p-2 text-sm transition-all outline-none focus-visible:ring-[3px] focus-visible:outline-1 [&_svg:not([class*='size-'])]:size-4",
				className,
			)}
			{...props}
		/>
	);
}

function NavigationMenuIndicator({
	className,
	...props
}: React.ComponentProps<typeof NavigationMenuPrimitive.Indicator>) {
	return (
		<NavigationMenuPrimitive.Indicator
			data-slot="navigation-menu-indicator"
			className={cn(
				"data-[state=visible]:animate-in data-[state=hidden]:animate-out data-[state=hidden]:fade-out data-[state=visible]:fade-in top-full z-[1] flex h-1.5 items-end justify-center overflow-hidden",
				className,
			)}
			{...props}
		>
			<div className="bg-border relative top-[60%] h-2 w-2 rotate-45 rounded-tl-sm shadow-md" />
		</NavigationMenuPrimitive.Indicator>
	);
}

export {
	NavigationMenu,
	NavigationMenuList,
	NavigationMenuItem,
	NavigationMenuContent,
	NavigationMenuTrigger,
	NavigationMenuLink,
	NavigationMenuIndicator,
	NavigationMenuViewport,
	navigationMenuTriggerStyle,
};
</file>

<file path="apps/web/src/components/ui/pagination.tsx">
import {
	ChevronLeftIcon,
	ChevronRightIcon,
	MoreHorizontalIcon,
} from "lucide-react";
import type * as React from "react";

import { type Button, buttonVariants } from "@/components/ui/button";
import { cn } from "@/lib/utils";

function Pagination({ className, ...props }: React.ComponentProps<"nav">) {
	return (
		<nav
			aria-label="pagination"
			data-slot="pagination"
			className={cn("mx-auto flex w-full justify-center", className)}
			{...props}
		/>
	);
}

function PaginationContent({
	className,
	...props
}: React.ComponentProps<"ul">) {
	return (
		<ul
			data-slot="pagination-content"
			className={cn("flex flex-row items-center gap-1", className)}
			{...props}
		/>
	);
}

function PaginationItem({ ...props }: React.ComponentProps<"li">) {
	return <li data-slot="pagination-item" {...props} />;
}

type PaginationLinkProps = {
	isActive?: boolean;
} & Pick<React.ComponentProps<typeof Button>, "size"> &
	React.ComponentProps<"a">;

function PaginationLink({
	className,
	isActive,
	size = "icon",
	...props
}: PaginationLinkProps) {
	return (
		<a
			aria-current={isActive ? "page" : undefined}
			data-slot="pagination-link"
			data-active={isActive}
			className={cn(
				buttonVariants({
					variant: isActive ? "outline" : "ghost",
					size,
				}),
				className,
			)}
			{...props}
		/>
	);
}

function PaginationPrevious({
	className,
	...props
}: React.ComponentProps<typeof PaginationLink>) {
	return (
		<PaginationLink
			aria-label="Go to previous page"
			size="default"
			className={cn("gap-1 px-2.5 sm:pl-2.5", className)}
			{...props}
		>
			<ChevronLeftIcon />
			<span className="hidden sm:block">Previous</span>
		</PaginationLink>
	);
}

function PaginationNext({
	className,
	...props
}: React.ComponentProps<typeof PaginationLink>) {
	return (
		<PaginationLink
			aria-label="Go to next page"
			size="default"
			className={cn("gap-1 px-2.5 sm:pr-2.5", className)}
			{...props}
		>
			<span className="hidden sm:block">Next</span>
			<ChevronRightIcon />
		</PaginationLink>
	);
}

function PaginationEllipsis({
	className,
	...props
}: React.ComponentProps<"span">) {
	return (
		<span
			aria-hidden
			data-slot="pagination-ellipsis"
			className={cn("flex size-9 items-center justify-center", className)}
			{...props}
		>
			<MoreHorizontalIcon className="size-4" />
			<span className="sr-only">More pages</span>
		</span>
	);
}

export {
	Pagination,
	PaginationContent,
	PaginationLink,
	PaginationItem,
	PaginationPrevious,
	PaginationNext,
	PaginationEllipsis,
};
</file>

<file path="apps/web/src/components/ui/popover.tsx">
"use client";

import * as PopoverPrimitive from "@radix-ui/react-popover";
import type * as React from "react";

import { cn } from "@/lib/utils";

function Popover({
	...props
}: React.ComponentProps<typeof PopoverPrimitive.Root>) {
	return <PopoverPrimitive.Root data-slot="popover" {...props} />;
}

function PopoverTrigger({
	...props
}: React.ComponentProps<typeof PopoverPrimitive.Trigger>) {
	return <PopoverPrimitive.Trigger data-slot="popover-trigger" {...props} />;
}

function PopoverContent({
	className,
	align = "center",
	sideOffset = 4,
	...props
}: React.ComponentProps<typeof PopoverPrimitive.Content>) {
	return (
		<PopoverPrimitive.Portal>
			<PopoverPrimitive.Content
				data-slot="popover-content"
				align={align}
				sideOffset={sideOffset}
				className={cn(
					"bg-popover text-popover-foreground data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2 z-50 w-72 origin-(--radix-popover-content-transform-origin) rounded-md border p-4 shadow-md outline-hidden",
					className,
				)}
				{...props}
			/>
		</PopoverPrimitive.Portal>
	);
}

function PopoverAnchor({
	...props
}: React.ComponentProps<typeof PopoverPrimitive.Anchor>) {
	return <PopoverPrimitive.Anchor data-slot="popover-anchor" {...props} />;
}

export { Popover, PopoverTrigger, PopoverContent, PopoverAnchor };
</file>

<file path="apps/web/src/components/ui/progress-steps.tsx">
import { cn } from "@/lib/utils";
import { CheckIcon, Loader2 } from "lucide-react";

export type Step = {
	id: string;
	label: string;
	description?: string;
	status: "pending" | "in_progress" | "completed" | "error";
	progress?: number; // 0-100
	errorMessage?: string;
};

type ProgressStepsProps = {
	steps: Step[];
	currentStepId?: string;
	className?: string;
};

export function ProgressSteps({
	steps,
	currentStepId,
	className,
}: ProgressStepsProps) {
	return (
		<div className={cn("w-full space-y-2", className)}>
			<div className="flex items-center justify-between">
				<div className="text-sm font-medium">Processing Status</div>
				<div className="text-xs text-muted-foreground">
					{steps.filter((step) => step.status === "completed").length} of{" "}
					{steps.length} completed
				</div>
			</div>
			<ul className="space-y-3">
				{steps.map((step, i) => {
					const isCurrent = step.id === currentStepId;
					const isCompleted = step.status === "completed";
					const isInProgress = step.status === "in_progress";
					const isError = step.status === "error";

					return (
						<li key={step.id} className="relative">
							<div
								className={cn(
									"group relative flex items-start",
									isCurrent && "animate-in fade-in duration-300",
								)}
							>
								<div
									className={cn(
										"flex h-9 w-9 shrink-0 items-center justify-center rounded-full border",
										isCompleted &&
											"bg-primary border-primary text-primary-foreground",
										isInProgress && "border-primary/70 bg-primary/10",
										isError &&
											"bg-destructive border-destructive text-destructive-foreground",
										!isCompleted &&
											!isInProgress &&
											!isError &&
											"border-muted-foreground/30",
									)}
								>
									{isCompleted && <CheckIcon className="h-4 w-4" />}
									{isInProgress && (
										<Loader2 className="h-4 w-4 animate-spin text-primary" />
									)}
									{isError && <span className="text-xs font-bold">!</span>}
									{!isCompleted && !isInProgress && !isError && (
										<span className="text-xs text-muted-foreground">
											{i + 1}
										</span>
									)}
								</div>

								<div className="ml-4 min-w-0 flex-1">
									<div className="flex items-center justify-between">
										<div
											className={cn(
												"text-sm font-medium",
												isCompleted && "text-foreground",
												isInProgress && "text-primary",
												isError && "text-destructive",
												!isCompleted &&
													!isInProgress &&
													!isError &&
													"text-muted-foreground",
											)}
										>
											{step.label}
										</div>
										{step.status === "in_progress" &&
											step.progress !== undefined && (
												<div className="text-xs text-muted-foreground">
													{step.progress}%
												</div>
											)}
									</div>

									{step.description && (
										<p className="text-xs text-muted-foreground mt-0.5">
											{step.description}
										</p>
									)}

									{isError && step.errorMessage && (
										<p className="text-xs text-destructive mt-1">
											{step.errorMessage}
										</p>
									)}

									{isInProgress && step.progress !== undefined && (
										<div className="mt-2 h-1.5 w-full overflow-hidden rounded-full bg-secondary">
											<div
												className="h-full bg-primary transition-all duration-300 ease-in-out"
												style={{ width: `${step.progress}%` }}
											/>
										</div>
									)}
								</div>
							</div>

							{i < steps.length - 1 && (
								<div
									className={cn(
										"absolute left-4 top-9 h-full w-px -translate-x-1/2 bg-border",
										isCompleted && "bg-primary",
									)}
									aria-hidden="true"
								/>
							)}
						</li>
					);
				})}
			</ul>
		</div>
	);
}
</file>

<file path="apps/web/src/components/ui/progress.tsx">
import * as ProgressPrimitive from "@radix-ui/react-progress";
import type * as React from "react";

import { cn } from "@/lib/utils";

function Progress({
	className,
	value,
	...props
}: React.ComponentProps<typeof ProgressPrimitive.Root>) {
	return (
		<ProgressPrimitive.Root
			data-slot="progress"
			className={cn(
				"bg-primary/20 relative h-2 w-full overflow-hidden rounded-full",
				className,
			)}
			{...props}
		>
			<ProgressPrimitive.Indicator
				data-slot="progress-indicator"
				className="bg-primary h-full w-full flex-1 transition-all"
				style={{ transform: `translateX(-${100 - (value || 0)}%)` }}
			/>
		</ProgressPrimitive.Root>
	);
}

export { Progress };
</file>

<file path="apps/web/src/components/ui/radio-group.tsx">
"use client";

import * as RadioGroupPrimitive from "@radix-ui/react-radio-group";
import { CircleIcon } from "lucide-react";
import type * as React from "react";

import { cn } from "@/lib/utils";

function RadioGroup({
	className,
	...props
}: React.ComponentProps<typeof RadioGroupPrimitive.Root>) {
	return (
		<RadioGroupPrimitive.Root
			data-slot="radio-group"
			className={cn("grid gap-3", className)}
			{...props}
		/>
	);
}

function RadioGroupItem({
	className,
	...props
}: React.ComponentProps<typeof RadioGroupPrimitive.Item>) {
	return (
		<RadioGroupPrimitive.Item
			data-slot="radio-group-item"
			className={cn(
				"border-input text-primary focus-visible:border-ring focus-visible:ring-ring/50 aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive dark:bg-input/30 aspect-square size-4 shrink-0 rounded-full border shadow-xs transition-[color,box-shadow] outline-none focus-visible:ring-[3px] disabled:cursor-not-allowed disabled:opacity-50",
				className,
			)}
			{...props}
		>
			<RadioGroupPrimitive.Indicator
				data-slot="radio-group-indicator"
				className="relative flex items-center justify-center"
			>
				<CircleIcon className="fill-primary absolute top-1/2 left-1/2 size-2 -translate-x-1/2 -translate-y-1/2" />
			</RadioGroupPrimitive.Indicator>
		</RadioGroupPrimitive.Item>
	);
}

export { RadioGroup, RadioGroupItem };
</file>

<file path="apps/web/src/components/ui/resizable.tsx">
import { GripVerticalIcon } from "lucide-react";
import type * as React from "react";
import * as ResizablePrimitive from "react-resizable-panels";

import { cn } from "@/lib/utils";

function ResizablePanelGroup({
	className,
	...props
}: React.ComponentProps<typeof ResizablePrimitive.PanelGroup>) {
	return (
		<ResizablePrimitive.PanelGroup
			data-slot="resizable-panel-group"
			className={cn(
				"flex h-full w-full data-[panel-group-direction=vertical]:flex-col",
				className,
			)}
			{...props}
		/>
	);
}

function ResizablePanel({
	...props
}: React.ComponentProps<typeof ResizablePrimitive.Panel>) {
	return <ResizablePrimitive.Panel data-slot="resizable-panel" {...props} />;
}

function ResizableHandle({
	withHandle,
	className,
	...props
}: React.ComponentProps<typeof ResizablePrimitive.PanelResizeHandle> & {
	withHandle?: boolean;
}) {
	return (
		<ResizablePrimitive.PanelResizeHandle
			data-slot="resizable-handle"
			className={cn(
				"bg-border focus-visible:ring-ring relative flex w-px items-center justify-center after:absolute after:inset-y-0 after:left-1/2 after:w-1 after:-translate-x-1/2 focus-visible:ring-1 focus-visible:ring-offset-1 focus-visible:outline-hidden data-[panel-group-direction=vertical]:h-px data-[panel-group-direction=vertical]:w-full data-[panel-group-direction=vertical]:after:left-0 data-[panel-group-direction=vertical]:after:h-1 data-[panel-group-direction=vertical]:after:w-full data-[panel-group-direction=vertical]:after:-translate-y-1/2 data-[panel-group-direction=vertical]:after:translate-x-0 [&[data-panel-group-direction=vertical]>div]:rotate-90",
				className,
			)}
			{...props}
		>
			{withHandle && (
				<div className="bg-border z-10 flex h-4 w-3 items-center justify-center rounded-xs border">
					<GripVerticalIcon className="size-2.5" />
				</div>
			)}
		</ResizablePrimitive.PanelResizeHandle>
	);
}

export { ResizablePanelGroup, ResizablePanel, ResizableHandle };
</file>

<file path="apps/web/src/components/ui/scroll-area.tsx">
"use client";

import * as ScrollAreaPrimitive from "@radix-ui/react-scroll-area";
import type * as React from "react";

import { cn } from "@/lib/utils";

function ScrollArea({
	className,
	children,
	...props
}: React.ComponentProps<typeof ScrollAreaPrimitive.Root>) {
	return (
		<ScrollAreaPrimitive.Root
			data-slot="scroll-area"
			className={cn("relative", className)}
			{...props}
		>
			<ScrollAreaPrimitive.Viewport
				data-slot="scroll-area-viewport"
				className="focus-visible:ring-ring/50 size-full rounded-[inherit] transition-[color,box-shadow] outline-none focus-visible:ring-[3px] focus-visible:outline-1"
			>
				{children}
			</ScrollAreaPrimitive.Viewport>
			<ScrollBar />
			<ScrollAreaPrimitive.Corner />
		</ScrollAreaPrimitive.Root>
	);
}

function ScrollBar({
	className,
	orientation = "vertical",
	...props
}: React.ComponentProps<typeof ScrollAreaPrimitive.ScrollAreaScrollbar>) {
	return (
		<ScrollAreaPrimitive.ScrollAreaScrollbar
			data-slot="scroll-area-scrollbar"
			orientation={orientation}
			className={cn(
				"flex touch-none p-px transition-colors select-none",
				orientation === "vertical" &&
					"h-full w-2.5 border-l border-l-transparent",
				orientation === "horizontal" &&
					"h-2.5 flex-col border-t border-t-transparent",
				className,
			)}
			{...props}
		>
			<ScrollAreaPrimitive.ScrollAreaThumb
				data-slot="scroll-area-thumb"
				className="bg-border relative flex-1 rounded-full"
			/>
		</ScrollAreaPrimitive.ScrollAreaScrollbar>
	);
}

export { ScrollArea, ScrollBar };
</file>

<file path="apps/web/src/components/ui/select.tsx">
import * as SelectPrimitive from "@radix-ui/react-select";
import { CheckIcon, ChevronDownIcon, ChevronUpIcon } from "lucide-react";
import type * as React from "react";

import { cn } from "@/lib/utils";

function Select({
	...props
}: React.ComponentProps<typeof SelectPrimitive.Root>) {
	return <SelectPrimitive.Root data-slot="select" {...props} />;
}

function SelectGroup({
	...props
}: React.ComponentProps<typeof SelectPrimitive.Group>) {
	return <SelectPrimitive.Group data-slot="select-group" {...props} />;
}

function SelectValue({
	...props
}: React.ComponentProps<typeof SelectPrimitive.Value>) {
	return <SelectPrimitive.Value data-slot="select-value" {...props} />;
}

function SelectTrigger({
	className,
	size = "default",
	children,
	...props
}: React.ComponentProps<typeof SelectPrimitive.Trigger> & {
	size?: "sm" | "default";
}) {
	return (
		<SelectPrimitive.Trigger
			data-slot="select-trigger"
			data-size={size}
			className={cn(
				"border-input data-[placeholder]:text-muted-foreground [&_svg:not([class*='text-'])]:text-muted-foreground focus-visible:border-ring focus-visible:ring-ring/50 aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive dark:bg-input/30 dark:hover:bg-input/50 flex w-fit items-center justify-between gap-2 rounded-md border bg-transparent px-3 py-2 text-sm whitespace-nowrap shadow-xs transition-[color,box-shadow] outline-none focus-visible:ring-[3px] disabled:cursor-not-allowed disabled:opacity-50 data-[size=default]:h-9 data-[size=sm]:h-8 *:data-[slot=select-value]:line-clamp-1 *:data-[slot=select-value]:flex *:data-[slot=select-value]:items-center *:data-[slot=select-value]:gap-2 [&_svg]:pointer-events-none [&_svg]:shrink-0 [&_svg:not([class*='size-'])]:size-4",
				className,
			)}
			{...props}
		>
			{children}
			<SelectPrimitive.Icon asChild>
				<ChevronDownIcon className="size-4 opacity-50" />
			</SelectPrimitive.Icon>
		</SelectPrimitive.Trigger>
	);
}

function SelectContent({
	className,
	children,
	position = "popper",
	...props
}: React.ComponentProps<typeof SelectPrimitive.Content>) {
	return (
		<SelectPrimitive.Portal>
			<SelectPrimitive.Content
				data-slot="select-content"
				className={cn(
					"bg-popover text-popover-foreground data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2 relative z-50 max-h-(--radix-select-content-available-height) min-w-[8rem] origin-(--radix-select-content-transform-origin) overflow-x-hidden overflow-y-auto rounded-md border shadow-md",
					position === "popper" &&
						"data-[side=bottom]:translate-y-1 data-[side=left]:-translate-x-1 data-[side=right]:translate-x-1 data-[side=top]:-translate-y-1",
					className,
				)}
				position={position}
				{...props}
			>
				<SelectScrollUpButton />
				<SelectPrimitive.Viewport
					className={cn(
						"p-1",
						position === "popper" &&
							"h-[var(--radix-select-trigger-height)] w-full min-w-[var(--radix-select-trigger-width)] scroll-my-1",
					)}
				>
					{children}
				</SelectPrimitive.Viewport>
				<SelectScrollDownButton />
			</SelectPrimitive.Content>
		</SelectPrimitive.Portal>
	);
}

function SelectLabel({
	className,
	...props
}: React.ComponentProps<typeof SelectPrimitive.Label>) {
	return (
		<SelectPrimitive.Label
			data-slot="select-label"
			className={cn("text-muted-foreground px-2 py-1.5 text-xs", className)}
			{...props}
		/>
	);
}

function SelectItem({
	className,
	children,
	...props
}: React.ComponentProps<typeof SelectPrimitive.Item>) {
	return (
		<SelectPrimitive.Item
			data-slot="select-item"
			className={cn(
				"focus:bg-accent focus:text-accent-foreground [&_svg:not([class*='text-'])]:text-muted-foreground relative flex w-full cursor-default items-center gap-2 rounded-sm py-1.5 pr-8 pl-2 text-sm outline-hidden select-none data-[disabled]:pointer-events-none data-[disabled]:opacity-50 [&_svg]:pointer-events-none [&_svg]:shrink-0 [&_svg:not([class*='size-'])]:size-4 *:[span]:last:flex *:[span]:last:items-center *:[span]:last:gap-2",
				className,
			)}
			{...props}
		>
			<span className="absolute right-2 flex size-3.5 items-center justify-center">
				<SelectPrimitive.ItemIndicator>
					<CheckIcon className="size-4" />
				</SelectPrimitive.ItemIndicator>
			</span>
			<SelectPrimitive.ItemText>{children}</SelectPrimitive.ItemText>
		</SelectPrimitive.Item>
	);
}

function SelectSeparator({
	className,
	...props
}: React.ComponentProps<typeof SelectPrimitive.Separator>) {
	return (
		<SelectPrimitive.Separator
			data-slot="select-separator"
			className={cn("bg-border pointer-events-none -mx-1 my-1 h-px", className)}
			{...props}
		/>
	);
}

function SelectScrollUpButton({
	className,
	...props
}: React.ComponentProps<typeof SelectPrimitive.ScrollUpButton>) {
	return (
		<SelectPrimitive.ScrollUpButton
			data-slot="select-scroll-up-button"
			className={cn(
				"flex cursor-default items-center justify-center py-1",
				className,
			)}
			{...props}
		>
			<ChevronUpIcon className="size-4" />
		</SelectPrimitive.ScrollUpButton>
	);
}

function SelectScrollDownButton({
	className,
	...props
}: React.ComponentProps<typeof SelectPrimitive.ScrollDownButton>) {
	return (
		<SelectPrimitive.ScrollDownButton
			data-slot="select-scroll-down-button"
			className={cn(
				"flex cursor-default items-center justify-center py-1",
				className,
			)}
			{...props}
		>
			<ChevronDownIcon className="size-4" />
		</SelectPrimitive.ScrollDownButton>
	);
}

export {
	Select,
	SelectContent,
	SelectGroup,
	SelectItem,
	SelectLabel,
	SelectScrollDownButton,
	SelectScrollUpButton,
	SelectSeparator,
	SelectTrigger,
	SelectValue,
};
</file>

<file path="apps/web/src/components/ui/separator.tsx">
"use client";

import * as SeparatorPrimitive from "@radix-ui/react-separator";
import type * as React from "react";

import { cn } from "@/lib/utils";

function Separator({
	className,
	orientation = "horizontal",
	decorative = true,
	...props
}: React.ComponentProps<typeof SeparatorPrimitive.Root>) {
	return (
		<SeparatorPrimitive.Root
			data-slot="separator-root"
			decorative={decorative}
			orientation={orientation}
			className={cn(
				"bg-border shrink-0 data-[orientation=horizontal]:h-px data-[orientation=horizontal]:w-full data-[orientation=vertical]:h-full data-[orientation=vertical]:w-px",
				className,
			)}
			{...props}
		/>
	);
}

export { Separator };
</file>

<file path="apps/web/src/components/ui/sheet.tsx">
import * as SheetPrimitive from "@radix-ui/react-dialog";
import { XIcon } from "lucide-react";
import type * as React from "react";

import { cn } from "@/lib/utils";

function Sheet({ ...props }: React.ComponentProps<typeof SheetPrimitive.Root>) {
	return <SheetPrimitive.Root data-slot="sheet" {...props} />;
}

function SheetTrigger({
	...props
}: React.ComponentProps<typeof SheetPrimitive.Trigger>) {
	return <SheetPrimitive.Trigger data-slot="sheet-trigger" {...props} />;
}

function SheetClose({
	...props
}: React.ComponentProps<typeof SheetPrimitive.Close>) {
	return <SheetPrimitive.Close data-slot="sheet-close" {...props} />;
}

function SheetPortal({
	...props
}: React.ComponentProps<typeof SheetPrimitive.Portal>) {
	return <SheetPrimitive.Portal data-slot="sheet-portal" {...props} />;
}

function SheetOverlay({
	className,
	...props
}: React.ComponentProps<typeof SheetPrimitive.Overlay>) {
	return (
		<SheetPrimitive.Overlay
			data-slot="sheet-overlay"
			className={cn(
				"data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 fixed inset-0 z-50 bg-black/50",
				className,
			)}
			{...props}
		/>
	);
}

function SheetContent({
	className,
	children,
	side = "right",
	...props
}: React.ComponentProps<typeof SheetPrimitive.Content> & {
	side?: "top" | "right" | "bottom" | "left";
}) {
	return (
		<SheetPortal>
			<SheetOverlay />
			<SheetPrimitive.Content
				data-slot="sheet-content"
				className={cn(
					"bg-background data-[state=open]:animate-in data-[state=closed]:animate-out fixed z-50 flex flex-col gap-4 shadow-lg transition ease-in-out data-[state=closed]:duration-300 data-[state=open]:duration-500",
					side === "right" &&
						"data-[state=closed]:slide-out-to-right data-[state=open]:slide-in-from-right inset-y-0 right-0 h-full w-3/4 border-l sm:max-w-sm",
					side === "left" &&
						"data-[state=closed]:slide-out-to-left data-[state=open]:slide-in-from-left inset-y-0 left-0 h-full w-3/4 border-r sm:max-w-sm",
					side === "top" &&
						"data-[state=closed]:slide-out-to-top data-[state=open]:slide-in-from-top inset-x-0 top-0 h-auto border-b",
					side === "bottom" &&
						"data-[state=closed]:slide-out-to-bottom data-[state=open]:slide-in-from-bottom inset-x-0 bottom-0 h-auto border-t",
					className,
				)}
				{...props}
			>
				{children}
				<SheetPrimitive.Close className="ring-offset-background focus:ring-ring data-[state=open]:bg-secondary absolute top-4 right-4 rounded-xs opacity-70 transition-opacity hover:opacity-100 focus:ring-2 focus:ring-offset-2 focus:outline-hidden disabled:pointer-events-none">
					<XIcon className="size-4" />
					<span className="sr-only">Close</span>
				</SheetPrimitive.Close>
			</SheetPrimitive.Content>
		</SheetPortal>
	);
}

function SheetHeader({ className, ...props }: React.ComponentProps<"div">) {
	return (
		<div
			data-slot="sheet-header"
			className={cn("flex flex-col gap-1.5 p-4", className)}
			{...props}
		/>
	);
}

function SheetFooter({ className, ...props }: React.ComponentProps<"div">) {
	return (
		<div
			data-slot="sheet-footer"
			className={cn("mt-auto flex flex-col gap-2 p-4", className)}
			{...props}
		/>
	);
}

function SheetTitle({
	className,
	...props
}: React.ComponentProps<typeof SheetPrimitive.Title>) {
	return (
		<SheetPrimitive.Title
			data-slot="sheet-title"
			className={cn("text-foreground font-semibold", className)}
			{...props}
		/>
	);
}

function SheetDescription({
	className,
	...props
}: React.ComponentProps<typeof SheetPrimitive.Description>) {
	return (
		<SheetPrimitive.Description
			data-slot="sheet-description"
			className={cn("text-muted-foreground text-sm", className)}
			{...props}
		/>
	);
}

export {
	Sheet,
	SheetTrigger,
	SheetClose,
	SheetContent,
	SheetHeader,
	SheetFooter,
	SheetTitle,
	SheetDescription,
};
</file>

<file path="apps/web/src/components/ui/sidebar.tsx">
"use client";

import { Slot } from "@radix-ui/react-slot";
import { type VariantProps, cva } from "class-variance-authority";
import { PanelLeftIcon } from "lucide-react";
import * as React from "react";

import { Button } from "@/components/ui/button";
import { Input } from "@/components/ui/input";
import { Separator } from "@/components/ui/separator";
import {
	Sheet,
	SheetContent,
	SheetDescription,
	SheetHeader,
	SheetTitle,
} from "@/components/ui/sheet";
import { Skeleton } from "@/components/ui/skeleton";
import {
	Tooltip,
	TooltipContent,
	TooltipProvider,
	TooltipTrigger,
} from "@/components/ui/tooltip";
import { useIsMobile } from "@/hooks/use-mobile";
import { cn } from "@/lib/utils";

const SIDEBAR_COOKIE_NAME = "sidebar_state";
const SIDEBAR_COOKIE_MAX_AGE = 60 * 60 * 24 * 7;
const SIDEBAR_WIDTH = "16rem";
const SIDEBAR_WIDTH_MOBILE = "18rem";
const SIDEBAR_WIDTH_ICON = "3rem";
const SIDEBAR_KEYBOARD_SHORTCUT = "b";

type SidebarContextProps = {
	state: "expanded" | "collapsed";
	open: boolean;
	setOpen: (open: boolean) => void;
	openMobile: boolean;
	setOpenMobile: (open: boolean) => void;
	isMobile: boolean;
	toggleSidebar: () => void;
};

const SidebarContext = React.createContext<SidebarContextProps | null>(null);

function useSidebar() {
	const context = React.useContext(SidebarContext);
	if (!context) {
		throw new Error("useSidebar must be used within a SidebarProvider.");
	}

	return context;
}

function SidebarProvider({
	defaultOpen = true,
	open: openProp,
	onOpenChange: setOpenProp,
	className,
	style,
	children,
	...props
}: React.ComponentProps<"div"> & {
	defaultOpen?: boolean;
	open?: boolean;
	onOpenChange?: (open: boolean) => void;
}) {
	const isMobile = useIsMobile();
	const [openMobile, setOpenMobile] = React.useState(false);

	// This is the internal state of the sidebar.
	// We use openProp and setOpenProp for control from outside the component.
	const [_open, _setOpen] = React.useState(defaultOpen);
	const open = openProp ?? _open;
	const setOpen = React.useCallback(
		(value: boolean | ((value: boolean) => boolean)) => {
			const openState = typeof value === "function" ? value(open) : value;
			if (setOpenProp) {
				setOpenProp(openState);
			} else {
				_setOpen(openState);
			}

			// This sets the cookie to keep the sidebar state.
			document.cookie = `${SIDEBAR_COOKIE_NAME}=${openState}; path=/; max-age=${SIDEBAR_COOKIE_MAX_AGE}`;
		},
		[setOpenProp, open],
	);

	// Helper to toggle the sidebar.
	const toggleSidebar = React.useCallback(() => {
		return isMobile ? setOpenMobile((open) => !open) : setOpen((open) => !open);
	}, [isMobile, setOpen, setOpenMobile]);

	// Adds a keyboard shortcut to toggle the sidebar.
	React.useEffect(() => {
		const handleKeyDown = (event: KeyboardEvent) => {
			if (
				event.key === SIDEBAR_KEYBOARD_SHORTCUT &&
				(event.metaKey || event.ctrlKey)
			) {
				event.preventDefault();
				toggleSidebar();
			}
		};

		window.addEventListener("keydown", handleKeyDown);
		return () => window.removeEventListener("keydown", handleKeyDown);
	}, [toggleSidebar]);

	// We add a state so that we can do data-state="expanded" or "collapsed".
	// This makes it easier to style the sidebar with Tailwind classes.
	const state = open ? "expanded" : "collapsed";

	const contextValue = React.useMemo<SidebarContextProps>(
		() => ({
			state,
			open,
			setOpen,
			isMobile,
			openMobile,
			setOpenMobile,
			toggleSidebar,
		}),
		[state, open, setOpen, isMobile, openMobile, setOpenMobile, toggleSidebar],
	);

	return (
		<SidebarContext.Provider value={contextValue}>
			<TooltipProvider delayDuration={0}>
				<div
					data-slot="sidebar-wrapper"
					style={
						{
							"--sidebar-width": SIDEBAR_WIDTH,
							"--sidebar-width-icon": SIDEBAR_WIDTH_ICON,
							...style,
						} as React.CSSProperties
					}
					className={cn(
						"group/sidebar-wrapper has-data-[variant=inset]:bg-sidebar flex min-h-svh w-full",
						className,
					)}
					{...props}
				>
					{children}
				</div>
			</TooltipProvider>
		</SidebarContext.Provider>
	);
}

function Sidebar({
	side = "left",
	variant = "sidebar",
	collapsible = "offcanvas",
	className,
	children,
	...props
}: React.ComponentProps<"div"> & {
	side?: "left" | "right";
	variant?: "sidebar" | "floating" | "inset";
	collapsible?: "offcanvas" | "icon" | "none";
}) {
	const { isMobile, state, openMobile, setOpenMobile } = useSidebar();

	if (collapsible === "none") {
		return (
			<div
				data-slot="sidebar"
				className={cn(
					"bg-sidebar text-sidebar-foreground flex h-full w-(--sidebar-width) flex-col",
					className,
				)}
				{...props}
			>
				{children}
			</div>
		);
	}

	if (isMobile) {
		return (
			<Sheet open={openMobile} onOpenChange={setOpenMobile} {...props}>
				<SheetContent
					data-sidebar="sidebar"
					data-slot="sidebar"
					data-mobile="true"
					className="bg-sidebar text-sidebar-foreground w-(--sidebar-width) p-0 [&>button]:hidden"
					style={
						{
							"--sidebar-width": SIDEBAR_WIDTH_MOBILE,
						} as React.CSSProperties
					}
					side={side}
				>
					<SheetHeader className="sr-only">
						<SheetTitle>Sidebar</SheetTitle>
						<SheetDescription>Displays the mobile sidebar.</SheetDescription>
					</SheetHeader>
					<div className="flex h-full w-full flex-col">{children}</div>
				</SheetContent>
			</Sheet>
		);
	}

	return (
		<div
			className="group peer text-sidebar-foreground hidden md:block"
			data-state={state}
			data-collapsible={state === "collapsed" ? collapsible : ""}
			data-variant={variant}
			data-side={side}
			data-slot="sidebar"
		>
			{/* This is what handles the sidebar gap on desktop */}
			<div
				data-slot="sidebar-gap"
				className={cn(
					"relative w-(--sidebar-width) bg-transparent transition-[width] duration-200 ease-linear",
					"group-data-[collapsible=offcanvas]:w-0",
					"group-data-[side=right]:rotate-180",
					variant === "floating" || variant === "inset"
						? "group-data-[collapsible=icon]:w-[calc(var(--sidebar-width-icon)+(--spacing(4)))]"
						: "group-data-[collapsible=icon]:w-(--sidebar-width-icon)",
				)}
			/>
			<div
				data-slot="sidebar-container"
				className={cn(
					"fixed inset-y-0 z-10 hidden h-svh w-(--sidebar-width) transition-[left,right,width] duration-200 ease-linear md:flex",
					side === "left"
						? "left-0 group-data-[collapsible=offcanvas]:left-[calc(var(--sidebar-width)*-1)]"
						: "right-0 group-data-[collapsible=offcanvas]:right-[calc(var(--sidebar-width)*-1)]",
					// Adjust the padding for floating and inset variants.
					variant === "floating" || variant === "inset"
						? "p-2 group-data-[collapsible=icon]:w-[calc(var(--sidebar-width-icon)+(--spacing(4))+2px)]"
						: "group-data-[collapsible=icon]:w-(--sidebar-width-icon) group-data-[side=left]:border-r group-data-[side=right]:border-l",
					className,
				)}
				{...props}
			>
				<div
					data-sidebar="sidebar"
					data-slot="sidebar-inner"
					className="bg-sidebar group-data-[variant=floating]:border-sidebar-border flex h-full w-full flex-col group-data-[variant=floating]:rounded-lg group-data-[variant=floating]:border group-data-[variant=floating]:shadow-sm"
				>
					{children}
				</div>
			</div>
		</div>
	);
}

function SidebarTrigger({
	className,
	onClick,
	...props
}: React.ComponentProps<typeof Button>) {
	const { toggleSidebar } = useSidebar();

	return (
		<Button
			data-sidebar="trigger"
			data-slot="sidebar-trigger"
			variant="ghost"
			size="icon"
			className={cn("size-7", className)}
			onClick={(event) => {
				onClick?.(event);
				toggleSidebar();
			}}
			{...props}
		>
			<PanelLeftIcon />
			<span className="sr-only">Toggle Sidebar</span>
		</Button>
	);
}

function SidebarRail({ className, ...props }: React.ComponentProps<"button">) {
	const { toggleSidebar } = useSidebar();

	return (
		<button
			data-sidebar="rail"
			data-slot="sidebar-rail"
			aria-label="Toggle Sidebar"
			tabIndex={-1}
			onClick={toggleSidebar}
			title="Toggle Sidebar"
			className={cn(
				"hover:after:bg-sidebar-border absolute inset-y-0 z-20 hidden w-4 -translate-x-1/2 transition-all ease-linear group-data-[side=left]:-right-4 group-data-[side=right]:left-0 after:absolute after:inset-y-0 after:left-1/2 after:w-[2px] sm:flex",
				"in-data-[side=left]:cursor-w-resize in-data-[side=right]:cursor-e-resize",
				"[[data-side=left][data-state=collapsed]_&]:cursor-e-resize [[data-side=right][data-state=collapsed]_&]:cursor-w-resize",
				"hover:group-data-[collapsible=offcanvas]:bg-sidebar group-data-[collapsible=offcanvas]:translate-x-0 group-data-[collapsible=offcanvas]:after:left-full",
				"[[data-side=left][data-collapsible=offcanvas]_&]:-right-2",
				"[[data-side=right][data-collapsible=offcanvas]_&]:-left-2",
				className,
			)}
			{...props}
		/>
	);
}

function SidebarInset({ className, ...props }: React.ComponentProps<"main">) {
	return (
		<main
			data-slot="sidebar-inset"
			className={cn(
				"bg-background relative flex w-full flex-1 flex-col",
				"md:peer-data-[variant=inset]:m-2 md:peer-data-[variant=inset]:ml-0 md:peer-data-[variant=inset]:rounded-xl md:peer-data-[variant=inset]:shadow-sm md:peer-data-[variant=inset]:peer-data-[state=collapsed]:ml-2",
				className,
			)}
			{...props}
		/>
	);
}

function SidebarInput({
	className,
	...props
}: React.ComponentProps<typeof Input>) {
	return (
		<Input
			data-slot="sidebar-input"
			data-sidebar="input"
			className={cn("bg-background h-8 w-full shadow-none", className)}
			{...props}
		/>
	);
}

function SidebarHeader({ className, ...props }: React.ComponentProps<"div">) {
	return (
		<div
			data-slot="sidebar-header"
			data-sidebar="header"
			className={cn("flex flex-col gap-2 p-2", className)}
			{...props}
		/>
	);
}

function SidebarFooter({ className, ...props }: React.ComponentProps<"div">) {
	return (
		<div
			data-slot="sidebar-footer"
			data-sidebar="footer"
			className={cn("flex flex-col gap-2 p-2", className)}
			{...props}
		/>
	);
}

function SidebarSeparator({
	className,
	...props
}: React.ComponentProps<typeof Separator>) {
	return (
		<Separator
			data-slot="sidebar-separator"
			data-sidebar="separator"
			className={cn("bg-sidebar-border mx-2 w-auto", className)}
			{...props}
		/>
	);
}

function SidebarContent({ className, ...props }: React.ComponentProps<"div">) {
	return (
		<div
			data-slot="sidebar-content"
			data-sidebar="content"
			className={cn(
				"flex min-h-0 flex-1 flex-col gap-2 overflow-auto group-data-[collapsible=icon]:overflow-hidden",
				className,
			)}
			{...props}
		/>
	);
}

function SidebarGroup({ className, ...props }: React.ComponentProps<"div">) {
	return (
		<div
			data-slot="sidebar-group"
			data-sidebar="group"
			className={cn("relative flex w-full min-w-0 flex-col p-2", className)}
			{...props}
		/>
	);
}

function SidebarGroupLabel({
	className,
	asChild = false,
	...props
}: React.ComponentProps<"div"> & { asChild?: boolean }) {
	const Comp = asChild ? Slot : "div";

	return (
		<Comp
			data-slot="sidebar-group-label"
			data-sidebar="group-label"
			className={cn(
				"text-sidebar-foreground/70 ring-sidebar-ring flex h-8 shrink-0 items-center rounded-md px-2 text-xs font-medium outline-hidden transition-[margin,opacity] duration-200 ease-linear focus-visible:ring-2 [&>svg]:size-4 [&>svg]:shrink-0",
				"group-data-[collapsible=icon]:-mt-8 group-data-[collapsible=icon]:opacity-0",
				className,
			)}
			{...props}
		/>
	);
}

function SidebarGroupAction({
	className,
	asChild = false,
	...props
}: React.ComponentProps<"button"> & { asChild?: boolean }) {
	const Comp = asChild ? Slot : "button";

	return (
		<Comp
			data-slot="sidebar-group-action"
			data-sidebar="group-action"
			className={cn(
				"text-sidebar-foreground ring-sidebar-ring hover:bg-sidebar-accent hover:text-sidebar-accent-foreground absolute top-3.5 right-3 flex aspect-square w-5 items-center justify-center rounded-md p-0 outline-hidden transition-transform focus-visible:ring-2 [&>svg]:size-4 [&>svg]:shrink-0",
				// Increases the hit area of the button on mobile.
				"after:absolute after:-inset-2 md:after:hidden",
				"group-data-[collapsible=icon]:hidden",
				className,
			)}
			{...props}
		/>
	);
}

function SidebarGroupContent({
	className,
	...props
}: React.ComponentProps<"div">) {
	return (
		<div
			data-slot="sidebar-group-content"
			data-sidebar="group-content"
			className={cn("w-full text-sm", className)}
			{...props}
		/>
	);
}

function SidebarMenu({ className, ...props }: React.ComponentProps<"ul">) {
	return (
		<ul
			data-slot="sidebar-menu"
			data-sidebar="menu"
			className={cn("flex w-full min-w-0 flex-col gap-1", className)}
			{...props}
		/>
	);
}

function SidebarMenuItem({ className, ...props }: React.ComponentProps<"li">) {
	return (
		<li
			data-slot="sidebar-menu-item"
			data-sidebar="menu-item"
			className={cn("group/menu-item relative", className)}
			{...props}
		/>
	);
}

const sidebarMenuButtonVariants = cva(
	"peer/menu-button flex w-full items-center gap-2 overflow-hidden rounded-md p-2 text-left text-sm outline-hidden ring-sidebar-ring transition-[width,height,padding] hover:bg-sidebar-accent hover:text-sidebar-accent-foreground focus-visible:ring-2 active:bg-sidebar-accent active:text-sidebar-accent-foreground disabled:pointer-events-none disabled:opacity-50 group-has-data-[sidebar=menu-action]/menu-item:pr-8 aria-disabled:pointer-events-none aria-disabled:opacity-50 data-[active=true]:bg-sidebar-accent data-[active=true]:font-medium data-[active=true]:text-sidebar-accent-foreground data-[state=open]:hover:bg-sidebar-accent data-[state=open]:hover:text-sidebar-accent-foreground group-data-[collapsible=icon]:size-8! group-data-[collapsible=icon]:p-2! [&>span:last-child]:truncate [&>svg]:size-4 [&>svg]:shrink-0",
	{
		variants: {
			variant: {
				default: "hover:bg-sidebar-accent hover:text-sidebar-accent-foreground",
				outline:
					"bg-background shadow-[0_0_0_1px_hsl(var(--sidebar-border))] hover:bg-sidebar-accent hover:text-sidebar-accent-foreground hover:shadow-[0_0_0_1px_hsl(var(--sidebar-accent))]",
			},
			size: {
				default: "h-8 text-sm",
				sm: "h-7 text-xs",
				lg: "h-12 text-sm group-data-[collapsible=icon]:p-0!",
			},
		},
		defaultVariants: {
			variant: "default",
			size: "default",
		},
	},
);

function SidebarMenuButton({
	asChild = false,
	isActive = false,
	variant = "default",
	size = "default",
	tooltip,
	className,
	...props
}: React.ComponentProps<"button"> & {
	asChild?: boolean;
	isActive?: boolean;
	tooltip?: string | React.ComponentProps<typeof TooltipContent>;
} & VariantProps<typeof sidebarMenuButtonVariants>) {
	const Comp = asChild ? Slot : "button";
	const { isMobile, state } = useSidebar();

	const button = (
		<Comp
			data-slot="sidebar-menu-button"
			data-sidebar="menu-button"
			data-size={size}
			data-active={isActive}
			className={cn(sidebarMenuButtonVariants({ variant, size }), className)}
			{...props}
		/>
	);

	if (!tooltip) {
		return button;
	}

	if (typeof tooltip === "string") {
		tooltip = {
			children: tooltip,
		};
	}

	return (
		<Tooltip>
			<TooltipTrigger asChild>{button}</TooltipTrigger>
			<TooltipContent
				side="right"
				align="center"
				hidden={state !== "collapsed" || isMobile}
				{...tooltip}
			/>
		</Tooltip>
	);
}

function SidebarMenuAction({
	className,
	asChild = false,
	showOnHover = false,
	...props
}: React.ComponentProps<"button"> & {
	asChild?: boolean;
	showOnHover?: boolean;
}) {
	const Comp = asChild ? Slot : "button";

	return (
		<Comp
			data-slot="sidebar-menu-action"
			data-sidebar="menu-action"
			className={cn(
				"text-sidebar-foreground ring-sidebar-ring hover:bg-sidebar-accent hover:text-sidebar-accent-foreground peer-hover/menu-button:text-sidebar-accent-foreground absolute top-1.5 right-1 flex aspect-square w-5 items-center justify-center rounded-md p-0 outline-hidden transition-transform focus-visible:ring-2 [&>svg]:size-4 [&>svg]:shrink-0",
				// Increases the hit area of the button on mobile.
				"after:absolute after:-inset-2 md:after:hidden",
				"peer-data-[size=sm]/menu-button:top-1",
				"peer-data-[size=default]/menu-button:top-1.5",
				"peer-data-[size=lg]/menu-button:top-2.5",
				"group-data-[collapsible=icon]:hidden",
				showOnHover &&
					"peer-data-[active=true]/menu-button:text-sidebar-accent-foreground group-focus-within/menu-item:opacity-100 group-hover/menu-item:opacity-100 data-[state=open]:opacity-100 md:opacity-0",
				className,
			)}
			{...props}
		/>
	);
}

function SidebarMenuBadge({
	className,
	...props
}: React.ComponentProps<"div">) {
	return (
		<div
			data-slot="sidebar-menu-badge"
			data-sidebar="menu-badge"
			className={cn(
				"text-sidebar-foreground pointer-events-none absolute right-1 flex h-5 min-w-5 items-center justify-center rounded-md px-1 text-xs font-medium tabular-nums select-none",
				"peer-hover/menu-button:text-sidebar-accent-foreground peer-data-[active=true]/menu-button:text-sidebar-accent-foreground",
				"peer-data-[size=sm]/menu-button:top-1",
				"peer-data-[size=default]/menu-button:top-1.5",
				"peer-data-[size=lg]/menu-button:top-2.5",
				"group-data-[collapsible=icon]:hidden",
				className,
			)}
			{...props}
		/>
	);
}

function SidebarMenuSkeleton({
	className,
	showIcon = false,
	...props
}: React.ComponentProps<"div"> & {
	showIcon?: boolean;
}) {
	// Random width between 50 to 90%.
	const width = React.useMemo(() => {
		return `${Math.floor(Math.random() * 40) + 50}%`;
	}, []);

	return (
		<div
			data-slot="sidebar-menu-skeleton"
			data-sidebar="menu-skeleton"
			className={cn("flex h-8 items-center gap-2 rounded-md px-2", className)}
			{...props}
		>
			{showIcon && (
				<Skeleton
					className="size-4 rounded-md"
					data-sidebar="menu-skeleton-icon"
				/>
			)}
			<Skeleton
				className="h-4 max-w-(--skeleton-width) flex-1"
				data-sidebar="menu-skeleton-text"
				style={
					{
						"--skeleton-width": width,
					} as React.CSSProperties
				}
			/>
		</div>
	);
}

function SidebarMenuSub({ className, ...props }: React.ComponentProps<"ul">) {
	return (
		<ul
			data-slot="sidebar-menu-sub"
			data-sidebar="menu-sub"
			className={cn(
				"border-sidebar-border mx-3.5 flex min-w-0 translate-x-px flex-col gap-1 border-l px-2.5 py-0.5",
				"group-data-[collapsible=icon]:hidden",
				className,
			)}
			{...props}
		/>
	);
}

function SidebarMenuSubItem({
	className,
	...props
}: React.ComponentProps<"li">) {
	return (
		<li
			data-slot="sidebar-menu-sub-item"
			data-sidebar="menu-sub-item"
			className={cn("group/menu-sub-item relative", className)}
			{...props}
		/>
	);
}

function SidebarMenuSubButton({
	asChild = false,
	size = "md",
	isActive = false,
	className,
	...props
}: React.ComponentProps<"a"> & {
	asChild?: boolean;
	size?: "sm" | "md";
	isActive?: boolean;
}) {
	const Comp = asChild ? Slot : "a";

	return (
		<Comp
			data-slot="sidebar-menu-sub-button"
			data-sidebar="menu-sub-button"
			data-size={size}
			data-active={isActive}
			className={cn(
				"text-sidebar-foreground ring-sidebar-ring hover:bg-sidebar-accent hover:text-sidebar-accent-foreground active:bg-sidebar-accent active:text-sidebar-accent-foreground [&>svg]:text-sidebar-accent-foreground flex h-7 min-w-0 -translate-x-px items-center gap-2 overflow-hidden rounded-md px-2 outline-hidden focus-visible:ring-2 disabled:pointer-events-none disabled:opacity-50 aria-disabled:pointer-events-none aria-disabled:opacity-50 [&>span:last-child]:truncate [&>svg]:size-4 [&>svg]:shrink-0",
				"data-[active=true]:bg-sidebar-accent data-[active=true]:text-sidebar-accent-foreground",
				size === "sm" && "text-xs",
				size === "md" && "text-sm",
				"group-data-[collapsible=icon]:hidden",
				className,
			)}
			{...props}
		/>
	);
}

export {
	Sidebar,
	SidebarContent,
	SidebarFooter,
	SidebarGroup,
	SidebarGroupAction,
	SidebarGroupContent,
	SidebarGroupLabel,
	SidebarHeader,
	SidebarInput,
	SidebarInset,
	SidebarMenu,
	SidebarMenuAction,
	SidebarMenuBadge,
	SidebarMenuButton,
	SidebarMenuItem,
	SidebarMenuSkeleton,
	SidebarMenuSub,
	SidebarMenuSubButton,
	SidebarMenuSubItem,
	SidebarProvider,
	SidebarRail,
	SidebarSeparator,
	SidebarTrigger,
	useSidebar,
};
</file>

<file path="apps/web/src/components/ui/skeleton.tsx">
import { cn } from "@/lib/utils";

function Skeleton({ className, ...props }: React.ComponentProps<"div">) {
	return (
		<div
			data-slot="skeleton"
			className={cn("bg-accent animate-pulse rounded-md", className)}
			{...props}
		/>
	);
}

export { Skeleton };
</file>

<file path="apps/web/src/components/ui/slider.tsx">
"use client";

import * as SliderPrimitive from "@radix-ui/react-slider";
import * as React from "react";

import { cn } from "@/lib/utils";

function Slider({
	className,
	defaultValue,
	value,
	min = 0,
	max = 100,
	...props
}: React.ComponentProps<typeof SliderPrimitive.Root>) {
	const _values = React.useMemo(
		() =>
			Array.isArray(value)
				? value
				: Array.isArray(defaultValue)
					? defaultValue
					: [min, max],
		[value, defaultValue, min, max],
	);

	return (
		<SliderPrimitive.Root
			data-slot="slider"
			defaultValue={defaultValue}
			value={value}
			min={min}
			max={max}
			className={cn(
				"relative flex w-full touch-none items-center select-none data-[disabled]:opacity-50 data-[orientation=vertical]:h-full data-[orientation=vertical]:min-h-44 data-[orientation=vertical]:w-auto data-[orientation=vertical]:flex-col",
				className,
			)}
			{...props}
		>
			<SliderPrimitive.Track
				data-slot="slider-track"
				className={cn(
					"bg-muted relative grow overflow-hidden rounded-full data-[orientation=horizontal]:h-1.5 data-[orientation=horizontal]:w-full data-[orientation=vertical]:h-full data-[orientation=vertical]:w-1.5",
				)}
			>
				<SliderPrimitive.Range
					data-slot="slider-range"
					className={cn(
						"bg-primary absolute data-[orientation=horizontal]:h-full data-[orientation=vertical]:w-full",
					)}
				/>
			</SliderPrimitive.Track>
			{Array.from({ length: _values.length }, (_, index) => (
				<SliderPrimitive.Thumb
					data-slot="slider-thumb"
					key={index}
					className="border-primary bg-background ring-ring/50 block size-4 shrink-0 rounded-full border shadow-sm transition-[color,box-shadow] hover:ring-4 focus-visible:ring-4 focus-visible:outline-hidden disabled:pointer-events-none disabled:opacity-50"
				/>
			))}
		</SliderPrimitive.Root>
	);
}

export { Slider };
</file>

<file path="apps/web/src/components/ui/sonner.tsx">
import { useTheme } from "next-themes";
import { Toaster as Sonner, type ToasterProps } from "sonner";

const Toaster = ({ ...props }: ToasterProps) => {
	const { theme = "system" } = useTheme();

	return (
		<Sonner
			theme={theme as ToasterProps["theme"]}
			className="toaster group"
			style={
				{
					"--normal-bg": "var(--popover)",
					"--normal-text": "var(--popover-foreground)",
					"--normal-border": "var(--border)",
				} as React.CSSProperties
			}
			{...props}
		/>
	);
};

export { Toaster };
</file>

<file path="apps/web/src/components/ui/switch.tsx">
"use client";

import * as SwitchPrimitive from "@radix-ui/react-switch";
import type * as React from "react";

import { cn } from "@/lib/utils";

function Switch({
	className,
	...props
}: React.ComponentProps<typeof SwitchPrimitive.Root>) {
	return (
		<SwitchPrimitive.Root
			data-slot="switch"
			className={cn(
				"peer data-[state=checked]:bg-primary data-[state=unchecked]:bg-input focus-visible:border-ring focus-visible:ring-ring/50 dark:data-[state=unchecked]:bg-input/80 inline-flex h-[1.15rem] w-8 shrink-0 items-center rounded-full border border-transparent shadow-xs transition-all outline-none focus-visible:ring-[3px] disabled:cursor-not-allowed disabled:opacity-50",
				className,
			)}
			{...props}
		>
			<SwitchPrimitive.Thumb
				data-slot="switch-thumb"
				className={cn(
					"bg-background dark:data-[state=unchecked]:bg-foreground dark:data-[state=checked]:bg-primary-foreground pointer-events-none block size-4 rounded-full ring-0 transition-transform data-[state=checked]:translate-x-[calc(100%-2px)] data-[state=unchecked]:translate-x-0",
				)}
			/>
		</SwitchPrimitive.Root>
	);
}

export { Switch };
</file>

<file path="apps/web/src/components/ui/table.tsx">
import type * as React from "react";

import { cn } from "@/lib/utils";

function Table({ className, ...props }: React.ComponentProps<"table">) {
	return (
		<div
			data-slot="table-container"
			className="relative w-full overflow-x-auto"
		>
			<table
				data-slot="table"
				className={cn("w-full caption-bottom text-sm", className)}
				{...props}
			/>
		</div>
	);
}

function TableHeader({ className, ...props }: React.ComponentProps<"thead">) {
	return (
		<thead
			data-slot="table-header"
			className={cn("[&_tr]:border-b", className)}
			{...props}
		/>
	);
}

function TableBody({ className, ...props }: React.ComponentProps<"tbody">) {
	return (
		<tbody
			data-slot="table-body"
			className={cn("[&_tr:last-child]:border-0", className)}
			{...props}
		/>
	);
}

function TableFooter({ className, ...props }: React.ComponentProps<"tfoot">) {
	return (
		<tfoot
			data-slot="table-footer"
			className={cn(
				"bg-muted/50 border-t font-medium [&>tr]:last:border-b-0",
				className,
			)}
			{...props}
		/>
	);
}

function TableRow({ className, ...props }: React.ComponentProps<"tr">) {
	return (
		<tr
			data-slot="table-row"
			className={cn(
				"hover:bg-muted/50 data-[state=selected]:bg-muted border-b transition-colors",
				className,
			)}
			{...props}
		/>
	);
}

function TableHead({ className, ...props }: React.ComponentProps<"th">) {
	return (
		<th
			data-slot="table-head"
			className={cn(
				"text-foreground h-10 px-2 text-left align-middle font-medium whitespace-nowrap [&:has([role=checkbox])]:pr-0 [&>[role=checkbox]]:translate-y-[2px]",
				className,
			)}
			{...props}
		/>
	);
}

function TableCell({ className, ...props }: React.ComponentProps<"td">) {
	return (
		<td
			data-slot="table-cell"
			className={cn(
				"p-2 align-middle whitespace-nowrap [&:has([role=checkbox])]:pr-0 [&>[role=checkbox]]:translate-y-[2px]",
				className,
			)}
			{...props}
		/>
	);
}

function TableCaption({
	className,
	...props
}: React.ComponentProps<"caption">) {
	return (
		<caption
			data-slot="table-caption"
			className={cn("text-muted-foreground mt-4 text-sm", className)}
			{...props}
		/>
	);
}

export {
	Table,
	TableHeader,
	TableBody,
	TableFooter,
	TableHead,
	TableRow,
	TableCell,
	TableCaption,
};
</file>

<file path="apps/web/src/components/ui/tabs.tsx">
"use client";

import * as TabsPrimitive from "@radix-ui/react-tabs";
import type * as React from "react";

import { cn } from "@/lib/utils";

function Tabs({
	className,
	...props
}: React.ComponentProps<typeof TabsPrimitive.Root>) {
	return (
		<TabsPrimitive.Root
			data-slot="tabs"
			className={cn("flex flex-col gap-2", className)}
			{...props}
		/>
	);
}

function TabsList({
	className,
	...props
}: React.ComponentProps<typeof TabsPrimitive.List>) {
	return (
		<TabsPrimitive.List
			data-slot="tabs-list"
			className={cn(
				"bg-muted text-muted-foreground inline-flex h-9 w-fit items-center justify-center rounded-lg p-[3px]",
				className,
			)}
			{...props}
		/>
	);
}

function TabsTrigger({
	className,
	...props
}: React.ComponentProps<typeof TabsPrimitive.Trigger>) {
	return (
		<TabsPrimitive.Trigger
			data-slot="tabs-trigger"
			className={cn(
				"data-[state=active]:bg-background dark:data-[state=active]:text-foreground focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:outline-ring dark:data-[state=active]:border-input dark:data-[state=active]:bg-input/30 text-foreground dark:text-muted-foreground inline-flex h-[calc(100%-1px)] flex-1 items-center justify-center gap-1.5 rounded-md border border-transparent px-2 py-1 text-sm font-medium whitespace-nowrap transition-[color,box-shadow] focus-visible:ring-[3px] focus-visible:outline-1 disabled:pointer-events-none disabled:opacity-50 data-[state=active]:shadow-sm [&_svg]:pointer-events-none [&_svg]:shrink-0 [&_svg:not([class*='size-'])]:size-4",
				className,
			)}
			{...props}
		/>
	);
}

function TabsContent({
	className,
	...props
}: React.ComponentProps<typeof TabsPrimitive.Content>) {
	return (
		<TabsPrimitive.Content
			data-slot="tabs-content"
			className={cn("flex-1 outline-none", className)}
			{...props}
		/>
	);
}

export { Tabs, TabsList, TabsTrigger, TabsContent };
</file>

<file path="apps/web/src/components/ui/textarea.tsx">
import type * as React from "react";

import { cn } from "@/lib/utils";

function Textarea({ className, ...props }: React.ComponentProps<"textarea">) {
	return (
		<textarea
			data-slot="textarea"
			className={cn(
				"border-input placeholder:text-muted-foreground focus-visible:border-ring focus-visible:ring-ring/50 aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive dark:bg-input/30 flex field-sizing-content min-h-16 w-full rounded-md border bg-transparent px-3 py-2 text-base shadow-xs transition-[color,box-shadow] outline-none focus-visible:ring-[3px] disabled:cursor-not-allowed disabled:opacity-50 md:text-sm",
				className,
			)}
			{...props}
		/>
	);
}

export { Textarea };
</file>

<file path="apps/web/src/components/ui/toast.tsx">
"use client";

import * as ToastPrimitives from "@radix-ui/react-toast";
import { type VariantProps, cva } from "class-variance-authority";
import { X } from "lucide-react";
import * as React from "react";

import { cn } from "../../lib/utils";

const ToastProvider = ToastPrimitives.Provider;

const ToastViewport = React.forwardRef<
	React.ElementRef<typeof ToastPrimitives.Viewport>,
	React.ComponentPropsWithoutRef<typeof ToastPrimitives.Viewport>
>(({ className, ...props }, ref) => (
	<ToastPrimitives.Viewport
		ref={ref}
		className={cn(
			"fixed top-0 z-[100] flex max-h-screen w-full flex-col-reverse p-4 sm:bottom-0 sm:right-0 sm:top-auto sm:flex-col md:max-w-[420px]",
			className,
		)}
		{...props}
	/>
));
ToastViewport.displayName = ToastPrimitives.Viewport.displayName;

const toastVariants = cva(
	"group pointer-events-auto relative flex w-full items-center justify-between space-x-2 overflow-hidden rounded-md border p-4 pr-6 shadow-lg transition-all data-[swipe=cancel]:translate-x-0 data-[swipe=end]:translate-x-[var(--radix-toast-swipe-end-x)] data-[swipe=move]:translate-x-[var(--radix-toast-swipe-move-x)] data-[swipe=move]:transition-none data-[state=open]:animate-in data-[state=closed]:animate-out data-[swipe=end]:animate-out data-[state=closed]:fade-out-80 data-[state=closed]:slide-out-to-right-full data-[state=open]:slide-in-from-top-full data-[state=open]:sm:slide-in-from-bottom-full",
	{
		variants: {
			variant: {
				default: "border bg-background text-foreground",
				destructive:
					"destructive group border-destructive bg-destructive text-destructive-foreground",
			},
		},
		defaultVariants: {
			variant: "default",
		},
	},
);

const Toast = React.forwardRef<
	React.ElementRef<typeof ToastPrimitives.Root>,
	React.ComponentPropsWithoutRef<typeof ToastPrimitives.Root> &
		VariantProps<typeof toastVariants>
>(({ className, variant, ...props }, ref) => {
	return (
		<ToastPrimitives.Root
			ref={ref}
			className={cn(toastVariants({ variant }), className)}
			{...props}
		/>
	);
});
Toast.displayName = ToastPrimitives.Root.displayName;

const ToastAction = React.forwardRef<
	React.ElementRef<typeof ToastPrimitives.Action>,
	React.ComponentPropsWithoutRef<typeof ToastPrimitives.Action>
>(({ className, ...props }, ref) => (
	<ToastPrimitives.Action
		ref={ref}
		className={cn(
			"inline-flex h-8 shrink-0 items-center justify-center rounded-md border bg-transparent px-3 text-sm font-medium transition-colors hover:bg-secondary focus:outline-none focus:ring-1 focus:ring-ring disabled:pointer-events-none disabled:opacity-50 group-[.destructive]:border-muted/40 group-[.destructive]:hover:border-destructive/30 group-[.destructive]:hover:bg-destructive group-[.destructive]:hover:text-destructive-foreground group-[.destructive]:focus:ring-destructive",
			className,
		)}
		{...props}
	/>
));
ToastAction.displayName = ToastPrimitives.Action.displayName;

const ToastClose = React.forwardRef<
	React.ElementRef<typeof ToastPrimitives.Close>,
	React.ComponentPropsWithoutRef<typeof ToastPrimitives.Close>
>(({ className, ...props }, ref) => (
	<ToastPrimitives.Close
		ref={ref}
		className={cn(
			"absolute right-1 top-1 rounded-md p-1 text-foreground/50 opacity-0 transition-opacity hover:text-foreground focus:opacity-100 focus:outline-none focus:ring-1 group-hover:opacity-100 group-[.destructive]:text-red-300 group-[.destructive]:hover:text-red-50 group-[.destructive]:focus:ring-red-400 group-[.destructive]:focus:ring-offset-red-600",
			className,
		)}
		toast-close=""
		{...props}
	>
		<X className="h-4 w-4" />
	</ToastPrimitives.Close>
));
ToastClose.displayName = ToastPrimitives.Close.displayName;

const ToastTitle = React.forwardRef<
	React.ElementRef<typeof ToastPrimitives.Title>,
	React.ComponentPropsWithoutRef<typeof ToastPrimitives.Title>
>(({ className, ...props }, ref) => (
	<ToastPrimitives.Title
		ref={ref}
		className={cn("text-sm font-semibold [&+div]:text-xs", className)}
		{...props}
	/>
));
ToastTitle.displayName = ToastPrimitives.Title.displayName;

const ToastDescription = React.forwardRef<
	React.ElementRef<typeof ToastPrimitives.Description>,
	React.ComponentPropsWithoutRef<typeof ToastPrimitives.Description>
>(({ className, ...props }, ref) => (
	<ToastPrimitives.Description
		ref={ref}
		className={cn("text-sm opacity-90", className)}
		{...props}
	/>
));
ToastDescription.displayName = ToastPrimitives.Description.displayName;

type ToastProps = React.ComponentPropsWithoutRef<typeof Toast>;

type ToastActionElement = React.ReactElement<typeof ToastAction>;

export {
	type ToastProps,
	type ToastActionElement,
	ToastProvider,
	ToastViewport,
	Toast,
	ToastTitle,
	ToastDescription,
	ToastClose,
	ToastAction,
};
</file>

<file path="apps/web/src/components/ui/toaster.tsx">
"use client";

import * as ToastPrimitives from "@radix-ui/react-toast";
import { type VariantProps, cva } from "class-variance-authority";
import { X } from "lucide-react";
import * as React from "react";

import { cn } from "../../lib/utils";

const ToastProvider = ToastPrimitives.Provider;

const ToastViewport = React.forwardRef<
	React.ElementRef<typeof ToastPrimitives.Viewport>,
	React.ComponentPropsWithoutRef<typeof ToastPrimitives.Viewport>
>(({ className, ...props }, ref) => (
	<ToastPrimitives.Viewport
		ref={ref}
		className={cn(
			"fixed top-0 z-[100] flex max-h-screen w-full flex-col-reverse p-4 sm:bottom-0 sm:right-0 sm:top-auto sm:flex-col md:max-w-[420px]",
			className,
		)}
		{...props}
	/>
));
ToastViewport.displayName = ToastPrimitives.Viewport.displayName;

const toastVariants = cva(
	"group pointer-events-auto relative flex w-full items-center justify-between space-x-2 overflow-hidden rounded-md border p-4 pr-6 shadow-lg transition-all data-[swipe=cancel]:translate-x-0 data-[swipe=end]:translate-x-[var(--radix-toast-swipe-end-x)] data-[swipe=move]:translate-x-[var(--radix-toast-swipe-move-x)] data-[swipe=move]:transition-none data-[state=open]:animate-in data-[state=closed]:animate-out data-[swipe=end]:animate-out data-[state=closed]:fade-out-80 data-[state=closed]:slide-out-to-right-full data-[state=open]:slide-in-from-top-full data-[state=open]:sm:slide-in-from-bottom-full",
	{
		variants: {
			variant: {
				default: "border bg-background text-foreground",
				destructive:
					"destructive group border-destructive bg-destructive text-destructive-foreground",
			},
		},
		defaultVariants: {
			variant: "default",
		},
	},
);

const Toast = React.forwardRef<
	React.ElementRef<typeof ToastPrimitives.Root>,
	React.ComponentPropsWithoutRef<typeof ToastPrimitives.Root> &
		VariantProps<typeof toastVariants>
>(({ className, variant, ...props }, ref) => {
	return (
		<ToastPrimitives.Root
			ref={ref}
			className={cn(toastVariants({ variant }), className)}
			{...props}
		/>
	);
});
Toast.displayName = ToastPrimitives.Root.displayName;

const ToastAction = React.forwardRef<
	React.ElementRef<typeof ToastPrimitives.Action>,
	React.ComponentPropsWithoutRef<typeof ToastPrimitives.Action>
>(({ className, ...props }, ref) => (
	<ToastPrimitives.Action
		ref={ref}
		className={cn(
			"inline-flex h-8 shrink-0 items-center justify-center rounded-md border bg-transparent px-3 text-sm font-medium transition-colors hover:bg-secondary focus:outline-none focus:ring-1 focus:ring-ring disabled:pointer-events-none disabled:opacity-50 group-[.destructive]:border-muted/40 group-[.destructive]:hover:border-destructive/30 group-[.destructive]:hover:bg-destructive group-[.destructive]:hover:text-destructive-foreground group-[.destructive]:focus:ring-destructive",
			className,
		)}
		{...props}
	/>
));
ToastAction.displayName = ToastPrimitives.Action.displayName;

const ToastClose = React.forwardRef<
	React.ElementRef<typeof ToastPrimitives.Close>,
	React.ComponentPropsWithoutRef<typeof ToastPrimitives.Close>
>(({ className, ...props }, ref) => (
	<ToastPrimitives.Close
		ref={ref}
		className={cn(
			"absolute right-1 top-1 rounded-md p-1 text-foreground/50 opacity-0 transition-opacity hover:text-foreground focus:opacity-100 focus:outline-none focus:ring-1 group-hover:opacity-100 group-[.destructive]:text-red-300 group-[.destructive]:hover:text-red-50 group-[.destructive]:focus:ring-red-400 group-[.destructive]:focus:ring-offset-red-600",
			className,
		)}
		toast-close=""
		{...props}
	>
		<X className="h-4 w-4" />
	</ToastPrimitives.Close>
));
ToastClose.displayName = ToastPrimitives.Close.displayName;

const ToastTitle = React.forwardRef<
	React.ElementRef<typeof ToastPrimitives.Title>,
	React.ComponentPropsWithoutRef<typeof ToastPrimitives.Title>
>(({ className, ...props }, ref) => (
	<ToastPrimitives.Title
		ref={ref}
		className={cn("text-sm font-semibold [&+div]:text-xs", className)}
		{...props}
	/>
));
ToastTitle.displayName = ToastPrimitives.Title.displayName;

const ToastDescription = React.forwardRef<
	React.ElementRef<typeof ToastPrimitives.Description>,
	React.ComponentPropsWithoutRef<typeof ToastPrimitives.Description>
>(({ className, ...props }, ref) => (
	<ToastPrimitives.Description
		ref={ref}
		className={cn("text-sm opacity-90", className)}
		{...props}
	/>
));
ToastDescription.displayName = ToastPrimitives.Description.displayName;

type ToastProps = React.ComponentPropsWithoutRef<typeof Toast>;

type ToastActionElement = React.ReactElement<typeof ToastAction>;

export {
	type ToastProps,
	type ToastActionElement,
	ToastProvider,
	ToastViewport,
	Toast,
	ToastTitle,
	ToastDescription,
	ToastClose,
	ToastAction,
};
</file>

<file path="apps/web/src/components/ui/toggle-group.tsx">
"use client";

import * as ToggleGroupPrimitive from "@radix-ui/react-toggle-group";
import type { VariantProps } from "class-variance-authority";
import * as React from "react";

import { toggleVariants } from "@/components/ui/toggle";
import { cn } from "@/lib/utils";

const ToggleGroupContext = React.createContext<
	VariantProps<typeof toggleVariants>
>({
	size: "default",
	variant: "default",
});

function ToggleGroup({
	className,
	variant,
	size,
	children,
	...props
}: React.ComponentProps<typeof ToggleGroupPrimitive.Root> &
	VariantProps<typeof toggleVariants>) {
	return (
		<ToggleGroupPrimitive.Root
			data-slot="toggle-group"
			data-variant={variant}
			data-size={size}
			className={cn(
				"group/toggle-group flex w-fit items-center rounded-md data-[variant=outline]:shadow-xs",
				className,
			)}
			{...props}
		>
			<ToggleGroupContext.Provider value={{ variant, size }}>
				{children}
			</ToggleGroupContext.Provider>
		</ToggleGroupPrimitive.Root>
	);
}

function ToggleGroupItem({
	className,
	children,
	variant,
	size,
	...props
}: React.ComponentProps<typeof ToggleGroupPrimitive.Item> &
	VariantProps<typeof toggleVariants>) {
	const context = React.useContext(ToggleGroupContext);

	return (
		<ToggleGroupPrimitive.Item
			data-slot="toggle-group-item"
			data-variant={context.variant || variant}
			data-size={context.size || size}
			className={cn(
				toggleVariants({
					variant: context.variant || variant,
					size: context.size || size,
				}),
				"min-w-0 flex-1 shrink-0 rounded-none shadow-none first:rounded-l-md last:rounded-r-md focus:z-10 focus-visible:z-10 data-[variant=outline]:border-l-0 data-[variant=outline]:first:border-l",
				className,
			)}
			{...props}
		>
			{children}
		</ToggleGroupPrimitive.Item>
	);
}

export { ToggleGroup, ToggleGroupItem };
</file>

<file path="apps/web/src/components/ui/toggle.tsx">
import * as TogglePrimitive from "@radix-ui/react-toggle";
import { type VariantProps, cva } from "class-variance-authority";
import type * as React from "react";

import { cn } from "@/lib/utils";

const toggleVariants = cva(
	"inline-flex items-center justify-center gap-2 rounded-md text-sm font-medium hover:bg-muted hover:text-muted-foreground disabled:pointer-events-none disabled:opacity-50 data-[state=on]:bg-accent data-[state=on]:text-accent-foreground [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 [&_svg]:shrink-0 focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] outline-none transition-[color,box-shadow] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive whitespace-nowrap",
	{
		variants: {
			variant: {
				default: "bg-transparent",
				outline:
					"border border-input bg-transparent shadow-xs hover:bg-accent hover:text-accent-foreground",
			},
			size: {
				default: "h-9 px-2 min-w-9",
				sm: "h-8 px-1.5 min-w-8",
				lg: "h-10 px-2.5 min-w-10",
			},
		},
		defaultVariants: {
			variant: "default",
			size: "default",
		},
	},
);

function Toggle({
	className,
	variant,
	size,
	...props
}: React.ComponentProps<typeof TogglePrimitive.Root> &
	VariantProps<typeof toggleVariants>) {
	return (
		<TogglePrimitive.Root
			data-slot="toggle"
			className={cn(toggleVariants({ variant, size, className }))}
			{...props}
		/>
	);
}

export { Toggle, toggleVariants };
</file>

<file path="apps/web/src/components/ui/tooltip.tsx">
import * as TooltipPrimitive from "@radix-ui/react-tooltip";
import type * as React from "react";

import { cn } from "@/lib/utils";

function TooltipProvider({
	delayDuration = 0,
	...props
}: React.ComponentProps<typeof TooltipPrimitive.Provider>) {
	return (
		<TooltipPrimitive.Provider
			data-slot="tooltip-provider"
			delayDuration={delayDuration}
			{...props}
		/>
	);
}

function Tooltip({
	...props
}: React.ComponentProps<typeof TooltipPrimitive.Root>) {
	return (
		<TooltipProvider>
			<TooltipPrimitive.Root data-slot="tooltip" {...props} />
		</TooltipProvider>
	);
}

function TooltipTrigger({
	...props
}: React.ComponentProps<typeof TooltipPrimitive.Trigger>) {
	return <TooltipPrimitive.Trigger data-slot="tooltip-trigger" {...props} />;
}

function TooltipContent({
	className,
	sideOffset = 0,
	children,
	...props
}: React.ComponentProps<typeof TooltipPrimitive.Content>) {
	return (
		<TooltipPrimitive.Portal>
			<TooltipPrimitive.Content
				data-slot="tooltip-content"
				sideOffset={sideOffset}
				className={cn(
					"bg-primary text-primary-foreground animate-in fade-in-0 zoom-in-95 data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=closed]:zoom-out-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2 z-50 w-fit origin-(--radix-tooltip-content-transform-origin) rounded-md px-3 py-1.5 text-xs text-balance",
					className,
				)}
				{...props}
			>
				{children}
				<TooltipPrimitive.Arrow className="bg-primary fill-primary z-50 size-2.5 translate-y-[calc(-50%_-_2px)] rotate-45 rounded-[2px]" />
			</TooltipPrimitive.Content>
		</TooltipPrimitive.Portal>
	);
}

export { Tooltip, TooltipTrigger, TooltipContent, TooltipProvider };
</file>

<file path="apps/web/src/components/ui/use-toast.tsx">
"use client";

import * as ToastPrimitives from "@radix-ui/react-toast";
import { type VariantProps, cva } from "class-variance-authority";
import { X } from "lucide-react";
import * as React from "react";

import { cn } from "../../lib/utils";

const ToastProvider = ToastPrimitives.Provider;

const ToastViewport = React.forwardRef<
	React.ElementRef<typeof ToastPrimitives.Viewport>,
	React.ComponentPropsWithoutRef<typeof ToastPrimitives.Viewport>
>(({ className, ...props }, ref) => (
	<ToastPrimitives.Viewport
		ref={ref}
		className={cn(
			"fixed top-0 z-[100] flex max-h-screen w-full flex-col-reverse p-4 sm:bottom-0 sm:right-0 sm:top-auto sm:flex-col md:max-w-[420px]",
			className,
		)}
		{...props}
	/>
));
ToastViewport.displayName = ToastPrimitives.Viewport.displayName;

const toastVariants = cva(
	"group pointer-events-auto relative flex w-full items-center justify-between space-x-2 overflow-hidden rounded-md border p-4 pr-6 shadow-lg transition-all data-[swipe=cancel]:translate-x-0 data-[swipe=end]:translate-x-[var(--radix-toast-swipe-end-x)] data-[swipe=move]:translate-x-[var(--radix-toast-swipe-move-x)] data-[swipe=move]:transition-none data-[state=open]:animate-in data-[state=closed]:animate-out data-[swipe=end]:animate-out data-[state=closed]:fade-out-80 data-[state=closed]:slide-out-to-right-full data-[state=open]:slide-in-from-top-full data-[state=open]:sm:slide-in-from-bottom-full",
	{
		variants: {
			variant: {
				default: "border bg-background text-foreground",
				destructive:
					"destructive group border-destructive bg-destructive text-destructive-foreground",
			},
		},
		defaultVariants: {
			variant: "default",
		},
	},
);

const Toast = React.forwardRef<
	React.ElementRef<typeof ToastPrimitives.Root>,
	React.ComponentPropsWithoutRef<typeof ToastPrimitives.Root> &
		VariantProps<typeof toastVariants>
>(({ className, variant, ...props }, ref) => {
	return (
		<ToastPrimitives.Root
			ref={ref}
			className={cn(toastVariants({ variant }), className)}
			{...props}
		/>
	);
});
Toast.displayName = ToastPrimitives.Root.displayName;

const ToastAction = React.forwardRef<
	React.ElementRef<typeof ToastPrimitives.Action>,
	React.ComponentPropsWithoutRef<typeof ToastPrimitives.Action>
>(({ className, ...props }, ref) => (
	<ToastPrimitives.Action
		ref={ref}
		className={cn(
			"inline-flex h-8 shrink-0 items-center justify-center rounded-md border bg-transparent px-3 text-sm font-medium transition-colors hover:bg-secondary focus:outline-none focus:ring-1 focus:ring-ring disabled:pointer-events-none disabled:opacity-50 group-[.destructive]:border-muted/40 group-[.destructive]:hover:border-destructive/30 group-[.destructive]:hover:bg-destructive group-[.destructive]:hover:text-destructive-foreground group-[.destructive]:focus:ring-destructive",
			className,
		)}
		{...props}
	/>
));
ToastAction.displayName = ToastPrimitives.Action.displayName;

const ToastClose = React.forwardRef<
	React.ElementRef<typeof ToastPrimitives.Close>,
	React.ComponentPropsWithoutRef<typeof ToastPrimitives.Close>
>(({ className, ...props }, ref) => (
	<ToastPrimitives.Close
		ref={ref}
		className={cn(
			"absolute right-1 top-1 rounded-md p-1 text-foreground/50 opacity-0 transition-opacity hover:text-foreground focus:opacity-100 focus:outline-none focus:ring-1 group-hover:opacity-100 group-[.destructive]:text-red-300 group-[.destructive]:hover:text-red-50 group-[.destructive]:focus:ring-red-400 group-[.destructive]:focus:ring-offset-red-600",
			className,
		)}
		toast-close=""
		{...props}
	>
		<X className="h-4 w-4" />
	</ToastPrimitives.Close>
));
ToastClose.displayName = ToastPrimitives.Close.displayName;

const ToastTitle = React.forwardRef<
	React.ElementRef<typeof ToastPrimitives.Title>,
	React.ComponentPropsWithoutRef<typeof ToastPrimitives.Title>
>(({ className, ...props }, ref) => (
	<ToastPrimitives.Title
		ref={ref}
		className={cn("text-sm font-semibold [&+div]:text-xs", className)}
		{...props}
	/>
));
ToastTitle.displayName = ToastPrimitives.Title.displayName;

const ToastDescription = React.forwardRef<
	React.ElementRef<typeof ToastPrimitives.Description>,
	React.ComponentPropsWithoutRef<typeof ToastPrimitives.Description>
>(({ className, ...props }, ref) => (
	<ToastPrimitives.Description
		ref={ref}
		className={cn("text-sm opacity-90", className)}
		{...props}
	/>
));
ToastDescription.displayName = ToastPrimitives.Description.displayName;

type ToastProps = React.ComponentPropsWithoutRef<typeof Toast>;

type ToastActionElement = React.ReactElement<typeof ToastAction>;

export {
	type ToastProps,
	type ToastActionElement,
	ToastProvider,
	ToastViewport,
	Toast,
	ToastTitle,
	ToastDescription,
	ToastClose,
	ToastAction,
};
</file>

<file path="apps/web/src/components/video/content-editor.tsx">
import { Button } from "@/components/ui/button";
import { Textarea } from "@/components/ui/textarea";
import { Loader2, Pencil, Save, X } from "lucide-react";
import { useEffect, useState } from "react";

type ContentEditorProps = {
	content: string;
	isLoading?: boolean;
	errorMessage?: string;
	onSave?: (content: string) => Promise<void>;
	className?: string;
	readOnly?: boolean;
	placeholder?: string;
	maxHeight?: string;
};

export function ContentEditor({
	content,
	isLoading = false,
	errorMessage,
	onSave,
	className = "",
	readOnly = false,
	placeholder = "Content not available yet",
	maxHeight = "400px",
}: ContentEditorProps) {
	const [isEditing, setIsEditing] = useState(false);
	const [editedContent, setEditedContent] = useState(content);
	const [isSaving, setIsSaving] = useState(false);
	const [saveError, setSaveError] = useState<string | null>(null);

	// Update edited content when the source content changes
	useEffect(() => {
		setEditedContent(content);
	}, [content]);

	const handleEdit = () => {
		setIsEditing(true);
		setSaveError(null);
	};

	const handleCancel = () => {
		setIsEditing(false);
		setEditedContent(content);
		setSaveError(null);
	};

	const handleSave = async () => {
		if (!onSave) return;
		setIsSaving(true);
		setSaveError(null);

		try {
			await onSave(editedContent);
			setIsEditing(false);
		} catch (error) {
			setSaveError(
				error instanceof Error ? error.message : "Error saving content",
			);
		} finally {
			setIsSaving(false);
		}
	};

	if (isLoading) {
		return (
			<div className="flex justify-center items-center p-8">
				<Loader2 className="h-6 w-6 animate-spin mr-2" />
				<span>Loading content...</span>
			</div>
		);
	}

	if (errorMessage) {
		return (
			<div className="p-4 border border-red-200 rounded-md bg-red-50 text-red-600">
				<p className="font-medium">Error loading content</p>
				<p className="text-sm mt-1">{errorMessage}</p>
			</div>
		);
	}

	if (!content.trim() && !isEditing) {
		return (
			<div className="flex flex-col items-center justify-center p-8 text-center text-muted-foreground">
				<p>{placeholder}</p>
				{!readOnly && onSave && (
					<Button
						variant="outline"
						size="sm"
						onClick={handleEdit}
						className="mt-4"
					>
						<Pencil className="h-4 w-4 mr-2" />
						Create Content
					</Button>
				)}
			</div>
		);
	}

	return (
		<div className={`relative ${className}`}>
			{isEditing ? (
				<>
					<Textarea
						value={editedContent}
						onChange={(e) => setEditedContent(e.target.value)}
						className={`min-h-[200px] font-mono text-sm p-4 ${saveError ? "border-red-300" : ""}`}
						placeholder="Enter content here..."
						disabled={isSaving}
					/>

					{saveError && (
						<div className="mt-2 p-2 text-sm text-red-600 bg-red-50 rounded-md">
							{saveError}
						</div>
					)}

					<div className="flex justify-end gap-2 mt-4">
						<Button
							variant="outline"
							size="sm"
							onClick={handleCancel}
							disabled={isSaving}
						>
							<X className="h-4 w-4 mr-1" />
							Cancel
						</Button>
						<Button size="sm" onClick={handleSave} disabled={isSaving}>
							{isSaving ? (
								<>
									<Loader2 className="h-4 w-4 mr-2 animate-spin" />
									Saving...
								</>
							) : (
								<>
									<Save className="h-4 w-4 mr-1" />
									Save Changes
								</>
							)}
						</Button>
					</div>
				</>
			) : (
				<>
					<div
						className={`overflow-y-auto p-4 font-mono text-sm whitespace-pre-wrap border rounded-md ${
							!readOnly
								? "hover:border-primary hover:bg-accent/10 transition-colors"
								: ""
						}`}
						style={{ maxHeight }}
					>
						{content}
					</div>

					{!readOnly && onSave && (
						<Button
							variant="ghost"
							size="sm"
							onClick={handleEdit}
							className="absolute top-2 right-2"
						>
							<Pencil className="h-4 w-4" />
						</Button>
					)}
				</>
			)}
		</div>
	);
}
</file>

<file path="apps/web/src/components/video/processing-steps.ts">
import type { Step } from "@/components/ui/progress-steps";

// Define the IDs for our processing steps
export enum ProcessingStepId {
	UPLOAD = "upload",
	AUDIO_EXTRACTION = "audio_extraction",
	TRANSCRIPT = "transcript",
	SUBTITLES = "subtitles",
	SHOWNOTES = "shownotes",
	CHAPTERS = "chapters",
	TITLE = "title",
	YOUTUBE_UPLOAD = "youtube_upload",
}

// Define static step metadata
export const PROCESSING_STEPS_META = {
	[ProcessingStepId.UPLOAD]: {
		label: "Video Upload",
		description: "Uploading video file to secure storage",
		progressPercentage: 10,
	},
	[ProcessingStepId.AUDIO_EXTRACTION]: {
		label: "Audio Extraction",
		description: "Extracting audio track for transcription",
		progressPercentage: 15,
	},
	[ProcessingStepId.TRANSCRIPT]: {
		label: "Transcript Generation",
		description: "Creating full text transcript from audio",
		progressPercentage: 15,
	},
	[ProcessingStepId.SUBTITLES]: {
		label: "Subtitles Generation",
		description: "Creating timestamped VTT subtitles",
		progressPercentage: 15,
	},
	[ProcessingStepId.SHOWNOTES]: {
		label: "Shownotes Generation",
		description: "Creating detailed show notes and summary",
		progressPercentage: 15,
	},
	[ProcessingStepId.CHAPTERS]: {
		label: "Chapters Generation",
		description: "Creating timestamped chapters",
		progressPercentage: 15,
	},
	[ProcessingStepId.TITLE]: {
		label: "Title & Keywords",
		description: "Creating optimized title and keywords",
		progressPercentage: 5,
	},
	[ProcessingStepId.YOUTUBE_UPLOAD]: {
		label: "YouTube Upload",
		description: "Uploading to YouTube with metadata",
		progressPercentage: 10,
	},
};

// Create the processing steps in the proper order
export const ORDERED_PROCESSING_STEPS = Object.keys(PROCESSING_STEPS_META).map(
	(id) => ({
		id,
		...PROCESSING_STEPS_META[id as ProcessingStepId],
	}),
);

// Initialize all steps to pending
export function initializeProcessingSteps(): Step[] {
	return Object.entries(PROCESSING_STEPS_META).map(([id, meta]) => ({
		id,
		label: meta.label,
		description: meta.description,
		status: "pending" as const,
	}));
}

// Update step status in the array
export function updateStepStatus(
	steps: Step[],
	stepId: ProcessingStepId,
	status: Step["status"],
	options?: { progress?: number; errorMessage?: string },
): Step[] {
	return steps.map((step) => {
		if (step.id === stepId) {
			return {
				...step,
				status,
				progress: options?.progress,
				errorMessage: options?.errorMessage,
			};
		}
		return step;
	});
}

// Calculate the overall progress percentage
export function calculateOverallProgress(steps: Step[]): number {
	// If all steps are completed, return 100%
	if (steps.every((step) => step.status === "completed")) {
		return 100;
	}

	let totalProgress = 0;
	let completedProgress = 0;

	steps.forEach((step) => {
		const stepMeta = PROCESSING_STEPS_META[step.id as ProcessingStepId];
		const weight = stepMeta.progressPercentage;

		totalProgress += weight;

		if (step.status === "completed") {
			completedProgress += weight;
		} else if (step.status === "in_progress" && step.progress !== undefined) {
			// For in-progress steps, add a portion based on its progress
			completedProgress += (weight * step.progress) / 100;
		}
	});

	return Math.round((completedProgress / totalProgress) * 100);
}

// Mock step progress for UI development/testing
export function createMockProcessingSteps(currentStepIndex: number): Step[] {
	const orderedStepIds = Object.keys(PROCESSING_STEPS_META);

	return orderedStepIds.map((id, index) => {
		const meta = PROCESSING_STEPS_META[id as ProcessingStepId];

		let status: Step["status"] = "pending";
		let progress: number | undefined = undefined;

		if (index < currentStepIndex) {
			status = "completed";
		} else if (index === currentStepIndex) {
			status = "in_progress";
			progress = Math.floor(Math.random() * 100);
		}

		return {
			id,
			label: meta.label,
			description: meta.description,
			status,
			progress,
		};
	});
}
</file>

<file path="apps/web/src/components/video/thumbnail-gallery.tsx">
import { Button } from "@/components/ui/button";
import {
	Card,
	CardContent,
	CardDescription,
	CardFooter,
	CardHeader,
	CardTitle,
} from "@/components/ui/card";
import { Input } from "@/components/ui/input";
import { Label } from "@/components/ui/label";
import { Tabs, TabsContent, TabsList, TabsTrigger } from "@/components/ui/tabs";
import { Textarea } from "@/components/ui/textarea";
import { cn } from "@/lib/utils";
import {
	Check,
	Edit2,
	Image,
	Loader2,
	RefreshCw,
	Sparkles,
} from "lucide-react";
import { useState } from "react";

export type Thumbnail = {
	id: string;
	url: string;
	prompt: string;
	isSelected?: boolean;
	status: "ready" | "generating" | "failed";
	error?: string;
};

type ThumbnailGalleryProps = {
	videoId: string;
	thumbnails: Thumbnail[];
	onSelect: (thumbnailId: string) => void;
	onRegenerateAll: () => void;
	onRegenerateSingle: (thumbnailId: string, newPrompt: string) => void;
	onApply: (thumbnailId: string) => void;
	onCustomUpload?: (file: File) => void;
	className?: string;
	isGenerating?: boolean;
};

export function ThumbnailGallery({
	videoId,
	thumbnails,
	onSelect,
	onRegenerateAll,
	onRegenerateSingle,
	onApply,
	onCustomUpload,
	className,
	isGenerating = false,
}: ThumbnailGalleryProps) {
	const [editingThumbnailId, setEditingThumbnailId] = useState<string | null>(
		null,
	);
	const [editPrompt, setEditPrompt] = useState("");
	const [customUploadFile, setCustomUploadFile] = useState<File | null>(null);

	const selectedThumbnail = thumbnails.find((thumb) => thumb.isSelected);

	const handleEditStart = (thumbnail: Thumbnail) => {
		setEditingThumbnailId(thumbnail.id);
		setEditPrompt(thumbnail.prompt);
	};

	const handleEditSave = (thumbnailId: string) => {
		if (editPrompt.trim()) {
			onRegenerateSingle(thumbnailId, editPrompt.trim());
			setEditingThumbnailId(null);
		}
	};

	const handleFileChange = (e: React.ChangeEvent<HTMLInputElement>) => {
		if (e.target.files && e.target.files.length > 0) {
			setCustomUploadFile(e.target.files[0]);
		}
	};

	const handleCustomUpload = () => {
		if (customUploadFile && onCustomUpload) {
			onCustomUpload(customUploadFile);
			setCustomUploadFile(null);
		}
	};

	return (
		<Card className={className}>
			<CardHeader>
				<CardTitle className="flex justify-between items-center">
					<span>Thumbnail Options</span>
					<Button
						size="sm"
						variant="outline"
						onClick={onRegenerateAll}
						disabled={isGenerating}
						className="gap-1"
					>
						{isGenerating ? (
							<>
								<Loader2 className="h-3.5 w-3.5 animate-spin" />
								Generating...
							</>
						) : (
							<>
								<Sparkles className="h-3.5 w-3.5" />
								Generate New Set
							</>
						)}
					</Button>
				</CardTitle>
				<CardDescription>
					Select or customize the thumbnail for your video
				</CardDescription>
			</CardHeader>

			<CardContent>
				<Tabs defaultValue="gallery">
					<TabsList className="grid w-full grid-cols-2 mb-4">
						<TabsTrigger value="gallery">AI Generated</TabsTrigger>
						<TabsTrigger value="custom">Custom Upload</TabsTrigger>
					</TabsList>

					<TabsContent value="gallery" className="space-y-4">
						{/* Selected Thumbnail Preview */}
						{selectedThumbnail && (
							<div className="space-y-2">
								<Label className="text-xs text-muted-foreground">
									Selected Thumbnail
								</Label>
								<div className="aspect-video bg-muted rounded-md overflow-hidden relative">
									<img
										src={selectedThumbnail.url}
										alt="Selected thumbnail"
										className="w-full h-full object-cover"
									/>
									<div className="absolute inset-0 bg-black/40 flex items-center justify-center opacity-0 hover:opacity-100 transition-opacity">
										<Button
											variant="secondary"
											size="sm"
											onClick={() => handleEditStart(selectedThumbnail)}
										>
											Edit Prompt
										</Button>
									</div>
								</div>
							</div>
						)}

						{/* Thumbnail Grid */}
						<div className="grid grid-cols-2 gap-3">
							{thumbnails.map((thumbnail) => (
								<div
									key={thumbnail.id}
									className={cn(
										"relative group rounded-md overflow-hidden border-2 aspect-video",
										thumbnail.isSelected
											? "border-primary"
											: "border-transparent",
									)}
								>
									{/* Thumbnail Image */}
									{thumbnail.status === "generating" ? (
										<div className="w-full h-full bg-muted flex items-center justify-center">
											<Loader2 className="h-6 w-6 animate-spin text-muted-foreground" />
										</div>
									) : thumbnail.status === "failed" ? (
										<div className="w-full h-full bg-destructive/10 flex flex-col items-center justify-center p-2">
											<span className="text-xs text-destructive font-medium">
												Generation Failed
											</span>
											{thumbnail.error && (
												<span className="text-[10px] text-muted-foreground text-center mt-1">
													{thumbnail.error}
												</span>
											)}
										</div>
									) : (
										<img
											src={thumbnail.url}
											alt={`Thumbnail option ${thumbnail.id}`}
											className="w-full h-full object-cover"
										/>
									)}

									{/* Overlay with Actions */}
									{thumbnail.status === "ready" && (
										<div className="absolute inset-0 bg-black/60 opacity-0 group-hover:opacity-100 transition-opacity flex flex-col items-center justify-center gap-2">
											<div className="flex gap-1">
												<Button
													size="sm"
													variant="secondary"
													className="h-8"
													onClick={() => handleEditStart(thumbnail)}
												>
													<Edit2 className="h-3.5 w-3.5 mr-1" />
													Edit
												</Button>
												<Button
													size="sm"
													variant={thumbnail.isSelected ? "default" : "outline"}
													className="h-8"
													onClick={() => onSelect(thumbnail.id)}
												>
													<Check className="h-3.5 w-3.5 mr-1" />
													{thumbnail.isSelected ? "Selected" : "Select"}
												</Button>
											</div>
											<p className="text-[10px] text-white/70 px-2 text-center line-clamp-2 mt-1">
												{thumbnail.prompt}
											</p>
										</div>
									)}
								</div>
							))}
						</div>

						{/* Edit Prompt Dialog */}
						{editingThumbnailId && (
							<div className="mt-4 border rounded-md p-3 space-y-3">
								<Label htmlFor="thumbnail-prompt">Edit Thumbnail Prompt</Label>
								<Textarea
									id="thumbnail-prompt"
									value={editPrompt}
									onChange={(e) => setEditPrompt(e.target.value)}
									placeholder="Describe what you want in this thumbnail..."
									className="resize-none min-h-[100px]"
								/>
								<div className="flex justify-end gap-2">
									<Button
										variant="outline"
										size="sm"
										onClick={() => setEditingThumbnailId(null)}
									>
										Cancel
									</Button>
									<Button
										size="sm"
										onClick={() => handleEditSave(editingThumbnailId)}
										disabled={!editPrompt.trim()}
									>
										Regenerate
									</Button>
								</div>
							</div>
						)}
					</TabsContent>

					<TabsContent value="custom" className="space-y-4">
						<div className="border-2 border-dashed rounded-md p-4 text-center">
							<div className="flex flex-col items-center justify-center gap-2">
								<Image className="h-8 w-8 text-muted-foreground" />
								<div className="space-y-1">
									<p className="text-sm font-medium">Upload Custom Thumbnail</p>
									<p className="text-xs text-muted-foreground">
										PNG, JPG, or WEBP up to 2MB
									</p>
								</div>

								<Input
									type="file"
									accept="image/png,image/jpeg,image/webp"
									className="max-w-xs mt-2"
									onChange={handleFileChange}
								/>

								{customUploadFile && (
									<div className="mt-2 w-full max-w-xs">
										<p className="text-xs text-muted-foreground truncate">
											{customUploadFile.name} (
											{Math.round(customUploadFile.size / 1024)} KB)
										</p>
										<Button
											className="w-full mt-2"
											size="sm"
											onClick={handleCustomUpload}
										>
											Upload & Use This Thumbnail
										</Button>
									</div>
								)}
							</div>
						</div>

						<div className="text-xs text-muted-foreground">
							<p>Image requirements:</p>
							<ul className="list-disc pl-5 space-y-1 mt-1">
								<li>1280x720 pixels (16:9 aspect ratio)</li>
								<li>Less than 2MB in size</li>
								<li>PNG, JPG, or WEBP format</li>
								<li>High-contrast images work best</li>
							</ul>
						</div>
					</TabsContent>
				</Tabs>
			</CardContent>

			<CardFooter>
				<Button
					className="w-full"
					disabled={!selectedThumbnail}
					onClick={() => selectedThumbnail && onApply(selectedThumbnail.id)}
				>
					Apply Selected Thumbnail
				</Button>
			</CardFooter>
		</Card>
	);
}
</file>

<file path="apps/web/src/components/video/title-selector.tsx">
import { Button } from "@/components/ui/button";
import {
	Card,
	CardContent,
	CardDescription,
	CardFooter,
	CardHeader,
	CardTitle,
} from "@/components/ui/card";
import { Input } from "@/components/ui/input";
import { cn } from "@/lib/utils";
import { Check, Edit2, RefreshCw, Sparkles, ThumbsUp } from "lucide-react";
import { useState } from "react";

export type TitleOption = {
	id: string;
	text: string;
	votes?: number;
	isSelected?: boolean;
};

type TitleSelectorProps = {
	videoId: string;
	titleOptions: TitleOption[];
	onSelect: (titleId: string) => void;
	onEdit: (titleId: string, newText: string) => void;
	onVote: (titleId: string) => void;
	onGenerateMore: () => void;
	onApply: (titleId: string) => void;
	className?: string;
	maxDisplayedOptions?: number;
};

export function TitleSelector({
	videoId,
	titleOptions,
	onSelect,
	onEdit,
	onVote,
	onGenerateMore,
	onApply,
	className,
	maxDisplayedOptions = 5,
}: TitleSelectorProps) {
	const [editingTitleId, setEditingTitleId] = useState<string | null>(null);
	const [editText, setEditText] = useState("");
	const [showAllOptions, setShowAllOptions] = useState(false);

	const sortedOptions = [...titleOptions].sort(
		(a, b) => (b.votes || 0) - (a.votes || 0),
	);

	const displayedOptions = showAllOptions
		? sortedOptions
		: sortedOptions.slice(0, maxDisplayedOptions);

	const selectedTitle = sortedOptions.find((title) => title.isSelected);

	const handleEditStart = (title: TitleOption) => {
		setEditingTitleId(title.id);
		setEditText(title.text);
	};

	const handleEditSave = () => {
		if (editingTitleId && editText.trim()) {
			onEdit(editingTitleId, editText.trim());
			setEditingTitleId(null);
		}
	};

	const handleEditCancel = () => {
		setEditingTitleId(null);
	};

	return (
		<Card className={className}>
			<CardHeader>
				<CardTitle className="flex justify-between items-center">
					<span>Video Title Options</span>
					<Button
						size="sm"
						variant="outline"
						onClick={onGenerateMore}
						className="gap-1"
					>
						<Sparkles className="h-3.5 w-3.5" />
						Generate More
					</Button>
				</CardTitle>
				<CardDescription>
					Select the best title for your video or create your own
				</CardDescription>
			</CardHeader>

			<CardContent className="space-y-4">
				{/* Selected Title Preview */}
				{selectedTitle && (
					<div className="p-3 border rounded-md bg-muted/50">
						<div className="text-xs text-muted-foreground mb-1">
							Selected Title
						</div>
						<div className="font-medium">{selectedTitle.text}</div>
					</div>
				)}

				{/* Title Options */}
				<div className="space-y-3">
					{displayedOptions.map((title) => (
						<div
							key={title.id}
							className={cn(
								"p-3 border rounded-md relative transition",
								title.isSelected
									? "border-primary/50 bg-primary/5"
									: "hover:border-border/80",
							)}
						>
							{editingTitleId === title.id ? (
								<div className="space-y-2">
									<Input
										value={editText}
										onChange={(e) => setEditText(e.target.value)}
										className="w-full"
										autoFocus
									/>
									<div className="flex space-x-2 justify-end">
										<Button
											size="sm"
											variant="ghost"
											onClick={handleEditCancel}
										>
											Cancel
										</Button>
										<Button size="sm" onClick={handleEditSave}>
											Save
										</Button>
									</div>
								</div>
							) : (
								<>
									<div className="mr-16 text-sm">{title.text}</div>
									<div className="absolute right-3 top-3 flex items-center space-x-2">
										<Button
											size="icon"
											variant="ghost"
											className="h-7 w-7"
											onClick={() => onVote(title.id)}
										>
											<ThumbsUp className="h-3.5 w-3.5" />
											{title.votes && title.votes > 0 && (
												<span className="absolute -top-0.5 -right-0.5 bg-primary text-[10px] text-primary-foreground w-4 h-4 flex items-center justify-center rounded-full">
													{title.votes}
												</span>
											)}
										</Button>
										<Button
											size="icon"
											variant="ghost"
											className="h-7 w-7"
											onClick={() => handleEditStart(title)}
										>
											<Edit2 className="h-3.5 w-3.5" />
										</Button>
										<Button
											size="icon"
											variant="ghost"
											className={cn(
												"h-7 w-7",
												title.isSelected && "text-primary",
											)}
											onClick={() => onSelect(title.id)}
										>
											<Check className="h-3.5 w-3.5" />
										</Button>
									</div>
								</>
							)}
						</div>
					))}

					{titleOptions.length > maxDisplayedOptions && (
						<Button
							variant="ghost"
							size="sm"
							className="w-full text-xs"
							onClick={() => setShowAllOptions(!showAllOptions)}
						>
							{showAllOptions
								? "Show Less"
								: `Show ${titleOptions.length - maxDisplayedOptions} More Options`}
						</Button>
					)}
				</div>
			</CardContent>

			<CardFooter>
				<Button
					className="w-full"
					disabled={!selectedTitle}
					onClick={() => selectedTitle && onApply(selectedTitle.id)}
				>
					Apply Selected Title
				</Button>
			</CardFooter>
		</Card>
	);
}
</file>

<file path="apps/web/src/components/video/video-detail.tsx">
import { Button } from "@/components/ui/button";
import {
	Card,
	CardContent,
	CardDescription,
	CardHeader,
	CardTitle,
} from "@/components/ui/card";
import { ProgressSteps, type Step } from "@/components/ui/progress-steps";
import { Tabs, TabsContent, TabsList, TabsTrigger } from "@/components/ui/tabs";
import { tryFetchContent } from "@/services/gcs-content";
import { Link } from "@tanstack/react-router";
import {
	ArrowLeft,
	CheckCircle,
	Download,
	ExternalLink,
	Loader2,
	type LucideIcon,
} from "lucide-react";
import { useEffect, useState } from "react";
import { ContentEditor } from "./content-editor";
import { calculateOverallProgress } from "./processing-steps";

type VideoAsset = {
	type: string;
	label: string;
	url: string;
	icon: LucideIcon;
	sizeKB?: number;
};

type VideoDetailProps = {
	videoId: string;
	videoTitle: string;
	videoUrl: string;
	thumbnailUrl?: string;
	processingSteps: Step[];
	currentStepId?: string;
	youtubeUrl?: string;
	status: "processing" | "completed" | "error" | "paused";
	assets?: VideoAsset[];
	className?: string;
	// Content related props
	output_files?: Record<string, string>;
	bucket_name?: string;
	onSaveContent?: (contentType: string, content: string) => Promise<void>;
};

export function VideoDetail({
	videoId,
	videoTitle,
	videoUrl,
	thumbnailUrl,
	processingSteps,
	currentStepId,
	youtubeUrl,
	status,
	assets = [],
	className,
	output_files = {},
	bucket_name,
	onSaveContent,
}: VideoDetailProps) {
	const overallProgress = calculateOverallProgress(processingSteps);
	const isCompleted = status === "completed";

	// State for content
	const [transcript, setTranscript] = useState<string>("");
	const [subtitles, setSubtitles] = useState<string>("");
	const [chapters, setChapters] = useState<string>("");
	const [shownotes, setShownotes] = useState<string>("");

	// Loading states
	const [loadingTranscript, setLoadingTranscript] = useState<boolean>(false);
	const [loadingSubtitles, setLoadingSubtitles] = useState<boolean>(false);
	const [loadingChapters, setLoadingChapters] = useState<boolean>(false);
	const [loadingShownotes, setLoadingShownotes] = useState<boolean>(false);

	// Error states
	const [transcriptError, setTranscriptError] = useState<string | null>(null);
	const [subtitlesError, setSubtitlesError] = useState<string | null>(null);
	const [chaptersError, setChaptersError] = useState<string | null>(null);
	const [shownotesError, setShownotesError] = useState<string | null>(null);

	// Active tab
	const [activeContentTab, setActiveContentTab] =
		useState<string>("transcript");

	// Effect to load content based on active tab
	useEffect(() => {
		if (!bucket_name || status !== "completed") return;

		const fetchContent = async () => {
			// Only fetch content for the active tab to save bandwidth
			switch (activeContentTab) {
				case "transcript":
					if (transcript || loadingTranscript) return;
					setLoadingTranscript(true);
					setTranscriptError(null);
					try {
						const content = await tryFetchContent(
							bucket_name,
							output_files,
							"transcript",
						);
						setTranscript(content || "");
					} catch (error) {
						setTranscriptError(
							error instanceof Error
								? error.message
								: "Failed to load transcript",
						);
					} finally {
						setLoadingTranscript(false);
					}
					break;

				case "subtitles":
					if (subtitles || loadingSubtitles) return;
					setLoadingSubtitles(true);
					setSubtitlesError(null);
					try {
						const content = await tryFetchContent(
							bucket_name,
							output_files,
							"subtitles",
						);
						setSubtitles(content || "");
					} catch (error) {
						setSubtitlesError(
							error instanceof Error
								? error.message
								: "Failed to load subtitles",
						);
					} finally {
						setLoadingSubtitles(false);
					}
					break;

				case "chapters":
					if (chapters || loadingChapters) return;
					setLoadingChapters(true);
					setChaptersError(null);
					try {
						const content = await tryFetchContent(
							bucket_name,
							output_files,
							"chapters",
						);
						setChapters(content || "");
					} catch (error) {
						setChaptersError(
							error instanceof Error
								? error.message
								: "Failed to load chapters",
						);
					} finally {
						setLoadingChapters(false);
					}
					break;

				case "shownotes":
					if (shownotes || loadingShownotes) return;
					setLoadingShownotes(true);
					setShownotesError(null);
					try {
						const content = await tryFetchContent(
							bucket_name,
							output_files,
							"shownotes",
						);
						setShownotes(content || "");
					} catch (error) {
						setShownotesError(
							error instanceof Error
								? error.message
								: "Failed to load shownotes",
						);
					} finally {
						setLoadingShownotes(false);
					}
					break;
			}
		};

		fetchContent();
	}, [
		activeContentTab,
		bucket_name,
		output_files,
		status,
		transcript,
		subtitles,
		chapters,
		shownotes,
		loadingTranscript,
		loadingSubtitles,
		loadingChapters,
		loadingShownotes,
	]);

	// Handle save operations for different content types
	const handleSaveTranscript = async (content: string) => {
		if (!onSaveContent) throw new Error("Save function not provided");
		await onSaveContent("transcript", content);
		setTranscript(content);
	};

	const handleSaveSubtitles = async (content: string) => {
		if (!onSaveContent) throw new Error("Save function not provided");
		await onSaveContent("subtitles", content);
		setSubtitles(content);
	};

	const handleSaveChapters = async (content: string) => {
		if (!onSaveContent) throw new Error("Save function not provided");
		await onSaveContent("chapters", content);
		setChapters(content);
	};

	const handleSaveShownotes = async (content: string) => {
		if (!onSaveContent) throw new Error("Save function not provided");
		await onSaveContent("shownotes", content);
		setShownotes(content);
	};

	return (
		<div className={className}>
			<div className="mb-6">
				<Link
					to="/dashboard"
					className="inline-flex items-center text-sm text-muted-foreground hover:text-foreground mb-4"
				>
					<ArrowLeft className="mr-2 h-4 w-4" />
					Back to dashboard
				</Link>

				<div className="flex items-center justify-between">
					<h1 className="text-2xl font-bold tracking-tight line-clamp-1">
						{videoTitle}
					</h1>
					{youtubeUrl && (
						<Button variant="outline" size="sm" asChild>
							<a href={youtubeUrl} target="_blank" rel="noopener noreferrer">
								<ExternalLink className="mr-2 h-4 w-4" />
								View on YouTube
							</a>
						</Button>
					)}
				</div>

				<div className="text-sm text-muted-foreground mt-1">ID: {videoId}</div>
			</div>

			<div className="grid gap-6 md:grid-cols-2">
				<div className="md:col-span-1 space-y-6">
					{/* Video Preview */}
					<Card>
						<CardHeader className="pb-2">
							<CardTitle className="text-base">Video Preview</CardTitle>
						</CardHeader>
						<CardContent>
							<div className="aspect-video bg-muted rounded-md overflow-hidden">
								{status === "completed" ? (
									<video
										src={videoUrl}
										poster={thumbnailUrl}
										controls
										className="w-full h-full object-cover"
									/>
								) : thumbnailUrl ? (
									<img
										src={thumbnailUrl}
										alt={videoTitle}
										className="w-full h-full object-cover"
									/>
								) : (
									<div className="w-full h-full flex items-center justify-center text-muted-foreground">
										Processing video...
									</div>
								)}
							</div>
						</CardContent>
					</Card>

					{/* Status Card */}
					<Card>
						<CardHeader className="pb-2">
							<div className="flex items-center justify-between">
								<CardTitle className="text-base">Processing Status</CardTitle>
								{isCompleted && (
									<div className="flex items-center text-xs font-medium text-primary">
										<CheckCircle className="mr-1 h-3 w-3" />
										Complete
									</div>
								)}
								{!isCompleted && (
									<div className="text-xs font-medium text-muted-foreground">
										{overallProgress}% complete
									</div>
								)}
							</div>
						</CardHeader>
						<CardContent>
							<ProgressSteps
								steps={processingSteps}
								currentStepId={currentStepId}
							/>
						</CardContent>
					</Card>
				</div>

				<div className="md:col-span-1 space-y-6">
					{/* Generated Content */}
					<Card>
						<CardHeader>
							<CardTitle className="text-base">Generated Content</CardTitle>
							<CardDescription>
								Assets created during video processing
							</CardDescription>
						</CardHeader>
						<CardContent>
							<Tabs
								value={activeContentTab}
								onValueChange={setActiveContentTab}
							>
								<TabsList className="grid grid-cols-4 mb-4">
									<TabsTrigger value="transcript">Transcript</TabsTrigger>
									<TabsTrigger value="subtitles">Subtitles</TabsTrigger>
									<TabsTrigger value="chapters">Chapters</TabsTrigger>
									<TabsTrigger value="shownotes">Shownotes</TabsTrigger>
								</TabsList>

								<TabsContent value="transcript" className="space-y-4">
									{isCompleted ? (
										<ContentEditor
											content={transcript}
											isLoading={loadingTranscript}
											errorMessage={transcriptError || undefined}
											onSave={onSaveContent ? handleSaveTranscript : undefined}
											readOnly={!onSaveContent}
											placeholder="Transcript not available"
											maxHeight="400px"
										/>
									) : (
										<div className="flex flex-col items-center justify-center py-8 text-center text-muted-foreground">
											<p>Transcript not yet generated</p>
											<p className="text-xs mt-1">
												This will be available once processing is complete
											</p>
										</div>
									)}
								</TabsContent>

								<TabsContent value="subtitles" className="space-y-4">
									{isCompleted ? (
										<ContentEditor
											content={subtitles}
											isLoading={loadingSubtitles}
											errorMessage={subtitlesError || undefined}
											onSave={onSaveContent ? handleSaveSubtitles : undefined}
											readOnly={!onSaveContent}
											placeholder="Subtitles not available"
											maxHeight="400px"
										/>
									) : (
										<div className="flex flex-col items-center justify-center py-8 text-center text-muted-foreground">
											<p>Subtitles not yet generated</p>
											<p className="text-xs mt-1">
												This will be available once processing is complete
											</p>
										</div>
									)}
								</TabsContent>

								<TabsContent value="chapters" className="space-y-4">
									{isCompleted ? (
										<ContentEditor
											content={chapters}
											isLoading={loadingChapters}
											errorMessage={chaptersError || undefined}
											onSave={onSaveContent ? handleSaveChapters : undefined}
											readOnly={!onSaveContent}
											placeholder="Chapters not available"
											maxHeight="400px"
										/>
									) : (
										<div className="flex flex-col items-center justify-center py-8 text-center text-muted-foreground">
											<p>Chapters not yet generated</p>
											<p className="text-xs mt-1">
												This will be available once processing is complete
											</p>
										</div>
									)}
								</TabsContent>

								<TabsContent value="shownotes" className="space-y-4">
									{isCompleted ? (
										<ContentEditor
											content={shownotes}
											isLoading={loadingShownotes}
											errorMessage={shownotesError || undefined}
											onSave={onSaveContent ? handleSaveShownotes : undefined}
											readOnly={!onSaveContent}
											placeholder="Show notes not available"
											maxHeight="400px"
										/>
									) : (
										<div className="flex flex-col items-center justify-center py-8 text-center text-muted-foreground">
											<p>Show notes not yet generated</p>
											<p className="text-xs mt-1">
												This will be available once processing is complete
											</p>
										</div>
									)}
								</TabsContent>
							</Tabs>
						</CardContent>
					</Card>

					{/* Download Assets */}
					{isCompleted && assets.length > 0 && (
						<Card>
							<CardHeader className="pb-2">
								<CardTitle className="text-base">Download Assets</CardTitle>
							</CardHeader>
							<CardContent>
								<ul className="divide-y">
									{assets.map((asset, index) => (
										<li key={index} className="py-2 first:pt-0 last:pb-0">
											<div className="flex items-center justify-between">
												<div className="flex items-center">
													<asset.icon className="h-4 w-4 text-muted-foreground mr-2" />
													<span className="text-sm">{asset.label}</span>
													{asset.sizeKB && (
														<span className="text-xs text-muted-foreground ml-2">
															({asset.sizeKB} KB)
														</span>
													)}
												</div>
												<Button variant="ghost" size="sm" asChild>
													<a href={asset.url} download>
														<Download className="h-4 w-4" />
													</a>
												</Button>
											</div>
										</li>
									))}
								</ul>
							</CardContent>
						</Card>
					)}
				</div>
			</div>
		</div>
	);
}
</file>

<file path="apps/web/src/components/video/video-progress-card.tsx">
import { Button } from "@/components/ui/button";
import {
	Card,
	CardContent,
	CardFooter,
	CardHeader,
	CardTitle,
} from "@/components/ui/card";
import { ProgressSteps, type Step } from "@/components/ui/progress-steps";
import { ExternalLink, PauseIcon, PlayIcon } from "lucide-react";
import { useState } from "react";

type VideoProgressCardProps = {
	videoId: string;
	videoTitle: string;
	thumbnailUrl?: string;
	uploadedAt: Date;
	status: "processing" | "completed" | "error" | "paused";
	processingSteps: Step[];
	currentStepId?: string;
	youtubeUrl?: string;
	onPauseResume?: () => void;
	className?: string;
};

export function VideoProgressCard({
	videoId,
	videoTitle,
	thumbnailUrl,
	uploadedAt,
	status,
	processingSteps,
	currentStepId,
	youtubeUrl,
	onPauseResume,
	className,
}: VideoProgressCardProps) {
	const [isPaused, setIsPaused] = useState(status === "paused");

	const handlePauseResume = () => {
		setIsPaused(!isPaused);
		onPauseResume?.();
	};

	const formatDate = (date: Date) => {
		return new Intl.DateTimeFormat("en-US", {
			month: "short",
			day: "numeric",
			hour: "numeric",
			minute: "numeric",
		}).format(date);
	};

	return (
		<Card className={className}>
			<CardHeader className="pb-2">
				<div className="flex items-start justify-between">
					<div className="space-y-1">
						<CardTitle className="line-clamp-1 text-base">
							{videoTitle}
						</CardTitle>
						<div className="flex items-center gap-2 text-xs text-muted-foreground">
							<span>Uploaded {formatDate(uploadedAt)}</span>
							<span></span>
							<span>ID: {videoId.slice(0, 8)}</span>
						</div>
					</div>

					{thumbnailUrl && (
						<div className="relative h-14 w-24 overflow-hidden rounded-md shrink-0">
							<img
								src={thumbnailUrl}
								alt={videoTitle}
								className="h-full w-full object-cover"
							/>
						</div>
					)}
				</div>
			</CardHeader>

			<CardContent>
				<ProgressSteps steps={processingSteps} currentStepId={currentStepId} />
			</CardContent>

			<CardFooter className="flex justify-between pt-0">
				{(status === "processing" || status === "paused") && (
					<Button variant="outline" size="sm" onClick={handlePauseResume}>
						{isPaused ? (
							<>
								<PlayIcon className="mr-1.5 h-3.5 w-3.5" />
								Resume
							</>
						) : (
							<>
								<PauseIcon className="mr-1.5 h-3.5 w-3.5" />
								Pause
							</>
						)}
					</Button>
				)}

				{status === "completed" && youtubeUrl && (
					<Button variant="outline" size="sm" asChild>
						<a href={youtubeUrl} target="_blank" rel="noopener noreferrer">
							<ExternalLink className="mr-1.5 h-3.5 w-3.5" />
							View on YouTube
						</a>
					</Button>
				)}

				<Button variant="ghost" size="sm" asChild>
					<a href={`/video/${videoId}`}>View Details</a>
				</Button>
			</CardFooter>
		</Card>
	);
}
</file>

<file path="apps/web/src/components/video/VideoListItem.tsx">
import { Link } from '@tanstack/react-router';
import type { VideoSummary } from '@/types/api';
import { Button } from '@/components/ui/button';

interface VideoListItemProps {
  video: VideoSummary;
}

export function VideoListItem({ video }: VideoListItemProps) {
  return (
    <div
      key={video.id} // Key should be on the top-level element returned by map in VideoList
      className="group relative rounded-lg border border-border p-4 hover:border-primary transition-colors"
    >
      <div className="flex justify-between items-start mb-3">
        <h3 className="font-semibold line-clamp-1 group-hover:text-primary transition-colors">
          {video.title || "Untitled Video"}
        </h3>
        {/* TODO: Add status badge or icon here based on video.status */}
      </div>
      {video.thumbnail_file_url && (
        <Link 
          to="/video/$videoId"
          params={{ videoId: String(video.id) }}
          className="block aspect-video w-full overflow-hidden rounded-md mb-3"
        >
          <img
            src={video.thumbnail_file_url}
            alt={video.title || 'Video thumbnail'}
            className="w-full h-full object-cover transition-transform group-hover:scale-105"
          />
        </Link>
      )}
      <div className="mt-2 space-y-1 text-sm text-muted-foreground">
        <p>
          Status: {video.status || "N/A"}
        </p>
        {/* Can add more details like created_at, duration etc. if needed */}
      </div>
      <div className="mt-3 flex space-x-2">
        <Button variant="outline" size="sm" asChild>
          <Link
            to="/video/$videoId"
            params={{ videoId: String(video.id) }}
          >
            View Details
          </Link>
        </Button>
        {/* TODO: Add other actions like Edit, Delete if applicable */}
      </div>
    </div>
  );
}
</file>

<file path="apps/web/src/components/auth.tsx">
export function Auth({
	actionText,
	onSubmit,
	status,
	afterSubmit,
}: {
	actionText: string;
	onSubmit: (e: React.FormEvent<HTMLFormElement>) => void;
	status: "pending" | "idle" | "success" | "error";
	afterSubmit?: React.ReactNode;
}) {
	return (
		<div className="fixed inset-0 bg-white dark:bg-black flex items-start justify-center p-8">
			<div className="bg-white dark:bg-gray-900 p-8 rounded-lg shadow-lg">
				<h1 className="text-2xl font-bold mb-4">{actionText}</h1>
				<form
					onSubmit={(e) => {
						e.preventDefault();
						onSubmit(e);
					}}
					className="space-y-4"
				>
					<div>
						<label htmlFor="email" className="block text-xs">
							Username
						</label>
						<input
							type="email"
							name="email"
							id="email"
							className="px-2 py-1 w-full rounded border border-gray-500/20 bg-white dark:bg-gray-800"
						/>
					</div>
					<div>
						<label htmlFor="password" className="block text-xs">
							Password
						</label>
						<input
							type="password"
							name="password"
							id="password"
							className="px-2 py-1 w-full rounded border border-gray-500/20 bg-white dark:bg-gray-800"
						/>
					</div>
					<button
						type="submit"
						className="w-full bg-cyan-600 text-white rounded py-2 font-black uppercase"
						disabled={status === "pending"}
					>
						{status === "pending" ? "..." : actionText}
					</button>
					{afterSubmit ? afterSubmit : null}
				</form>
			</div>
		</div>
	);
}
</file>

<file path="apps/web/src/components/default-catch-boundary.tsx">
import {
	ErrorComponent,
	Link,
	rootRouteId,
	useMatch,
	useRouter,
} from "@tanstack/react-router";
import type { ErrorComponentProps } from "@tanstack/react-router";
import * as React from "react";

export function DefaultCatchBoundary({ error }: ErrorComponentProps) {
	const router = useRouter();
	const isRoot = useMatch({
		strict: false,
		select: (state) => state.id === rootRouteId,
	});

	console.error(error);

	return (
		<div className="min-w-0 flex-1 p-4 flex flex-col items-center justify-center gap-6">
			<ErrorComponent error={error} />
			<div className="flex gap-2 items-center flex-wrap">
				<button
					onClick={() => {
						router.invalidate();
					}}
					className={
						"px-2 py-1 bg-gray-600 dark:bg-gray-700 rounded text-white uppercase font-extrabold"
					}
				>
					Try Again
				</button>
				{isRoot ? (
					<Link
						to="/"
						className={
							"px-2 py-1 bg-gray-600 dark:bg-gray-700 rounded text-white uppercase font-extrabold"
						}
					>
						Home
					</Link>
				) : (
					<Link
						to="/"
						className={
							"px-2 py-1 bg-gray-600 dark:bg-gray-700 rounded text-white uppercase font-extrabold"
						}
						onClick={(e) => {
							e.preventDefault();
							window.history.back();
						}}
					>
						Go Back
					</Link>
				)}
			</div>
		</div>
	);
}
</file>

<file path="apps/web/src/components/not-found.tsx">
import { Link } from '@tanstack/react-router'

export function NotFound({ children }: { children?: any }) {
  return (
    <div className="space-y-2 p-2">
      <div className="text-gray-600 dark:text-gray-400">
        {children || <p>The page you are looking for does not exist.</p>}
      </div>
      <p className="flex items-center gap-2 flex-wrap">
        <button
          onClick={() => window.history.back()}
          className="bg-emerald-500 text-white px-2 py-1 rounded uppercase font-black text-sm"
        >
          Go back
        </button>
        <Link
          to="/"
          className="bg-cyan-600 text-white px-2 py-1 rounded uppercase font-black text-sm"
        >
          Start Over
        </Link>
      </p>
    </div>
  )
}
</file>

<file path="apps/web/src/components/post-error.tsx">
import { ErrorComponent, ErrorComponentProps } from '@tanstack/react-router'

export function PostErrorComponent({ error }: ErrorComponentProps) {
  return <ErrorComponent error={error} />
}
</file>

<file path="apps/web/src/components/ProtectedLayout.tsx">
import React, { useEffect, useState } from "react";
import { useNavigate } from "@tanstack/react-router";

type ProtectedLayoutProps = {
  children: React.ReactNode;
};

export function ProtectedLayout({ children }: ProtectedLayoutProps) {
  const [loading, setLoading] = useState(true);
  const [authenticated, setAuthenticated] = useState(false);
  const navigate = useNavigate();

  useEffect(() => {
    async function checkAuth() {
      try {
        const res = await fetch("/auth/me", {
          credentials: "include",
        });
        if (res.ok) {
          setAuthenticated(true);
        } else {
          setAuthenticated(false);
          navigate({ to: "/login" });
        }
      } catch (err) {
        setAuthenticated(false);
        navigate({ to: "/login" });
      } finally {
        setLoading(false);
      }
    }
    checkAuth();
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, []);

  if (loading) {
    return <div>Loading...</div>;
  }

  if (!authenticated) {
    return null;
  }

  return <>{children}</>;
}

export default ProtectedLayout;
</file>

<file path="apps/web/src/components/user-error.tsx">
import { ErrorComponent, ErrorComponentProps } from '@tanstack/react-router'

export function UserErrorComponent({ error }: ErrorComponentProps) {
  return <ErrorComponent error={error} />
}
</file>

<file path="apps/web/src/hooks/use-mobile.ts">
import * as React from "react";

const MOBILE_BREAKPOINT = 768;

export function useIsMobile() {
	const [isMobile, setIsMobile] = React.useState<boolean | undefined>(
		undefined,
	);

	React.useEffect(() => {
		const mql = window.matchMedia(`(max-width: ${MOBILE_BREAKPOINT - 1}px)`);
		const onChange = () => {
			setIsMobile(window.innerWidth < MOBILE_BREAKPOINT);
		};
		mql.addEventListener("change", onChange);
		setIsMobile(window.innerWidth < MOBILE_BREAKPOINT);
		return () => mql.removeEventListener("change", onChange);
	}, []);

	return !!isMobile;
}
</file>

<file path="apps/web/src/hooks/useAuth.ts">
import { useState, useEffect, useCallback } from 'react';
import { supabase } from '@echo/db/clients/client';
import type { AuthError, Session, User, SignInWithPasswordCredentials, SignUpWithPasswordCredentials, AuthChangeEvent, OAuthResponse, UserResponse } from '@supabase/supabase-js';

interface UseAuthState {
  user: User | null;
  session: Session | null;
  isLoading: boolean;
  error: AuthError | null;
  isInitialized: boolean; // To track if the initial auth state has been loaded
}

interface UseAuthActions {
  loginWithPassword: (credentials: SignInWithPasswordCredentials) => Promise<{ error: AuthError | null }>;
  signUpWithEmailPassword: (credentials: SignUpWithPasswordCredentials) => Promise<{ error: AuthError | null }>;
  signOut: () => Promise<{ error: AuthError | null }>;
  signInWithGoogle: () => Promise<OAuthResponse>;
}

export function useAuth(): UseAuthState & UseAuthActions {
  const [user, setUser] = useState<User | null>(null);
  const [session, setSession] = useState<Session | null>(null);
  const [isLoading, setIsLoading] = useState<boolean>(false); // For active operations like login/signup
  const [error, setError] = useState<AuthError | null>(null);
  const [isInitialized, setIsInitialized] = useState<boolean>(false); // Tracks initial auth state check

  useEffect(() => {
    setIsLoading(true);
    supabase.auth.getSession().then(({ data, error }: { data: { session: Session | null }, error: AuthError | null }) => {
      if (error) {
        console.error('Error getting initial session:', error);
        setError(error);
      }
      setSession(data.session);
      setUser(data.session?.user ?? null);
      setIsInitialized(true);
      setIsLoading(false);
    });

    const { data: authListenerData } = supabase.auth.onAuthStateChange(
      async (event: AuthChangeEvent, newSession: Session | null) => {
        setSession(newSession);
        setUser(newSession?.user ?? null);
        setError(null);
        setIsInitialized(true);
        setIsLoading(false);
      }
    );

    return () => {
      authListenerData.subscription.unsubscribe();
    };
  }, []);

  const handleAuthOperation = useCallback(
    async (authPromiseFactory: () => Promise<{ data?: any; error: AuthError | null }>) => {
      setIsLoading(true);
      setError(null);
      try {
        const { error: opError } = await authPromiseFactory();
        if (opError) {
          setError(opError);
          return { error: opError };
        }
        // Session and user state will be updated by onAuthStateChange listener
        return { error: null };
      } catch (e: any) {
        const err = { name: 'AuthOperationError', message: e.message || 'An unknown error occurred' } as AuthError;
        setError(err);
        return { error: err };
      } finally {
        setIsLoading(false);
      }
    },
    []
  );

  const loginWithPassword = useCallback(
    async (credentials: SignInWithPasswordCredentials) => {
      return handleAuthOperation(() => supabase.auth.signInWithPassword(credentials));
    },
    [handleAuthOperation]
  );

  const signUpWithEmailPassword = useCallback(
    async (credentials: SignUpWithPasswordCredentials) => {
      return handleAuthOperation(() => supabase.auth.signUp(credentials));
    },
    [handleAuthOperation]
  );

  const signOut = useCallback(async () => {
    return handleAuthOperation(() => supabase.auth.signOut());
  }, [handleAuthOperation]);

  const signInWithGoogle = useCallback(async (): Promise<OAuthResponse> => {
    setIsLoading(true);
    setError(null);
    const result = await supabase.auth.signInWithOAuth({
        provider: 'google',
        options: {
            redirectTo: `${window.location.origin}/auth/callback`,
        },
    });
    
    if (result.error) {
        setError(result.error);
        setIsLoading(false);
    }
    return result; 
  }, []);


  return { user, session, isLoading, error, isInitialized, loginWithPassword, signUpWithEmailPassword, signOut, signInWithGoogle };
}
</file>

<file path="apps/web/src/hooks/useJobStatusManager.ts">
import { useState, useEffect, useRef } from 'react';
import { useQueryClient } from '@tanstack/react-query';
import { useAppWebSocket } from './useAppWebSocket';
import { supabase } from '@echo/db';
import type { Session, AuthChangeEvent } from '@supabase/supabase-js';
// VideoSummary was re-added, VideoJobSchema aliased to VideoJob, WebSocketJobUpdate removed from this import
import type { VideoJobSchema as VideoJob, ProcessingStatus, VideoSummary } from '../types/api'; 

// Define WebSocketJobUpdate locally
// It represents the expected shape of job update messages from the WebSocket.
// Ensures job_id is present for targeting cache updates, video_id for lists.
export type WebSocketJobUpdate = Partial<VideoJob> & { 
  job_id: number; 
  video_id?: number; 
  // Include any other fields that are guaranteed or essential in a WS message for job updates.
  // For example, if status is always sent for an update:
  // status?: ProcessingStatus;
};

export function useJobStatusManager() {
  const queryClient = useQueryClient();
  const [currentSession, setCurrentSession] = useState<Session | null>(null);
  const prevUserIdRef = useRef<string | null>(null);

  useEffect(() => {
    supabase.auth.getSession().then(({ data }: { data: { session: Session | null } }) => {
      setCurrentSession(data.session);
      const currentUserId = data.session?.user?.id ?? null;
      if (currentUserId) {
        console.log("JobStatusManager: Initial session found, user ID:", currentUserId);
      } else {
        console.log("JobStatusManager: No initial session found.");
      }
      prevUserIdRef.current = currentUserId;
    });

    const { data: authListenerData } = supabase.auth.onAuthStateChange(
      (_event: AuthChangeEvent, session: Session | null) => {
        setCurrentSession(session);
        const newUserId = session?.user?.id ?? null;
        if (newUserId) {
          console.log("JobStatusManager: Auth state changed, new user ID:", newUserId);
        } else {
          console.log("JobStatusManager: Auth state changed, user logged out or no session.");
        }
        prevUserIdRef.current = newUserId;
      }
    );

    return () => {
      authListenerData.subscription.unsubscribe();
    };
  }, []);

  const handleTypedWebSocketMessage = (wsUpdateData: WebSocketJobUpdate) => {
    console.log("JobStatusManager: Received parsed job update via WebSocket", wsUpdateData);

    if (!wsUpdateData.job_id) {
        console.warn("JobStatusManager: WebSocket update missing job_id, skipping cache update.", wsUpdateData);
        return;
    }

    const jobDetailsQueryKey = ['jobDetails', String(wsUpdateData.job_id)];
    queryClient.setQueryData<VideoJob | undefined>(
      jobDetailsQueryKey,
      (oldData) => {
        if (oldData) {
          const updatedData = { ...oldData, ...wsUpdateData };
          // Ensure all fields from VideoJob are potentially present if wsUpdateData is partial
          // This is a bit simplistic; a more robust merge might be needed depending on data structure
          return updatedData as VideoJob;
        }
        // If the job details were not already cached, we might not want to create it here,
        // or we might want to fetch it if it's a new job we should know about.
        // For now, only update if existing.
        return oldData; 
      }
    );

    const myVideosQueryKey = ['myVideos'];
    queryClient.setQueryData<VideoSummary[] | undefined>(
      myVideosQueryKey,
      (oldVideoList) => {
        if (!oldVideoList) return undefined;
        return oldVideoList.map(videoSummary => {
          // Assuming video_id is present in wsUpdateData if it's relevant to a video entry
          if (wsUpdateData.video_id && videoSummary.id === wsUpdateData.video_id) {
            const newStatus = wsUpdateData.status as ProcessingStatus | undefined; // Type assertion
            return {
              ...videoSummary,
              // Only update fields that are present in the WebSocket update
              ...(wsUpdateData.metadata?.title && { title: wsUpdateData.metadata.title }),
              ...(newStatus && { status: newStatus }),
              // Add other relevant fields from VideoSummary that might be updated
            };
          }
          return videoSummary;
        });
      }
    );

    const processingJobsQueryKey = ['processingJobs'];
    queryClient.setQueryData<VideoJob[] | undefined>(
      processingJobsQueryKey,
      (oldData) => {
        if (!oldData) return undefined; // If no cache exists, do nothing or consider fetching

        const jobExists = oldData.some(job => job.id === wsUpdateData.job_id);
        
        if (jobExists) {
          return oldData.map(job => {
            if (job.id === wsUpdateData.job_id) {
              // Merge existing job data with update
              // Ensure status is correctly typed if present in wsUpdateData
              const updatedJob = { ...job, ...wsUpdateData };
              if (wsUpdateData.status) {
                updatedJob.status = wsUpdateData.status as ProcessingStatus;
              }
              return updatedJob;
            }
            return job;
          });
        } else {
          // If the job is new and its status implies it should be on the processing dashboard
          // (e.g., PENDING, PROCESSING), add it to the list.
          // This requires wsUpdateData to be a more complete representation of a VideoJob.
          // For now, we'll assume if it's a new job_id, we might want to add it if it's a full object.
          // This might need refinement based on what data the WS sends for "new" jobs.
          if (wsUpdateData.status && (wsUpdateData.status === "PENDING" || wsUpdateData.status === "PROCESSING")) {
             // We need to be careful about partial updates vs full new job objects.
             // Let's assume wsUpdateData could be a full new job if it's not in oldData.
             // The type WebSocketJobUpdate is Partial<VideoJob>, so we need to ensure
             // that if we add it, it has all necessary fields for a VideoJob.
             // This part is tricky without knowing the exact WS message contents for new jobs.
             // A safer approach for "new" jobs might be to invalidate the query and let it refetch,
             // or ensure the WS sends the full job object.
             // For now, let's try to add it if it has a status.
             // This cast might be unsafe if wsUpdateData is truly partial.
            return [...oldData, wsUpdateData as VideoJob];
          }
        }
        return oldData;
      }
    );

    console.log(`JobStatusManager: Updated cache for job ${wsUpdateData.job_id} and relevant lists (processingJobs, myVideos).`);
  };

  const { isConnected, lastJsonMessage } = useAppWebSocket({
    onOpen: () => console.log("JobStatusManager: WebSocket connection established."),
    onClose: (event) => console.log("JobStatusManager: WebSocket connection closed.", event),
    onError: (event) => console.error("JobStatusManager: WebSocket error.", event),
  });

  useEffect(() => {
    if (lastJsonMessage) {
        // Validate the structure of the message more carefully
        if (typeof lastJsonMessage === 'object' && 
            lastJsonMessage !== null && 
            'job_id' in lastJsonMessage &&
            typeof (lastJsonMessage as any).job_id === 'number' // Ensure job_id is a number
            // Potentially add more checks if other fields are critical for routing/typing
            ) {
            const updateData = lastJsonMessage as WebSocketJobUpdate; 
            handleTypedWebSocketMessage(updateData);
        } else {
            console.warn("JobStatusManager: Received WebSocket message of unexpected shape or missing/invalid job_id:", lastJsonMessage);
        }
    }
  }, [lastJsonMessage, queryClient]); // Added queryClient to dependencies of useEffect if it's used in handleTypedWebSocketMessage through closure

  useEffect(() => {
    const currentUserId = prevUserIdRef.current;
    if (currentUserId) {
      console.log(`JobStatusManager: Active for user ${currentUserId}. WebSocket connected: ${isConnected}`);
    } else {
      console.log(`JobStatusManager: Waiting for user ID to be determined. WebSocket connected: ${isConnected}`);
    }
  }, [isConnected]); // Removed prevUserIdRef.current from deps as it's a ref.

  return { isWebSocketConnected: isConnected, currentUserId: prevUserIdRef.current, session: currentSession };
}
</file>

<file path="apps/web/src/hooks/useMutation.ts">
import * as React from "react";

export function useMutation<TVariables, TData, TError = Error>(opts: {
	fn: (variables: TVariables) => Promise<TData>;
	onSuccess?: (ctx: { data: TData }) => void | Promise<void>;
}) {
	const [submittedAt, setSubmittedAt] = React.useState<number | undefined>();
	const [variables, setVariables] = React.useState<TVariables | undefined>();
	const [error, setError] = React.useState<TError | undefined>();
	const [data, setData] = React.useState<TData | undefined>();
	const [status, setStatus] = React.useState<
		"idle" | "pending" | "success" | "error"
	>("idle");

	const mutate = React.useCallback(
		async (variables: TVariables): Promise<TData | undefined> => {
			setStatus("pending");
			setSubmittedAt(Date.now());
			setVariables(variables);
			//
			try {
				const data = await opts.fn(variables);
				await opts.onSuccess?.({ data });
				setStatus("success");
				setError(undefined);
				setData(data);
				return data;
			} catch (err: any) {
				setStatus("error");
				setError(err);
			}
		},
		[opts.fn],
	);

	return {
		status,
		variables,
		submittedAt,
		mutate,
		error,
		data,
	};
}
</file>

<file path="apps/web/src/lib/utils.ts">
import { type ClassValue, clsx } from "clsx";
import { twMerge } from "tailwind-merge";

/**
 * Combines multiple class names into a single string, merging Tailwind CSS classes properly.
 * Uses clsx for conditional classes and tailwind-merge to handle Tailwind class conflicts.
 *
 * @param inputs - Class values to be combined
 * @returns A string of combined class names
 */
export function cn(...inputs: ClassValue[]): string {
	return twMerge(clsx(inputs));
}
</file>

<file path="apps/web/src/routes/_pathlessLayout.tsx">
import { Outlet, createFileRoute } from '@tanstack/react-router'

export const Route = createFileRoute('/_pathlessLayout')({
  component: LayoutComponent,
})

function LayoutComponent() {
  return (
    <div className="p-2">
      <div className="border-b">I'm a layout</div>
      <div>
        <Outlet />
      </div>
    </div>
  )
}
</file>

<file path="apps/web/src/routes/dashboard.e2e.test.tsx">
// E2E smoke test for login  upload  dashboard video listing
// This test assumes the backend and Supabase are running locally and accessible.

import { describe, it, expect, beforeAll } from "vitest";
import { fetchMyVideos } from "@/lib/api";

// NOTE: This is a placeholder for a true E2E test. In a real E2E setup, use Playwright or Cypress for browser automation.
// Here, we test the API integration and dashboard listing logic.

describe("E2E: Dashboard video listing", () => {
  let initialVideos: Awaited<ReturnType<typeof fetchMyVideos>>;

  beforeAll(async () => {
    // Simulate user login here if possible (e.g., set cookie/JWT)
    // For now, assume the test runner is authenticated or running with a test user session.
    initialVideos = await fetchMyVideos();
  });

  it("should list videos for the current user", async () => {
    const videos = await fetchMyVideos();
    expect(Array.isArray(videos)).toBe(true);
    // If there are no videos, the test still passes (empty state)
    // If there are videos, check structure
    if (videos.length > 0) {
      const video = videos[0];
      expect(video).toHaveProperty("id");
      expect(video).toHaveProperty("title");
      expect(video).toHaveProperty("status");
      expect(video).toHaveProperty("created_at");
    }
  });

  // Additional steps for upload and dashboard refresh would require browser automation.
  // This test can be expanded with Playwright/Cypress for full E2E coverage.
});
</file>

<file path="apps/web/src/routes/index.tsx">
import { createFileRoute, Link } from "@tanstack/react-router";
import { Button } from "@/components/ui/button";
import { useAuth } from "../hooks/useAuth";
import { GoogleLoginButton } from "@/components/GoogleLoginButton"; // Assuming this is the correct path

export const Route = createFileRoute("/")({
  component: Home,
});

function Home() {
  const { session, user, isLoading } = useAuth();

  return (
    <div className="container mx-auto p-4 sm:p-6 md:p-8 max-w-3xl text-center">
      <header className="mb-12">
        <h1 className="text-4xl sm:text-5xl font-bold mb-4 text-gray-800">
          Welcome to EchoStream
        </h1>
        <p className="text-lg sm:text-xl text-gray-600">
          Transform your raw video content into polished, engaging material with automated transcripts, summaries, chapters, and more. Perfect for creators and businesses looking to maximize their video impact.
        </p>
      </header>

      <section className="mb-12">
        <h2 className="text-2xl sm:text-3xl font-semibold mb-6 text-gray-700">How It Works</h2>
        <div className="grid md:grid-cols-3 gap-6 text-left">
          <div className="p-6 bg-white rounded-lg shadow-md">
            <h3 className="text-xl font-semibold mb-2 text-blue-600">1. Upload</h3>
            <p className="text-gray-600">Simply upload your video file. We handle the rest, kicking off our powerful AI processing pipeline.</p>
          </div>
          <div className="p-6 bg-white rounded-lg shadow-md">
            <h3 className="text-xl font-semibold mb-2 text-blue-600">2. Process</h3>
            <p className="text-gray-600">Our AI gets to work generating accurate transcripts, insightful summaries, logical chapters, and even potential titles for your content.</p>
          </div>
          <div className="p-6 bg-white rounded-lg shadow-md">
            <h3 className="text-xl font-semibold mb-2 text-blue-600">3. Publish</h3>
            <p className="text-gray-600">Review the generated content, make any tweaks, and get ready to share your enhanced video with the world or use the assets in your workflow.</p>
          </div>
        </div>
      </section>

      {isLoading ? (
        <div className="my-8">
          <p className="text-gray-500">Loading authentication status...</p>
          {/* You can add a spinner here if you like */}
        </div>
      ) : session ? (
        <div className="my-8 p-6 bg-green-50 rounded-lg shadow">
          <h2 className="text-2xl font-semibold mb-3 text-green-700">
            Welcome back, {user?.email || "Creator"}!
          </h2>
          <p className="text-gray-700 mb-4">
            You are logged in. Head to your dashboard to manage your videos.
          </p>
          <Button asChild size="lg">
            <Link to="/dashboard" className="font-semibold">
              Go to Dashboard
            </Link>
          </Button>
        </div>
      ) : (
        <div className="my-8 p-6 bg-blue-50 rounded-lg shadow">
          <h2 className="text-2xl font-semibold mb-4 text-blue-700">
            Ready to Get Started?
          </h2>
          <p className="text-gray-700 mb-6">
            Sign in with Google to start processing your videos and unlock powerful AI features.
          </p>
          <div className="max-w-xs mx-auto">
            <GoogleLoginButton />
          </div>
        </div>
      )}

      <footer className="mt-16 pt-8 border-t border-gray-200">
        <p className="text-sm text-gray-500">
          &copy; {new Date().getFullYear()} EchoStream. All rights reserved.
        </p>
      </footer>
    </div>
  );
}
</file>

<file path="apps/web/src/routes/logout.tsx">
import { createFileRoute, redirect } from "@tanstack/react-router";
import { createServerFn } from "@tanstack/react-start";
import { getSupabaseServerClient } from "../utils/supabase";

const logoutFn = createServerFn().handler(async () => {
	const supabase = await getSupabaseServerClient();
	const { error } = await supabase.auth.signOut();

	if (error) {
		return {
			error: true,
			message: error.message,
		};
	}

	throw redirect({
		href: "/",
	});
});

export const Route = createFileRoute("/logout")({
	preload: false,
	loader: () => logoutFn(),
});
</file>

<file path="apps/web/src/routes/settings.tsx">
import { createFileRoute } from "@tanstack/react-router";

import { Button } from "../components/ui/button";
import {
	Card,
	CardContent,
	CardHeader,
	CardTitle,
} from "../components/ui/card";
import { Input } from "../components/ui/input";
import { Label } from "../components/ui/label";
import { Separator } from "../components/ui/separator";
import { Switch } from "../components/ui/switch";
// import { Select, SelectContent, SelectItem, SelectTrigger, SelectValue } from "../components/ui/select"; // Placeholder for future use

function SettingsComponent() {
	return (
		<div className="container mx-auto py-8 max-w-4xl space-y-8">
			<h1 className="text-3xl font-bold">Settings</h1>

			{/* Profile & Preferences Section */}
			<Card>
				<CardHeader>
					<CardTitle>Profile & Preferences</CardTitle>
				</CardHeader>
				<CardContent className="space-y-4">
					<div className="space-y-2">
						<Label htmlFor="displayName">Display Name</Label>
						<Input
							id="displayName"
							placeholder="Your Name (coming soon)"
							disabled
						/>
					</div>
					<div className="space-y-2">
						<Label htmlFor="email">Email</Label>
						<Input
							id="email"
							type="email"
							placeholder="your.email@example.com (coming soon)"
							disabled
						/>
					</div>
					<div className="flex items-center justify-between space-x-2 pt-2">
						<Label htmlFor="darkMode" className="flex flex-col space-y-1">
							<span>Dark Mode</span>
							<span className="font-normal leading-snug text-muted-foreground">
								Adjust the appearance to reduce eye strain.
							</span>
						</Label>
						<Switch id="darkMode" disabled />
					</div>
				</CardContent>
			</Card>

			{/* YouTube Integration Section */}
			<Card>
				<CardHeader>
					<CardTitle>YouTube Integration</CardTitle>
				</CardHeader>
				<CardContent className="space-y-4">
					<div className="space-y-2">
						<Label>Connected Account</Label>
						<div className="flex items-center justify-between">
							<p className="text-sm text-muted-foreground">
								youtube-channel-name (coming soon)
							</p>
							<Button variant="outline" size="sm" disabled>
								Reconnect
							</Button>
						</div>
					</div>
					<Separator />
					<div className="space-y-2">
						<Label htmlFor="defaultPrivacy">Default Upload Privacy</Label>
						{/* Placeholder for Select component */}
						<Input
							id="defaultPrivacy"
							placeholder="Private (coming soon)"
							disabled
						/>
						{/* <Select disabled>
              <SelectTrigger id="defaultPrivacy">
                <SelectValue placeholder="Select default privacy" />
              </SelectTrigger>
              <SelectContent>
                <SelectItem value="private">Private</SelectItem>
                <SelectItem value="unlisted">Unlisted</SelectItem>
                <SelectItem value="public">Public</SelectItem>
              </SelectContent>
            </Select> */}
					</div>
					<div className="space-y-2">
						<Label htmlFor="defaultCategory">Default Video Category</Label>
						{/* Placeholder for Select component */}
						<Input
							id="defaultCategory"
							placeholder="Science & Technology (coming soon)"
							disabled
						/>
					</div>
				</CardContent>
			</Card>

			{/* AI Configuration Section */}
			<Card>
				<CardHeader>
					<CardTitle>AI Configuration</CardTitle>
				</CardHeader>
				<CardContent className="space-y-4">
					<div className="space-y-2">
						<Label>AI Model</Label>
						<p className="text-sm text-muted-foreground">
							Gemini Pro (Current Model - read-only)
						</p>
					</div>
					<Separator />
					<div className="space-y-2">
						<Label htmlFor="titleSuggestions">
							Number of Title Suggestions
						</Label>
						<Input
							id="titleSuggestions"
							type="number"
							placeholder="3 (coming soon)"
							disabled
						/>
					</div>
					<div className="space-y-2">
						<Label htmlFor="thumbnailStyles">Thumbnail Generation Style</Label>
						{/* Placeholder for Select component */}
						<Input
							id="thumbnailStyles"
							placeholder="Default (coming soon)"
							disabled
						/>
					</div>
				</CardContent>
			</Card>

			{/* Notification Settings Section */}
			<Card>
				<CardHeader>
					<CardTitle>Notifications</CardTitle>
				</CardHeader>
				<CardContent className="space-y-4">
					<div className="flex items-center justify-between space-x-2">
						<Label
							htmlFor="emailNotifications"
							className="flex flex-col space-y-1"
						>
							<span>Email Notifications</span>
							<span className="font-normal leading-snug text-muted-foreground">
								Receive email updates for processing events.
							</span>
						</Label>
						<Switch id="emailNotifications" disabled />
					</div>
					<Separator />
					<p className="text-sm font-medium text-muted-foreground">
						Notify me when:
					</p>
					<div className="flex items-center justify-between space-x-2 pl-4">
						<Label htmlFor="notifyUploadComplete">Upload Complete</Label>
						<Switch id="notifyUploadComplete" disabled />
					</div>
					<div className="flex items-center justify-between space-x-2 pl-4">
						<Label htmlFor="notifyProcessingComplete">
							Processing Complete
						</Label>
						<Switch id="notifyProcessingComplete" disabled />
					</div>
					<div className="flex items-center justify-between space-x-2 pl-4">
						<Label htmlFor="notifyProcessingError">Processing Error</Label>
						<Switch id="notifyProcessingError" disabled />
					</div>
				</CardContent>
			</Card>

			{/* Storage & Credentials Section */}
			<Card>
				<CardHeader>
					<CardTitle>Storage & Credentials</CardTitle>
				</CardHeader>
				<CardContent className="space-y-4">
					<div className="space-y-2">
						<Label>Google Cloud Storage</Label>
						<p className="text-sm text-muted-foreground">
							Input Bucket: `your-input-bucket` (read-only)
						</p>
						<p className="text-sm text-muted-foreground">
							Output Bucket: `your-output-bucket` (read-only)
						</p>
					</div>
					<Separator />
					<div className="space-y-2">
						<Label>Credential Status</Label>
						<p className="text-sm text-muted-foreground">
							Google Cloud: Connected 
						</p>
						<p className="text-sm text-muted-foreground">
							YouTube API: Connected 
						</p>
					</div>
				</CardContent>
			</Card>
		</div>
	);
}

export const Route = createFileRoute("/settings")({
	component: SettingsComponent,
});
</file>

<file path="apps/web/src/services/gcs-content.ts">
/**
 * Service to handle fetching content from Google Cloud Storage
 */

// Utility to fetch content from GCS using a publicly accessible URL
export async function fetchGCSContent(url: string): Promise<string> {
	try {
		const response = await fetch(url);

		if (!response.ok) {
			throw new Error(
				`Failed to fetch content: ${response.status} ${response.statusText}`,
			);
		}

		return await response.text();
	} catch (error) {
		console.error("Error fetching GCS content:", error);
		throw error;
	}
}

// Generate a publicly accessible URL for a GCS file (if permission allows)
export function getGCSPublicUrl(bucketName: string, filePath: string): string {
	return `https://storage.googleapis.com/${bucketName}/${filePath}`;
}

// Process the output_files paths from Firestore to generate usable URLs
export function getContentUrls(
	bucketName: string,
	outputFiles: Record<string, string> = {},
) {
	const urls: Record<string, string> = {};

	Object.entries(outputFiles).forEach(([key, path]) => {
		if (path) {
			urls[key] = getGCSPublicUrl(bucketName, path);
		}
	});

	return urls;
}

// Try to fetch content, returns null if not available
export async function tryFetchContent(
	bucketName: string | undefined,
	outputFiles: Record<string, string>,
	contentType: string,
): Promise<string | null> {
	if (!bucketName || !outputFiles[contentType]) {
		return null;
	}

	try {
		const url = getGCSPublicUrl(bucketName, outputFiles[contentType]);
		const content = await fetchGCSContent(url);
		return content;
	} catch (error) {
		console.error(`Error fetching ${contentType}:`, error);
		return null;
	}
}
</file>

<file path="apps/web/src/styles/app.css">
@import "tailwindcss";

@plugin 'tailwindcss-animate';

@custom-variant dark (&:is(.dark *));

@theme inline {
  --radius-lg: var(--radius);
  --radius-md: calc(var(--radius) - 2px);
  --radius-sm: calc(var(--radius) - 4px);

  --color-background: var(--background);
  --color-foreground: var(--foreground);

  --color-card: var(--card);
  --color-card-foreground: var(--card-foreground);

  --color-popover: var(--popover);
  --color-popover-foreground: var(--popover-foreground);

  --color-primary: var(--primary);
  --color-primary-foreground: var(--primary-foreground);

  --color-secondary: var(--secondary);
  --color-secondary-foreground: var(--secondary-foreground);

  --color-muted: var(--muted);
  --color-muted-foreground: var(--muted-foreground);

  --color-accent: var(--accent);
  --color-accent-foreground: var(--accent-foreground);

  --color-destructive: var(--destructive);
  --color-destructive-foreground: var(--destructive-foreground);

  --color-border: var(--border);
  --color-input: var(--input);
  --color-ring: var(--ring);

  --color-chart-1: var(--chart-1);
  --color-chart-2: var(--chart-2);
  --color-chart-3: var(--chart-3);
  --color-chart-4: var(--chart-4);
  --color-chart-5: var(--chart-5);
}

:root {
  --radius: 0.5rem;
  --background: oklch(1 0 0);
  --foreground: oklch(0.141 0.005 285.823);
  --card: oklch(1 0 0);
  --card-foreground: oklch(0.141 0.005 285.823);
  --popover: oklch(1 0 0);
  --popover-foreground: oklch(0.141 0.005 285.823);
  --primary: oklch(0.645 0.246 16.439);
  --primary-foreground: oklch(0.969 0.015 12.422);
  --secondary: oklch(0.967 0.001 286.375);
  --secondary-foreground: oklch(0.21 0.006 285.885);
  --muted: oklch(0.967 0.001 286.375);
  --muted-foreground: oklch(0.552 0.016 285.938);
  --accent: oklch(0.967 0.001 286.375);
  --accent-foreground: oklch(0.21 0.006 285.885);
  --destructive: oklch(0.577 0.245 27.325);
  --border: oklch(0.92 0.004 286.32);
  --input: oklch(0.92 0.004 286.32);
  --ring: oklch(0.645 0.246 16.439);
  --chart-1: oklch(0.646 0.222 41.116);
  --chart-2: oklch(0.6 0.118 184.704);
  --chart-3: oklch(0.398 0.07 227.392);
  --chart-4: oklch(0.828 0.189 84.429);
  --chart-5: oklch(0.769 0.188 70.08);
  --sidebar: oklch(0.985 0 0);
  --sidebar-foreground: oklch(0.141 0.005 285.823);
  --sidebar-primary: oklch(0.645 0.246 16.439);
  --sidebar-primary-foreground: oklch(0.969 0.015 12.422);
  --sidebar-accent: oklch(0.967 0.001 286.375);
  --sidebar-accent-foreground: oklch(0.21 0.006 285.885);
  --sidebar-border: oklch(0.92 0.004 286.32);
  --sidebar-ring: oklch(0.645 0.246 16.439);
}

.dark {
  --background: oklch(0.141 0.005 285.823);
  --foreground: oklch(0.985 0 0);
  --card: oklch(0.21 0.006 285.885);
  --card-foreground: oklch(0.985 0 0);
  --popover: oklch(0.21 0.006 285.885);
  --popover-foreground: oklch(0.985 0 0);
  --primary: oklch(0.645 0.246 16.439);
  --primary-foreground: oklch(0.969 0.015 12.422);
  --secondary: oklch(0.274 0.006 286.033);
  --secondary-foreground: oklch(0.985 0 0);
  --muted: oklch(0.274 0.006 286.033);
  --muted-foreground: oklch(0.705 0.015 286.067);
  --accent: oklch(0.274 0.006 286.033);
  --accent-foreground: oklch(0.985 0 0);
  --destructive: oklch(0.704 0.191 22.216);
  --border: oklch(1 0 0 / 10%);
  --input: oklch(1 0 0 / 15%);
  --ring: oklch(0.645 0.246 16.439);
  --chart-1: oklch(0.488 0.243 264.376);
  --chart-2: oklch(0.696 0.17 162.48);
  --chart-3: oklch(0.769 0.188 70.08);
  --chart-4: oklch(0.627 0.265 303.9);
  --chart-5: oklch(0.645 0.246 16.439);
  --sidebar: oklch(0.21 0.006 285.885);
  --sidebar-foreground: oklch(0.985 0 0);
  --sidebar-primary: oklch(0.645 0.246 16.439);
  --sidebar-primary-foreground: oklch(0.969 0.015 12.422);
  --sidebar-accent: oklch(0.274 0.006 286.033);
  --sidebar-accent-foreground: oklch(0.985 0 0);
  --sidebar-border: oklch(1 0 0 / 10%);
  --sidebar-ring: oklch(0.645 0.246 16.439);
}

@layer base {
  * {
    @apply border-border;
  }
  body {
    @apply bg-background text-foreground;
  }
}
</file>

<file path="apps/web/src/utils/loggingMiddleware.ts">
import { createMiddleware } from '@tanstack/react-start'

const preLogMiddleware = createMiddleware()
  .client(async (ctx) => {
    const clientTime = new Date()

    return ctx.next({
      context: {
        clientTime,
      },
      sendContext: {
        clientTime,
      },
    })
  })
  .server(async (ctx) => {
    const serverTime = new Date()

    return ctx.next({
      sendContext: {
        serverTime,
        durationToServer:
          serverTime.getTime() - ctx.context.clientTime.getTime(),
      },
    })
  })

export const logMiddleware = createMiddleware()
  .middleware([preLogMiddleware])
  .client(async (ctx) => {
    const res = await ctx.next()

    const now = new Date()
    console.log('Client Req/Res:', {
      duration: res.context.clientTime.getTime() - now.getTime(),
      durationToServer: res.context.durationToServer,
      durationFromServer: now.getTime() - res.context.serverTime.getTime(),
    })

    return res
  })
</file>

<file path="apps/web/src/utils/seo.ts">
export const seo = ({
	title,
	description,
	keywords,
	image,
  }: {
	title: string
	description?: string
	image?: string
	keywords?: string
  }) => {
	const tags = [
	  { title },
	  { name: 'description', content: description },
	  { name: 'keywords', content: keywords },
	  { name: 'twitter:title', content: title },
	  { name: 'twitter:description', content: description },
	  { name: 'twitter:creator', content: '@tannerlinsley' },
	  { name: 'twitter:site', content: '@tannerlinsley' },
	  { name: 'og:type', content: 'website' },
	  { name: 'og:title', content: title },
	  { name: 'og:description', content: description },
	  ...(image
		? [
			{ name: 'twitter:image', content: image },
			{ name: 'twitter:card', content: 'summary_large_image' },
			{ name: 'og:image', content: image },
		  ]
		: []),
	]
  
	return tags
  }
</file>

<file path="apps/web/src/utils/supabase.ts">
import { createServerClient } from "@supabase/ssr";
import { parseCookies, setCookie } from "@tanstack/react-start/server";

export function getSupabaseServerClient() {
	return createServerClient(
		process.env.SUPABASE_URL!,
		process.env.SUPABASE_ANON_KEY!,
		{
			cookies: {
				// @ts-ignore Wait till Supabase overload works
				getAll() {
					return Object.entries(parseCookies()).map(([name, value]) => ({
						name,
						value,
					}));
				},
				setAll(cookies) {
					cookies.forEach((cookie) => {
						setCookie(cookie.name, cookie.value);
					});
				},
			},
		},
	);
}
</file>

<file path="apps/web/src/utils/users.tsx">
export type User = {
    id: number
    name: string
    email: string
  }
  
  export const DEPLOY_URL = 'http://localhost:3000'
</file>

<file path="apps/web/src/api.ts">
import {
	createStartAPIHandler,
	defaultAPIFileRouteHandler,
  } from '@tanstack/react-start/api'
  
  export default createStartAPIHandler(defaultAPIFileRouteHandler)
</file>

<file path="apps/web/src/client.tsx">
/// <reference types="vinxi/types/client" />
import { hydrateRoot } from 'react-dom/client'
import { StartClient } from '@tanstack/react-start'
import { createRouter } from './router'

const router = createRouter()

hydrateRoot(document, <StartClient router={router} />)
</file>

<file path="apps/web/src/global-middleware.ts">
import { registerGlobalMiddleware } from '@tanstack/react-start'
import { logMiddleware } from './utils/loggingMiddleware'

registerGlobalMiddleware({
  middleware: [logMiddleware],
})
</file>

<file path="apps/web/src/router.tsx">
import { createRouter as createTanStackRouter } from '@tanstack/react-router'
import { routeTree } from './routeTree.gen'
import { DefaultCatchBoundary } from '@/components/default-catch-boundary'
import { NotFound } from '@/components/not-found'

export function createRouter() {
  const router = createTanStackRouter({
    routeTree,
    defaultPreload: 'intent',
    defaultErrorComponent: DefaultCatchBoundary,
    defaultNotFoundComponent: () => <NotFound />,
    scrollRestoration: true,
  })

  return router
}

declare module '@tanstack/react-router' {
  interface Register {
    router: ReturnType<typeof createRouter>
  }
}
</file>

<file path="apps/web/src/ssr.tsx">
/// <reference types="vinxi/types/server" />
import {
	createStartHandler,
	defaultStreamHandler,
  } from "@tanstack/react-start/server";
  import { getRouterManifest } from "@tanstack/react-start/router-manifest";
  
  import { createRouter } from "./router";
  
  export default createStartHandler({
	createRouter,
	getRouterManifest,
  })(defaultStreamHandler);
</file>

<file path="apps/web/.env.example">
VITE_BASE_URL=http://localhost:3000
VITE_SUPABASE_URL="your_supabase_url_here"
VITE_SUPABASE_ANON_KEY="your_supabase_anon_key_here"
VITE_API_BASE_URL="http://localhost:8000/api/v1" # Or your production API base URL

DATABASE_URL="postgresql://user:password@localhost:5432/your_db_name"
# You can also use Docker Compose to set up a local PostgreSQL database:
# docker-compose up -d

# https://www.better-auth.com/docs/installation
BETTER_AUTH_SECRET="your_better_auth_secret"

# OAuth2 Providers, optional
GITHUB_CLIENT_ID="your_github_client_id"
GITHUB_CLIENT_SECRET="your_github_client_secret"
GOOGLE_CLIENT_ID="your_google_client_id"
GOOGLE_CLIENT_SECRET="your_google_client_secret"

# NOTE:
# In your OAuth2 apps, set callback/redirect URIs to`http://localhost:3000/api/auth/callback/<provider>`
# e.g. http://localhost:3000/api/auth/callback/github
</file>

<file path="apps/web/.gitattributes">
* text=auto eol=lf
</file>

<file path="apps/web/biome.json">
{
	"$schema": "https://biomejs.dev/schemas/1.9.4/schema.json",
	"vcs": {
		"enabled": false,
		"clientKind": "git",
		"useIgnoreFile": false
	},
	"files": {
		"ignoreUnknown": false,
		"ignore": []
	},
	"formatter": {
		"enabled": true,
		"indentStyle": "tab",
		"indentWidth": 2,
		"lineWidth": 80
	},
	"organizeImports": {
		"enabled": true
	},
	"linter": {
		"enabled": true,
		"rules": {
			"recommended": true,
			"a11y": {
				"useSemanticElements": "off"
			}
		}
	},
	"javascript": {
		"formatter": {
			"quoteStyle": "double",
			"trailingCommas": "all"
		}
	}
}
</file>

<file path="apps/web/components.json">
{
	"$schema": "https://ui.shadcn.com/schema.json",
	"style": "new-york",
	"rsc": false,
	"tsx": true,
	"tailwind": {
		"config": "tailwind.config.mjs",
		"css": "src/styles/app.css",
		"baseColor": "zinc",
		"cssVariables": true,
		"prefix": ""
	},
	"aliases": {
		"components": "@/components",
		"utils": "@/lib/utils",
		"ui": "@/components/ui",
		"lib": "@/lib",
		"hooks": "@/hooks"
	},
	"iconLibrary": "lucide"
}
</file>

<file path="apps/web/Dockerfile">
# Use Node.js as the base image
FROM node:20-slim

# Install pnpm
RUN npm install -g pnpm

# Set working directory
WORKDIR /app

# Copy package files
COPY package.json pnpm-lock.yaml ./

# Install dependencies
RUN pnpm install

# Copy the rest of the application
COPY . .

# Expose the default Vite port
EXPOSE 3000

# Command to run the development server
CMD ["pnpm", "dev", "--host", "0.0.0.0"]
</file>

<file path="apps/web/postcss.config.mjs">
export default {
	plugins: {
	  "@tailwindcss/postcss": {},
	},
  };
</file>

<file path="apps/web/README.md">
needs update
</file>

<file path="apps/web/vitest.config.ts">
import react from "@vitejs/plugin-react";
import { defineConfig } from "vitest/config";

export default defineConfig({
	plugins: [react()],
	test: {
		environment: "jsdom",
		globals: true,
		setupFiles: [],
		coverage: {
			reporter: ["text", "html"],
		},
		include: ["app/components/**/*.test.{ts,tsx}"],
	},
});
</file>

<file path="supabase/.temp/cli-latest">
v2.22.12
</file>

<file path="supabase/clients/client.ts">
import { createClient, SupabaseClient } from '@supabase/supabase-js';

// These variables are expected to be set in the environment of the consuming application (apps/web)
const supabaseUrl = import.meta.env.VITE_SUPABASE_URL;
const supabaseAnonKey = import.meta.env.VITE_SUPABASE_ANON_KEY;

if (!supabaseUrl || !supabaseAnonKey) {
    // In a shared package, throwing an error might be too aggressive during build.
    // Consider logging a warning or having a more robust config strategy if used in multiple apps with different envs.
    // For now, we assume apps/web will provide these.
    console.warn("Supabase URL or Anon Key is missing for shared client. Ensure .env is set up in the apps/web project.");
}

// Initialize with a check to prevent errors if env vars are missing during certain build/import scenarios
export const supabase: SupabaseClient = createClient(
    supabaseUrl || "", // Provide a fallback or handle more gracefully
    supabaseAnonKey || "" // Provide a fallback or handle more gracefully
);

// It's crucial that the consuming app (apps/web) ensures these ENV VARS are correctly populated.
</file>

<file path="supabase/clients/server.ts">
import { createServerClient } from "@supabase/ssr";
import { parseCookies, setCookie } from "@tanstack/react-start/server";

export function getSupabaseServerClient() {
	return createServerClient(
		process.env.SUPABASE_URL!,
		process.env.SUPABASE_ANON_KEY!,
		{
			cookies: {
				// @ts-ignore Wait till Supabase overload works
				getAll() {
					return Object.entries(parseCookies()).map(([name, value]) => ({
						name,
						value,
					}));
				},
				setAll(cookies) {
					cookies.forEach((cookie) => {
						setCookie(cookie.name, cookie.value);
					});
				},
			},
		},
	);
}
</file>

<file path="supabase/migrations/20250514044259_create_videos_table.sql">
-- Migration: Create videos table with RLS for Echo platform
-- Created: 2025-05-14T04:42:59Z UTC
-- Purpose: Introduces the core 'videos' table for user-uploaded video records, with row-level security policies for user isolation.

-- 1. Create the videos table
create table if not exists public.videos (
  id uuid primary key default gen_random_uuid(),
  user_id uuid references auth.users(id) on delete cascade,
  title text,
  description text,
  tags text[],
  subtitles text,
  thumbnail_gcs_path text,
  original_video_gcs_path text not null,
  processing_status text default 'pending',
  created_at timestamptz default timezone('utc'::text, now()),
  updated_at timestamptz default timezone('utc'::text, now())
);

-- 2. Trigger to auto-update updated_at
create or replace function public.update_updated_at_column()
returns trigger
language plpgsql
security invoker
set search_path = ''
as $$
begin
  new.updated_at = timezone('utc'::text, now());
  return new;
end;
$$;

drop trigger if exists update_videos_updated_at on public.videos;
create trigger update_videos_updated_at
before update on public.videos
for each row
execute function public.update_updated_at_column();

-- 3. Enable Row Level Security (RLS)
alter table public.videos enable row level security;

-- 4. RLS Policies

-- SELECT: Users can select their own videos
create policy "Users can select their own videos"
on public.videos
for select
to authenticated
using ((select auth.uid()) = user_id);

-- INSERT: Users can insert their own videos
create policy "Users can insert their own videos"
on public.videos
for insert
to authenticated
with check ((select auth.uid()) = user_id);

-- UPDATE: Users can update their own videos
create policy "Users can update their own videos"
on public.videos
for update
to authenticated
using ((select auth.uid()) = user_id)
with check ((select auth.uid()) = user_id);

-- DELETE: Users can delete their own videos
create policy "Users can delete their own videos"
on public.videos
for delete
to authenticated
using ((select auth.uid()) = user_id);

-- Index for user_id to optimize policy checks
create index if not exists idx_videos_user_id on public.videos(user_id);

-- End of migration
</file>

<file path="supabase/mutations/index.ts">
// This is where the mutations will when working with the db
</file>

<file path="supabase/supabase/.branches/_current_branch">
main
</file>

<file path="supabase/supabase/.temp/cli-latest">
v2.22.12
</file>

<file path="supabase/index.ts">
export * from "./clients/client";
export * from "./clients/server";
// export * from "./types/db";
</file>

<file path=".cursorignore">
# Add directories or file patterns to ignore during indexing (e.g. foo/ or *.csv)
</file>

<file path=".dockerignore">
# Git
.git
.gitignore

# Node
node_modules
.next
npm-debug.log*
yarn-error.log*
pnpm-debug.log*
.pnpm-store/
.env*
!.env.example
*.tsbuildinfo

# Python
venv/
.venv/
__pycache__/
*.py[cod]
*$py.class
.pytest_cache/
.mypy_cache/
.ruff_cache/

# IDE/OS specific
.vscode/
.idea/
*.swp
.DS_Store
</file>

<file path=".gcloudignore">
# This file specifies files that are *not* uploaded to Google Cloud
# using gcloud. It follows the same syntax as .gitignore, with the addition of
# "#!include" directives (which insert the entries of the given .gitignore-style
# file at that point).
#
# For more information, run:
#   $ gcloud topic gcloudignore
#
.gcloudignore
# If you would like to upload your .git directory, .gitignore file or files
# from your .gitignore file, remove the corresponding line
# below:
.git
.gitignore

node_modules
#!include:.gitignore
</file>

<file path=".python-version">
3.13.0
</file>

<file path=".ai_docs/prompts/new.md">
### Step 1:
- ask a million questions about codebase
- ask how to make it better 
- ask more questions on why to take that direction
- compare model outputs
- Take another **pass** at it, including relative paths, of existing functionality. include as many referneces to code as possible so it's not liek we're starting from scratch. print the existing file tree before and he after version of the file tree. Expected outputs: 
  - prd.md: Full requirements document
  - [file-structure-comparison.md](http://file-structure-comparison.md): Current vs. proposed structure
  - [implementation-tasks.md](http://implementation-tasks.md): Prioritized implementation tasks
  - examples/: Sample implementations of key components
- Now take everything we spoke about and ensure our @implementation-tasks.md are broken into atomic steps for a junior dev. Include the specific data from the original checklist such that nothing is conceptual, and this list can be given to a developer to implement for production. Do not write any code. If necessary though you can use pseudocode.

### Step 2: 
- Ask `what would be a good prompt be for telling an anthropic coding assistant on how to get started with this refactor`
- Open new chat 
- paste that in
- review answers
- `Nice - update our @implementation-tasks.md list. Then continue on the next highest leverage ones until its a good stoppping point`
- do this 2x 

**step 3 (new window)**

I need help continuing the refactoring of our video processing pipeline from a monolithic architecture to a clean/hexagonal architecture. We're following a detailed implementation plan and have completed several initial tasks.

Please review these key documents:
- ai_docs/implementation-tasks.md- Contains the step-by-step refactoring plan with completed tasks marked with 
- ai_docs/examples/- Contains code examples for reference
- ai_docs/overview.md- Provides architectural overview and design principles
- ai_docs/file-structure-comparison.md- Shows the before/after structure
- ai_docs/prd.md- Original requirements document
</file>

<file path=".ai_docs/prompts/prime.md">
# Context Prime
> Follow the instructions to understand the context of the project.

## Run the following command

eza . --tree --git-ignore

## Read the following files
> Read the files below and nothing else.
apps/web/
supabase/
</file>

<file path=".cursor/rules/memory_bank.mdc">
---
description: 
globs: 
alwaysApply: false
---
---
description: Describes Cline's Memory Bank system, its structure, and workflows for maintaining project knowledge across sessions.
author: https://github.com/nickbaumann98
version: 1.0
tags: ["memory-bank", "knowledge-base", "core-behavior", "documentation-protocol"]
globs: ["memory-bank/**/*.md", "*"]
---
# Cline's Memory Bank

I am Cline, an expert software engineer with a unique characteristic: my memory resets completely between sessions. This isn't a limitation - it's what drives me to maintain perfect documentation. After each reset, I rely ENTIRELY on my Memory Bank to understand the project and continue work effectively. I MUST read ALL memory bank files at the start of EVERY task - this is not optional.

## Memory Bank Structure

The Memory Bank consists of core files and optional context files, all in Markdown format. Files build upon each other in a clear hierarchy:

```mermaid
flowchart TD
    PB[projectbrief.md] --> PC[productContext.md]
    PB --> SP[systemPatterns.md]
    PB --> TC[techContext.md]
    
    PC --> AC[activeContext.md]
    SP --> AC
    TC --> AC
    
    AC --> P[tasks.md]
```

### Core Files (Required)
1. `projectbrief.md`
   - Foundation document that shapes all other files
   - Created at project start if it doesn't exist
   - Defines core requirements and goals
   - Source of truth for project scope

2. `productContext.md`
   - Why this project exists
   - Problems it solves
   - How it should work
   - User experience goals

3. `activeContext.md`
   - Current work focus
   - Recent changes
   - Next steps
   - Active decisions and considerations
   - Important patterns and preferences
   - Learnings and project insights

4. `systemPatterns.md`
   - System architecture
   - Key technical decisions
   - Design patterns in use
   - Component relationships
   - Critical implementation paths

5. `techContext.md`
   - Technologies used
   - Development setup
   - Technical constraints
   - Dependencies
   - Tool usage patterns

6. `tasks.md`
   - What works
   - What's left to build
   - Current status
   - Known issues
   - Evolution of project decisions

### Additional Context
Create additional files/folders within .ai_docs/ when they help organize:
- Complex feature documentation
- Integration specifications
- API documentation
- Testing strategies
- Deployment procedures

## Core Workflows

### Plan Mode
```mermaid
flowchart TD
    Start[Start] --> ReadFiles[Read Memory Bank]
    ReadFiles --> CheckFiles{Files Complete?}
    
    CheckFiles -->|No| Plan[Create Plan]
    Plan --> Document[Document in Chat]
    
    CheckFiles -->|Yes| Verify[Verify Context]
    Verify --> Strategy[Develop Strategy]
    Strategy --> Present[Present Approach]
```

### Act Mode
```mermaid
flowchart TD
    Start[Start] --> Context[Check Memory Bank]
    Context --> Update[Update Documentation]
    Update --> Execute[Execute Task]
    Execute --> Document[Document Changes]
```

## Documentation Updates

Memory Bank updates occur when:
1. Discovering new project patterns
2. After implementing significant changes
3. When user requests with **update memory bank** (MUST review ALL files)
4. When context needs clarification

```mermaid
flowchart TD
    Start[Update Process]
    
    subgraph Process
        P1[Review ALL Files]
        P2[Document Current State]
        P3[Clarify Next Steps]
        P4[Document Insights & Patterns]
        
        P1 --> P2 --> P3 --> P4
    end
    
    Start --> Process
```

Note: When triggered by **update memory bank**, I MUST review every memory bank file, even if some don't require updates. Focus particularly on activeContext.md and tasks.md as they track current state.

REMEMBER: After every memory reset, I begin completely fresh. The Memory Bank is my only link to previous work. It must be maintained with precision and clarity, as my effectiveness depends entirely on its accuracy.
</file>

<file path="apps/core/api/endpoints/jobs_endpoints.py">
from typing import List, Optional

from fastapi import APIRouter, Depends, Query
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import Session

from apps.core.api.schemas.video_processing_schemas import VideoJobSchema
from apps.core.lib.auth.supabase_auth import AuthenticatedUser, get_current_user
from apps.core.lib.database.connection import get_async_db_session, get_db_session
from apps.core.models.enums import ProcessingStatus
from apps.core.services.job_service import get_user_jobs_by_statuses

# Placeholder for the service function we will create in Task 3.X.2
# from apps.core.services.job_service import get_user_jobs_by_statuses

router = APIRouter()


@router.get(
    "/",
    response_model=List[VideoJobSchema],
    summary="Get User's Processing Jobs",
    description="Retrieve a list of video processing jobs for the authenticated user, filtered by status.",
)
async def get_my_processing_jobs(
    *,  # Enforces keyword-only arguments for clarity
    db: AsyncSession = Depends(get_async_db_session),
    current_user: AuthenticatedUser = Depends(get_current_user),
    status: Optional[List[ProcessingStatus]] = Query(
        default=None,  # Default to None, meaning all non-terminal if not specified by service
        description="Filter jobs by status (e.g., PENDING, PROCESSING). If not provided, by default PENDING and PROCESSING jobs are returned.",
        example=[ProcessingStatus.PENDING, ProcessingStatus.PROCESSING],
    ),
):
    """
    Retrieves video processing jobs for the current authenticated user.

    Allows filtering by one or more job statuses.
    If no statuses are provided, the service layer will return PENDING and PROCESSING jobs.
    The user_id passed to the service layer will be current_user.id (string UUID from Supabase).
    """
    # Call the service layer function.
    # Note: get_user_jobs_by_statuses is async, so we await it.
    # The underlying repository call is synchronous for now.
    jobs = await get_user_jobs_by_statuses(
        db=db,
        user_id=current_user.id,  # Pass the string ID from AuthenticatedUser
        statuses=status,
    )
    return jobs


# Future job-related endpoints will be added here
</file>

<file path="apps/core/api/endpoints/video_processing_endpoints.py">
"""
API endpoints for video processing functionality.

This module defines FastAPI routes for video upload, processing, and status retrieval.
It provides a clean RESTful interface for the video processing pipeline, handling
authentication, request validation, response formatting, and error handling.

The endpoints handle:
- Video file uploads with validation
- Asynchronous video processing via background tasks
- Status checking for ongoing and completed jobs
- Authorization to ensure users only access their own data

All business logic is delegated to the VideoProcessingService, with this module
focusing solely on HTTP concerns.
"""

from fastapi import APIRouter, BackgroundTasks, Depends, File, HTTPException, UploadFile
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import Session

from apps.core.api.schemas.video_processing_schemas import (
    VideoJobSchema,
    VideoUploadResponseSchema,
)
from apps.core.core.config import settings
from apps.core.lib.ai.ai_client_factory import get_ai_adapter
from apps.core.lib.auth.supabase_auth import AuthenticatedUser, get_current_user
from apps.core.lib.database.connection import get_async_db_session, get_db_session
from apps.core.lib.storage.file_storage import FileStorageService
from apps.core.lib.utils.ffmpeg_utils import FfmpegUtils
from apps.core.lib.utils.file_utils import FileUtils
from apps.core.lib.utils.subtitle_utils import SubtitleUtils
from apps.core.operations.video_job_repository import (
    VideoJobRepository,
    get_video_job_repository,
)
from apps.core.operations.video_metadata_repository import VideoMetadataRepository
from apps.core.operations.video_repository import VideoRepository, get_video_repository
from apps.core.services.video_processing_service import VideoProcessingService

router = APIRouter()


async def get_video_processing_service(
    # Repositories are now injected via their async getters
    video_repo: VideoRepository = Depends(get_video_repository),
    job_repo: VideoJobRepository = Depends(get_video_job_repository),
    # VideoMetadataRepository uses static methods, so no instance needed for them directly
    # However, VideoProcessingService expects an instance. We can pass a dummy or refactor service.
    # For now, let's assume VideoProcessingService can handle VideoMetadataRepository methods being called statically if needed
    # or we can instantiate it if it has a simple __init__.
    # VideoMetadataRepository has no __init__, its methods are static.
    # The service VideoProcessingService instantiates it as `metadata_repo: VideoMetadataRepository`. Let's keep that for now.
    # If VideoProcessingService directly calls VideoMetadataRepository.create_or_update (which it does), that's fine.
    storage_service: FileStorageService = Depends(
        FileStorageService
    ),  # Assuming FileStorageService can be Depended on directly or has a getter
    ai_adapter_instance=Depends(
        get_ai_adapter
    ),  # Renamed to avoid conflict, uses existing getter
    ffmpeg_utils_instance: FfmpegUtils = Depends(
        FfmpegUtils
    ),  # Assuming FfmpegUtils can be Depended on
    subtitle_utils_instance: SubtitleUtils = Depends(
        SubtitleUtils
    ),  # Assuming SubtitleUtils can be Depended on
    file_utils_instance: FileUtils = Depends(
        FileUtils
    ),  # Assuming FileUtils can be Depended on
) -> VideoProcessingService:
    # Dependency injection for the service and its dependencies
    return VideoProcessingService(
        video_repo=video_repo,  # Injected async repo
        job_repo=job_repo,  # Injected async repo
        metadata_repo=VideoMetadataRepository(),  # Instantiated as before (static methods)
        storage=storage_service,
        ai_adapter=ai_adapter_instance,
        ffmpeg_utils=ffmpeg_utils_instance,
        subtitle_utils=subtitle_utils_instance,
        file_utils=file_utils_instance,
    )


@router.post(
    "/upload",
    response_model=VideoUploadResponseSchema,
    summary="Upload a video and initiate processing",
)
async def upload_video(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    current_user: AuthenticatedUser = Depends(get_current_user),
    db: AsyncSession = Depends(get_async_db_session),
    service: VideoProcessingService = Depends(get_video_processing_service),
):
    """
    Upload a video file and initiate the processing pipeline.

    This endpoint allows users to upload a video file, which is then processed to:
    - Extract metadata (duration, resolution, format)
    - Generate a transcript
    - Create AI-generated title, description, and tags
    - Generate subtitle files (VTT and SRT formats)
    - Extract a thumbnail image

    The processing occurs asynchronously in the background, and clients can
    check the processing status using the returned job_id.

    Returns:
        VideoUploadResponseSchema: Contains the job_id and initial PENDING status

    Raises:
        400: If the uploaded file is not a video or is missing metadata
        401: If the user is not authenticated
    """
    if not file.content_type or not file.content_type.startswith("video/"):
        raise HTTPException(status_code=400, detail="File must be a video")
    if not file.filename or not file.content_type:
        raise HTTPException(status_code=400, detail="Missing file metadata")
    video_content = await file.read()
    job = await service.initiate_video_processing(
        db=db,
        original_filename=file.filename,
        video_content=video_content,
        content_type=file.content_type,
        uploader_user_id=current_user.id,
        background_tasks=background_tasks,
    )
    return VideoUploadResponseSchema(job_id=job.id, status=job.status)


@router.get(
    "/jobs/{job_id}",
    response_model=VideoJobSchema,
    summary="Get video processing job details",
)
async def get_job_details(
    job_id: int,
    current_user: AuthenticatedUser = Depends(get_current_user),
    db: AsyncSession = Depends(get_async_db_session),
    service: VideoProcessingService = Depends(get_video_processing_service),
):
    """
    Retrieve details and status of a video processing job.

    This endpoint allows users to check the status of a video processing job and retrieve
    all associated metadata. It includes authorization checks to ensure users can only
    access their own video processing jobs.

    The response includes:
    - Job status (PENDING, PROCESSING, COMPLETED, FAILED)
    - Processing stages information
    - Error messages (if any)
    - Video metadata (title, description, tags, etc.)
    - Generated asset URLs (transcript, subtitles, thumbnail)
    - Technical metadata (duration, resolution, format)

    Path Parameters:
        job_id (int): The ID of the video processing job to retrieve

    Returns:
        VideoJobSchema: Complete job information with nested video and metadata

    Raises:
        401: If the user is not authenticated
        403/404: If the job doesn't exist or belongs to another user
    """
    job = await service.get_job_details(db=db, job_id=job_id, user_id=current_user.id)
    return job
</file>

<file path="apps/core/api/schemas/video_processing_schemas.py">
"""
Pydantic schemas for video processing API requests and responses.

This module defines the data models used for API input validation and response
serialization in the video processing API endpoints. These schemas provide a
contract between the backend services and API clients, ensuring type safety
and proper documentation.

The schemas are designed to map cleanly to the database models while providing
appropriate defaults, validation rules, and documentation for the API layer.
"""

from datetime import datetime
from typing import Any, Dict, List, Optional, Union

from pydantic import BaseModel, ConfigDict, Field

from apps.core.models.enums import ProcessingStatus

# --- API Client Specific Schemas ---


class ApiErrorDetail(BaseModel):
    """Individual error detail, often part of a list in ApiErrorResponse."""

    loc: Optional[List[Union[str, int]]] = Field(
        default=None, description="Location of the error (e.g., field path)"
    )
    msg: str = Field(..., description="Error message")
    type: str = Field(..., description="Type of error (e.g., 'value_error')")
    ctx: Optional[Dict[str, Any]] = Field(
        default=None, description="Additional context for the error"
    )


class ApiErrorResponse(BaseModel):
    """Standard error response structure for API errors."""

    detail: Union[str, List[ApiErrorDetail]] = Field(
        ..., description="Error message or list of error details"
    )


class SignedUploadUrlRequest(BaseModel):
    """Request schema for obtaining a signed URL for video upload."""

    filename: str = Field(..., description="Original filename of the video.")
    contentType: str = Field(
        ..., alias="content_type", description="MIME type of the video file."
    )
    # Using alias for contentType to match frontend usage, backend might prefer content_type

    model_config = ConfigDict(populate_by_name=True)


class SignedUploadUrlResponse(BaseModel):
    """Response schema after requesting a signed URL."""

    uploadUrl: str = Field(
        ..., alias="upload_url", description="The GCS signed URL for direct PUT upload."
    )
    videoId: str = Field(
        ...,
        alias="video_id",
        description="The unique ID assigned to this video upload attempt/record.",
    )
    # Assuming videoId is a string identifier generated for the upload process,
    # which might later be associated with an integer DB ID.

    model_config = ConfigDict(populate_by_name=True)


class UploadCompleteRequest(BaseModel):
    """Request schema to notify the backend that a direct upload is complete."""

    videoId: str = Field(
        ..., alias="video_id", description="The unique ID of the video upload."
    )
    originalFilename: str = Field(
        ...,
        alias="original_filename",
        description="Original filename of the uploaded video.",
    )
    contentType: str = Field(
        ..., alias="content_type", description="MIME type of the video file."
    )
    sizeBytes: int = Field(
        ..., alias="size_bytes", description="Size of the video file in bytes."
    )
    storagePath: Optional[str] = Field(
        default=None,
        alias="storage_path",
        description="Canonical path in GCS if known by uploader; backend may infer.",
    )
    # Aliases to match frontend camelCase, Python typically uses snake_case.

    model_config = ConfigDict(populate_by_name=True)


class VideoSummary(BaseModel):
    """Summarized video information, typically for lists."""

    id: int = Field(..., description="Unique identifier for the video.")
    original_filename: str = Field(..., description="Original filename of the video.")
    title: Optional[str] = Field(default=None, description="Title of the video.")
    created_at: Optional[datetime] = Field(
        default=None, description="Timestamp of video creation."
    )
    status: Optional[ProcessingStatus] = Field(
        default=None, description="Current processing status of the video."
    )
    thumbnail_file_url: Optional[str] = Field(
        default=None, description="URL to the video's thumbnail."
    )
    # Add other fields like duration, title if available and desired in summary

    model_config = ConfigDict(from_attributes=True)


class VideoMetadataUpdateRequest(BaseModel):
    """Request schema for updating video metadata."""

    title: Optional[str] = Field(default=None, description="New title for the video.")
    description: Optional[str] = Field(
        default=None, description="New description for the video."
    )
    tags: Optional[List[str]] = Field(
        default=None, description="New list of tags for the video."
    )
    # Add other editable metadata fields from VideoMetadataSchema as needed

    model_config = ConfigDict(extra="forbid")  # Prevent unspecified fields


# --- Existing Schemas (ensure they are compatible) ---


class VideoUploadResponseSchema(BaseModel):
    """
    Response schema for the video upload endpoint. (Existing)
    NOTE: This might be for when upload starts processing, not the signed URL itself.
    If getSignedUploadUrl returns SignedUploadUrlResponse, this might be for a different step.
    For now, keeping as is, assuming it's used by an endpoint.
    """

    job_id: int = Field(..., description="The ID of the created video processing job.")
    status: ProcessingStatus = Field(..., description="The initial status of the job.")


class VideoSchema(BaseModel):
    """Schema representing a video file in the system. (Existing)"""

    id: int
    uploader_user_id: str
    original_filename: str
    storage_path: str
    content_type: str
    size_bytes: int
    created_at: Optional[datetime] = None
    updated_at: Optional[datetime] = None

    model_config = ConfigDict(from_attributes=True)


class VideoMetadataSchema(BaseModel):
    """Schema representing metadata extracted from a processed video. (Existing)"""

    id: Optional[int] = None
    job_id: Optional[int] = None
    title: Optional[str] = None
    description: Optional[str] = None
    tags: Optional[List[str]] = Field(default_factory=list)
    transcript_text: Optional[str] = None
    transcript_file_url: Optional[str] = None
    subtitle_files_urls: Optional[Dict[str, Any]] = Field(default_factory=dict)
    thumbnail_file_url: Optional[str] = None
    extracted_video_duration_seconds: Optional[float] = None
    extracted_video_resolution: Optional[str] = None
    extracted_video_format: Optional[str] = None
    show_notes_text: Optional[str] = None
    created_at: Optional[datetime] = None
    updated_at: Optional[datetime] = None

    model_config = ConfigDict(from_attributes=True)


class VideoJobSchema(BaseModel):
    """Schema representing a video processing job. (Existing)"""

    id: int
    video_id: int
    status: ProcessingStatus
    processing_stages: Optional[Union[List[str], Dict[str, Any]]] = None
    error_message: Optional[str] = None
    created_at: Optional[datetime] = None
    updated_at: Optional[datetime] = None
    video: Optional[VideoSchema] = None
    metadata: Optional[VideoMetadataSchema] = None

    model_config = ConfigDict(from_attributes=True, extra="ignore")


class VideoDetailsResponse(
    BaseModel
):  # Renamed from VideoJobSchema for clarity if it's the primary response for video details
    """
    Comprehensive details for a specific video, including its job and metadata.
    This often mirrors VideoJobSchema if that schema is the primary source of truth.
    Alternatively, it can be a composition of VideoSchema, VideoMetadataSchema, and job details.
    Let's make it closely related to VideoJobSchema for now.
    """

    id: int = Field(
        ...,
        description="Video Job ID. If this is for Video Details, this might be Video ID with Job details nested or vice-versa",
    )  # Clarify if this ID is Video or Job
    video_id: int = Field(..., description="Associated Video ID")
    uploader_user_id: Optional[str] = Field(
        None, description="ID of the user who uploaded the video. (Derived from video)"
    )
    original_filename: Optional[str] = Field(
        None, description="Original filename from the upload. (Derived from video)"
    )

    status: ProcessingStatus = Field(
        ..., description="Current status of the processing job."
    )
    processing_stages: Optional[Union[List[str], Dict[str, Any]]] = Field(
        default=None, description="Progress information."
    )
    error_message: Optional[str] = Field(
        default=None, description="Error details if the job failed."
    )

    created_at: Optional[datetime] = Field(
        default=None, description="When the job (or video) was created."
    )
    updated_at: Optional[datetime] = Field(
        default=None, description="When the job (or video) was last updated."
    )

    # Nested video and metadata details
    video: Optional[VideoSchema] = Field(
        default=None, description="Associated video details."
    )
    metadata: Optional[VideoMetadataSchema] = Field(
        default=None, description="Associated metadata details."
    )

    model_config = ConfigDict(from_attributes=True, extra="ignore")

    # If VideoDetailsResponse is intended to be populated from a VideoJobModel that has
    # related VideoModel and VideoMetadataModel, `from_attributes=True` helps.
    # The direct fields like uploader_user_id and original_filename can be populated if
    # the ORM query for VideoJobModel also loads related VideoModel and these are exposed.
    # Otherwise, they might be better inside the nested `video: VideoSchema`.
    # For now, keeping them potentially at top level for flexibility in how backend serves it.


# Ensure all schemas intended for generation are defined above this line or imported.
# ProcessingStatus is an enum and should be handled correctly by pydantic-to-typescript.
</file>

<file path="apps/core/bin/dev.sh">
#!/bin/bash

# Get the app directory
APP_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
cd "$APP_DIR"

# Determine the project root directory (which contains the 'apps' folder)
PROJECT_ROOT_DIR="$(cd "$APP_DIR/../.." && pwd)"

# Set PYTHONPATH to include the project root directory
export PYTHONPATH=$PROJECT_ROOT_DIR

# Run the application with Python directly
python main.py
</file>

<file path="apps/core/lib/database/connection.py">
from typing import AsyncGenerator, Generator

from sqlalchemy import create_engine
from sqlalchemy.ext.asyncio import AsyncSession, async_sessionmaker, create_async_engine
from sqlalchemy.orm import Session, declarative_base, sessionmaker

from core.config import settings

# Create SQLAlchemy engine
# Add connect_args for SQLite compatibility
connect_args = {}
if settings.DATABASE_URL.startswith("sqlite"):
    connect_args = {"check_same_thread": False}

engine = create_engine(settings.DATABASE_URL, connect_args=connect_args)


# Async SQLAlchemy engine
async_db_url = settings.DATABASE_URL
if settings.DATABASE_URL.startswith("postgresql://"):
    async_db_url = settings.DATABASE_URL.replace(
        "postgresql://", "postgresql+asyncpg://"
    )
elif settings.DATABASE_URL.startswith(
    "sqlite:///"
):  # Assuming 'sqlite:////path/to/db.sqlite'
    async_db_url = settings.DATABASE_URL.replace("sqlite:///", "sqlite+aiosqlite:///")
elif settings.DATABASE_URL.startswith(
    "sqlite://"
):  # Assuming relative path 'sqlite://./db.sqlite'
    async_db_url = settings.DATABASE_URL.replace("sqlite://", "sqlite+aiosqlite://")


async_engine = create_async_engine(async_db_url)

# SessionLocal class
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

# Async SessionLocal class
AsyncSessionLocal = async_sessionmaker(
    bind=async_engine,
    class_=AsyncSession,
    expire_on_commit=False,
    autocommit=False,  # Explicitly set for clarity, though default for AsyncSession
    autoflush=False,  # Explicitly set for clarity, though default for AsyncSession
)

# Base class for models
Base = declarative_base()


def get_db_session() -> Generator[Session, None, None]:
    """
    Dependency function to get DB session
    Usage: db: Session = Depends(get_db_session)
    """
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()


async def get_async_db_session() -> AsyncGenerator[AsyncSession, None]:
    """
    Dependency function to get an async DB session.
    Usage: db: AsyncSession = Depends(get_async_db_session)
    """
    # The type checker might complain here, but this is the standard way
    # to create an async session with SQLAlchemy.
    async_session_local_instance = AsyncSessionLocal()

    async with async_session_local_instance as session:
        try:
            yield session
        except Exception:
            await session.rollback()
            raise
        finally:
            # Closing the session is handled by the async context manager
            pass


def create_session() -> Session:
    """Create and return a new session"""
    return SessionLocal()


# It's good practice to also have an async version if needed outside FastAPI Depends
async def create_async_session() -> AsyncSession:
    """Create and return a new async session"""
    # The type checker might complain here as well.
    return AsyncSessionLocal()
</file>

<file path="apps/core/operations/chat_repository.py">
from typing import Any, Dict, List, Optional
from uuid import UUID

from fastapi import Depends

# from lib.database import create_session # Will be replaced
from sqlalchemy import delete, desc, select, update  # Added select, delete, update
from sqlalchemy.ext.asyncio import AsyncSession  # Added AsyncSession
from sqlalchemy.orm import Session

from apps.core.lib.database.connection import (
    get_async_db_session,  # Import async session getter
)
from apps.core.models.chat_model import Chat, Message


class ChatRepository:
    def __init__(self, db: AsyncSession):  # Changed to AsyncSession
        self.db = db

    # Chat operations
    async def get_chat(self, chat_id: UUID) -> Optional[Chat]:  # async def
        result = await self.db.execute(select(Chat).filter(Chat.id == chat_id))
        return result.scalars().first()

    async def get_chats(
        self, skip: int = 0, limit: int = 20
    ) -> List[Chat]:  # async def
        result = await self.db.execute(
            select(Chat).order_by(desc(Chat.updated_at)).offset(skip).limit(limit)
        )
        return list(result.scalars().all())

    async def create_chat(self, chat_data: Dict[str, Any]) -> Chat:  # async def
        chat = Chat(**chat_data)
        self.db.add(chat)
        await self.db.commit()  # await commit
        await self.db.refresh(chat)  # await refresh
        return chat

    async def update_chat(
        self, chat_id: UUID, chat_data: Dict[str, Any]
    ) -> Optional[Chat]:  # async def
        # chat = await self.get_chat(chat_id) # Fetch first approach
        # if chat:
        #     for key, value in chat_data.items():
        #         setattr(chat, key, value)
        #     await self.db.commit()
        #     await self.db.refresh(chat)
        # return chat
        # Direct update approach (more efficient for partial updates)
        stmt = update(Chat).where(Chat.id == chat_id).values(**chat_data)
        result = await self.db.execute(stmt)
        if result.rowcount == 0:
            return None
        await self.db.commit()
        return await self.get_chat(
            chat_id
        )  # Re-fetch to get updated model with all fields

    async def delete_chat(self, chat_id: UUID) -> bool:  # async def
        stmt = delete(Chat).where(Chat.id == chat_id)
        result = await self.db.execute(stmt)
        await self.db.commit()
        return result.rowcount > 0

    # Message operations
    async def get_message(self, message_id: UUID) -> Optional[Message]:  # async def
        result = await self.db.execute(select(Message).filter(Message.id == message_id))
        return result.scalars().first()

    async def get_messages_by_chat(  # async def
        self, chat_id: UUID, skip: int = 0, limit: int = 50
    ) -> List[Message]:
        result = await self.db.execute(
            select(Message)
            .filter(Message.chat_id == chat_id)
            .order_by(Message.created_at)
            .offset(skip)
            .limit(limit)
        )
        return list(result.scalars().all())

    async def create_message(
        self, message_data: Dict[str, Any]
    ) -> Message:  # async def
        message = Message(**message_data)
        self.db.add(message)

        # Update the chat's updated_at timestamp
        # The comment "The updated_at will be automatically updated due to onupdate" implies
        # that the database might handle this. If not, an explicit update might be needed.
        # Forcing an update via ORM could be:
        chat = await self.get_chat(message_data["chat_id"])
        if chat:
            # If Chat model has a listener or if there's a DB trigger for updated_at,
            # simply adding the message and committing might be enough.
            # If we need to explicitly mark chat as dirty for ORM to update `updated_at` (if it's ORM-managed):
            # from sqlalchemy.orm.attributes import flag_modified
            # chat.some_field_to_trigger_update = chat.some_field_to_trigger_update # or flag_modified(chat, "updated_at") if it is not autogenerated by DB
            # self.db.add(chat) # Ensure chat is part of the session if it needs an update.
            pass  # Assuming DB onupdate handles Chat.updated_at, or ORM handles it on relationship changes.

        await self.db.commit()  # await commit
        await self.db.refresh(message)  # await refresh
        return message

    async def delete_message(self, message_id: UUID) -> bool:  # async def
        stmt = delete(Message).where(Message.id == message_id)
        result = await self.db.execute(stmt)
        await self.db.commit()
        return result.rowcount > 0


async def get_chat_repository(  # async def
    db: AsyncSession = Depends(get_async_db_session),  # Use get_async_db_session
) -> ChatRepository:
    return ChatRepository(db)
</file>

<file path="apps/core/operations/transaction_repo.py">
from typing import Any, Awaitable, Callable

from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import Session


class TransactionRepository:
    def __init__(self, db: AsyncSession):
        self.db = db

    async def run_in_transaction(
        self, callback: Callable[[AsyncSession], Awaitable[Any]]
    ) -> Any:
        """
        Execute an async function within a database transaction.

        Args:
            callback: An async function that takes the async session as parameter and returns a result

        Returns:
            The result of the callback function
        """
        try:
            result = await callback(self.db)
            await self.db.commit()
            return result
        except Exception as e:
            await self.db.rollback()
            raise e
</file>

<file path="apps/core/operations/user_repository.py">
from typing import List, Optional

from fastapi import Depends
from sqlalchemy import delete, select, update  # Added select, delete, update
from sqlalchemy.ext.asyncio import AsyncSession  # Added AsyncSession

# from lib.database import create_session # Will be replaced by async version
from sqlalchemy.orm import Session

from apps.core.lib.database.connection import (
    get_async_db_session,  # Import async session getter
)
from apps.core.models.user_model import User


class UserRepository:
    def __init__(self, db: AsyncSession):  # Changed to AsyncSession
        self.db = db

    async def get_user(self, user_id: int) -> Optional[User]:  # async def
        result = await self.db.execute(select(User).filter(User.id == user_id))
        return result.scalars().first()

    async def get_user_by_email(self, email: str) -> Optional[User]:  # async def
        result = await self.db.execute(select(User).filter(User.email == email))
        return result.scalars().first()

    async def get_user_by_username(self, username: str) -> Optional[User]:  # async def
        result = await self.db.execute(select(User).filter(User.username == username))
        return result.scalars().first()

    async def get_users(
        self, skip: int = 0, limit: int = 100
    ) -> List[User]:  # async def
        result = await self.db.execute(select(User).offset(skip).limit(limit))
        return list(result.scalars().all())

    async def create_user(self, user_data: dict) -> User:  # async def
        user = User(**user_data)
        self.db.add(user)
        await self.db.commit()  # await commit
        await self.db.refresh(user)  # await refresh
        return user

    async def update_user(
        self, user_id: int, user_data: dict
    ) -> Optional[User]:  # async def
        # We need to be careful here. self.get_user is now async.
        # SQLAlchemy 2.0 style update is preferred for partial updates.
        # However, to maintain similar logic of fetch then update for now:
        user = await self.get_user(user_id)
        if user:
            for key, value in user_data.items():
                setattr(user, key, value)
            await self.db.commit()  # await commit
            await self.db.refresh(user)  # await refresh
        return user
        # Alternative using SQLAlchemy update:
        # stmt = update(User).where(User.id == user_id).values(**user_data)
        # result = await self.db.execute(stmt)
        # if result.rowcount == 0:
        #     return None
        # await self.db.commit()
        # return await self.get_user(user_id) # Re-fetch to get updated model

    async def delete_user(self, user_id: int) -> bool:  # async def
        # SQLAlchemy 2.0 style delete is preferred.
        stmt = delete(User).where(User.id == user_id)
        result = await self.db.execute(stmt)
        await self.db.commit()  # await commit
        return result.rowcount > 0
        # Old logic:
        # user = await self.get_user(user_id)
        # if user:
        #     await self.db.delete(user) # await delete
        #     await self.db.commit()
        #     return True
        # return False


async def get_user_repository(  # async def
    db: AsyncSession = Depends(get_async_db_session),  # Use get_async_db_session
) -> UserRepository:
    return UserRepository(db)
</file>

<file path="apps/core/operations/video_metadata_repository.py">
"""
video_metadata_repository.py: Repository for VideoMetadataModel data access.

- Provides methods for creating/updating and retrieving VideoMetadataModel instances.
- Accepts SQLAlchemy Session as the first argument.
- No business logic; data access only.

Directory: apps/core/operations/video_metadata_repository.py
Layer: Operations
"""

from typing import Any, Optional

from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import Session

from apps.core.models.video_metadata_model import VideoMetadataModel


class VideoMetadataRepository:
    """
    Repository for VideoMetadataModel data access.
    """

    @staticmethod
    async def create_or_update(
        db: AsyncSession, job_id: int, **kwargs: Any
    ) -> VideoMetadataModel:
        """
        Create or update a VideoMetadataModel for a given job_id.

        Args:
            db (AsyncSession): SQLAlchemy async session.
            job_id (int): Associated job ID.
            **kwargs: Fields to update or set.

        Returns:
            VideoMetadataModel: The created or updated metadata model.
        """
        result = await db.execute(
            select(VideoMetadataModel).filter(VideoMetadataModel.job_id == job_id)
        )
        metadata = result.scalars().first()

        if metadata is None:
            metadata = VideoMetadataModel(job_id=job_id, **kwargs)
            db.add(metadata)
        else:
            for key, value in kwargs.items():
                setattr(metadata, key, value)
        await db.flush()
        return metadata

    @staticmethod
    async def get_by_job_id(
        db: AsyncSession, job_id: int
    ) -> Optional[VideoMetadataModel]:
        """
        Retrieve a VideoMetadataModel by its job_id.

        Args:
            db (AsyncSession): SQLAlchemy async session.
            job_id (int): Job ID.

        Returns:
            Optional[VideoMetadataModel]: The metadata model, or None if not found.
        """
        result = await db.execute(
            select(VideoMetadataModel).filter(VideoMetadataModel.job_id == job_id)
        )
        return result.scalars().first()
</file>

<file path="apps/core/services/ai_service.py">
import asyncio
import json
from typing import Any, AsyncGenerator, Dict, List, cast
from uuid import UUID

from fastapi import Depends

# Import and configure OpenAI
from openai import AsyncOpenAI
from openai.types.chat import ChatCompletionMessageParam
from operations.chat_repository import ChatRepository, get_chat_repository

from apps.core.core.config import settings

# NOTE: OpenAI client is now instantiated inside methods for easier testing/mocking.


class AIService:
    def __init__(self, chat_repository: ChatRepository):
        self.chat_repository = chat_repository

    async def generate_ai_response(
        self, message_content: str, chat_id: UUID
    ) -> Dict[str, Any]:
        """
        Generate AI response using OpenAI API
        """
        # Get chat history for context
        chat_history = await self.get_chat_history(chat_id)

        # Add the current message to history
        chat_history.append({"role": "user", "content": message_content})

        # Call OpenAI API to get a response
        client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
        response = await client.chat.completions.create(
            model="o4-mini",
            messages=cast(List[ChatCompletionMessageParam], chat_history),
        )

        ai_response = response.choices[0].message.content

        # Create and store the AI response message
        message_data = {
            "content": ai_response,
            "is_from_ai": True,
            "chat_id": chat_id,
        }

        ai_message = await self.chat_repository.create_message(message_data)

        return {
            "id": ai_message.id,
            "content": ai_message.content,
            "is_from_ai": bool(ai_message.is_from_ai),
            "created_at": ai_message.created_at,
            "chat_id": ai_message.chat_id,
        }

    async def get_chat_history(self, chat_id: UUID) -> List[Dict[str, Any]]:
        """Get the chat history in a format usable for AI context"""
        messages = await self.chat_repository.get_messages_by_chat(chat_id)
        result = []

        for msg in messages:
            # Create a new Python dictionary instead of modifying one
            if hasattr(msg, "is_from_ai"):
                source_value = getattr(msg, "is_from_ai")
                if source_value == 1:
                    role = "assistant"
                else:
                    role = "user"
            else:
                role = "user"

            # Create new dict with all values at once
            message_dict = {"role": role, "content": msg.content}

            result.append(message_dict)

        return result

    async def stream_ai_response(
        self, message_content: str, chat_id: UUID, protocol: str = "data"
    ) -> AsyncGenerator[str, None]:
        """
        Stream an AI response according to the Vercel AI protocol
        """
        # Get chat history for context
        chat_history = await self.get_chat_history(chat_id)

        # Add the current message to history
        chat_history.append({"role": "user", "content": message_content})

        # Save the user message to the database
        user_message_data = {
            "content": message_content,
            "is_from_ai": False,
            "chat_id": chat_id,
        }
        await self.chat_repository.create_message(user_message_data)

        # Use OpenAI streaming API
        complete_response = ""

        # Create a streaming response from OpenAI
        client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
        stream = await client.chat.completions.create(
            model="o4-mini",
            messages=cast(List[ChatCompletionMessageParam], chat_history),
            stream=True,
        )

        if protocol == "text":
            async for chunk in stream:
                if chunk.choices and chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    complete_response += content
                    yield content

        elif protocol == "data":
            async for chunk in stream:
                if chunk.choices and chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    complete_response += content
                    yield f"0:{json.dumps(content)}\n"

            # Send the completion message
            yield f'd:{{"finishReason":"stop","usage":{{"promptTokens":0,"completionTokens":{len(complete_response)}}}}}\n'

        # Save the complete AI response to the database
        ai_message_data = {
            "content": complete_response,
            "is_from_ai": True,
            "chat_id": chat_id,
        }
        await self.chat_repository.create_message(ai_message_data)

    async def convert_to_openai_messages(self, client_messages: List[Dict[str, Any]]):
        """
        Convert client messages to OpenAI format
        In a real implementation, you would handle attachments and tool invocations
        """
        openai_messages = []

        for message in client_messages:
            # Basic conversion for text content
            openai_messages.append(
                {
                    "role": message.get("role", "user"),
                    "content": message.get("content", ""),
                }
            )

        return openai_messages

    async def process_chat_stream(
        self,
        client_messages: List[Dict[str, Any]],
        chat_id: UUID,
        protocol: str = "data",
    ) -> AsyncGenerator[str, None]:
        """
        Process a complete chat interaction with streaming response
        This matches the example's pattern for handling chat requests
        """
        # In a real app, you would process the full message history
        # For this dummy implementation, we'll just use the last message from the user
        if client_messages and client_messages[-1]["role"] == "user":
            last_message = client_messages[-1]
            message_content = last_message.get("content", "")

            # Get a streaming response
            async for chunk in self.stream_ai_response(
                message_content, chat_id, protocol
            ):
                yield chunk
        else:
            # No valid messages found
            yield "No valid user messages found"


async def get_ai_service(
    chat_repository: ChatRepository = Depends(get_chat_repository),
) -> AIService:
    return AIService(chat_repository)
</file>

<file path="apps/core/services/auth_service.py">
import hashlib
from typing import Any, Dict, Optional

from fastapi import Depends, HTTPException, status

# Corrected import paths
from apps.core.operations.user_repository import UserRepository, get_user_repository


class AuthService:
    def __init__(self, user_repository: UserRepository):
        self.user_repository = user_repository

    async def authenticate_user(
        self, username: str, password: str
    ) -> Optional[Dict[str, Any]]:
        # Find the user
        user = await self.user_repository.get_user_by_username(username)
        if not user:
            return None

        # Validate password (in real app, use proper password hashing)
        hashed_password = hashlib.sha256(password.encode()).hexdigest()
        if str(user.hashed_password) != hashed_password:
            return None

        # Return user without sensitive information
        return {
            "id": user.id,
            "username": user.username,
            "email": user.email,
            "full_name": user.full_name,
            "is_active": user.is_active,
        }

    async def register_user(self, user_data: Dict[str, Any]) -> Dict[str, Any]:
        # Check if username or email already exists
        email = user_data.get("email")
        if not email:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Email is required",
            )
        if await self.user_repository.get_user_by_email(email):
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Email already registered",
            )

        username = user_data.get("username")
        if not username:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Username is required",
            )
        if await self.user_repository.get_user_by_username(username):
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Username already taken",
            )

        # Hash password (in real app, use proper password hashing)
        password = user_data.pop("password", "")
        user_data["hashed_password"] = hashlib.sha256(password.encode()).hexdigest()

        # Create user
        user = await self.user_repository.create_user(user_data)

        # Return user without sensitive information
        return {
            "id": user.id,
            "username": user.username,
            "email": user.email,
            "full_name": user.full_name,
            "is_active": user.is_active,
        }


async def get_auth_service(
    user_repository: UserRepository = Depends(get_user_repository),
) -> AuthService:
    return AuthService(user_repository)
</file>

<file path="apps/core/services/chat_service.py">
from typing import Any, Dict, List
from uuid import UUID

from fastapi import Depends, HTTPException, status

# Corrected import path
from apps.core.operations.chat_repository import ChatRepository, get_chat_repository


class ChatService:
    def __init__(self, chat_repository: ChatRepository):
        self.chat_repository = chat_repository

    # Chat operations
    async def get_chats(self, skip: int = 0, limit: int = 20) -> List[Dict[str, Any]]:
        chats = await self.chat_repository.get_chats(skip=skip, limit=limit)
        return [
            {
                "id": chat.id,
                "title": chat.title
                if chat.title is not None
                else f"Chat {chat.id}",  # Default title if none is provided
                "created_at": chat.created_at,
                "updated_at": chat.updated_at,
            }
            for chat in chats
        ]

    async def get_chat(self, chat_id: UUID) -> Dict[str, Any]:
        chat = await self.chat_repository.get_chat(chat_id)
        if not chat:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND, detail="Chat not found"
            )

        # Get messages for this chat
        messages = await self.chat_repository.get_messages_by_chat(chat_id)

        return {
            "id": chat.id,
            "title": chat.title if chat.title is not None else f"Chat {chat.id}",
            "created_at": chat.created_at,
            "updated_at": chat.updated_at,
            "messages": [
                {
                    "id": message.id,
                    "content": message.content,
                    "is_from_ai": bool(message.is_from_ai),
                    "created_at": message.created_at,
                    "chat_id": message.chat_id,
                }
                for message in messages
            ],
        }

    async def create_chat(self, chat_data: Dict[str, Any]) -> Dict[str, Any]:
        # Create new chat
        chat = await self.chat_repository.create_chat(chat_data)

        return {
            "id": chat.id,
            "title": chat.title if chat.title is not None else f"Chat {chat.id}",
            "created_at": chat.created_at,
            "updated_at": chat.updated_at,
            "messages": [],
        }

    async def update_chat(
        self, chat_id: UUID, chat_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        # Check if chat exists
        chat = await self.chat_repository.get_chat(chat_id)
        if not chat:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND, detail="Chat not found"
            )

        # Update chat
        updated_chat = await self.chat_repository.update_chat(chat_id, chat_data)
        if not updated_chat:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Failed to update chat",
            )

        return {
            "id": updated_chat.id,
            "title": updated_chat.title
            if updated_chat.title is not None
            else f"Chat {updated_chat.id}",
            "created_at": updated_chat.created_at,
            "updated_at": updated_chat.updated_at,
        }

    async def delete_chat(self, chat_id: UUID) -> Dict[str, Any]:
        # Check if chat exists
        chat = await self.chat_repository.get_chat(chat_id)
        if not chat:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND, detail="Chat not found"
            )

        # Delete chat
        await self.chat_repository.delete_chat(chat_id)

        return {"success": True, "message": "Chat deleted successfully"}

    # Message operations
    async def create_message(self, message_data: Dict[str, Any]) -> Dict[str, Any]:
        # Check if chat exists
        chat_id = message_data.get("chat_id")
        if not chat_id:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Chat ID is required",
            )

        chat = await self.chat_repository.get_chat(chat_id)
        if not chat:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND, detail="Chat not found"
            )

        # Create message
        message = await self.chat_repository.create_message(message_data)

        return {
            "id": message.id,
            "content": message.content,
            "is_from_ai": bool(message.is_from_ai),
            "created_at": message.created_at,
            "chat_id": message.chat_id,
        }

    async def get_chat_messages(
        self, chat_id: UUID, skip: int = 0, limit: int = 50
    ) -> List[Dict[str, Any]]:
        # Check if chat exists
        chat = await self.chat_repository.get_chat(chat_id)
        if not chat:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND, detail="Chat not found"
            )

        # Get messages
        messages = await self.chat_repository.get_messages_by_chat(
            chat_id, skip=skip, limit=limit
        )

        return [
            {
                "id": message.id,
                "content": message.content,
                "is_from_ai": bool(message.is_from_ai),
                "created_at": message.created_at,
                "chat_id": message.chat_id,
            }
            for message in messages
        ]


def get_chat_service(
    chat_repository: ChatRepository = Depends(get_chat_repository),
) -> ChatService:
    return ChatService(chat_repository)
</file>

<file path="apps/core/services/job_service.py">
"""
Service layer for job-related operations.
"""

from typing import List, Optional

from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import Session

from apps.core.models.enums import ProcessingStatus
from apps.core.models.video_job_model import VideoJobModel
from apps.core.operations.video_job_repository import VideoJobRepository

# Placeholder for CRUD operations if we create a crud_jobs.py
# from apps.core.crud.crud_jobs import get_jobs_by_user_and_statuses


async def get_user_jobs_by_statuses(
    db: AsyncSession,  # Changed to AsyncSession
    user_id: str,  # Supabase User ID (string/UUID)
    statuses: Optional[List[ProcessingStatus]] = None,
) -> List[VideoJobModel]:
    """
    Retrieves video processing jobs for a specific user, filtered by specified statuses.

    Args:
        db: SQLAlchemy async session.
        user_id: The ID of the user (Supabase string UUID).
        statuses: An optional list of ProcessingStatus enums to filter by.
                  If None or empty, this service will default to PENDING and PROCESSING.

    Returns:
        A list of VideoJobModel objects matching the criteria.
    """
    # If no specific statuses are requested by the client,
    # default to fetching PENDING and PROCESSING jobs.
    if not statuses:
        statuses_to_fetch = [ProcessingStatus.PENDING, ProcessingStatus.PROCESSING]
    else:
        statuses_to_fetch = statuses

    # Repository method is now async, so await the call.
    jobs = await VideoJobRepository.get_by_user_id_and_statuses(
        db=db,
        user_id=user_id,
        statuses=statuses_to_fetch,
        # We can add limit/offset parameters here if needed by the API endpoint later
    )
    return jobs
</file>

<file path="apps/core/services/user_service.py">
from typing import Any, Dict, Optional

from fastapi import Depends, HTTPException, status
from sqlalchemy.ext.asyncio import AsyncSession

from apps.core.lib.auth.supabase_auth import AuthenticatedUser
from apps.core.models.user_model import User
from apps.core.operations.user_repository import UserRepository, get_user_repository


class UserService:
    def __init__(self, user_repository: UserRepository):
        self.user_repository = user_repository

    async def get_user_profile(self, user_id: int) -> Dict[str, Any]:
        user = await self.user_repository.get_user(user_id)
        if not user:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND, detail="User not found"
            )

        # Business logic: exclude sensitive information
        return {
            "id": user.id,
            "username": user.username,
            "email": user.email,
            "full_name": user.full_name,
            "is_active": user.is_active,
        }

    async def get_or_create_user_profile(self, auth_user: AuthenticatedUser) -> User:
        """
        Ensures a local user profile exists for the authenticated user.
        Looks up by email; creates a new user if not found.

        Args:
            auth_user (AuthenticatedUser): Authenticated user from Supabase JWT.

        Returns:
            User: The user model instance.
        """
        if not auth_user.email:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Authenticated user missing email",
            )
        user = await self.user_repository.get_user_by_email(auth_user.email)
        if user:
            return user

        # Generate a username from email prefix or fallback to user id string
        username = (
            auth_user.email.split("@")[0]
            if "@" in auth_user.email
            else f"user_{auth_user.id}"
        )
        user_data = {
            "username": username,
            "email": auth_user.email,
            "full_name": "",
            "hashed_password": "",  # Not used for Supabase-auth users
            "is_active": True,
        }
        user = await self.user_repository.create_user(user_data)
        return user

    async def create_user(self, user_data: Dict[str, Any]) -> Dict[str, Any]:
        # Check if username or email already exists
        email = user_data.get("email")
        if not email:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Email is required",
            )
        if await self.user_repository.get_user_by_email(email):
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Email already registered",
            )

        username = user_data.get("username")
        if not username:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Username is required",
            )
        if await self.user_repository.get_user_by_username(username):
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Username already taken",
            )

        # Create user
        user = await self.user_repository.create_user(user_data)

        # Return user without sensitive information
        return {
            "id": user.id,
            "username": user.username,
            "email": user.email,
            "full_name": user.full_name,
            "is_active": user.is_active,
        }


async def get_user_service(
    user_repository: UserRepository = Depends(get_user_repository),
) -> UserService:
    return UserService(user_repository)
</file>

<file path="apps/core/services/video_processing_service.py">
"""
Service for orchestrating the video processing pipeline.

This module implements the core business logic for video processing workflows,
handling the entire pipeline from initial upload through processing stages to
completion. It coordinates between repositories, storage systems, AI services,
and utility libraries to process videos and extract metadata.

The service manages background tasks, error handling, and provides status tracking
of processing jobs, implementing a robust pipeline for video analysis.

Usage:
    from fastapi import BackgroundTasks, Depends
    from sqlalchemy.orm import Session
    from apps.core.services.video_processing_service import VideoProcessingService

    # Inject dependencies and create service
    video_processing_service = VideoProcessingService(
        video_repo=VideoRepository(),
        job_repo=VideoJobRepository(),
        metadata_repo=VideoMetadataRepository(),
        storage=file_storage_service,
        ai_adapter=ai_adapter,
        ffmpeg_utils=ffmpeg_utils,
        subtitle_utils=subtitle_utils,
        file_utils=file_utils
    )

    # Initiate video processing
    job = await video_processing_service.initiate_video_processing(
        db=db_session,
        original_filename="video.mp4",
        video_content=file_bytes,
        content_type="video/mp4",
        uploader_user_id=user_id,
        background_tasks=background_tasks
    )

    # Get job status
    job_details = await video_processing_service.get_job_details(
        db=db_session,
        job_id=job.id,
        user_id=user_id
    )
"""

from typing import Optional

from fastapi import BackgroundTasks, HTTPException
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import Session

from apps.core.core.exceptions import VideoProcessingError
from apps.core.lib.ai.base_adapter import AIAdapterInterface
from apps.core.lib.auth.supabase_auth import AuthenticatedUser
from apps.core.lib.database.connection import AsyncSessionLocal, create_async_session
from apps.core.lib.storage.file_storage import FileStorageService
from apps.core.lib.utils.ffmpeg_utils import FfmpegUtils
from apps.core.lib.utils.file_utils import FileUtils
from apps.core.lib.utils.subtitle_utils import SubtitleUtils
from apps.core.models.enums import ProcessingStatus
from apps.core.models.video_job_model import VideoJobModel
from apps.core.models.video_metadata_model import VideoMetadataModel
from apps.core.models.video_model import VideoModel
from apps.core.operations.video_job_repository import VideoJobRepository
from apps.core.operations.video_metadata_repository import VideoMetadataRepository
from apps.core.operations.video_repository import VideoRepository


class VideoProcessingService:
    """
    Service layer for orchestrating the video processing pipeline.

    This service coordinates the end-to-end processing of video uploads, including
    storage, metadata extraction, AI processing, and status tracking. It manages the
    complex workflow of video analysis while keeping the API layer simple.

    Attributes:
        video_repo (VideoRepository): Repository for video data access.
        job_repo (VideoJobRepository): Repository for job data access.
        metadata_repo (VideoMetadataRepository): Repository for metadata access.
        storage (FileStorageService): Service for file storage operations.
        ai_adapter (AIAdapterInterface): AI service for text generation and transcription.
        ffmpeg_utils (FfmpegUtils): Utilities for video processing operations.
        subtitle_utils (SubtitleUtils): Utilities for subtitle generation.
        file_utils (FileUtils): Utilities for file system operations.
    """

    def __init__(
        self,
        video_repo: VideoRepository,
        job_repo: VideoJobRepository,
        metadata_repo: VideoMetadataRepository,
        storage: FileStorageService,
        ai_adapter: AIAdapterInterface,
        ffmpeg_utils: FfmpegUtils,
        subtitle_utils: SubtitleUtils,
        file_utils: FileUtils,
    ):
        self.video_repo = video_repo
        self.job_repo = job_repo
        self.metadata_repo = metadata_repo
        self.storage = storage
        self.ai_adapter = ai_adapter
        self.ffmpeg_utils = ffmpeg_utils
        self.subtitle_utils = subtitle_utils
        self.file_utils = file_utils

    async def initiate_video_processing(
        self,
        db: AsyncSession,
        original_filename: str,
        video_content: bytes,
        content_type: str,
        uploader_user_id: str,
        background_tasks: BackgroundTasks,
    ) -> VideoJobModel:
        """
        Initiate the video processing pipeline for a newly uploaded video.

        This method handles the initial steps of the video processing workflow:
        1. Saves the uploaded video to storage
        2. Creates database records for the video and processing job
        3. Schedules the full processing pipeline as a background task

        Args:
            db (AsyncSession): Database async session for database operations
            original_filename (str): Original filename of the uploaded video
            video_content (bytes): Raw video file content
            content_type (str): MIME type of the video file (e.g., "video/mp4")
            uploader_user_id (str): ID of the user who uploaded the video
            background_tasks (BackgroundTasks): FastAPI background tasks manager

        Returns:
            VideoJobModel: Created job model with PENDING status

        Note:
            This method commits the transaction before returning, ensuring the
            records are visible to the background task that will process the video.
        """
        # Save video file
        stored_video_path = await self.storage.save_file(
            file_content=video_content,
            filename=original_filename,
            subdir=f"uploads/{uploader_user_id}",
        )
        # Create VideoModel
        video = await self.video_repo.create(
            db=db,
            uploader_user_id=uploader_user_id,
            original_filename=original_filename,
            storage_path=stored_video_path,
            content_type=content_type,
            size_bytes=len(video_content),
        )
        # Create VideoJobModel
        job = await self.job_repo.create(
            db=db,
            video_id=video.id,  # type: ignore[arg-type]
            status=ProcessingStatus.PENDING,
            processing_stages=None,
            error_message=None,
        )
        await db.commit()
        # Schedule background processing
        background_tasks.add_task(
            self._execute_processing_pipeline,
            job.id,  # type: ignore[arg-type]
            stored_video_path,  # type: ignore[arg-type]
        )
        return job

    async def _execute_processing_pipeline(
        self,
        job_id: int,
        video_storage_path: str,
    ):
        """
        Execute the full video processing pipeline as a background task.

        This method is the core of the video processing workflow, handling all
        steps from downloading the video through processing to metadata generation.
        It's designed to run as a background task and updates the job status
        throughout the process.

        Processing steps:
        1. Set up a database session and temporary directory
        2. Download the video from storage to local temp directory
        3. Extract basic video metadata (duration, resolution, format)
        4. Extract audio track from the video
        5. Transcribe audio to text using AI service
        6. Generate content metadata (title, description, tags) using AI
        7. Generate subtitle files in multiple formats
        8. Extract thumbnail image from the video
        9. Update job status to COMPLETED when done

        Args:
            job_id (int): ID of the video processing job
            video_storage_path (str): Storage path of the uploaded video

        Note:
            This method handles its own error management, updating the job status to FAILED
            if any step encounters an exception. It also ensures proper cleanup of temporary
            files and database connections.
        """
        async with AsyncSessionLocal() as db_bg:
            temp_dir = self.file_utils.create_temp_dir()
            try:
                job = await self.job_repo.get_by_id(db_bg, job_id)
                if not job:
                    raise VideoProcessingError(
                        f"Job {job_id} not found or video relationship missing"
                    )

                await self.job_repo.update_status(
                    db_bg, job_id, ProcessingStatus.PROCESSING
                )
                await db_bg.commit()

                # Download video to temp dir
                if job.video is None:
                    raise VideoProcessingError(
                        f"Video data not found for job {job_id}. Ensure video relationship is loaded."
                    )
                local_video_path = f"{temp_dir}/{job.video.original_filename}"
                await self.storage.download_file(video_storage_path, local_video_path)

                # Step 1: Basic Metadata
                video_metadata = self.ffmpeg_utils.get_video_metadata_sync(
                    local_video_path
                )
                await self.metadata_repo.create_or_update(
                    db_bg,
                    job_id,
                    extracted_video_duration_seconds=video_metadata.get("duration"),
                    extracted_video_resolution=video_metadata.get("resolution"),
                    extracted_video_format=video_metadata.get("format"),
                )
                await db_bg.commit()

                # Step 2: Extract Audio
                audio_path = f"{temp_dir}/audio.wav"
                self.ffmpeg_utils.extract_audio_sync(local_video_path, audio_path)

                # Step 3: Transcript
                transcript_text = await self.ai_adapter.transcribe_audio(audio_path)
                transcript_file_path = f"{temp_dir}/transcript.txt"
                with open(transcript_file_path, "w") as f:
                    f.write(transcript_text)
                transcript_url = await self.storage.save_file(
                    file_content=transcript_text.encode("utf-8"),
                    filename="transcript.txt",
                    subdir=f"transcripts/{job.video.uploader_user_id}",
                )

                await self.metadata_repo.create_or_update(
                    db_bg,
                    job_id,
                    transcript_text=transcript_text,
                    transcript_file_url=transcript_url,
                )
                await db_bg.commit()

                # Step 4: Content Metadata
                title = await self.ai_adapter.generate_text(
                    prompt="Generate a YouTube-style title for this video.",
                    context=transcript_text,
                )
                description = await self.ai_adapter.generate_text(
                    prompt="Generate a YouTube-style description for this video.",
                    context=transcript_text,
                )
                tags = await self.ai_adapter.generate_text(
                    prompt="Generate a comma-separated list of tags for this video.",
                    context=transcript_text,
                )
                show_notes = await self.ai_adapter.generate_text(
                    prompt="Generate show notes for this video.",
                    context=transcript_text,
                )
                await self.metadata_repo.create_or_update(
                    db_bg,
                    job_id,
                    title=title,
                    description=description,
                    tags=[t.strip() for t in tags.split(",")],
                    show_notes_text=show_notes,
                )
                await db_bg.commit()

                # Step 5: Subtitles
                segments = [
                    {"text": line, "start_time": 0.0, "end_time": 0.0}
                    for line in transcript_text.splitlines()
                ]
                vtt_content = self.subtitle_utils.generate_vtt(segments)
                srt_content = self.subtitle_utils.generate_srt(segments)
                vtt_url = await self.storage.save_file(
                    file_content=vtt_content.encode("utf-8"),
                    filename="subtitles.vtt",
                    subdir=f"subtitles/{job.video.uploader_user_id}",
                )
                srt_url = await self.storage.save_file(
                    file_content=srt_content.encode("utf-8"),
                    filename="subtitles.srt",
                    subdir=f"subtitles/{job.video.uploader_user_id}",
                )
                await self.metadata_repo.create_or_update(
                    db_bg,
                    job_id,
                    subtitle_files_urls={"vtt": vtt_url, "srt": srt_url},
                )
                await db_bg.commit()

                # Step 6: Thumbnail
                thumbnail_path = f"{temp_dir}/thumbnail.jpg"
                self.ffmpeg_utils.extract_frame_sync(
                    local_video_path, 1.0, thumbnail_path
                )
                with open(thumbnail_path, "rb") as f:
                    thumbnail_bytes = f.read()
                thumbnail_url = await self.storage.save_file(
                    file_content=thumbnail_bytes,
                    filename="thumbnail.jpg",
                    subdir=f"thumbnails/{job.video.uploader_user_id}",
                )
                await self.metadata_repo.create_or_update(
                    db_bg,
                    job_id,
                    thumbnail_file_url=thumbnail_url,
                )
                await db_bg.commit()

                # Mark job as completed
                await self.job_repo.update_status(
                    db_bg, job_id, ProcessingStatus.COMPLETED
                )
                await db_bg.commit()
            except Exception as e:
                await self.job_repo.update_status(
                    db_bg, job_id, ProcessingStatus.FAILED
                )
                await self.job_repo.add_processing_stage(
                    db_bg, job_id, f"Error: {str(e)}"
                )
                await db_bg.commit()
            finally:
                self.file_utils.cleanup_temp_dir(temp_dir)
                await db_bg.close()

    async def get_job_details(
        self, db: AsyncSession, job_id: int, user_id: str
    ) -> Optional[VideoJobModel]:
        """
        Retrieve details of a specific video processing job.

        Ensures that the job belongs to the requesting user before returning details.

        Args:
            db (AsyncSession): Database async session for database operations.
            job_id (int): ID of the video processing job.
            user_id (str): ID of the user requesting the job details.

        Returns:
            Optional[VideoJobModel]: The job model if found and user is authorized,
                                     otherwise None or raises HTTPException.
        """
        job = await self.job_repo.get_by_id(db, job_id)
        if not job:
            raise HTTPException(status_code=404, detail="Job not found")

        # Ensure video relationship is loaded by get_by_id or handle potential None
        if job.video is None or job.video.uploader_user_id != user_id:
            raise HTTPException(
                status_code=403, detail="User not authorized to view this job"
            )
        return job
</file>

<file path="apps/core/tests/api/test_jobs_api.py">
"""
Integration tests for the Jobs API endpoints.
"""

from typing import List, Optional
from unittest.mock import ANY, AsyncMock, MagicMock, patch

import pytest
from fastapi import HTTPException

from apps.core.api.schemas.video_processing_schemas import (
    VideoJobSchema,  # For response validation
    VideoMetadataSchema,
    VideoSchema,
)
from apps.core.lib.auth.supabase_auth import (
    AuthenticatedUser,  # For mocking get_current_user
    get_current_user,
)
from apps.core.models.enums import ProcessingStatus


@pytest.fixture
def mock_authenticated_user_fixture() -> AuthenticatedUser:
    return AuthenticatedUser(
        id="test-user-id", email="test@example.com", aud="authenticated"
    )


@pytest.fixture
def auth_override(mock_authenticated_user_fixture: AuthenticatedUser):
    """Overrides the get_current_user dependency for testing."""

    # It's important that this inner function matches the signature of the original dependency
    async def _override():
        return mock_authenticated_user_fixture

    return _override


@pytest.fixture
def unauth_override():
    async def _override_raise_401():
        raise HTTPException(status_code=401, detail="Not authenticated")

    return _override_raise_401


# Test cases for GET /api/v1/jobs/


def test_get_my_processing_jobs_success_default_statuses(
    test_client_fixture,
    auth_override,
    mock_authenticated_user_fixture: AuthenticatedUser,
):
    """Test successful retrieval of jobs with default status handling in service."""
    app = test_client_fixture.app
    app.dependency_overrides[get_current_user] = auth_override

    # Mock the service call
    # Provide None for optional nested schemas if not testing their content
    mock_job1 = VideoJobSchema(
        id=1, video_id=1, status=ProcessingStatus.PENDING, video=None, metadata=None
    )
    mock_job2 = VideoJobSchema(
        id=2, video_id=2, status=ProcessingStatus.PROCESSING, video=None, metadata=None
    )
    mock_service_response = [mock_job1, mock_job2]

    with patch(
        "apps.core.api.endpoints.jobs_endpoints.get_user_jobs_by_statuses",
        new_callable=AsyncMock,
    ) as mock_service_call:

        async def async_mock_service(*args, **kwargs):
            return mock_service_response

        mock_service_call.return_value = mock_service_response

        response = test_client_fixture.get("/api/v1/jobs/")

        assert response.status_code == 200
        response_data = response.json()
        assert len(response_data) == 2
        assert response_data[0]["id"] == mock_job1.id
        assert response_data[1]["id"] == mock_job2.id
        # Service should be called with current_user.id and default statuses (handled by service)
        mock_service_call.assert_awaited_once_with(
            user_id=mock_authenticated_user_fixture.id,
            db=ANY,
            statuses=None,
        )

    app.dependency_overrides.clear()  # Clear overrides


def test_get_my_processing_jobs_with_specific_statuses(
    test_client_fixture,
    auth_override,
    mock_authenticated_user_fixture: AuthenticatedUser,
):
    app = test_client_fixture.app
    app.dependency_overrides[get_current_user] = auth_override

    mock_job_completed = VideoJobSchema(
        id=3, video_id=3, status=ProcessingStatus.COMPLETED, video=None, metadata=None
    )
    mock_service_response_completed = [mock_job_completed]

    with patch(
        "apps.core.api.endpoints.jobs_endpoints.get_user_jobs_by_statuses",
        new_callable=AsyncMock,
    ) as mock_service_call:

        async def async_mock_service_completed(*args, **kwargs):
            if kwargs.get("statuses") == [ProcessingStatus.COMPLETED]:
                return mock_service_response_completed
            return []

        mock_service_call.return_value = mock_service_response_completed

        response = test_client_fixture.get("/api/v1/jobs/?status=COMPLETED")
        assert response.status_code == 200
        response_data = response.json()
        assert len(response_data) == 1
        assert response_data[0]["id"] == mock_job_completed.id
        mock_service_call.assert_awaited_once_with(
            user_id=mock_authenticated_user_fixture.id,
            db=ANY,
            statuses=[ProcessingStatus.COMPLETED],
        )

    app.dependency_overrides.clear()  # Clear overrides


def test_get_my_processing_jobs_unauthenticated(test_client_fixture, unauth_override):
    """Test API returns 401 if user is not authenticated."""

    app = test_client_fixture.app
    # Override get_current_user to simulate unauthenticated state
    app.dependency_overrides[get_current_user] = unauth_override

    response = test_client_fixture.get("/api/v1/jobs/")
    assert response.status_code == 401

    app.dependency_overrides.clear()


def test_get_my_processing_jobs_invalid_status_parameter(
    test_client_fixture, auth_override
):
    """Test API returns 422 for invalid status query parameter."""
    app = test_client_fixture.app
    app.dependency_overrides[get_current_user] = auth_override

    response = test_client_fixture.get("/api/v1/jobs/?status=INVALID_STATUS_VALUE")
    assert response.status_code == 422  # Unprocessable Entity

    app.dependency_overrides.clear()  # Clear overrides
</file>

<file path="apps/core/tests/integration/api/test_job_status_retrieval.py">
import pytest
from fastapi import FastAPI, status
from httpx import AsyncClient

from apps.core.lib.auth.supabase_auth import get_current_user
from apps.core.models.enums import ProcessingStatus

pytestmark = pytest.mark.integration


@pytest.mark.asyncio
async def test_get_job_status_successful(client: AsyncClient, populated_db):
    """Test successful retrieval of job status for an authorized user."""
    # Get the job from the populated database
    job = populated_db["job"]

    # Make the request
    response = await client.get(f"/api/v1/videos/jobs/{job.id}")

    # Assert response status
    assert response.status_code == status.HTTP_200_OK

    # Verify response format
    json_response = response.json()
    assert "id" in json_response
    assert "status" in json_response
    assert "processing_stages" in json_response
    assert "video" in json_response

    # Verify job data
    assert json_response["id"] == job.id
    assert json_response["status"] == ProcessingStatus.COMPLETED.value

    # Verify video data
    assert json_response["video"]["id"] == populated_db["video"].id
    assert json_response["video"]["uploader_user_id"] == "test-user-id"
    assert json_response["video"]["original_filename"] == "test_video.mp4"


@pytest.mark.asyncio
async def test_get_job_status_nonexistent_job(client: AsyncClient):
    """Test error handling for non-existent job ID."""
    # Use a job ID that doesn't exist
    non_existent_job_id = 9999

    # Make the request
    response = await client.get(f"/api/v1/videos/jobs/{non_existent_job_id}")

    # Should return 404 Not Found
    assert response.status_code == status.HTTP_404_NOT_FOUND


@pytest.mark.asyncio
async def test_get_job_status_unauthorized_user(client: AsyncClient, populated_db):
    """Test unauthorized access to a job owned by another user."""
    # Get the job from the populated database
    job = populated_db["job"]

    # Temporarily override the auth dependency to return a different user
    from apps.core.lib.auth.supabase_auth import AuthenticatedUser
    from apps.core.main import app

    # Create a different user
    async def mock_different_user():
        return AuthenticatedUser(
            id="different-user-id", email="different@example.com", aud="authenticated"
        )

    # Apply the override
    original_override = app.dependency_overrides.get(get_current_user)
    app.dependency_overrides[get_current_user] = mock_different_user

    try:
        # Make the request
        response = await client.get(f"/api/v1/videos/jobs/{job.id}")

        # Should return 403 Forbidden since the job belongs to another user
        assert response.status_code == status.HTTP_403_FORBIDDEN

    finally:
        # Restore the original override
        if original_override:
            app.dependency_overrides[get_current_user] = original_override
        else:
            del app.dependency_overrides[get_current_user]


@pytest.mark.asyncio
async def test_get_job_status_unauthenticated(client: AsyncClient, populated_db):
    """Test job status retrieval for unauthenticated user."""
    # Get the job from the populated database
    job = populated_db["job"]

    # Temporarily override the auth dependency to simulate unauthenticated request
    from fastapi import HTTPException

    from apps.core.main import app

    # Create a function that raises an authentication error
    async def mock_unauthenticated_user():
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Could not validate credentials",
            headers={"WWW-Authenticate": "Bearer"},
        )

    # Apply the override
    original_override = app.dependency_overrides.get(get_current_user)
    app.dependency_overrides[get_current_user] = mock_unauthenticated_user

    try:
        # Make the request
        response = await client.get(f"/api/v1/videos/jobs/{job.id}")

        # Should return 401 Unauthorized
        assert response.status_code == status.HTTP_401_UNAUTHORIZED

    finally:
        # Restore the original override
        if original_override:
            app.dependency_overrides[get_current_user] = original_override
        else:
            del app.dependency_overrides[get_current_user]
</file>

<file path="apps/core/tests/integration/api/test_video_upload_flow.py">
import io
import os

import pytest
from fastapi import FastAPI, status
from httpx import AsyncClient
from sqlalchemy.ext.asyncio import AsyncSession

from apps.core.api.schemas.video_processing_schemas import VideoUploadResponseSchema
from apps.core.lib.database.connection import get_db_session
from apps.core.models.enums import ProcessingStatus

# Import the fixtures from conftest.py
pytestmark = pytest.mark.integration


@pytest.mark.asyncio
async def test_upload_video_successful(
    client: AsyncClient, test_video_file: str, db_session: AsyncSession
):
    """Test successful video upload and initial job creation"""
    # Read the test video file
    with open(test_video_file, "rb") as f:
        file_content = f.read()

    # Create file-like object for multipart upload
    file = io.BytesIO(file_content)

    # Make the request
    response = await client.post(
        "/api/v1/videos/upload",
        files={"file": ("test_video.mp4", file, "video/mp4")},
    )

    # Assert response status and format
    assert response.status_code == status.HTTP_200_OK
    json_response = response.json()

    # Verify response format matches VideoUploadResponseSchema
    assert "job_id" in json_response
    assert "status" in json_response

    # Get the job ID
    job_id = json_response["job_id"]

    # Verify job was created with PENDING status
    assert json_response["status"] == ProcessingStatus.PENDING.value

    # Verify job exists in database
    from apps.core.operations.video_job_repository import VideoJobRepository

    job = await VideoJobRepository.get_by_id(db_session, job_id)

    assert job is not None
    assert job.status == ProcessingStatus.PENDING

    # Verify video was created in database
    from apps.core.operations.video_repository import VideoRepository

    video_id_value: int = job.video_id
    video = await VideoRepository.get_by_id(db_session, video_id_value)

    assert video is not None
    assert video.uploader_user_id == "test-user-id"
    assert video.original_filename == "test_video.mp4"
    assert video.content_type == "video/mp4"


@pytest.mark.asyncio
async def test_upload_video_no_file(client: AsyncClient):
    """Test uploading without a file returns 422"""
    response = await client.post("/api/v1/videos/upload")
    assert response.status_code == status.HTTP_422_UNPROCESSABLE_ENTITY


@pytest.mark.asyncio
async def test_upload_video_unsupported_file_type(client: AsyncClient):
    """Test uploading an unsupported file type"""
    # Create a text file
    file = io.BytesIO(b"This is not a video")

    response = await client.post(
        "/api/v1/videos/upload",
        files={"file": ("test.txt", file, "text/plain")},
    )

    # Should return 400 Bad Request for unsupported content type
    assert response.status_code == status.HTTP_400_BAD_REQUEST


@pytest.mark.asyncio
async def test_upload_video_unauthenticated(client: AsyncClient, test_video_file: str):
    """Test upload fails with invalid authentication"""
    # Temporarily override the auth dependency to simulate unauthenticated request
    from fastapi import HTTPException

    from apps.core.lib.auth.supabase_auth import get_current_user
    from apps.core.main import app

    # Create a function that raises an authentication error
    async def mock_unauthenticated_user():
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Could not validate credentials",
            headers={"WWW-Authenticate": "Bearer"},
        )

    # Apply the override
    original_override = app.dependency_overrides.get(get_current_user)
    app.dependency_overrides[get_current_user] = mock_unauthenticated_user

    try:
        # Read the test video file
        with open(test_video_file, "rb") as f:
            file_content = f.read()

        file = io.BytesIO(file_content)

        # Make the request
        response = await client.post(
            "/api/v1/videos/upload",
            files={"file": ("test_video.mp4", file, "video/mp4")},
        )

        # Should return 401 Unauthorized
        assert response.status_code == status.HTTP_401_UNAUTHORIZED

    finally:
        # Restore the original override
        if original_override:
            app.dependency_overrides[get_current_user] = original_override
        else:
            del app.dependency_overrides[get_current_user]


@pytest.mark.asyncio
async def test_upload_large_video(client: AsyncClient):
    """Test handling of large video files"""
    # Create a large file (10MB)
    large_file = io.BytesIO(b"0" * (10 * 1024 * 1024))

    response = await client.post(
        "/api/v1/videos/upload",
        files={"file": ("large_video.mp4", large_file, "video/mp4")},
    )

    # Should still succeed (assuming no file size limit in the API)
    assert response.status_code == status.HTTP_200_OK
</file>

<file path="apps/core/tests/integration/test_video_processing_api.py">
import os
import tempfile
from typing import Optional
from unittest.mock import AsyncMock, patch

import pytest
from fastapi import Header, HTTPException, status
from fastapi.testclient import TestClient
from httpx import AsyncClient

import apps.core.models  # Ensure all models are registered before app/db usage
from apps.core.lib.auth.supabase_auth import AuthenticatedUser, get_current_user
from apps.core.main import app


# Dummy AI Adapter for testing
class DummyAIAdapter:
    async def generate_text(self, prompt: str, context: Optional[str] = None) -> str:
        return "dummy ai response"

    async def transcribe_audio(self, audio_file_path: str) -> str:
        return "dummy transcript"


def override_get_ai_adapter(settings_instance=None):
    return DummyAIAdapter()


import sys
from unittest.mock import patch

from apps.core.lib.ai import ai_client_factory


@pytest.fixture(autouse=True, scope="module")
def patch_get_ai_adapter():
    # Patch get_ai_adapter at the import location used by the endpoint
    with patch(
        "apps.core.api.endpoints.video_processing_endpoints.get_ai_adapter",
        new=override_get_ai_adapter,
    ):
        yield


app.dependency_overrides[ai_client_factory.get_ai_adapter] = override_get_ai_adapter

# NOTE: This is a scaffold for integration/E2E tests of the video processing API.
#       Actual test logic, fixtures, and mocks should be implemented as the next step.


# Override get_current_user dependency to bypass JWT validation for tests
# while still requiring an Authorization header to be present
async def override_get_current_user(authorization: Optional[str] = Header(None)):
    if not authorization or not authorization.startswith("Bearer "):
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid authentication credentials",
            headers={"WWW-Authenticate": "Bearer"},
        )
    # If we have an Authorization header, return a test user
    return AuthenticatedUser(
        id="test-user-id", email="test@example.com", aud="authenticated"
    )


app.dependency_overrides = getattr(app, "dependency_overrides", {})
app.dependency_overrides[get_current_user] = override_get_current_user

# client = TestClient(app) # REMOVE: Client will be injected by pytest fixture


# Patch OpenAI client for all tests to avoid real API calls and missing API key errors
@pytest.fixture(autouse=True, scope="module")
def patch_openai_client():
    with patch(
        "openai.resources.chat.completions.AsyncCompletions.create",
        new_callable=AsyncMock,
    ) as mock_create:
        # Return a dummy response structure matching OpenAI's API
        mock_create.return_value = AsyncMock(
            choices=[
                type(
                    "obj",
                    (),
                    {"message": type("msg", (), {"content": "dummy response"})()},
                )()
            ]
        )
        yield mock_create


@pytest.fixture(scope="module")
def test_video_file():
    # Create a temporary dummy video file for upload tests
    fd, path = tempfile.mkstemp(suffix=".mp4")
    with os.fdopen(fd, "wb") as f:
        f.write(os.urandom(1024 * 1024))  # 1MB random data
    yield path
    os.remove(path)


@pytest.mark.asyncio
async def test_upload_video_success(test_video_file, client: AsyncClient):
    """
    Test uploading a video with valid authentication and check response structure.
    """
    # TODO: Replace with a real or properly mocked JWT for Supabase
    valid_token = os.environ.get("TEST_AUTH_TOKEN", "test-token")
    with open(test_video_file, "rb") as f:
        response = await client.post(
            "/api/v1/videos/upload",
            files={"file": ("test.mp4", f, "video/mp4")},
            headers={"Authorization": f"Bearer {valid_token}"},
        )
    assert response.status_code == 200, (
        f"Unexpected status: {response.status_code}, body: {response.text}"
    )
    data = response.json()
    assert "job_id" in data and isinstance(data["job_id"], int)
    assert "status" in data and data["status"] in ("PENDING", "PROCESSING", "COMPLETED")
    # Store job_id for potential use in dependent tests but don't return it
    job_id = data["job_id"]
    assert job_id > 0


@pytest.mark.asyncio
async def test_upload_video_unauthorized(test_video_file, client: AsyncClient):
    """
    Test uploading a video without authentication should fail.
    """
    with open(test_video_file, "rb") as f:
        response = await client.post(
            "/api/v1/videos/upload",
            files={"file": ("test.mp4", f, "video/mp4")},
        )
    assert response.status_code in (401, 403)


@pytest.mark.asyncio
async def test_get_job_details_success(
    monkeypatch, test_video_file, client: AsyncClient
):
    """
    Test retrieving job details after uploading a video.
    """
    # Upload a video and get job_id
    valid_token = os.environ.get("TEST_AUTH_TOKEN", "test-token")
    with open(test_video_file, "rb") as f:
        upload_response = await client.post(
            "/api/v1/videos/upload",
            files={"file": ("test.mp4", f, "video/mp4")},
            headers={"Authorization": f"Bearer {valid_token}"},
        )
    if upload_response.status_code != 200:
        pytest.skip("Upload failed, cannot test job details.")
    job_id = upload_response.json()["job_id"]

    # Retrieve job details
    response = await client.get(
        f"/api/v1/videos/jobs/{job_id}",
        headers={"Authorization": f"Bearer {valid_token}"},
    )
    assert response.status_code == 200, (
        f"Unexpected status: {response.status_code}, body: {response.text}"
    )
    data = response.json()
    assert "id" in data and data["id"] == job_id
    assert "status" in data
    assert "video" in data
    assert "metadata" in data


@pytest.mark.asyncio
async def test_get_job_details_unauthorized(client: AsyncClient):
    """
    Test retrieving job details without authentication should fail.
    """
    job_id = 1  # Arbitrary
    response = await client.get(
        f"/api/v1/videos/jobs/{job_id}",
    )
    assert response.status_code in (401, 403)


@pytest.mark.asyncio
async def test_upload_invalid_file_type(client: AsyncClient):
    """
    Test uploading a non-video file should fail or be handled gracefully.
    """
    valid_token = os.environ.get("TEST_AUTH_TOKEN", "test-token")
    with tempfile.NamedTemporaryFile(suffix=".txt") as f:
        f.write(b"not a video")
        f.flush()
        f.seek(0)
        response = await client.post(
            "/api/v1/videos/upload",
            files={"file": ("test.txt", f, "text/plain")},
            headers={"Authorization": f"Bearer {valid_token}"},
        )
    # Acceptable: 400 Bad Request, 422 Unprocessable Entity, or handled error
    assert response.status_code in (400, 422, 415, 500)


# TODO: Add more E2E scenarios (full pipeline, error cases, etc.)
</file>

<file path="apps/core/tests/operations/test_video_job_repository.py">
"""
Unit tests for the VideoJobRepository.
"""

from datetime import datetime, timedelta
from typing import AsyncGenerator, List

import pytest
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from apps.core.lib.database.connection import Base
from apps.core.models.enums import ProcessingStatus
from apps.core.models.video_job_model import VideoJobModel
from apps.core.models.video_model import VideoModel
from apps.core.operations.video_job_repository import VideoJobRepository

# Setup in-memory SQLite database for testing
# engine = create_engine("sqlite:///:memory:")
# TestingSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

# db_session fixture - REMOVED (now comes from conftest.py as db_session_fixture)
# @pytest.fixture(scope="function")
# def db_session() -> Generator[Session, None, None]:
#     """Creates a new database session for a test."""
#     Base.metadata.create_all(bind=engine)  # Create tables
#     session = TestingSessionLocal()
#     try:
#         yield session
#     finally:
#         session.close()
#         Base.metadata.drop_all(bind=engine)  # Drop tables after test


class TestVideoJobRepository:
    @pytest.mark.asyncio
    async def test_get_by_user_id_and_statuses_no_jobs(
        self, db_session_fixture: AsyncSession
    ):
        """Test retrieving jobs when no jobs exist for the user."""
        user_id = "test_user_1"
        # Ensure VideoModel for the user exists, otherwise join will fail if jobs were present
        # For this specific test (no jobs), it's fine, but good practice for others.
        video_for_user = VideoModel(
            uploader_user_id=user_id,
            original_filename="test.mp4",
            storage_path="test/path",
            content_type="video/mp4",
            size_bytes=100,
        )
        db_session_fixture.add(video_for_user)
        await db_session_fixture.commit()  # Commit the video

        jobs = await VideoJobRepository.get_by_user_id_and_statuses(
            db=db_session_fixture, user_id=user_id, statuses=[ProcessingStatus.PENDING]
        )
        assert len(jobs) == 0

    @pytest.mark.asyncio
    async def test_get_by_user_id_and_statuses_filters_by_user(
        self, db_session_fixture: AsyncSession
    ):
        """Test that jobs are correctly filtered by user_id."""
        user1_id = "user_1"
        user2_id = "user_2"

        # Video for user 1
        video1 = VideoModel(
            uploader_user_id=user1_id,
            original_filename="video1.mp4",
            storage_path="path1",
            content_type="video/mp4",
            size_bytes=100,
        )
        db_session_fixture.add(video1)
        await db_session_fixture.commit()  # Commit video1 before creating job1
        job1 = VideoJobModel(video_id=video1.id, status=ProcessingStatus.PENDING)
        db_session_fixture.add(job1)
        # await db_session_fixture.commit() # Commit job1 separately if needed, or commit all at once

        # Video for user 2
        video2 = VideoModel(
            uploader_user_id=user2_id,
            original_filename="video2.mp4",
            storage_path="path2",
            content_type="video/mp4",
            size_bytes=100,
        )
        db_session_fixture.add(video2)
        await db_session_fixture.commit()  # Commit video2 before creating job2
        job2 = VideoJobModel(video_id=video2.id, status=ProcessingStatus.PENDING)
        db_session_fixture.add(job2)
        await db_session_fixture.commit()  # Commit jobs

        jobs_user1 = await VideoJobRepository.get_by_user_id_and_statuses(
            db=db_session_fixture, user_id=user1_id, statuses=[ProcessingStatus.PENDING]
        )
        assert len(jobs_user1) == 1
        assert bool(jobs_user1[0].id == job1.id)
        assert bool(jobs_user1[0].video.uploader_user_id == user1_id)

        jobs_user2 = await VideoJobRepository.get_by_user_id_and_statuses(
            db=db_session_fixture, user_id=user2_id, statuses=[ProcessingStatus.PENDING]
        )
        assert len(jobs_user2) == 1
        assert bool(jobs_user2[0].id == job2.id)
        assert bool(jobs_user2[0].video.uploader_user_id == user2_id)

    @pytest.mark.asyncio
    async def test_get_by_user_id_and_statuses_filters_by_multiple_statuses(
        self, db_session_fixture: AsyncSession
    ):
        user_id = "multi_status_user"
        video = VideoModel(
            uploader_user_id=user_id,
            original_filename="video.mp4",
            storage_path="path",
            content_type="video/mp4",
            size_bytes=100,
        )
        db_session_fixture.add(video)
        await db_session_fixture.commit()

        job_pending = VideoJobModel(
            video_id=video.id,
            status=ProcessingStatus.PENDING,
            created_at=datetime.utcnow() - timedelta(hours=2),
        )
        job_processing = VideoJobModel(
            video_id=video.id,
            status=ProcessingStatus.PROCESSING,
            created_at=datetime.utcnow() - timedelta(hours=1),
        )
        job_completed = VideoJobModel(
            video_id=video.id,
            status=ProcessingStatus.COMPLETED,
            created_at=datetime.utcnow(),
        )
        db_session_fixture.add_all([job_pending, job_processing, job_completed])
        await db_session_fixture.commit()

        # Test fetching PENDING and PROCESSING
        jobs_pending_processing = await VideoJobRepository.get_by_user_id_and_statuses(
            db=db_session_fixture,
            user_id=user_id,
            statuses=[ProcessingStatus.PENDING, ProcessingStatus.PROCESSING],
        )
        assert len(jobs_pending_processing) == 2
        job_ids_pending_processing = {job.id for job in jobs_pending_processing}
        assert job_pending.id in job_ids_pending_processing
        assert job_processing.id in job_ids_pending_processing

        # Test fetching only COMPLETED
        jobs_completed_list = await VideoJobRepository.get_by_user_id_and_statuses(
            db=db_session_fixture,
            user_id=user_id,
            statuses=[ProcessingStatus.COMPLETED],
        )
        assert len(jobs_completed_list) == 1
        assert bool(jobs_completed_list[0].id == job_completed.id)

    @pytest.mark.asyncio
    async def test_get_by_user_id_and_statuses_no_status_filter(
        self, db_session_fixture: AsyncSession
    ):
        """Test behavior when statuses is None (should return all for user)."""
        user_id = "no_status_filter_user"
        video = VideoModel(
            uploader_user_id=user_id,
            original_filename="video.mp4",
            storage_path="path",
            content_type="video/mp4",
            size_bytes=100,
        )
        db_session_fixture.add(video)
        await db_session_fixture.commit()

        job_pending = VideoJobModel(video_id=video.id, status=ProcessingStatus.PENDING)
        job_completed = VideoJobModel(
            video_id=video.id, status=ProcessingStatus.COMPLETED
        )
        db_session_fixture.add_all([job_pending, job_completed])
        await db_session_fixture.commit()

        jobs = await VideoJobRepository.get_by_user_id_and_statuses(
            db=db_session_fixture, user_id=user_id, statuses=None
        )
        assert len(jobs) == 2

    @pytest.mark.asyncio
    async def test_get_by_user_id_and_statuses_ordering(
        self, db_session_fixture: AsyncSession
    ):
        user_id = "ordering_user"
        video = VideoModel(
            uploader_user_id=user_id,
            original_filename="video.mp4",
            storage_path="path",
            content_type="video/mp4",
            size_bytes=100,
        )
        db_session_fixture.add(video)
        await db_session_fixture.commit()

        # Timestamps are important here
        job1_older = VideoJobModel(
            video_id=video.id,
            status=ProcessingStatus.PENDING,
            created_at=datetime.utcnow() - timedelta(minutes=10),
        )
        job2_newer = VideoJobModel(
            video_id=video.id,
            status=ProcessingStatus.PENDING,
            created_at=datetime.utcnow() - timedelta(minutes=1),
        )
        job3_oldest = VideoJobModel(
            video_id=video.id,
            status=ProcessingStatus.PROCESSING,
            created_at=datetime.utcnow() - timedelta(minutes=20),
        )
        db_session_fixture.add_all([job1_older, job2_newer, job3_oldest])
        await db_session_fixture.commit()

        jobs = await VideoJobRepository.get_by_user_id_and_statuses(
            db=db_session_fixture, user_id=user_id, statuses=None
        )
        assert len(jobs) == 3
        assert bool(jobs[0].id == job2_newer.id)  # Newest first
        assert bool(jobs[1].id == job1_older.id)
        assert bool(jobs[2].id == job3_oldest.id)  # Oldest last

    @pytest.mark.asyncio
    async def test_get_by_user_id_and_statuses_pagination(
        self, db_session_fixture: AsyncSession
    ):
        user_id = "pagination_user"
        video = VideoModel(
            uploader_user_id=user_id,
            original_filename="video.mp4",
            storage_path="path",
            content_type="video/mp4",
            size_bytes=100,
        )
        db_session_fixture.add(video)
        await db_session_fixture.commit()

        all_jobs_data = []
        for i in range(5):  # Create 5 jobs
            job = VideoJobModel(
                video_id=video.id,
                status=ProcessingStatus.PENDING,
                created_at=datetime.utcnow() - timedelta(minutes=i),
            )
            all_jobs_data.append(job)
        db_session_fixture.add_all(all_jobs_data)
        await db_session_fixture.commit()

        # Test fetching first page (limit 2)
        jobs_page1 = await VideoJobRepository.get_by_user_id_and_statuses(
            db=db_session_fixture,
            user_id=user_id,
            statuses=[ProcessingStatus.PENDING],
            limit=2,
            offset=0,
        )
        assert len(jobs_page1) == 2
        # Assuming jobs are ordered by created_at desc by default (newest first)
        # Newest is -0 minutes ago, then -1 minutes ago
        assert bool(jobs_page1[0].created_at > jobs_page1[1].created_at)

        # Test fetching second page (limit 2, offset 2)
        jobs_page2 = await VideoJobRepository.get_by_user_id_and_statuses(
            db=db_session_fixture,
            user_id=user_id,
            statuses=[ProcessingStatus.PENDING],
            limit=2,
            offset=2,
        )
        assert len(jobs_page2) == 2
        assert bool(
            jobs_page1[1].created_at > jobs_page2[0].created_at
        )  # End of page 1 vs start of page 2

        # Test fetching a page that exceeds total items
        jobs_page_exceed = await VideoJobRepository.get_by_user_id_and_statuses(
            db=db_session_fixture,
            user_id=user_id,
            statuses=[ProcessingStatus.PENDING],
            limit=2,
            offset=4,  # 5 items total, offset 4 means 5th item
        )
        assert len(jobs_page_exceed) == 1

        # Test fetching with limit only (no offset)
        jobs_limit_only = await VideoJobRepository.get_by_user_id_and_statuses(
            db=db_session_fixture,
            user_id=user_id,
            statuses=[ProcessingStatus.PENDING],
            limit=3,
        )
        assert len(jobs_limit_only) == 3

    @pytest.mark.asyncio
    async def test_get_by_user_id_and_statuses_no_matching_status(
        self, db_session_fixture: AsyncSession
    ):
        user_id = "no_match_status_user"
        video = VideoModel(
            uploader_user_id=user_id,
            original_filename="video.mp4",
            storage_path="path",
            content_type="video/mp4",
            size_bytes=100,
        )
        db_session_fixture.add(video)
        await db_session_fixture.commit()

        job_pending = VideoJobModel(video_id=video.id, status=ProcessingStatus.PENDING)
        db_session_fixture.add(job_pending)
        await db_session_fixture.commit()

        jobs = await VideoJobRepository.get_by_user_id_and_statuses(
            db=db_session_fixture,
            user_id=user_id,
            statuses=[ProcessingStatus.COMPLETED],
        )
        assert len(jobs) == 0

    @pytest.mark.asyncio
    async def test_eager_loading(self, db_session_fixture: AsyncSession):
        """Test that video and video_metadata are eagerly loaded."""
        user_id = "eager_loading_user"
        video = VideoModel(
            uploader_user_id=user_id,
            original_filename="video_eager.mp4",
            storage_path="path_eager",
            content_type="video/mp4",
            size_bytes=100,
            # video_metadata field is implicitly handled via backref if VideoMetadataModel has one
            # or can be explicitly set if VideoMetadataModel instance is created and linked
        )
        db_session_fixture.add(video)
        await db_session_fixture.commit()  # Commit video first to get its ID

        # Create a job linked to this video
        job = VideoJobModel(video_id=video.id, status=ProcessingStatus.PENDING)
        db_session_fixture.add(job)
        await db_session_fixture.commit()

        # Clear the session to ensure objects are loaded from DB, not cache, for testing eager loading
        db_session_fixture.expunge_all()
        # Alternatively, and often better for testing eager loads against the DB:
        # await db_session_fixture.close()
        # db_session_fixture = await anext(db_session_fixture_gen()) # if db_session_fixture was a generator

        # Retrieve the job.
        # The get_by_user_id_and_statuses method should use joinedload/selectinload
        retrieved_jobs = await VideoJobRepository.get_by_user_id_and_statuses(
            db=db_session_fixture, user_id=user_id, statuses=[ProcessingStatus.PENDING]
        )
        assert len(retrieved_jobs) == 1
        retrieved_job = retrieved_jobs[0]

        # Accessing .video and .video.video_metadata should not trigger new SQL queries
        # This is hard to assert directly without inspecting SQL queries (e.g. using an event listener)
        # For now, we assume the repository's query options handle this.
        # A simple check is that the attributes are accessible:
        assert retrieved_job.video is not None
        assert retrieved_job.video.original_filename == "video_eager.mp4"
        # If VideoMetadataModel was setup and linked, you would check it too:
        # assert retrieved_job.video.video_metadata is not None
        # assert retrieved_job.video.video_metadata.title == "Some Title" # if metadata was created and linked

        # To truly test eager loading, you'd need to mock the session or use SQLAlchemy events
        # to count queries. For this test, we rely on the implementation detail of the repository.
        # One indirect way is to check if the relationship is loaded without error after expunging.
        # If it wasn't eagerly loaded, accessing it might raise a DetachedInstanceError or similar,
        # or it would be None if the session was completely closed and reopened.
        # Since we are just expunging, the objects are detached but still in memory.
        # A more robust test would involve a fresh session or query counting.

        # For this test, let's ensure the related video's uploader_user_id is correct,
        # which implies 'video' was loaded.
        assert retrieved_job.video.uploader_user_id == user_id

    # TODO:
    # - (No outstanding major categories from previous TODO, covered above)
</file>

<file path="apps/core/tests/services/test_job_service.py">
"""
Unit tests for the JobService.
"""

from typing import List, Optional
from unittest.mock import AsyncMock, MagicMock, patch

import pytest
from sqlalchemy.ext.asyncio import AsyncSession

from apps.core.models.enums import ProcessingStatus
from apps.core.models.video_job_model import VideoJobModel
from apps.core.services.job_service import get_user_jobs_by_statuses


@pytest.mark.asyncio
async def test_get_user_jobs_by_statuses_with_specific_statuses():
    """Test service passes specific statuses to the repository."""
    mock_db_session = AsyncMock(spec=AsyncSession)
    user_id = "test_user"
    specific_statuses = [ProcessingStatus.COMPLETED]
    expected_jobs_data = [
        VideoJobModel(id=1, video_id=101, status=ProcessingStatus.COMPLETED)
    ]

    with patch(
        "apps.core.services.job_service.VideoJobRepository.get_by_user_id_and_statuses",
        new_callable=AsyncMock,
    ) as mock_repo_call:
        mock_repo_call.return_value = expected_jobs_data

        result_jobs = await get_user_jobs_by_statuses(
            db=mock_db_session, user_id=user_id, statuses=specific_statuses
        )

        mock_repo_call.assert_awaited_once_with(
            db=mock_db_session, user_id=user_id, statuses=specific_statuses
        )
        assert result_jobs == expected_jobs_data


@pytest.mark.asyncio
async def test_get_user_jobs_by_statuses_with_no_statuses_defaults_correctly():
    """Test service defaults to PENDING and PROCESSING when no statuses are provided."""
    mock_db_session = AsyncMock(spec=AsyncSession)
    user_id = "test_user_default"
    default_statuses = [ProcessingStatus.PENDING, ProcessingStatus.PROCESSING]
    expected_jobs_data = [
        VideoJobModel(id=2, video_id=102, status=ProcessingStatus.PENDING),
        VideoJobModel(id=3, video_id=103, status=ProcessingStatus.PROCESSING),
    ]

    with patch(
        "apps.core.services.job_service.VideoJobRepository.get_by_user_id_and_statuses",
        new_callable=AsyncMock,
    ) as mock_repo_call:
        mock_repo_call.return_value = expected_jobs_data

        # Test with statuses=None
        result_jobs_none = await get_user_jobs_by_statuses(
            db=mock_db_session, user_id=user_id, statuses=None
        )
        mock_repo_call.assert_awaited_with(
            db=mock_db_session,
            user_id=user_id,
            statuses=default_statuses,
        )
        assert result_jobs_none == expected_jobs_data

        # Test with statuses=[] (empty list)
        result_jobs_empty = await get_user_jobs_by_statuses(
            db=mock_db_session, user_id=user_id, statuses=[]
        )
        mock_repo_call.assert_awaited_with(
            db=mock_db_session,
            user_id=user_id,
            statuses=default_statuses,
        )
        assert result_jobs_empty == expected_jobs_data

        # To be very precise about both calls:
        assert mock_repo_call.await_count == 2
        call_args_list = mock_repo_call.await_args_list
        assert call_args_list[0].kwargs["statuses"] == default_statuses
        assert call_args_list[1].kwargs["statuses"] == default_statuses


@pytest.mark.asyncio
async def test_get_user_jobs_by_statuses_returns_repository_result():
    """Test that the service function returns whatever the repository provides."""
    mock_db_session = AsyncMock(spec=AsyncSession)
    user_id = "test_user_return"
    mock_jobs_from_repo = [
        VideoJobModel(id=1, video_id=1, status=ProcessingStatus.PENDING),
        VideoJobModel(id=2, video_id=2, status=ProcessingStatus.PROCESSING),
    ]

    with patch(
        "apps.core.services.job_service.VideoJobRepository.get_by_user_id_and_statuses",
        new_callable=AsyncMock,
    ) as mock_repo_call:
        mock_repo_call.return_value = mock_jobs_from_repo

        result_jobs = await get_user_jobs_by_statuses(
            db=mock_db_session, user_id=user_id, statuses=[ProcessingStatus.PENDING]
        )
        mock_repo_call.assert_awaited_once_with(
            db=mock_db_session, user_id=user_id, statuses=[ProcessingStatus.PENDING]
        )
        assert result_jobs == mock_jobs_from_repo
</file>

<file path="apps/core/tests/unit/lib/storage/test_file_storage.py">
"""
Unit tests for the file storage service.
"""

import asyncio
import os
import shutil
import tempfile
from pathlib import Path
from unittest.mock import MagicMock, patch

import pytest
from core.config import Settings

from apps.core.lib.storage.file_storage import FileStorageService


@pytest.fixture
def temp_dir():
    """Create a temporary directory for testing local storage."""
    temp_dir = tempfile.mkdtemp()
    yield temp_dir
    # Clean up
    shutil.rmtree(temp_dir)


@pytest.fixture
def mock_settings_local(temp_dir):
    """Create mock settings configured for local storage."""
    settings = Settings()
    settings.STORAGE_BACKEND = "local"
    settings.LOCAL_STORAGE_PATH = temp_dir
    settings.BASE_DIR = Path("/fake/base/dir")  # Not used for local tests
    return settings


@pytest.fixture
def mock_settings_gcs():
    """Create mock settings configured for GCS storage."""
    settings = Settings()
    settings.STORAGE_BACKEND = "gcs"
    settings.GCS_BUCKET_NAME = "test-bucket"
    settings.GOOGLE_APPLICATION_CREDENTIALS_PATH = None  # Use default credentials
    return settings


@pytest.fixture
def mock_gcs_client():
    """Create a mock GCS client."""
    with patch("apps.core.lib.storage.file_storage.storage") as mock_storage:
        # Create mock bucket and blob
        mock_blob = MagicMock()
        mock_bucket = MagicMock()
        mock_bucket.blob.return_value = mock_blob

        # Create mock client
        mock_client = MagicMock()
        mock_client.bucket.return_value = mock_bucket

        # Set up return value for the Client constructor
        mock_storage.Client.return_value = mock_client

        # Set GCS_AVAILABLE to True
        with patch("apps.core.lib.storage.file_storage.GCS_AVAILABLE", True):
            yield mock_client


class TestFileStorageService:
    """Test cases for the FileStorageService class."""

    def test_init_local_storage(self, mock_settings_local):
        """Test initialization with local storage backend."""
        service = FileStorageService(mock_settings_local)

        # Verify the local storage path was set correctly
        assert service.local_storage_path == Path(
            mock_settings_local.LOCAL_STORAGE_PATH
        )

        # Verify the directory was created
        assert os.path.exists(mock_settings_local.LOCAL_STORAGE_PATH)

    def test_init_gcs_storage(self, mock_settings_gcs, mock_gcs_client):
        """Test initialization with GCS storage backend."""
        service = FileStorageService(mock_settings_gcs)

        # Verify the GCS client was initialized
        assert service.gcs_client is not None

    def test_init_gcs_without_dependencies(self, mock_settings_gcs):
        """Test initialization fails when GCS is selected but dependencies aren't available."""
        with patch("apps.core.lib.storage.file_storage.GCS_AVAILABLE", False):
            with pytest.raises(ImportError) as excinfo:
                FileStorageService(mock_settings_gcs)

            assert "Google Cloud Storage dependencies are not installed" in str(
                excinfo.value
            )

    def test_init_gcs_without_bucket(self, mock_gcs_client):
        """Test initialization fails when GCS is selected but no bucket is specified."""
        settings = Settings()
        settings.STORAGE_BACKEND = "gcs"
        settings.GCS_BUCKET_NAME = None

        with pytest.raises(ValueError) as excinfo:
            FileStorageService(settings)

        assert "GCS_BUCKET_NAME must be set" in str(excinfo.value)

    @pytest.mark.asyncio
    async def test_init_unsupported_backend(self):
        """Test initialization fails with an unsupported storage backend."""
        settings = Settings()
        settings.STORAGE_BACKEND = "unsupported"

        # This should not raise an error on init, only when methods are called
        service = FileStorageService(settings)

        # But operations should fail
        with pytest.raises(ValueError) as excinfo:
            await service.save_file(b"test content", "test.txt")

        assert "Unsupported storage backend" in str(excinfo.value)

    @pytest.mark.asyncio
    async def test_save_file_local(self, mock_settings_local):
        """Test saving a file to local storage."""
        service = FileStorageService(mock_settings_local)

        # Test file data
        test_content = b"This is test content"
        test_filename = "test.txt"

        # Save the file
        relative_path = await service.save_file(test_content, test_filename)

        # Verify the path is returned correctly
        assert relative_path.startswith("uploads/")
        assert relative_path.endswith(".txt")

        # Verify the file was saved
        saved_path = os.path.join(mock_settings_local.LOCAL_STORAGE_PATH, relative_path)
        assert os.path.exists(saved_path)

        # Verify the content
        with open(saved_path, "rb") as f:
            saved_content = f.read()
        assert saved_content == test_content

    @pytest.mark.asyncio
    async def test_save_file_gcs(self, mock_settings_gcs, mock_gcs_client):
        """Test saving a file to GCS."""
        service = FileStorageService(mock_settings_gcs)

        # Test file data
        test_content = b"This is test content"
        test_filename = "test.txt"

        # Save the file
        storage_path = await service.save_file(test_content, test_filename)

        # Verify the path is returned correctly
        assert storage_path.startswith(
            f"gs://{mock_settings_gcs.GCS_BUCKET_NAME}/uploads/"
        )
        assert storage_path.endswith(".txt")

        # Verify the GCS upload was called
        mock_bucket = mock_gcs_client.bucket.return_value
        mock_blob = mock_bucket.blob.return_value
        mock_blob.upload_from_string.assert_called_once()

        # Verify the correct parameters were used
        call_args = mock_blob.upload_from_string.call_args[0]
        assert call_args[0] == test_content

    @pytest.mark.asyncio
    async def test_download_file_local(self, mock_settings_local):
        """Test downloading a file from local storage."""
        service = FileStorageService(mock_settings_local)

        # Create a test file
        test_content = b"This is test content for download"
        test_filename = "test_download.txt"
        source_path = await service.save_file(test_content, test_filename)

        # Create a temporary directory for the download destination
        with tempfile.TemporaryDirectory() as dest_dir:
            dest_path = os.path.join(dest_dir, "downloaded.txt")

            # Download the file
            result_path = await service.download_file(source_path, dest_path)

            # Verify the correct path was returned
            assert result_path == dest_path

            # Verify the file was downloaded
            assert os.path.exists(dest_path)

            # Verify the content
            with open(dest_path, "rb") as f:
                downloaded_content = f.read()
            assert downloaded_content == test_content

    @pytest.mark.asyncio
    async def test_download_file_gcs(self, mock_settings_gcs, mock_gcs_client):
        """Test downloading a file from GCS."""
        service = FileStorageService(mock_settings_gcs)

        # Mock GCS URI
        gcs_uri = f"gs://{mock_settings_gcs.GCS_BUCKET_NAME}/test_folder/test.txt"

        # Create a temporary directory for the download destination
        with tempfile.TemporaryDirectory() as dest_dir:
            dest_path = os.path.join(dest_dir, "downloaded.txt")

            # Download the file
            result_path = await service.download_file(gcs_uri, dest_path)

            # Verify the correct path was returned
            assert result_path == dest_path

            # Verify download_to_filename was called on the blob
            mock_bucket = mock_gcs_client.bucket.return_value
            mock_blob = mock_bucket.blob.return_value
            mock_blob.download_to_filename.assert_called_once_with(dest_path)

    @pytest.mark.asyncio
    async def test_get_public_url_local(self, mock_settings_local):
        """Test getting a public URL for a local file."""
        service = FileStorageService(mock_settings_local)

        # Local files don't have public URLs
        url = await service.get_public_url("uploads/test.txt")
        assert url is None

    @pytest.mark.asyncio
    async def test_get_public_url_gcs(self, mock_settings_gcs, mock_gcs_client):
        """Test getting a public URL for a GCS file."""
        service = FileStorageService(mock_settings_gcs)

        # Test with a GCS URI
        gcs_uri = f"gs://{mock_settings_gcs.GCS_BUCKET_NAME}/test_folder/test.txt"
        url = await service.get_public_url(gcs_uri)

        # Verify the URL format
        assert (
            url
            == f"https://storage.googleapis.com/{mock_settings_gcs.GCS_BUCKET_NAME}/test_folder/test.txt"
        )

        # Test with just a blob path
        blob_path = "test_folder/test.txt"
        url = await service.get_public_url(blob_path)

        # Verify the URL format
        assert (
            url
            == f"https://storage.googleapis.com/{mock_settings_gcs.GCS_BUCKET_NAME}/{blob_path}"
        )

    def test_parse_gcs_uri(self, mock_settings_gcs, mock_gcs_client):
        """Test parsing a GCS URI."""
        service = FileStorageService(mock_settings_gcs)

        # Valid URI
        bucket_name, blob_name = service._parse_gcs_uri(
            "gs://test-bucket/path/to/file.txt"
        )
        assert bucket_name == "test-bucket"
        assert blob_name == "path/to/file.txt"

        # Invalid URI (no gs:// prefix)
        with pytest.raises(ValueError) as excinfo:
            service._parse_gcs_uri("invalid-uri")
        assert "Invalid GCS URI" in str(excinfo.value)

        # Invalid URI (no blob name)
        with pytest.raises(ValueError) as excinfo:
            service._parse_gcs_uri("gs://test-bucket")
        assert "Invalid GCS URI format" in str(excinfo.value)

    def test_upload_from_string_local(self, mock_settings_local):
        """Test uploading a string to local storage."""
        service = FileStorageService(mock_settings_local)

        # Test content
        test_content = "This is string content"
        storage_path = "test_folder/string_file.txt"

        # Upload the string
        service.upload_from_string(test_content, storage_path)

        # Verify the file was created
        full_path = os.path.join(mock_settings_local.LOCAL_STORAGE_PATH, storage_path)
        assert os.path.exists(full_path)

        # Verify the content
        with open(full_path, "r", encoding="utf-8") as f:
            saved_content = f.read()
        assert saved_content == test_content

    def test_upload_from_string_gcs(self, mock_settings_gcs, mock_gcs_client):
        """Test uploading a string to GCS."""
        service = FileStorageService(mock_settings_gcs)

        # Test content
        test_content = "This is string content"
        storage_path = "test_folder/string_file.txt"
        content_type = "text/plain"

        # Upload the string
        service.upload_from_string(test_content, storage_path, content_type)

        # Verify upload_from_string was called on the blob
        mock_bucket = mock_gcs_client.bucket.return_value
        mock_blob = mock_bucket.blob.return_value
        mock_blob.upload_from_string.assert_called_once_with(
            test_content, content_type=content_type
        )
</file>

<file path="apps/core/tests/unit/lib/utils/test_ffmpeg_utils.py">
"""
Unit tests for the FfmpegUtils class.
"""

import json
import os
import subprocess
import tempfile
from unittest.mock import AsyncMock, MagicMock, patch

import pytest

from apps.core.lib.utils.ffmpeg_utils import FfmpegUtils


class TestFfmpegUtils:
    """Test cases for the FfmpegUtils class."""

    @patch("apps.core.lib.utils.ffmpeg_utils.subprocess.run")
    def test_extract_audio_sync_success(self, mock_run):
        """Test successful audio extraction from a video file."""
        # Set up the mock
        mock_result = MagicMock()
        mock_result.returncode = 0
        mock_run.return_value = mock_result

        # Call the method
        FfmpegUtils.extract_audio_sync("test_video.mp4", "output_audio.wav")

        # Verify subprocess.run was called with correct arguments
        mock_run.assert_called_once()
        args, kwargs = mock_run.call_args

        # Check the command list
        cmd = args[0]
        assert cmd[0] == "ffmpeg"
        assert "-i" in cmd
        assert "test_video.mp4" in cmd
        assert "output_audio.wav" in cmd

        # Check that stdout and stderr are captured
        assert kwargs["capture_output"] is True
        assert kwargs["text"] is True
        assert kwargs["check"] is False

    @patch("apps.core.lib.utils.ffmpeg_utils.subprocess.run")
    def test_extract_audio_sync_failure(self, mock_run):
        """Test handling of ffmpeg failure during audio extraction."""
        # Set up the mock for failure
        mock_result = MagicMock()
        mock_result.returncode = 1
        mock_result.stderr = "ffmpeg error message"
        mock_run.return_value = mock_result

        # Call the method and expect an exception
        with pytest.raises(RuntimeError) as excinfo:
            FfmpegUtils.extract_audio_sync("test_video.mp4", "output_audio.wav")

        # Verify the error message
        assert "FFmpeg audio extraction failed" in str(excinfo.value)
        assert "ffmpeg error message" in str(excinfo.value)

    @patch("apps.core.lib.utils.ffmpeg_utils.subprocess.run")
    def test_extract_frame_sync_success(self, mock_run):
        """Test successful frame extraction from a video file."""
        # Set up the mock
        mock_result = MagicMock()
        mock_result.returncode = 0
        mock_run.return_value = mock_result

        # Call the method
        FfmpegUtils.extract_frame_sync("test_video.mp4", 10.5, "output_frame.jpg")

        # Verify subprocess.run was called with correct arguments
        mock_run.assert_called_once()
        args, kwargs = mock_run.call_args

        # Check the command list
        cmd = args[0]
        assert cmd[0] == "ffmpeg"
        assert "-ss" in cmd
        assert "10.5" in cmd
        assert "-i" in cmd
        assert "test_video.mp4" in cmd
        assert "output_frame.jpg" in cmd

        assert kwargs["capture_output"] is True
        assert kwargs["text"] is True
        assert kwargs["check"] is False

    @patch("apps.core.lib.utils.ffmpeg_utils.subprocess.run")
    def test_extract_frame_sync_failure(self, mock_run):
        """Test handling of ffmpeg failure during frame extraction."""
        # Set up the mock for failure
        mock_result = MagicMock()
        mock_result.returncode = 1
        mock_result.stderr = "ffmpeg error message"
        mock_run.return_value = mock_result

        # Call the method and expect an exception
        with pytest.raises(RuntimeError) as excinfo:
            FfmpegUtils.extract_frame_sync("test_video.mp4", 10.5, "output_frame.jpg")

        # Verify the error message
        assert "FFmpeg frame extraction failed" in str(excinfo.value)
        assert "ffmpeg error message" in str(excinfo.value)

    @patch("apps.core.lib.utils.ffmpeg_utils.subprocess.run")
    def test_get_video_metadata_sync_success(self, mock_run):
        """Test successful metadata extraction from a video file."""
        # Sample ffprobe JSON output
        sample_metadata = {
            "streams": [
                {
                    "codec_type": "video",
                    "width": 1920,
                    "height": 1080,
                    "duration": "60.000000",
                    "r_frame_rate": "30/1",
                },
                {"codec_type": "audio", "sample_rate": "44100", "channels": 2},
            ],
            "format": {
                "duration": "60.000000",
                "bit_rate": "1000000",
                "format_name": "mp4",
            },
        }

        # Set up the mock
        mock_result = MagicMock()
        mock_result.returncode = 0
        mock_result.stdout = json.dumps(sample_metadata)
        mock_run.return_value = mock_result

        # Call the method
        metadata = FfmpegUtils.get_video_metadata_sync("test_video.mp4")

        # Verify subprocess.run was called with correct arguments
        mock_run.assert_called_once()
        args, kwargs = mock_run.call_args

        # Check the command list
        cmd = args[0]
        assert cmd[0] == "ffprobe"
        assert "-print_format" in cmd
        assert "json" in cmd
        assert "test_video.mp4" in cmd

        assert kwargs["capture_output"] is True
        assert kwargs["text"] is True
        assert kwargs["check"] is False

        # Verify the parsed metadata
        assert metadata == sample_metadata
        assert metadata["streams"][0]["width"] == 1920
        assert metadata["format"]["duration"] == "60.000000"

    @patch("apps.core.lib.utils.ffmpeg_utils.subprocess.run")
    def test_get_video_metadata_sync_failure(self, mock_run):
        """Test handling of ffprobe failure during metadata extraction."""
        # Set up the mock for failure
        mock_result = MagicMock()
        mock_result.returncode = 1
        mock_result.stderr = "ffprobe error message"
        mock_run.return_value = mock_result

        # Call the method and expect an exception
        with pytest.raises(RuntimeError) as excinfo:
            FfmpegUtils.get_video_metadata_sync("test_video.mp4")

        # Verify the error message
        assert "ffprobe metadata extraction failed" in str(excinfo.value)
        assert "ffprobe error message" in str(excinfo.value)

    def test_real_command_structure(self):
        """
        Test the actual structure of commands without executing them.
        This verifies the command construction logic without mocking.
        """

        # Extract audio command
        with patch("apps.core.lib.utils.ffmpeg_utils.subprocess.run") as mock_run:
            FfmpegUtils.extract_audio_sync("video.mp4", "audio.wav")
            cmd = mock_run.call_args[0][0]
            assert cmd[0:3] == ["ffmpeg", "-y", "-i"]
            assert "video.mp4" in cmd
            assert "-vn" in cmd
            assert "audio.wav" in cmd

        # Extract frame command
        with patch("apps.core.lib.utils.ffmpeg_utils.subprocess.run") as mock_run:
            FfmpegUtils.extract_frame_sync("video.mp4", 5.5, "frame.jpg")
            cmd = mock_run.call_args[0][0]
            assert cmd[0:3] == ["ffmpeg", "-y", "-ss"]
            assert cmd[3] == "5.5"
            assert "video.mp4" in cmd
            assert "-frames:v" in cmd
            assert "1" in cmd
            assert "frame.jpg" in cmd

        # Get metadata command
        with patch("apps.core.lib.utils.ffmpeg_utils.subprocess.run") as mock_run:
            FfmpegUtils.get_video_metadata_sync("video.mp4")
            cmd = mock_run.call_args[0][0]
            assert cmd[0:3] == ["ffprobe", "-v", "error"]
            assert "-show_entries" in cmd
            assert "format:stream" in cmd
            assert "-print_format" in cmd
            assert "json" in cmd
            assert "video.mp4" in cmd
</file>

<file path="apps/core/tests/unit/operations/test_video_job_repository.py">
"""
Unit tests for the VideoJobRepository.
"""

import json
from unittest.mock import AsyncMock, MagicMock, patch

import pytest
from sqlalchemy.ext.asyncio import AsyncSession

from apps.core.models.enums import ProcessingStatus
from apps.core.models.video_job_model import VideoJobModel
from apps.core.operations.video_job_repository import VideoJobRepository


@pytest.fixture
def mock_db_session_async() -> AsyncMock:
    """Create a mock SQLAlchemy AsyncSession."""
    mock_session = AsyncMock(spec=AsyncSession)

    return mock_session


@pytest.fixture
def sample_job_data():
    """Sample video job data for testing."""
    return {
        "video_id": 42,
        "status": ProcessingStatus.PENDING,
        "processing_stages": [],
        "error_message": None,
    }


class TestVideoJobRepository:
    """Test cases for the VideoJobRepository class."""

    @pytest.mark.asyncio
    async def test_create(self, mock_db_session_async: AsyncMock, sample_job_data):
        """Test creating a new video job."""
        job = await VideoJobRepository.create(
            mock_db_session_async,
            video_id=sample_job_data["video_id"],
            status=sample_job_data["status"],
        )

        assert isinstance(job, VideoJobModel)
        assert job.video_id == sample_job_data["video_id"]
        assert job.status == sample_job_data["status"]

        mock_db_session_async.add.assert_called_once()
        mock_db_session_async.flush.assert_awaited_once()
        mock_db_session_async.refresh.assert_awaited_once()

    @pytest.mark.asyncio
    async def test_create_with_defaults(self, mock_db_session_async: AsyncMock):
        """Test creating a job with default values."""
        job = await VideoJobRepository.create(mock_db_session_async, video_id=42)

        assert isinstance(job, VideoJobModel)
        assert job.video_id == 42
        assert job.status == ProcessingStatus.PENDING

        mock_db_session_async.add.assert_called_once()
        mock_db_session_async.flush.assert_awaited_once()
        mock_db_session_async.refresh.assert_awaited_once()

    @pytest.mark.asyncio
    async def test_get_by_id_found(
        self, mock_db_session_async: AsyncMock, sample_job_data
    ):
        """Test retrieving a job by ID when it exists."""
        mock_job_instance = VideoJobModel(**sample_job_data)
        mock_job_instance.id = 123

        mock_execute_result = AsyncMock()
        mock_scalars_result = AsyncMock()

        mock_db_session_async.execute.return_value = mock_execute_result
        mock_execute_result.scalars.return_value = mock_scalars_result
        mock_scalars_result.first.return_value = mock_job_instance

        job = await VideoJobRepository.get_by_id(mock_db_session_async, 123)

        assert job is mock_job_instance
        assert job.id == 123
        mock_db_session_async.execute.assert_awaited_once()

    @pytest.mark.asyncio
    async def test_get_by_id_not_found(self, mock_db_session_async: AsyncMock):
        """Test retrieving a job by ID when it doesn't exist."""
        mock_execute_result = AsyncMock()
        mock_scalars_result = AsyncMock()

        mock_db_session_async.execute.return_value = mock_execute_result
        mock_execute_result.scalars.return_value = mock_scalars_result
        mock_scalars_result.first.return_value = None

        job = await VideoJobRepository.get_by_id(mock_db_session_async, 999)

        assert job is None
        mock_db_session_async.execute.assert_awaited_once()

    @pytest.mark.asyncio
    async def test_update_status(
        self, mock_db_session_async: AsyncMock, sample_job_data
    ):
        """Test updating a job's status."""
        mock_job_to_update = VideoJobModel(**sample_job_data)  # type: ignore
        mock_job_to_update.id = 123

        with patch(
            "apps.core.operations.video_job_repository.VideoJobRepository.get_by_id",
            new_callable=AsyncMock,
        ) as mock_get_by_id:
            mock_get_by_id.return_value = mock_job_to_update

            updated_job = await VideoJobRepository.update_status(
                mock_db_session_async, 123, ProcessingStatus.PROCESSING
            )

            mock_get_by_id.assert_awaited_once_with(mock_db_session_async, 123)
            assert updated_job is not None  # Explicitly assert not None
            if updated_job:  # Guard subsequent attribute access
                assert updated_job is mock_job_to_update
                assert updated_job.status == ProcessingStatus.PROCESSING
                assert updated_job.error_message is None

            mock_db_session_async.flush.assert_awaited_once()
            # Ensure refresh is called with the object that get_by_id returned (which is mock_job_to_update)
            mock_db_session_async.refresh.assert_awaited_once_with(mock_job_to_update)

    @pytest.mark.asyncio
    async def test_update_status_with_error(
        self, mock_db_session_async: AsyncMock, sample_job_data
    ):
        """Test updating a job's status with an error message."""
        mock_job_to_update = VideoJobModel(**sample_job_data)  # type: ignore
        mock_job_to_update.id = 123

        with patch(
            "apps.core.operations.video_job_repository.VideoJobRepository.get_by_id",
            new_callable=AsyncMock,
        ) as mock_get_by_id:
            mock_get_by_id.return_value = mock_job_to_update

            error_msg = "Something went wrong"
            updated_job = await VideoJobRepository.update_status(
                mock_db_session_async,
                123,
                ProcessingStatus.FAILED,
                error_message=error_msg,
            )

            mock_get_by_id.assert_awaited_once_with(mock_db_session_async, 123)
            assert updated_job is not None  # Explicitly assert not None
            if updated_job:  # Guard subsequent attribute access
                assert updated_job is mock_job_to_update
                assert updated_job.status == ProcessingStatus.FAILED
                assert updated_job.error_message == error_msg

            mock_db_session_async.flush.assert_awaited_once()
            mock_db_session_async.refresh.assert_awaited_once_with(mock_job_to_update)

    @pytest.mark.asyncio
    async def test_update_status_not_found(self, mock_db_session_async: AsyncMock):
        """Test updating status when job doesn't exist."""
        with patch(
            "apps.core.operations.video_job_repository.VideoJobRepository.get_by_id",
            new_callable=AsyncMock,
        ) as mock_get_by_id:
            mock_get_by_id.return_value = None  # Simulate job not found

            updated_job = await VideoJobRepository.update_status(
                mock_db_session_async, 999, ProcessingStatus.PROCESSING
            )

            mock_get_by_id.assert_awaited_once_with(mock_db_session_async, 999)
            assert updated_job is None
            assert mock_db_session_async.flush.await_count == 0
            assert mock_db_session_async.refresh.await_count == 0

    @pytest.mark.asyncio
    async def test_add_processing_stage_new_list(
        self, mock_db_session_async: AsyncMock
    ):
        """Test adding a processing stage to a job with no existing stages."""
        mock_job = VideoJobModel(video_id=42, processing_stages=None)  # type: ignore
        mock_job.id = 123

        with patch(
            "apps.core.operations.video_job_repository.VideoJobRepository.get_by_id",
            new_callable=AsyncMock,
        ) as mock_get_by_id:
            mock_get_by_id.return_value = mock_job

            stage = "transcription_started"
            updated_job = await VideoJobRepository.add_processing_stage(
                mock_db_session_async, 123, stage
            )

            mock_get_by_id.assert_awaited_once_with(mock_db_session_async, 123)
            assert updated_job is not None
            if updated_job:
                assert updated_job is mock_job
                # The repository method should handle initializing the list
                assert updated_job.processing_stages == [stage]

            mock_db_session_async.flush.assert_awaited_once()
            mock_db_session_async.refresh.assert_awaited_once_with(mock_job)

    @pytest.mark.asyncio
    async def test_add_processing_stage_existing_list(
        self, mock_db_session_async: AsyncMock
    ):
        """Test adding a processing stage to a job with existing stages."""
        existing_stages = ["upload_complete", "validation_complete"]
        mock_job = VideoJobModel(video_id=42, processing_stages=existing_stages)  # type: ignore
        mock_job.id = 123

        with patch(
            "apps.core.operations.video_job_repository.VideoJobRepository.get_by_id",
            new_callable=AsyncMock,
        ) as mock_get_by_id:
            mock_get_by_id.return_value = mock_job

            new_stage = "transcription_started"
            updated_job = await VideoJobRepository.add_processing_stage(
                mock_db_session_async, 123, new_stage
            )

            mock_get_by_id.assert_awaited_once_with(mock_db_session_async, 123)
            assert updated_job is not None
            if updated_job:
                assert updated_job is mock_job  # The same instance is modified
                assert updated_job.processing_stages == existing_stages + [new_stage]

            mock_db_session_async.flush.assert_awaited_once()
            mock_db_session_async.refresh.assert_awaited_once_with(mock_job)

    @pytest.mark.asyncio
    async def test_add_processing_stage_json_string(
        self, mock_db_session_async: AsyncMock
    ):
        """Test adding a processing stage when stages are stored as a JSON string."""
        existing_stages_json = json.dumps(["upload_complete"])
        # Ensure the mock_job.processing_stages is set as if read from DB (i.e., already a string)
        mock_job = VideoJobModel(video_id=42, processing_stages=existing_stages_json)  # type: ignore
        mock_job.id = 123

        with patch(
            "apps.core.operations.video_job_repository.VideoJobRepository.get_by_id",
            new_callable=AsyncMock,
        ) as mock_get_by_id:
            mock_get_by_id.return_value = mock_job

            new_stage = "validation_complete"
            updated_job = await VideoJobRepository.add_processing_stage(
                mock_db_session_async, 123, new_stage
            )

            mock_get_by_id.assert_awaited_once_with(mock_db_session_async, 123)
            assert updated_job is not None
            if updated_job:  # Guard attribute access
                assert updated_job is mock_job
                # The repository method should deserialize, append, and reserialize (or store as list if column type allows)
                # Assuming the repository method correctly handles JSON string to list conversion and back, or stores as Python list directly.
                # If it stores as Python list, then this assertion is fine.
                # If it converts back to JSON string, this assertion needs to change.
                # Based on VideoJobModel.processing_stages being potentially `JSON` type which SQLAlchemy handles,
                # it likely becomes a Python list in the model instance after load/modification.
                current_stages = json.loads(
                    existing_stages_json
                )  # Before adding new_stage
                current_stages.append(new_stage)
                assert updated_job.processing_stages == current_stages

            mock_db_session_async.flush.assert_awaited_once()
            mock_db_session_async.refresh.assert_awaited_once_with(mock_job)

    @pytest.mark.asyncio
    async def test_add_processing_stage_invalid_json(
        self, mock_db_session_async: AsyncMock
    ):
        """Test adding a stage when processing_stages is an invalid JSON string."""
        mock_job = VideoJobModel(video_id=42, processing_stages="not_a_valid_json[")  # type: ignore
        mock_job.id = 123

        with patch(
            "apps.core.operations.video_job_repository.VideoJobRepository.get_by_id",
            new_callable=AsyncMock,
        ) as mock_get_by_id:
            mock_get_by_id.return_value = mock_job

            new_stage = "validation_complete"
            # Depending on how add_processing_stage handles json.JSONDecodeError,
            # this might raise an exception, or log an error and initialize stages to [new_stage].
            # For this test, let's assume it logs error and initializes a new list.
            # If it's expected to raise, use pytest.raises.
            updated_job = await VideoJobRepository.add_processing_stage(
                mock_db_session_async, 123, new_stage
            )

            mock_get_by_id.assert_awaited_once_with(mock_db_session_async, 123)
            assert updated_job is not None
            if updated_job:
                assert updated_job is mock_job
                # Assuming repository initializes to [new_stage] on JSON error
                assert updated_job.processing_stages == [new_stage]

            mock_db_session_async.flush.assert_awaited_once()
            mock_db_session_async.refresh.assert_awaited_once_with(mock_job)

    @pytest.mark.asyncio
    async def test_add_processing_stage_not_found(
        self, mock_db_session_async: AsyncMock
    ):
        """Test adding a processing stage when the job doesn't exist."""
        with patch(
            "apps.core.operations.video_job_repository.VideoJobRepository.get_by_id",
            new_callable=AsyncMock,
        ) as mock_get_by_id:
            mock_get_by_id.return_value = None  # Simulate job not found

            new_stage = "transcription_started"
            updated_job = await VideoJobRepository.add_processing_stage(
                mock_db_session_async, 999, new_stage
            )

            mock_get_by_id.assert_awaited_once_with(mock_db_session_async, 999)
            assert updated_job is None
            assert mock_db_session_async.flush.await_count == 0
            assert mock_db_session_async.refresh.await_count == 0

    @pytest.mark.asyncio
    async def test_add_processing_stage_non_list_object(
        self, mock_db_session_async: AsyncMock
    ):
        """Test adding a stage when processing_stages is not a list or JSON list (e.g., a dict)."""
        # This scenario depends on how robust the add_processing_stage method is.
        # It might raise a TypeError, or attempt to handle it gracefully.
        # Let's assume it attempts to initialize to a new list with the stage.
        mock_job = VideoJobModel(video_id=42, processing_stages={"key": "value"})  # type: ignore
        mock_job.id = 123

        with patch(
            "apps.core.operations.video_job_repository.VideoJobRepository.get_by_id",
            new_callable=AsyncMock,
        ) as mock_get_by_id:
            mock_get_by_id.return_value = mock_job

            new_stage = "processing_initiated"
            updated_job = await VideoJobRepository.add_processing_stage(
                mock_db_session_async, 123, new_stage
            )

            mock_get_by_id.assert_awaited_once_with(mock_db_session_async, 123)
            assert updated_job is not None
            if updated_job:
                assert updated_job is mock_job
                # Assuming it re-initializes or similar graceful handling
                assert updated_job.processing_stages == [new_stage]

            mock_db_session_async.flush.assert_awaited_once()
            mock_db_session_async.refresh.assert_awaited_once_with(mock_job)
</file>

<file path="apps/core/tests/unit/operations/test_video_metadata_repository.py">
"""
Unit tests for the VideoMetadataRepository.
"""

from unittest.mock import AsyncMock, MagicMock, patch

import pytest
from sqlalchemy.ext.asyncio import AsyncSession

from apps.core.models.video_metadata_model import VideoMetadataModel
from apps.core.operations.video_metadata_repository import VideoMetadataRepository


@pytest.fixture
def mock_db_session_async() -> AsyncMock:
    """Create a mock SQLAlchemy AsyncSession."""
    mock_session = AsyncMock(spec=AsyncSession)
    # Specific chain setup (execute, scalars, first) will be done in tests as needed.
    return mock_session


@pytest.fixture
def sample_metadata_data():
    """Sample video metadata for testing."""
    return {
        "job_id": 42,
        "title": "Test Video Title",
        "description": "A test video description",
        "tags": ["test", "video", "unittest"],
        "transcript_text": "This is a test transcript.",
        "extracted_video_duration_seconds": 120.5,
        "extracted_video_resolution": "1920x1080",
    }


class TestVideoMetadataRepository:
    """Test cases for the VideoMetadataRepository class."""

    @pytest.mark.asyncio
    async def test_create_new_metadata(
        self, mock_db_session_async: AsyncMock, sample_metadata_data
    ):
        """Test creating new metadata when none exists."""
        # Configure the mock for the internal get_by_job_id call to return None
        mock_execute_result_get = AsyncMock()
        mock_scalars_result_get = AsyncMock()
        mock_db_session_async.execute.return_value = (
            mock_execute_result_get  # This will be for the get call
        )
        mock_execute_result_get.scalars.return_value = mock_scalars_result_get
        mock_scalars_result_get.first.return_value = None  # Simulate metadata not found

        # Call the repository method (which is now async)
        metadata = await VideoMetadataRepository.create_or_update(
            mock_db_session_async,  # Use the async mock session
            job_id=sample_metadata_data["job_id"],
            title=sample_metadata_data["title"],
            description=sample_metadata_data["description"],
        )

        # Verify the metadata model was created correctly
        assert isinstance(metadata, VideoMetadataModel)
        assert metadata.job_id == sample_metadata_data["job_id"]
        assert metadata.title == sample_metadata_data["title"]
        assert metadata.description == sample_metadata_data["description"]
        assert metadata.tags is None  # Not provided in this call
        assert metadata.transcript_text is None  # Not provided

        # Verify session operations
        # execute was called once for the initial get_by_job_id check
        mock_db_session_async.execute.assert_awaited_once()
        mock_db_session_async.add.assert_called_once()  # metadata instance passed to add
        mock_db_session_async.flush.assert_awaited_once()
        # Assuming create_or_update also refreshes the new instance
        mock_db_session_async.refresh.assert_awaited_once_with(metadata)

    @pytest.mark.asyncio
    async def test_update_existing_metadata(
        self, mock_db_session_async: AsyncMock, sample_metadata_data
    ):
        """Test updating existing metadata."""
        existing_metadata_instance = VideoMetadataModel(
            job_id=sample_metadata_data["job_id"],
            title="Old Title",
            description="Old Description",
        )

        # Configure the mock for the internal get_by_job_id call to return existing_metadata_instance
        mock_execute_result_get = AsyncMock()
        mock_scalars_result_get = AsyncMock()
        # Reset execute mock if it's the same instance from a previous test/setup in a class
        # For function-scoped mock_db_session_async, it's a fresh mock each time.
        mock_db_session_async.execute.return_value = mock_execute_result_get
        mock_execute_result_get.scalars.return_value = mock_scalars_result_get
        mock_scalars_result_get.first.return_value = existing_metadata_instance

        new_title = "Updated Title"
        new_tags = ["updated", "tags"]
        updated_metadata = await VideoMetadataRepository.create_or_update(
            mock_db_session_async,
            job_id=sample_metadata_data["job_id"],
            title=new_title,
            tags=new_tags,
        )

        # Verify the metadata was updated correctly
        assert updated_metadata is existing_metadata_instance
        assert updated_metadata.title == new_title
        assert updated_metadata.tags == new_tags
        assert updated_metadata.description == "Old Description"  # Unchanged

        # Verify session operations
        mock_db_session_async.execute.assert_awaited_once()  # For get_by_job_id
        mock_db_session_async.add.assert_not_called()  # No new object added
        mock_db_session_async.flush.assert_awaited_once()
        mock_db_session_async.refresh.assert_awaited_once_with(
            existing_metadata_instance
        )

    @pytest.mark.asyncio
    async def test_update_multiple_fields(
        self, mock_db_session_async: AsyncMock, sample_metadata_data
    ):
        """Test updating multiple fields at once."""
        existing_metadata_instance = VideoMetadataModel(
            job_id=sample_metadata_data["job_id"],
        )

        # Mock the internal get_by_job_id call
        mock_execute_result_get = AsyncMock()
        mock_scalars_result_get = AsyncMock()
        mock_db_session_async.execute.return_value = mock_execute_result_get
        mock_execute_result_get.scalars.return_value = mock_scalars_result_get
        mock_scalars_result_get.first.return_value = existing_metadata_instance

        # Call the repository method with multiple fields
        updated_metadata = await VideoMetadataRepository.create_or_update(
            mock_db_session_async,
            job_id=sample_metadata_data["job_id"],
            **sample_metadata_data,  # Update with all fields from the fixture
        )

        assert updated_metadata is existing_metadata_instance
        for key, value in sample_metadata_data.items():
            # job_id is part of the key for lookup, not an updatable field by kwargs in this manner usually
            # if key != "job_id":
            # The create_or_update likely iterates through kwargs and sets them.
            assert getattr(updated_metadata, key) == value

        mock_db_session_async.execute.assert_awaited_once()
        mock_db_session_async.add.assert_not_called()
        mock_db_session_async.flush.assert_awaited_once()
        mock_db_session_async.refresh.assert_awaited_once_with(
            existing_metadata_instance
        )

    @pytest.mark.asyncio
    async def test_get_by_job_id_found(
        self, mock_db_session_async: AsyncMock, sample_metadata_data
    ):
        """Test retrieving metadata by job_id when it exists."""
        mock_metadata_instance = VideoMetadataModel(**sample_metadata_data)  # type: ignore

        mock_execute_result = AsyncMock()
        mock_scalars_result = AsyncMock()
        mock_db_session_async.execute.return_value = mock_execute_result
        mock_execute_result.scalars.return_value = mock_scalars_result
        mock_scalars_result.first.return_value = mock_metadata_instance

        metadata = await VideoMetadataRepository.get_by_job_id(
            mock_db_session_async, sample_metadata_data["job_id"]
        )

        assert metadata is mock_metadata_instance
        if metadata:  # Guard for linter
            assert metadata.job_id == sample_metadata_data["job_id"]

        mock_db_session_async.execute.assert_awaited_once()

    @pytest.mark.asyncio
    async def test_get_by_job_id_not_found(self, mock_db_session_async: AsyncMock):
        """Test retrieving metadata by job_id when it doesn't exist."""
        mock_execute_result = AsyncMock()
        mock_scalars_result = AsyncMock()
        mock_db_session_async.execute.return_value = mock_execute_result
        mock_execute_result.scalars.return_value = mock_scalars_result
        mock_scalars_result.first.return_value = None  # Simulate not found

        metadata = await VideoMetadataRepository.get_by_job_id(
            mock_db_session_async, 999
        )

        assert metadata is None
        mock_db_session_async.execute.assert_awaited_once()

    @pytest.mark.asyncio
    async def test_create_with_empty_kwargs(self, mock_db_session_async: AsyncMock):
        """Test creating metadata with only job_id."""
        # Mock the internal get_by_job_id call to return None
        mock_execute_result_get = AsyncMock()
        mock_scalars_result_get = AsyncMock()
        mock_db_session_async.execute.return_value = mock_execute_result_get
        mock_execute_result_get.scalars.return_value = mock_scalars_result_get
        mock_scalars_result_get.first.return_value = None  # No existing metadata

        job_id_val = 123
        metadata = await VideoMetadataRepository.create_or_update(
            mock_db_session_async,
            job_id=job_id_val,
            # No other kwargs passed
        )

        assert isinstance(metadata, VideoMetadataModel)
        if metadata:  # Guard for linter
            assert metadata.job_id == job_id_val
            assert metadata.title is None
            assert metadata.description is None
            assert metadata.tags is None

        mock_db_session_async.execute.assert_awaited_once()
        mock_db_session_async.add.assert_called_once()  # With the new metadata instance
        mock_db_session_async.flush.assert_awaited_once()
        if metadata:  # metadata will not be None here due to creation path
            mock_db_session_async.refresh.assert_awaited_once_with(metadata)
</file>

<file path="apps/core/tests/unit/operations/test_video_repository.py">
"""
Unit tests for the VideoRepository.
"""

from unittest.mock import AsyncMock, MagicMock, patch

import pytest
from sqlalchemy.ext.asyncio import AsyncSession

from apps.core.models.video_model import VideoModel
from apps.core.operations.video_repository import VideoRepository


@pytest.fixture
def mock_db_session_async() -> AsyncMock:
    """Create a mock SQLAlchemy AsyncSession."""
    mock_session = AsyncMock(spec=AsyncSession)
    return mock_session


@pytest.fixture
def sample_video_data():
    """Sample video data for testing."""
    return {
        "uploader_user_id": "test-user-123",
        "original_filename": "test_video.mp4",
        "storage_path": "uploads/test-user-123/abc123.mp4",
        "content_type": "video/mp4",
        "size_bytes": 1024000,
    }


class TestVideoRepository:
    """Test cases for the VideoRepository class."""

    @pytest.mark.asyncio
    async def test_create(self, mock_db_session_async: AsyncMock, sample_video_data):
        """Test creating a new video."""
        video = await VideoRepository.create(mock_db_session_async, **sample_video_data)

        assert isinstance(video, VideoModel)
        assert video.uploader_user_id == sample_video_data["uploader_user_id"]
        assert video.original_filename == sample_video_data["original_filename"]
        assert video.storage_path == sample_video_data["storage_path"]
        assert video.content_type == sample_video_data["content_type"]
        assert video.size_bytes == sample_video_data["size_bytes"]

        mock_db_session_async.add.assert_called_once_with(video)
        mock_db_session_async.flush.assert_awaited_once()
        mock_db_session_async.refresh.assert_awaited_once_with(video)

    @pytest.mark.asyncio
    async def test_get_by_id_found(
        self, mock_db_session_async: AsyncMock, sample_video_data
    ):
        """Test retrieving a video by ID when it exists."""
        mock_video_instance = VideoModel(**sample_video_data)
        mock_video_instance.id = 123

        mock_execute_result = AsyncMock()
        mock_scalars_result = AsyncMock()
        mock_db_session_async.execute.return_value = mock_execute_result
        mock_execute_result.scalars.return_value = mock_scalars_result
        mock_scalars_result.first.return_value = mock_video_instance

        video = await VideoRepository.get_by_id(mock_db_session_async, 123)

        assert video is mock_video_instance
        if video:
            assert video.id == 123
        mock_db_session_async.execute.assert_awaited_once()

    @pytest.mark.asyncio
    async def test_get_by_id_not_found(self, mock_db_session_async: AsyncMock):
        """Test retrieving a video by ID when it doesn't exist."""
        mock_execute_result = AsyncMock()
        mock_scalars_result = AsyncMock()
        mock_db_session_async.execute.return_value = mock_execute_result
        mock_execute_result.scalars.return_value = mock_scalars_result
        mock_scalars_result.first.return_value = None

        video = await VideoRepository.get_by_id(mock_db_session_async, 999)

        assert video is None
        mock_db_session_async.execute.assert_awaited_once()

    @pytest.mark.asyncio
    async def test_db_error_handling(
        self, mock_db_session_async: AsyncMock, sample_video_data
    ):
        """Test handling of database errors during create's flush."""
        mock_db_session_async.flush = AsyncMock(side_effect=Exception("Database error"))

        with pytest.raises(Exception) as excinfo:
            await VideoRepository.create(mock_db_session_async, **sample_video_data)

        assert "Database error" in str(excinfo.value)
        mock_db_session_async.add.assert_called_once()
        mock_db_session_async.flush.assert_awaited_once()
</file>

<file path="apps/core/tests/unit/services/test_user_service.py">
"""
Unit tests for the UserService.
"""

from unittest.mock import AsyncMock, MagicMock, patch

import pytest
from fastapi import HTTPException
from sqlalchemy.ext.asyncio import AsyncSession

from apps.core.lib.auth.supabase_auth import AuthenticatedUser
from apps.core.models.user_model import User
from apps.core.operations.user_repository import UserRepository
from apps.core.services.user_service import UserService


@pytest.fixture
def mock_user_repository_async() -> AsyncMock:
    """Create a mock async UserRepository."""
    return AsyncMock(spec=UserRepository)


@pytest.fixture
def user_service_async(mock_user_repository_async: AsyncMock) -> UserService:
    """Create a UserService instance with mock async dependencies."""
    return UserService(user_repository=mock_user_repository_async)


@pytest.fixture
def mock_db_async() -> AsyncMock:
    """Mock SQLAlchemy AsyncSession."""
    return AsyncMock(spec=AsyncSession)


@pytest.fixture
def sample_user_data() -> dict:
    """Sample user data for testing."""
    return {
        "id": 1,
        "username": "testuser",
        "email": "test@example.com",
        "full_name": "Test User",
        "is_active": True,
        "supabase_auth_id": "some-auth-id-123",
    }


@pytest.fixture
def sample_user_async(sample_user_data: dict) -> AsyncMock:
    """Create a sample async mock User model instance."""
    user = AsyncMock(spec=User)
    for key, value in sample_user_data.items():
        setattr(user, key, value)
    user.hashed_password = "hashed_password"
    return user


@pytest.fixture
def auth_user() -> AuthenticatedUser:
    """Create a sample AuthenticatedUser for get_or_create tests."""
    return AuthenticatedUser(
        id="some-auth-id-123",
        email="test@example.com",
        aud="authenticated",
    )


class TestUserService:
    """Test cases for the UserService class."""

    @pytest.mark.asyncio
    async def test_get_user_profile_found(
        self,
        user_service_async: UserService,
        mock_user_repository_async: AsyncMock,
        sample_user_async: AsyncMock,
    ):
        """Test getting a user profile when the user exists."""
        test_user_id = sample_user_async.id
        mock_user_repository_async.get_user.return_value = sample_user_async

        user_profile_dict = await user_service_async.get_user_profile(test_user_id)

        assert user_profile_dict is not None
        if user_profile_dict:
            assert user_profile_dict["id"] == sample_user_async.id
            assert user_profile_dict["username"] == sample_user_async.username
            assert user_profile_dict["email"] == sample_user_async.email
            assert user_profile_dict["full_name"] == sample_user_async.full_name
            assert user_profile_dict["is_active"] == sample_user_async.is_active
            assert "hashed_password" not in user_profile_dict

        mock_user_repository_async.get_user.assert_awaited_once_with(test_user_id)

    @pytest.mark.asyncio
    async def test_get_user_profile_not_found(
        self, user_service_async: UserService, mock_user_repository_async: AsyncMock
    ):
        """Test getting a user profile when the user doesn't exist."""
        test_user_id = 999
        mock_user_repository_async.get_user.return_value = None

        with pytest.raises(HTTPException) as excinfo:
            await user_service_async.get_user_profile(test_user_id)

        assert excinfo.value.status_code == 404
        assert excinfo.value.detail == "User not found"
        mock_user_repository_async.get_user.assert_awaited_once_with(test_user_id)

    @pytest.mark.asyncio
    async def test_get_or_create_user_profile_existing(
        self,
        user_service_async: UserService,
        mock_user_repository_async: AsyncMock,
        sample_user_async: AsyncMock,
        auth_user: AuthenticatedUser,
    ):
        """Test getting an existing user profile via get_or_create_user_profile."""
        mock_user_repository_async.get_user_by_email.return_value = sample_user_async

        result_user = await user_service_async.get_or_create_user_profile(auth_user)

        assert result_user is sample_user_async
        mock_user_repository_async.get_user_by_email.assert_awaited_once_with(
            auth_user.email
        )
        mock_user_repository_async.create_user.assert_not_awaited()  # Should not be called if user exists

    @pytest.mark.asyncio
    async def test_get_or_create_user_profile_new(
        self,
        user_service_async: UserService,
        mock_user_repository_async: AsyncMock,
        sample_user_async: AsyncMock,
        auth_user: AuthenticatedUser,
    ):
        """Test creating a new user profile via get_or_create_user_profile."""
        mock_user_repository_async.get_user_by_email.return_value = (
            None  # Simulate user not found by email
        )
        mock_user_repository_async.create_user.return_value = (
            sample_user_async  # Mock the created user
        )

        result_user = await user_service_async.get_or_create_user_profile(auth_user)

        assert result_user is sample_user_async
        mock_user_repository_async.get_user_by_email.assert_awaited_once_with(
            auth_user.email
        )
        mock_user_repository_async.create_user.assert_awaited_once()

        # Verify the data passed to create_user (it's the first positional arg to the mock)
        # call_args[0] is for positional args, call_args[1] for kwargs
        # create_user in repo takes (user_data: dict)
        created_user_data_arg = mock_user_repository_async.create_user.await_args[0][0]
        assert created_user_data_arg["email"] == auth_user.email
        assert (
            auth_user.email is not None
        )  # Ensure email is not None before split for linter
        assert created_user_data_arg["username"] == auth_user.email.split("@")[0]
        assert created_user_data_arg["is_active"] is True
        # Assuming supabase_auth_id is also set, if that's part of User model and create logic
        # If UserService itself sets supabase_auth_id, it should be checked here.
        # The current UserRepo.create_user takes user_data dict, so it depends on what UserService puts in that dict.
        # The service logic for username generation is: auth_user.email.split("@")[0]
        # The service logic for other fields might be specific. For example, hashed_password = "" in the service.
        assert created_user_data_arg["hashed_password"] == ""

    @pytest.mark.asyncio
    async def test_get_or_create_user_profile_no_email(
        self, user_service_async: UserService, mock_user_repository_async: AsyncMock
    ):
        """Test handling when auth_user has no email."""
        # Create auth user without email
        auth_user_no_email = AuthenticatedUser(
            id="auth123",
            email=None,
            aud="authenticated",
        )

        # Call the service method and expect exception
        with pytest.raises(HTTPException) as excinfo:
            await user_service_async.get_or_create_user_profile(auth_user_no_email)

        # Verify exception
        assert excinfo.value.status_code == 400
        assert excinfo.value.detail == "Authenticated user missing email"

        # Verify repository not called
        mock_user_repository_async.get_user_by_email.assert_not_awaited()

    @pytest.mark.asyncio
    async def test_get_or_create_user_profile_with_fallback_username(
        self,
        user_service_async: UserService,
        mock_user_repository_async: AsyncMock,
        sample_user_async: AsyncMock,
    ):
        """Test creating a user with fallback username when email has no @ symbol."""
        # Create auth user with unusual email (no @)
        auth_user_unusual_email = AuthenticatedUser(
            id="auth123",
            email="invalid-email",
            aud="authenticated",
        )

        # Set up mock returns
        mock_user_repository_async.get_user_by_email.return_value = None
        mock_user_repository_async.create_user.return_value = sample_user_async

        # Call the service method
        result = await user_service_async.get_or_create_user_profile(
            auth_user_unusual_email
        )

        # Verify results
        assert result is sample_user_async

        # Verify repository calls
        mock_user_repository_async.get_user_by_email.assert_awaited_once_with(
            "invalid-email"
        )
        mock_user_repository_async.create_user.assert_awaited_once()

        # Verify the created user data uses fallback username
        created_user_data = mock_user_repository_async.create_user.await_args[0][0]
        assert created_user_data["username"] == f"user_{auth_user_unusual_email.id}"
        assert created_user_data["email"] == "invalid-email"

    @pytest.mark.asyncio
    async def test_create_user_success(
        self,
        user_service_async: UserService,
        mock_user_repository_async: AsyncMock,
        sample_user_async: AsyncMock,
        sample_user_data: dict,
    ):
        """Test creating a new user successfully."""
        # Set up mock returns for validation checks
        mock_user_repository_async.get_user_by_email.return_value = None
        mock_user_repository_async.get_user_by_username.return_value = None
        mock_user_repository_async.create_user.return_value = sample_user_async

        # Remove id as it's not part of input data
        user_input_dict = sample_user_data.copy()
        # Ensure we are creating a new user, so ID from sample_user_data (which implies existing) should not be used.
        # UserService.create_user expects a dict. Let's simulate the Pydantic model or dict it might receive.
        user_input_for_create = {
            "username": user_input_dict["username"],
            "email": user_input_dict["email"],
            "full_name": user_input_dict.get(
                "full_name"
            ),  # Use .get for optional fields
            "hashed_password": "hashed_password",  # create_user service method expects this
            "is_active": user_input_dict.get(
                "is_active", True
            ),  # Default if not in sample
            "supabase_auth_id": user_input_dict.get("supabase_auth_id"),
        }

        # Call the service method
        result_dict = await user_service_async.create_user(user_input_for_create)

        # Verify results
        assert result_dict["id"] == sample_user_async.id
        assert result_dict["username"] == sample_user_async.username
        assert result_dict["email"] == sample_user_async.email
        assert result_dict["full_name"] == sample_user_async.full_name
        assert result_dict["is_active"] == sample_user_async.is_active
        assert "hashed_password" not in result_dict  # Sensitive info excluded

        # Verify repository calls
        mock_user_repository_async.get_user_by_email.assert_awaited_once_with(
            user_input_for_create["email"]
        )
        mock_user_repository_async.get_user_by_username.assert_awaited_once_with(
            user_input_for_create["username"]
        )
        # Verify create_user was called with the correct structure
        # The service method constructs user_create_dict then passes its .model_dump() (if pydantic)
        # or the dict itself to repository's create_user.
        # Assuming user_input_for_create is what's ultimately passed (or its equivalent after Pydantic processing)
        mock_user_repository_async.create_user.assert_awaited_once_with(
            user_input_for_create
        )

    @pytest.mark.asyncio
    async def test_create_user_email_exists(
        self,
        user_service_async: UserService,
        mock_user_repository_async: AsyncMock,
        sample_user_async: AsyncMock,
        sample_user_data: dict,
    ):
        """Test creating a user when the email already exists."""
        # Set up mock returns for validation checks
        mock_user_repository_async.get_user_by_email.return_value = (
            sample_user_async  # Email exists
        )
        mock_user_repository_async.get_user_by_username.return_value = None

        user_input_dict = sample_user_data.copy()
        user_input_for_create = {
            "username": "newusername",  # Different username
            "email": user_input_dict["email"],  # Existing email
            "full_name": user_input_dict.get("full_name"),
            "hashed_password": "hashed_password",
            "is_active": user_input_dict.get("is_active", True),
            "supabase_auth_id": user_input_dict.get("supabase_auth_id"),
        }

        # Call the service method and expect exception
        with pytest.raises(HTTPException) as excinfo:
            await user_service_async.create_user(user_input_for_create)

        # Verify exception
        assert excinfo.value.status_code == 400
        assert excinfo.value.detail == "Email already registered"

        # Verify repository calls
        mock_user_repository_async.get_user_by_email.assert_awaited_once_with(
            user_input_for_create["email"]
        )
        mock_user_repository_async.get_user_by_username.assert_not_awaited()  # Should not be called if email check fails first
        mock_user_repository_async.create_user.assert_not_awaited()

    @pytest.mark.asyncio
    async def test_create_user_username_exists(
        self,
        user_service_async: UserService,
        mock_user_repository_async: AsyncMock,
        sample_user_async: AsyncMock,
        sample_user_data: dict,
    ):
        """Test creating a user when the username already exists."""
        # Set up mock returns for validation checks
        mock_user_repository_async.get_user_by_email.return_value = (
            None  # Email is unique
        )
        mock_user_repository_async.get_user_by_username.return_value = (
            sample_user_async  # Username exists
        )

        user_input_dict = sample_user_data.copy()
        user_input_for_create = {
            "username": user_input_dict["username"],  # Existing username
            "email": "newemail@example.com",  # Different email
            "full_name": user_input_dict.get("full_name"),
            "hashed_password": "hashed_password",
            "is_active": user_input_dict.get("is_active", True),
            "supabase_auth_id": user_input_dict.get("supabase_auth_id"),
        }

        # Call the service method and expect exception
        with pytest.raises(HTTPException) as excinfo:
            await user_service_async.create_user(user_input_for_create)

        # Verify exception
        assert excinfo.value.status_code == 400
        assert excinfo.value.detail == "Username already registered"

        # Verify repository calls
        mock_user_repository_async.get_user_by_email.assert_awaited_once_with(
            user_input_for_create["email"]
        )
        mock_user_repository_async.get_user_by_username.assert_awaited_once_with(
            user_input_for_create["username"]
        )
        mock_user_repository_async.create_user.assert_not_awaited()

    @pytest.mark.asyncio
    async def test_create_user_missing_email(
        self, user_service_async: UserService, mock_user_repository_async: AsyncMock
    ):
        """Test creating a user with missing email (should be caught by Pydantic model if UserCreate schema is used)."""
        user_input_for_create = {
            "username": "someuser",
            # "email": "missing@example.com", # Email is missing
            "full_name": "Some User",
            "hashed_password": "hashed_password",
        }
        # Depending on how UserService.create_user handles input (e.g. if it uses a Pydantic model UserCreate for validation)
        # this might raise a Pydantic ValidationError or an HTTPException if the service catches it.
        # For this test, let's assume the service raises HTTPException for simplicity if basic fields are missing,
        # or Pydantic ValidationError if it uses a model.
        # If UserService create_user uses a Pydantic model UserCreate that requires email:
        # from pydantic import ValidationError
        # with pytest.raises(ValidationError): # Or HTTPException if service translates it
        #     await user_service_async.create_user(user_input_for_create)

        # If the service's create_user method itself checks for 'email' in the dict:
        with pytest.raises(
            HTTPException
        ) as excinfo:  # Or TypeError if dict access fails like user_data["email"]
            await user_service_async.create_user(user_input_for_create)  # Pass the dict

        # This assertion depends on the actual error raised by create_user for missing fields.
        # If UserCreate Pydantic model is used, it would be a ValidationError.
        # If the service manually checks and raises HTTPException:
        assert (
            excinfo.value.status_code == 422
        )  # Unprocessable Entity (typical for validation errors)
        # Or if a KeyError/TypeError happens before validation and is not caught:
        # assert isinstance(excinfo.value, (KeyError, TypeError))

        mock_user_repository_async.create_user.assert_not_awaited()

    @pytest.mark.asyncio
    async def test_create_user_missing_username(
        self, user_service_async: UserService, mock_user_repository_async: AsyncMock
    ):
        """Test creating a user with missing username."""
        user_input_for_create = {
            # "username": "someuser", # Username is missing
            "email": "test@example.com",
            "full_name": "Some User",
            "hashed_password": "hashed_password",
        }

        with pytest.raises(
            HTTPException
        ) as excinfo:  # Assuming HTTPException for validation
            await user_service_async.create_user(user_input_for_create)

        assert excinfo.value.status_code == 422  # Unprocessable Entity
        mock_user_repository_async.create_user.assert_not_awaited()
</file>

<file path="apps/core/tests/unit/services/test_video_processing_service.py">
"""
Unit tests for the VideoProcessingService.
"""

import os
from unittest.mock import AsyncMock, MagicMock, Mock, call, patch

import pytest
from fastapi import BackgroundTasks, HTTPException

# Assuming AsyncSession is needed for type hints if db mocks are AsyncSession
from sqlalchemy.ext.asyncio import AsyncSession

from apps.core.core.exceptions import VideoProcessingError
from apps.core.models.enums import ProcessingStatus
from apps.core.models.video_job_model import VideoJobModel
from apps.core.models.video_metadata_model import VideoMetadataModel
from apps.core.models.video_model import VideoModel
from apps.core.services.video_processing_service import VideoProcessingService


@pytest.fixture
def mock_dependencies():
    """Create mock dependencies for the VideoProcessingService."""
    return {
        "video_repo": AsyncMock(),  # Repositories are now async
        "job_repo": AsyncMock(),  # Repositories are now async
        "metadata_repo": AsyncMock(),  # Repositories are now async
        "storage": AsyncMock(),  # Already AsyncMock
        "ai_adapter": AsyncMock(),  # Already AsyncMock
        # Assuming these utils remain synchronous. If any method called by service is async, these would need adjustment.
        "ffmpeg_utils": MagicMock(),
        "subtitle_utils": MagicMock(),
        "file_utils": MagicMock(),
    }


@pytest.fixture
def service(mock_dependencies):
    """Create a VideoProcessingService instance with mock dependencies."""
    return VideoProcessingService(
        video_repo=mock_dependencies["video_repo"],
        job_repo=mock_dependencies["job_repo"],
        metadata_repo=mock_dependencies["metadata_repo"],
        storage=mock_dependencies["storage"],
        ai_adapter=mock_dependencies["ai_adapter"],
        ffmpeg_utils=mock_dependencies["ffmpeg_utils"],
        subtitle_utils=mock_dependencies["subtitle_utils"],
        file_utils=mock_dependencies["file_utils"],
    )


@pytest.fixture
def mock_db_async() -> AsyncMock:  # Renamed and returns AsyncMock
    """Mock SQLAlchemy AsyncSession."""
    return AsyncMock(spec=AsyncSession)


@pytest.fixture
def mock_background_tasks():
    """Mock FastAPI BackgroundTasks."""
    # BackgroundTasks itself is not async, its add_task method is sync.
    return MagicMock(spec=BackgroundTasks)


@pytest.fixture
def sample_video_data():
    """Sample video data for testing."""
    return {
        "original_filename": "test_video.mp4",
        "video_content": b"test video content",
        "content_type": "video/mp4",
        "uploader_user_id": "test-user-123",
    }


class TestVideoProcessingService:
    """Test cases for the VideoProcessingService class."""

    async def test_initiate_video_processing(
        self,
        service: VideoProcessingService,
        mock_db_async: AsyncMock,  # Use the async db mock
        mock_background_tasks: MagicMock,
        mock_dependencies: dict,
        sample_video_data: dict,
    ):
        """Test initiating video processing pipeline."""
        # Set up mock returns
        # storage.save_file is async, ensure it's awaited in service or its return is awaitable if mock is AsyncMock
        mock_dependencies[
            "storage"
        ].save_file.return_value = "gs://bucket/uploads/test-user-123/test_video.mp4"

        mock_video = AsyncMock(
            spec=VideoModel
        )  # Using AsyncMock if its attributes are accessed or methods called async
        mock_video.id = 1
        # repo.create is now async, so its mock should handle await or be an AsyncMock itself
        mock_dependencies["video_repo"].create.return_value = mock_video

        mock_job = AsyncMock(spec=VideoJobModel)
        mock_job.id = 42
        mock_dependencies["job_repo"].create.return_value = mock_job

        # Call the service method (already awaited)
        result = await service.initiate_video_processing(
            db=mock_db_async,  # Pass async_db_mock
            **sample_video_data,
            background_tasks=mock_background_tasks,
        )

        assert result is mock_job

        # storage.save_file is async
        mock_dependencies["storage"].save_file.assert_awaited_once_with(
            file_content=sample_video_data["video_content"],
            filename=sample_video_data["original_filename"],
            subdir=f"uploads/{sample_video_data['uploader_user_id']}",
        )

        # video_repo.create is async
        mock_dependencies["video_repo"].create.assert_awaited_once_with(
            db=mock_db_async,
            uploader_user_id=sample_video_data["uploader_user_id"],
            original_filename=sample_video_data["original_filename"],
            storage_path="gs://bucket/uploads/test-user-123/test_video.mp4",
            content_type=sample_video_data["content_type"],
            size_bytes=len(sample_video_data["video_content"]),
        )

        # job_repo.create is async
        mock_dependencies["job_repo"].create.assert_awaited_once_with(
            db=mock_db_async,
            video_id=mock_video.id,
            status=ProcessingStatus.PENDING,
            processing_stages=None,  # Assuming default
            error_message=None,  # Assuming default
        )

        # DB commit is async
        mock_db_async.commit.assert_awaited_once()

        # BackgroundTasks.add_task is synchronous
        mock_background_tasks.add_task.assert_called_once_with(
            service._execute_processing_pipeline,  # Target function
            mock_job.id,  # args for the target function
            "gs://bucket/uploads/test-user-123/test_video.mp4",  # args for the target function
        )

    @patch("apps.core.services.video_processing_service.AsyncSessionLocal")
    async def test_execute_processing_pipeline_success(
        self,
        mock_AsyncSessionLocal: MagicMock,  # Patched AsyncSessionLocal factory
        service: VideoProcessingService,
        mock_dependencies: dict,
    ):
        """Test successful execution of video processing pipeline."""
        # Setup mock for the async context manager produced by AsyncSessionLocal()
        mock_db_async_cm = AsyncMock()  # This is the context manager object
        mock_db_async_session = AsyncMock(
            spec=AsyncSession
        )  # This is the session yielded by __aenter__
        mock_AsyncSessionLocal.return_value = mock_db_async_cm
        mock_db_async_cm.__aenter__.return_value = mock_db_async_session

        # Mock job and related objects
        mock_video = AsyncMock(spec=VideoModel)  # Use AsyncMock for spec consistency
        mock_video.original_filename = "test_video.mp4"
        mock_video.uploader_user_id = "test-user-123"

        mock_job = AsyncMock(spec=VideoJobModel)
        mock_job.id = 42
        mock_job.video = mock_video  # Assuming direct attribute assignment for mock

        # job_repo.get_by_id is async
        mock_dependencies["job_repo"].get_by_id.return_value = mock_job

        # Mock file utilities (assuming these remain synchronous)
        mock_dependencies["file_utils"].create_temp_dir.return_value = "/tmp/test_dir"

        # Mock ffmpeg utilities (assuming sync)
        mock_dependencies["ffmpeg_utils"].get_video_metadata_sync.return_value = {
            "duration": 120.5,
            "resolution": "1920x1080",
            "format": "mp4",
        }

        # Mock AI adapter (methods are async)
        mock_dependencies[
            "ai_adapter"
        ].transcribe_audio.return_value = "This is a test transcript."
        mock_dependencies["ai_adapter"].generate_text.side_effect = [
            "Test Video Title",
            "Test video description.",
            "test, video, processing",
            "Show notes for the test video.",
        ]

        # Mock subtitle utils (assuming sync)
        mock_dependencies[
            "subtitle_utils"
        ].generate_vtt.return_value = (
            "WEBVTT\n\n00:00:00.000 --> 00:00:01.000\nThis is a test transcript."
        )
        mock_dependencies[
            "subtitle_utils"
        ].generate_srt.return_value = (
            "1\n00:00:00,000 --> 00:00:01,000\nThis is a test transcript."
        )

        # Mock storage.save_file (async)
        # Use a list for side_effect if called multiple times with different return values in sequence
        mock_dependencies["storage"].save_file.side_effect = [
            "gs://bucket/transcripts/test-user-123/transcript.txt",
            "gs://bucket/subtitles/test-user-123/subtitles.vtt",
            "gs://bucket/subtitles/test-user-123/subtitles.srt",
            "gs://bucket/thumbnails/test-user-123/thumbnail.jpg",
        ]

        mock_open = MagicMock()  # For patching builtins.open (sync)
        mock_file_content = MagicMock()
        mock_open.return_value.__enter__.return_value = mock_file_content
        mock_file_content.read.return_value = b"test thumbnail data"

        with patch("builtins.open", mock_open):
            await service._execute_processing_pipeline(
                42, "gs://bucket/uploads/test-user-123/test_video.mp4"
            )

        # --- Assertions ---

        # Step 0: Job status update to PROCESSING
        mock_dependencies["job_repo"].update_status.assert_any_await_with(
            mock_db_async_session, 42, ProcessingStatus.PROCESSING
        )

        # Step 1: Download video (if service._download_video is called and is async)
        # service._download_video might call storage.download_file_to_temp, which is async.
        # Assuming it's called, and storage.download_file_to_temp is the underlying async call.
        # This depends on the internal structure of _execute_processing_pipeline and _download_video.
        # For now, let's assume _download_video is called and it uses storage.download_file_to_temp.
        # If _download_video is a helper that orchestrates this, we might not mock storage.download directly here
        # unless _download_video itself is mocked. Given it's a private method, we test its effects.
        # The existing test doesn't mock/assert a download step explicitly other than file_utils.create_temp_dir.
        # So, we'll stick to what was previously being asserted or implied.
        mock_dependencies[
            "file_utils"
        ].create_temp_dir.assert_called_once()  # For local processing

        # Step 2: Metadata extraction (ffmpeg - sync)
        mock_dependencies["ffmpeg_utils"].get_video_metadata_sync.assert_called_once()
        mock_dependencies["metadata_repo"].create_or_update.assert_any_await_with(
            mock_db_async_session,
            42,
            extracted_video_duration_seconds=120.5,
            extracted_video_resolution="1920x1080",
            extracted_video_format="mp4",
        )

        # Step 3: Audio extraction (ffmpeg - sync) and transcription (AI - async)
        mock_dependencies[
            "ffmpeg_utils"
        ].extract_audio_sync.assert_called_once()  # Sync
        mock_dependencies["ai_adapter"].transcribe_audio.assert_awaited_once()  # Async
        #   Save transcript text (storage.save_file - async)
        mock_dependencies["storage"].save_file.assert_any_await_with(
            file_content="This is a test transcript.",
            filename="transcript.txt",
            subdir=f"transcripts/{mock_video.uploader_user_id}",
            content_type="text/plain",
        )
        mock_dependencies["metadata_repo"].create_or_update.assert_any_await_with(
            mock_db_async_session,
            42,
            transcript_text="This is a test transcript.",
            transcript_file_url="gs://bucket/transcripts/test-user-123/transcript.txt",  # From save_file side_effect
        )

        # Step 4: Content metadata generation (AI - async)
        assert mock_dependencies["ai_adapter"].generate_text.await_count == 4
        mock_dependencies["metadata_repo"].create_or_update.assert_any_await_with(
            mock_db_async_session,
            42,
            title="Test Video Title",
            description="Test video description.",
            tags=["test", "video", "processing"],
            # show_notes_text="Show notes for the test video.", # If this is a field
        )

        # Step 5: Subtitles generation (sync) and saving (async)
        mock_dependencies["subtitle_utils"].generate_vtt.assert_called_once()
        mock_dependencies["subtitle_utils"].generate_srt.assert_called_once()
        #   Save VTT (storage.save_file - async)
        mock_dependencies["storage"].save_file.assert_any_await_with(
            file_content="WEBVTT\n\n00:00:00.000 --> 00:00:01.000\nThis is a test transcript.",
            filename="subtitles.vtt",
            subdir=f"subtitles/{mock_video.uploader_user_id}",
            content_type="text/vtt",
        )
        #   Save SRT (storage.save_file - async)
        mock_dependencies["storage"].save_file.assert_any_await_with(
            file_content="1\n00:00:00,000 --> 00:00:01,000\nThis is a test transcript.",
            filename="subtitles.srt",
            subdir=f"subtitles/{mock_video.uploader_user_id}",
            content_type="application/x-subrip",
        )
        mock_dependencies["metadata_repo"].create_or_update.assert_any_await_with(
            mock_db_async_session,
            42,
            subtitle_files_urls={
                "vtt": "gs://bucket/subtitles/test-user-123/subtitles.vtt",  # From save_file side_effect
                "srt": "gs://bucket/subtitles/test-user-123/subtitles.srt",  # From save_file side_effect
            },
        )

        # Step 6: Thumbnail extraction (ffmpeg - sync) and saving (async)
        # Assuming extract_frame_sync takes the local video path and output path
        # The exact args depend on how ffmpeg_utils.extract_frame_sync is structured.
        # Let's assume it takes an output path like "/tmp/test_dir/thumbnail.jpg"
        mock_dependencies["ffmpeg_utils"].extract_frame_sync.assert_called_once()
        #   Patching builtins.open was done to simulate reading this thumbnail file.
        #   Save thumbnail (storage.save_file - async)
        mock_dependencies["storage"].save_file.assert_any_await_with(
            file_content=b"test thumbnail data",  # From the mock_file.read.return_value
            filename="thumbnail.jpg",
            subdir=f"thumbnails/{mock_video.uploader_user_id}",
            content_type="image/jpeg",
        )
        mock_dependencies["metadata_repo"].create_or_update.assert_any_await_with(
            mock_db_async_session,
            42,
            thumbnail_file_url="gs://bucket/thumbnails/test-user-123/thumbnail.jpg",  # From save_file side_effect
        )

        # Final step: Mark job as completed
        mock_dependencies["job_repo"].update_status.assert_any_await_with(
            mock_db_async_session, 42, ProcessingStatus.COMPLETED
        )

        # DB Commit at the end of successful pipeline
        mock_db_async_session.commit.assert_awaited_once()

        # Cleanup local temp files (sync)
        mock_dependencies["file_utils"].remove_dir.assert_called_once_with(
            "/tmp/test_dir"
        )

    @patch("apps.core.services.video_processing_service.AsyncSessionLocal")
    async def test_execute_processing_pipeline_error(
        self,
        mock_AsyncSessionLocal: MagicMock,  # Patched AsyncSessionLocal factory
        service: VideoProcessingService,
        mock_dependencies: dict,
    ):
        """Test handling of errors in the processing pipeline."""
        # Setup mock for the async context manager produced by AsyncSessionLocal()
        mock_db_async_cm = AsyncMock()  # This is the context manager object
        mock_db_async_session = AsyncMock(
            spec=AsyncSession
        )  # This is the session yielded by __aenter__
        mock_AsyncSessionLocal.return_value = mock_db_async_cm
        mock_db_async_cm.__aenter__.return_value = mock_db_async_session

        # Mock job and related objects
        mock_video = AsyncMock(spec=VideoModel)
        mock_video.original_filename = "test_video.mp4"
        mock_video.uploader_user_id = "test-user-123"

        mock_job = AsyncMock(spec=VideoJobModel)
        mock_job.id = 42
        mock_job.video = mock_video

        # job_repo.get_by_id is async
        mock_dependencies["job_repo"].get_by_id.return_value = mock_job

        # Mock file utilities
        mock_dependencies["file_utils"].create_temp_dir.return_value = "/tmp/test_dir"

        # Set up an error during processing
        mock_dependencies[
            "ffmpeg_utils"
        ].get_video_metadata_sync.side_effect = Exception("FFmpeg error")

        # Call the method
        await service._execute_processing_pipeline(
            42, "gs://bucket/uploads/test-user-123/test_video.mp4"
        )

        # Verify error handling
        mock_dependencies["job_repo"].update_status.assert_any_call(
            mock_db_async_session, 42, ProcessingStatus.FAILED
        )
        mock_dependencies["job_repo"].add_processing_stage.assert_awaited_once_with(
            mock_db_async_session, 42, "Error: FFmpeg error"
        )

        # Verify cleanup still happens
        mock_dependencies["file_utils"].remove_dir.assert_called_once_with(
            "/tmp/test_dir"
        )
        mock_db_async_session.commit.assert_awaited_once()

    @patch("apps.core.services.video_processing_service.AsyncSessionLocal")
    async def test_execute_processing_pipeline_job_not_found(
        self,
        mock_AsyncSessionLocal: MagicMock,
        service: VideoProcessingService,
        mock_dependencies: dict,
    ):
        """Test pipeline behavior when the initial job ID is not found."""
        mock_db_async_cm = AsyncMock()
        mock_db_async_session = AsyncMock(spec=AsyncSession)
        mock_AsyncSessionLocal.return_value = mock_db_async_cm
        mock_db_async_cm.__aenter__.return_value = mock_db_async_session

        # Simulate job_repo.get_by_id returning None
        mock_dependencies["job_repo"].get_by_id.return_value = None

        # Temp dir might still be created before job is fetched, or not, depending on service logic.
        # If it is, we need to mock its creation and assert its removal.
        temp_dir_path = "/tmp/job_not_found_dir"
        mock_dependencies["file_utils"].create_temp_dir.return_value = temp_dir_path

        job_id_not_found = 999
        video_url = "gs://bucket/uploads/some_video.mp4"

        await service._execute_processing_pipeline(job_id_not_found, video_url)

        # Verify that no processing steps were attempted if job not found early.
        # For example, ffmpeg_utils should not have been called.
        mock_dependencies["ffmpeg_utils"].extract_audio_sync.assert_not_called()
        mock_dependencies["ffmpeg_utils"].get_video_metadata_sync.assert_not_called()
        mock_dependencies["ai_adapter"].transcribe_audio.assert_not_awaited()

        # Verify no status updates were attempted for a non-existent job ID on job_repo.
        # If job_repo.update_status was called with job_id_not_found, it would likely fail or be a no-op.
        # We are checking that the service logic bails out before trying to update status.
        # Count calls to job_repo.update_status with specific non-existent job_id
        update_status_calls = [
            call_args
            for call_args in mock_dependencies["job_repo"].update_status.await_args_list
            if call_args[0][1] == job_id_not_found  # args[0] is db, args[1] is job_id
        ]
        assert len(update_status_calls) == 0

        # Check that commit/rollback were not called as no db changes should have been made related to this job.
        mock_db_async_session.commit.assert_not_awaited()
        mock_db_async_session.rollback.assert_not_awaited()

        # Check if temp dir cleanup was still called if it was created.
        # This depends on the service's try/finally structure for temp dir management.
        # If create_temp_dir is called regardless, remove_dir should also be called.
        if mock_dependencies["file_utils"].create_temp_dir.called:
            mock_dependencies["file_utils"].remove_dir.assert_called_once_with(
                temp_dir_path
            )
        else:
            mock_dependencies["file_utils"].remove_dir.assert_not_called()

    async def test_get_job_details_found(
        self,
        service: VideoProcessingService,
        mock_db_async: AsyncMock,
        mock_dependencies: dict,
    ):
        """Test retrieving job details when job exists and user is owner."""
        mock_video = AsyncMock(spec=VideoModel)
        mock_video.uploader_user_id = "test-user-123"

        mock_job = AsyncMock(spec=VideoJobModel)
        mock_job.id = 42
        mock_job.video = mock_video  # Link video to job for uploader_user_id check

        mock_dependencies["job_repo"].get_by_id.return_value = mock_job

        result = await service.get_job_details(mock_db_async, 42, "test-user-123")

        assert result is mock_job
        mock_dependencies["job_repo"].get_by_id.assert_awaited_once_with(
            mock_db_async, 42
        )

    async def test_get_job_details_not_found(
        self,
        service: VideoProcessingService,
        mock_db_async: AsyncMock,
        mock_dependencies: dict,
    ):
        """Test retrieving job details when job does not exist."""
        mock_dependencies["job_repo"].get_by_id.return_value = None

        with pytest.raises(HTTPException) as excinfo:
            await service.get_job_details(mock_db_async, 999, "test-user-123")

        assert excinfo.value.status_code == 404
        assert excinfo.value.detail == "Job not found"
        mock_dependencies["job_repo"].get_by_id.assert_awaited_once_with(
            mock_db_async, 999
        )

    async def test_get_job_details_unauthorized(
        self,
        service: VideoProcessingService,
        mock_db_async: AsyncMock,
        mock_dependencies: dict,
    ):
        """Test retrieving job details when user is not the owner."""
        mock_video = AsyncMock(spec=VideoModel)
        mock_video.uploader_user_id = "other-user-456"  # Different owner

        mock_job = AsyncMock(spec=VideoJobModel)
        mock_job.id = 42
        mock_job.video = mock_video

        mock_dependencies["job_repo"].get_by_id.return_value = mock_job

        with pytest.raises(HTTPException) as excinfo:
            await service.get_job_details(
                mock_db_async, 42, "test-user-123"
            )  # Current user is test-user-123

        assert (
            excinfo.value.status_code == 404
        )  # Or 403 Forbidden, depending on service logic
        # The original test asserted 404, so we keep it consistent.
        # A 403 might be more semantically correct if the job exists but user can't access.
        # However, often services return 404 to avoid revealing existence of a resource.
        assert excinfo.value.detail == "Job not found"
        mock_dependencies["job_repo"].get_by_id.assert_awaited_once_with(
            mock_db_async, 42
        )
</file>

<file path="apps/core/tests/test_api.py">
import pytest
from httpx import AsyncClient  # Changed from fastapi.testclient import TestClient
from sqlalchemy.ext.asyncio import AsyncSession

# main.app and its dependency_overrides will be handled by test_client_fixture from conftest
# from main import app
# from lib.database import Base, create_session # Not used directly anymore
# from sqlalchemy import create_engine # Not used directly anymore
# from sqlalchemy.orm import sessionmaker # Not used directly anymore
# from sqlalchemy.pool import StaticPool # Not used directly anymore

# Setup in-memory SQLite for testing - This is now handled by conftest.py
# SQLALCHEMY_DATABASE_URL = "sqlite:///:memory:"
# engine = create_engine(
# SQLALCHEMY_DATABASE_URL,
# connect_args={"check_same_thread": False},
# poolclass=StaticPool,
# )
# TestingSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)


# Override the get_db dependency - This is now handled by test_client_fixture in conftest.py
# def override_get_db():
# db = TestingSessionLocal()
# try:
# yield db
# finally:
# db.close()


# app.dependency_overrides[create_session] = override_get_db

# client = TestClient(app) # Client will come from test_client_fixture


# @pytest.fixture(scope="function")
# def test_db():
# # Create the database tables - Handled by async_engine_fixture in conftest.py
# Base.metadata.create_all(bind=engine)
# yield
# # Drop the database tables
# Base.metadata.drop_all(bind=engine)


@pytest.mark.asyncio
async def test_create_user(
    client: AsyncClient,  # Changed from TestClient
):  # client injected from conftest.test_client_fixture
    response = await client.post(  # Added await, client is now AsyncClient
        "/api/v1/users/",
        json={
            "username": "testuser",
            "email": "test@example.com",
            "full_name": "Test User",
            "password": "password123",
        },
    )
    assert response.status_code == 201
    data = response.json()
    assert data["username"] == "testuser"
    assert data["email"] == "test@example.com"
    assert data["full_name"] == "Test User"
    assert "id" in data
    assert "password" not in data


@pytest.mark.asyncio
async def test_get_user(
    client: AsyncClient,  # Changed from TestClient
):  # client injected from conftest.test_client_fixture
    # First create a user
    response_create = await client.post(  # Added await
        "/api/v1/users/",
        json={
            "username": "testuser_get",  # Use a different username to avoid conflicts if tests run in parallel or state leaks
            "email": "testget@example.com",
            "full_name": "Test User Get",
            "password": "password123",
        },
    )
    assert response_create.status_code == 201
    user_id = response_create.json()["id"]

    # Now get the user
    response_get = await client.get(f"/api/v1/users/{user_id}")  # Added await
    assert response_get.status_code == 200
    data_get = response_get.json()
    assert data_get["username"] == "testuser_get"
    assert data_get["email"] == "testget@example.com"
    assert data_get["id"] == user_id
</file>

<file path="apps/core/README.md">
# Echo Core Application

## Project Overview

The Echo Core Application is a backend service for video processing featuring:

- Video upload, processing, and metadata extraction
- Integration with AI services for transcription and content analysis
- Secure storage in cloud or local filesystem
- Support for Supabase authentication and PostgreSQL
- Standardized API for frontend integration

## Architecture

The application follows a clean, layered architecture:

```
apps/core/
 api/             # API Layer (FastAPI endpoints, Pydantic schemas)
 core/            # Core configurations and shared utilities
 lib/             # Common libraries and adapters
    ai/             # AI service adapters (Gemini, cache)
    publishing/     # Publishing adapters (YouTube)
    storage/        # Storage adapters (GCS, local)
    utils/          # Helper utilities (FFmpeg, file, subtitle)
 models/          # SQLAlchemy data models
 operations/      # Data access layer (repositories)
 services/        # Business logic and orchestration
 tests/           # Automated tests
     integration/    # API and service integration tests
     unit/           # Component-level unit tests
```

### Architecture Layers

1. **API Layer** - HTTP interface using FastAPI
2. **Service Layer** - Business logic orchestration
3. **Operations Layer** - Data access through repositories
4. **Model Layer** - Database schema definitions
5. **Library Layer** - Common utilities and external integrations:
   - **AI Adapters** - Integrations with AI services (Gemini)
   - **Storage Adapters** - File storage implementations (GCS, local)
   - **Publishing Adapters** - Distribution channel integrations (YouTube)
   - **Utilities** - Common tools for video/audio processing

## Local Development Setup

### Prerequisites

- Python 3.10+ installed
- [uv](https://github.com/astral-sh/uv) for package management
- [Docker](https://www.docker.com/) for running Supabase locally
- [Supabase CLI](https://supabase.com/docs/guides/cli) for local development

### Environment Setup

1. Clone the repository and navigate to the core application:

```bash
cd apps/core
```

2. Create and activate a virtual environment:

```bash
uv venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

3. Install dependencies:

```bash
uv pip install -r pyproject.toml
```

### Supabase Local Setup

1. Install Supabase CLI:

```bash
npm install supabase --save-dev
```

2. Initialize and start local Supabase:

```bash
cd ../../  # Navigate to the project root
supabase init
supabase start
```

3. Copy the environment variables displayed by the Supabase CLI.

### Environment Configuration

1. Create a `.env` file in `apps/core/`:

```bash
cp .env.example .env
```

2. Edit the `.env` file with your local Supabase credentials and other environment variables.

### Database Migrations

Apply migrations to your local database:

```bash
cd apps/core
alembic upgrade head
```

### Starting the API Server

```bash
cd apps/core
uvicorn api.main:app --reload
```

The API will be available at `http://localhost:8000`.

## API Endpoints

### Authentication

Authentication is handled by Supabase. The backend expects a JWT token in the `Authorization` header.

### Video Processing API

#### Upload a Video

```
POST /api/v1/videos/upload
```

**Request:**
- Multipart form data with `file` field containing the video

**Response:**
```json
{
  "job_id": 123,
  "status": "PENDING"
}
```

#### Get Job Status

```
GET /api/v1/videos/jobs/{job_id}
```

**Response:**
```json
{
  "id": 123,
  "video_id": 456,
  "status": "COMPLETED",
  "processing_stages": {
    "transcription": true,
    "metadata": true
  },
  "error_message": null,
  "created_at": "2025-05-16T12:00:00Z",
  "updated_at": "2025-05-16T12:05:00Z",
  "video": {
    "id": 456,
    "uploader_user_id": "user-uuid",
    "original_filename": "my-video.mp4",
    "storage_path": "uploads/user-uuid/my-video.mp4",
    "content_type": "video/mp4",
    "size_bytes": 1024000,
    "created_at": "2025-05-16T12:00:00Z",
    "updated_at": "2025-05-16T12:00:00Z"
  },
  "metadata": {
    "id": 789,
    "job_id": 123,
    "title": "Auto-generated title",
    "description": "Auto-generated description",
    "tags": ["tag1", "tag2"],
    "transcript_text": "Full transcript of the video...",
    "transcript_file_url": "https://example.com/transcripts/123.txt",
    "subtitle_files_urls": {
      "vtt": "https://example.com/subtitles/123.vtt",
      "srt": "https://example.com/subtitles/123.srt"
    },
    "thumbnail_file_url": "https://example.com/thumbnails/123.jpg",
    "extracted_video_duration_seconds": 120.5,
    "extracted_video_resolution": "1920x1080",
    "extracted_video_format": "mp4",
    "show_notes_text": "Auto-generated show notes...",
    "created_at": "2025-05-16T12:05:00Z",
    "updated_at": "2025-05-16T12:05:00Z"
  }
}
```

## Environment Variables

| Variable                         | Description                        | Default          |
| -------------------------------- | ---------------------------------- | ---------------- |
| `ENVIRONMENT`                    | Development/production environment | `development`    |
| `DATABASE_URL`                   | PostgreSQL connection string       |                  |
| `SUPABASE_URL`                   | Supabase project URL               |                  |
| `SUPABASE_ANON_KEY`              | Supabase anonymous key             |                  |
| `SUPABASE_SERVICE_ROLE_KEY`      | Supabase service role key          |                  |
| `SUPABASE_JWT_SECRET`            | Supabase JWT secret                |                  |
| `STORAGE_BACKEND`                | Storage backend (`local` or `gcs`) | `local`          |
| `LOCAL_STORAGE_PATH`             | Path for local file storage        | `./output_files` |
| `GCS_BUCKET_NAME`                | Google Cloud Storage bucket name   |                  |
| `GOOGLE_APPLICATION_CREDENTIALS` | Path to GCP credentials JSON       |                  |
| `GEMINI_API_KEY`                 | Google Gemini AI API key           |                  |
| `REDIS_URL`                      | Redis connection URL for caching   |                  |

## Testing

### Running Tests

```bash
cd apps/core

# Run all tests
pytest

# Run unit tests only
pytest tests/unit

# Run integration tests only
pytest tests/integration

# Run with coverage report
pytest --cov=.
```

### Test Structure

- **Unit Tests**: Test individual components in isolation
  - Mock dependencies to focus on component behavior
  - Fast and independent of external systems
  
- **Integration Tests**: Test API endpoints and database interactions
  - `tests/integration/api/`: Tests for API endpoints
  - `tests/integration/conftest.py`: Common fixtures for database and API testing
  - Test authentication, error handling, and success paths
  
- **Test Fixtures**: Common test utilities and setup in `conftest.py` files
  - Database session management
  - Authentication helpers
  - File upload utilities
  - Mocked services

## Video Processing Architecture

The video processing functionality is distributed across several modules following clean architecture principles:

1. **Domain Models** (`models/`): Define the core entities
   - Video, VideoJob, VideoMetadata models with SQLAlchemy
   - Pydantic schemas for API validation and responses

2. **Service Layer** (`services/`): Contains core business logic
   - Video processing orchestration
   - Transcription and metadata generation
   - Job status management

3. **Adapters** (`lib/`): Interface implementations
   - AI adapters for transcription and content analysis
   - Storage adapters for file management
   - Publishing adapters for distribution (YouTube)

4. **API Layer** (`api/endpoints/`): External interface
   - FastAPI endpoints for video upload and processing
   - Authentication and validation middleware
   - Response formatting

This architecture ensures:
- **Testability**: Each layer can be tested in isolation
- **Maintainability**: Changes in one layer don't affect others
- **Flexibility**: Easy to swap implementations (e.g., storage providers)

## Deployment

The application can be deployed as a container or as a standard Python application. For production, ensure:

1. Set `ENVIRONMENT=production` in environment variables
2. Use a production-ready ASGI server like Gunicorn with Uvicorn workers
3. Set up proper logging and monitoring
4. Configure proper security settings for Supabase
</file>

<file path="apps/web/src/components/video/processing-dashboard.tsx">
import { Button } from "@/components/ui/button";
import { PlusIcon } from "lucide-react";
import { useEffect, useState, useRef } from "react";
import { Alert, AlertDescription, AlertTitle } from "@/components/ui/alert";
import { Skeleton } from "@/components/ui/skeleton";
import { Card, CardContent, CardHeader, CardFooter } from "@/components/ui/card";
import { useQuery, useQueryClient } from "@tanstack/react-query";
import { getProcessingJobs, getSignedUploadUrl, notifyUploadComplete } from "@/lib/api";
import type { VideoJobSchema, SignedUploadUrlRequest, SignedUploadUrlResponse, UploadCompleteRequest } from "@/types/api";
import type { Step } from "@/components/ui/progress-steps";
import { VideoProgressCard } from "./video-progress-card";
import { useJobStatusManager } from "@/hooks/useJobStatusManager";
import { toast } from "sonner";

// Map backend processing stages to frontend steps
const stageToStepMap: Record<string, number> = {
	uploaded: 0,
	audio_extraction: 1,
	transcript_generation: 2,
	subtitle_generation: 3,
	shownote_generation: 4,
	chapter_generation: 5,
	title_generation: 6,
	youtube_upload: 7,
	completed: 8,
};

// Helper to determine the current step index based on backend stage
function getCurrentStepIndex(jobStatus: VideoJobSchema['status'], stages?: VideoJobSchema['processing_stages']): number {
	if (jobStatus === "COMPLETED") return 8; // Max steps
	if (jobStatus === "FAILED") return 0; // Or a specific error step
	if (typeof stages === 'object' && stages !== null && !Array.isArray(stages)) {
		return Object.keys(stages).length;
	}
	if (Array.isArray(stages)) {
		return stages.length;
	}
	return 0;
}

function getProgressCardStatus(
	jobStatus: VideoJobSchema['status'],
): "processing" | "completed" | "error" | "paused" {
	switch (jobStatus) {
		case "COMPLETED": return "completed";
		case "FAILED": return "error";
		case "PENDING": return "processing"; // Or a specific "pending" visual state if desired
		case "PROCESSING": return "processing";
		default: return "processing"; // Fallback
	}
}

type ProcessingDashboardProps = {
	className?: string;
};

export function ProcessingDashboard({ className }: ProcessingDashboardProps) {
	useJobStatusManager();
	const queryClient = useQueryClient();
	const inputRef = useRef<HTMLInputElement>(null);
	const [isUploading, setIsUploading] = useState(false);

	const { 
		data: processingJobs, 
		isLoading, 
		error: fetchError,
	} = useQuery<VideoJobSchema[], Error>({
		queryKey: ['processingJobs'],
		queryFn: () => getProcessingJobs(),
	});

	const handleUploadButtonClick = () => {
		if (isUploading) return;
		inputRef.current?.click();
	};

	const handleFileChange = async (event: React.ChangeEvent<HTMLInputElement>) => {
		if (event.target.files && event.target.files.length > 0) {
			const file = event.target.files[0];
			if (inputRef.current) {
				inputRef.current.value = "";
			}
			
			setIsUploading(true);
			const uploadPromise = async () => {
				let signedUrlResponse: SignedUploadUrlResponse;
				try {
					const requestData: SignedUploadUrlRequest = {
						filename: file.name,
						content_type: file.type,
					};
					signedUrlResponse = await getSignedUploadUrl(requestData);
					if (!signedUrlResponse.upload_url || !signedUrlResponse.video_id) {
						throw new Error("Invalid response from signed URL endpoint");
					}
				} catch (err: any) {
					console.error("Failed to get upload URL:", err);
					throw new Error(err.message || "Failed to get upload URL");
				}

				const { upload_url: uploadUrl, video_id: videoId } = signedUrlResponse;

				try {
					await new Promise<void>((resolve, reject) => {
						const xhr = new XMLHttpRequest();
						xhr.open("PUT", uploadUrl);
						xhr.setRequestHeader("Content-Type", file.type);
						xhr.onload = () => {
							if (xhr.status >= 200 && xhr.status < 300) {
								resolve();
							} else {
								reject(new Error(`Upload failed with status ${xhr.status}: ${xhr.statusText}`));
							}
						};
						xhr.onerror = () => reject(new Error("Upload failed due to network error"));
						xhr.send(file);
					});
				} catch (err: any) {
					console.error("Upload failed:", err);
					throw new Error(err.message || "Upload to storage failed");
				}

				try {
					const completeRequestData: UploadCompleteRequest = {
						video_id: String(videoId),
						original_filename: file.name,
						content_type: file.type,
						size_bytes: file.size,
					};
					await notifyUploadComplete(completeRequestData);
				} catch (err: any) {
					console.error("Failed to finalize upload:", err);
					throw new Error(err.message || "Failed to finalize upload with backend");
				}
				return videoId;
			};

			toast.promise(uploadPromise(), {
				loading: "Uploading video...",
				success: (videoId) => {
					queryClient.invalidateQueries({ queryKey: ['processingJobs'] });
					queryClient.invalidateQueries({ queryKey: ['myVideos'] });
					return `Video "${file.name}" uploaded successfully! (ID: ${videoId})`;
				},
				error: (err) => `Upload failed: ${err.message}`,
				finally: () => {
					setIsUploading(false);
				}
			});
		}
	};
	
	const handlePauseResume = (videoId: string) => {
		console.log(
			"Pause/resume functionality is not currently supported by the backend.",
			videoId,
		);
	};

	return (
		<div className={className}>
			<div className="flex items-center justify-between mb-8">
				<div>
					<h1 className="text-2xl font-bold tracking-tight">
						Video Processing
					</h1>
					<p className="text-muted-foreground text-sm mt-1">
						Track the status of your video processing tasks
					</p>
				</div>
				<Button onClick={handleUploadButtonClick} disabled={isUploading}>
					<PlusIcon className="h-4 w-4 mr-2" />
					{isUploading ? "Uploading..." : "Upload New Video"}
				</Button>
			</div>

			<input
				type="file"
				ref={inputRef}
				onChange={handleFileChange}
				style={{ display: "none" }}
				accept="video/*" 
				disabled={isUploading}
			/>

			{fetchError && (
				<div className="py-10">
					<Alert variant="destructive" className="max-w-lg mx-auto">
						<AlertTitle>Error Fetching Processing Videos</AlertTitle>
						<AlertDescription>{fetchError.message}</AlertDescription>
					</Alert>
				</div>
			)}

			{isLoading && !fetchError ? (
				<div className="grid gap-6 sm:grid-cols-1 lg:grid-cols-2 xl:grid-cols-3">
					{[...Array(3)].map((_, index) => (
						<Card key={index} className="animate-pulse">
							<CardHeader className="pb-2">
								<div className="flex items-start justify-between">
									<div className="space-y-1 flex-grow pr-2">
										<Skeleton className="h-5 w-3/4" />
										<Skeleton className="h-4 w-1/2" />
									</div>
									<Skeleton className="h-14 w-24 shrink-0 rounded-md" />
								</div>
							</CardHeader>
							<CardContent>
								<Skeleton className="h-6 w-full mb-2" />
								<Skeleton className="h-4 w-1/2" /> 
							</CardContent>
							<CardFooter className="flex justify-between pt-0">
								<Skeleton className="h-8 w-20" />
								<Skeleton className="h-8 w-24" />
							</CardFooter>
						</Card>
					))}
				</div>
			) : !fetchError && processingJobs && processingJobs.length === 0 ? (
				<div className="flex flex-col items-center justify-center py-20 text-center">
					<p className="text-muted-foreground mb-4">
						No videos currently processing
					</p>
					<Button variant="outline" onClick={handleUploadButtonClick} disabled={isUploading}>
						{isUploading ? "Uploading..." : "Upload a new video"}
					</Button>
				</div>
			) : !fetchError && processingJobs ? (
				<div className="grid gap-6 sm:grid-cols-1 lg:grid-cols-2 xl:grid-cols-3">
					{processingJobs.map((job) => {
						const cardStatus = getProgressCardStatus(job.status);
						
						const simplifiedSteps: Step[] = [];
						let currentDisplayStepId: string | undefined = undefined;

						if (cardStatus === 'processing' && job.processing_stages) {
							if (Array.isArray(job.processing_stages)) {
							} else if (typeof job.processing_stages === 'object' && job.processing_stages !== null) {
							}
						} else if (cardStatus === 'completed') {
						}

						return (
							<VideoProgressCard
								key={job.id}
								videoId={String(job.id)}
								videoTitle={job.metadata?.title || job.video?.original_filename || "Untitled Video"}
								thumbnailUrl={job.metadata?.thumbnail_file_url || undefined}
								uploadedAt={job.created_at ? new Date(job.created_at) : new Date()}
								status={cardStatus}
								processingSteps={simplifiedSteps}
								currentStepId={currentDisplayStepId}
								onPauseResume={() => handlePauseResume(String(job.id))}
							/>
						);
					})}
				</div>
			) : null}
		</div>
	);
}
</file>

<file path="apps/web/src/components/video/VideoList.tsx">
import type { VideoSummary } from "@/types/api";
import { VideoListItem } from "./VideoListItem";
import { Button } from "@/components/ui/button";
import { Skeleton } from "@/components/ui/skeleton"; // For loading state
import { Alert, AlertDescription, AlertTitle } from "@/components/ui/alert"; // Added for error display

interface VideoListProps {
  videos: VideoSummary[];
  isLoading: boolean;
  error?: Error | null;
  hasNextPage?: boolean;
  isFetchingNextPage?: boolean;
  fetchNextPage?: () => void;
  showUploadButton?: boolean; // Optional: if the empty state upload button is handled here
  onTriggerUpload?: () => void; // Callback to open upload dialog
}

export function VideoList({
  videos,
  isLoading,
  error,
  hasNextPage,
  isFetchingNextPage,
  fetchNextPage,
  showUploadButton = false, // Default to false, dashboard can control its own dialog trigger
  onTriggerUpload,
}: VideoListProps) {
  if (isLoading && videos.length === 0) { // Initial loading state
    return (
      <div className="grid gap-6 sm:grid-cols-1 lg:grid-cols-2 xl:grid-cols-3">
        {[...Array(6)].map((_, index) => (
          <div key={index} className="rounded-lg border border-border p-4 space-y-3">
            <Skeleton className="h-6 w-3/4" />
            <Skeleton className="aspect-video w-full rounded-md" />
            <Skeleton className="h-4 w-1/2" />
            <div className="flex space-x-2">
              <Skeleton className="h-8 w-24" />
            </div>
          </div>
        ))}
      </div>
    );
  }

  if (error) {
    return (
      <div className="text-center py-10">
        <Alert variant="destructive" className="max-w-md mx-auto">
          <AlertTitle>Error Loading Videos</AlertTitle>
          <AlertDescription>{error.message}</AlertDescription>
        </Alert>
      </div>
    );
  }

  if (videos.length === 0) {
    return (
      <div className="flex flex-col items-center justify-center py-20 text-center">
        <p className="text-muted-foreground mb-4">
          No videos found in your library.
        </p>
        {showUploadButton && onTriggerUpload && (
            <Button variant="outline" onClick={onTriggerUpload}>Upload your first video</Button>
        )}
      </div>
    );
  }

  return (
    <>
      <div className="grid gap-6 sm:grid-cols-1 lg:grid-cols-2 xl:grid-cols-3">
        {videos.map((video) => (
          <VideoListItem key={video.id} video={video} />
        ))}
      </div>
      {hasNextPage && fetchNextPage && (
        <div className="flex justify-center mt-6">
          <Button onClick={fetchNextPage} disabled={isFetchingNextPage || isLoading}>
            {isFetchingNextPage ? "Loading more..." : "Load More"}
          </Button>
        </div>
      )}
    </>
  );
}
</file>

<file path="apps/web/src/components/login.tsx">
import { useAuth } from '../hooks/useAuth';
import { GoogleLoginButton } from './GoogleLoginButton';
import { useForm } from 'react-hook-form';
import { zodResolver } from '@hookform/resolvers/zod';
import * as z from 'zod';
import { Button } from '@/components/ui/button';
import {
	Form,
	FormControl,
	FormField,
	FormItem,
	FormLabel,
	FormMessage,
} from '@/components/ui/form';
import { Input } from '@/components/ui/input';
import { Link, useRouter } from '@tanstack/react-router';
import { toast } from 'sonner'; // Assuming sonner is used for toasts as per Phase 5

const loginSchema = z.object({
	email: z.string().email({ message: 'Invalid email address.' }),
	password: z.string().min(1, { message: 'Password is required.' }),
});

type LoginFormValues = z.infer<typeof loginSchema>;

export function Login() {
	const router = useRouter();
	const { loginWithPassword, isLoading, error: authError, isInitialized } // `error` from useAuth renamed to `authError` to avoid conflict
		= useAuth();

	const form = useForm<LoginFormValues>({
		resolver: zodResolver(loginSchema),
		defaultValues: {
			email: '',
			password: '',
		},
	});

	const onSubmit = async (values: LoginFormValues) => {
		const { error } = await loginWithPassword({
			email: values.email,
			password: values.password,
		});

		if (error) {
			// Error is already set in `authError` state by `useAuth`
			// Optionally, show a toast for more immediate feedback if authError isn't displayed directly
			toast.error(error.message || 'Login failed. Please check your credentials.');
		} else {
			toast.success('Login successful!');
			// router.invalidate() might be needed if loader data depends on auth state
			await router.invalidate();
			router.navigate({ to: '/dashboard' }); // Navigate to dashboard as per task
		}
	};

	// Redirect if already logged in and initialized
	// This logic might be better placed in the route component or a layout
	// useEffect(() => {
	//   if (isInitialized && session) {
	//     router.navigate({ to: '/dashboard' });
	//   }
	// }, [isInitialized, session, router]);

	return (
		<div className='max-w-md mx-auto mt-8 p-6 border rounded-lg shadow-md'>
			<h2 className='text-2xl font-semibold text-center mb-6'>Login</h2>

			<GoogleLoginButton />

			<div className='my-4 flex items-center'>
				<hr className='flex-grow border-gray-300' />
				<span className='mx-2 text-gray-500 text-xs uppercase'>or</span>
				<hr className='flex-grow border-gray-300' />
			</div>

			<Form {...form}>
				<form onSubmit={form.handleSubmit(onSubmit)} className='space-y-4'>
					<FormField
						control={form.control}
						name='email'
						render={({ field }) => (
							<FormItem>
								<FormLabel>Email</FormLabel>
								<FormControl>
									<Input placeholder='your@email.com' {...field} />
								</FormControl>
								<FormMessage />
							</FormItem>
						)}
					/>
					<FormField
						control={form.control}
						name='password'
						render={({ field }) => (
							<FormItem>
								<FormLabel>Password</FormLabel>
								<FormControl>
									<Input type='password' placeholder='' {...field} />
								</FormControl>
								<FormMessage />
							</FormItem>
						)}
					/>

					{authError && (
						<p className='text-sm text-red-500 text-center'>{authError.message}</p>
					)}

					<Button type='submit' className='w-full' disabled={isLoading}>
						{isLoading ? (
							<>
								<svg className='animate-spin -ml-1 mr-3 h-5 w-5 text-white' xmlns='http://www.w3.org/2000/svg' fill='none' viewBox='0 0 24 24'>
									<circle className='opacity-25' cx='12' cy='12' r='10' stroke='currentColor' strokeWidth='4'></circle>
									<path className='opacity-75' fill='currentColor' d='M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z'></path>
								</svg>
								Logging in...
							</>
						) : 'Login'}
					</Button>
				</form>
			</Form>
			<p className='mt-6 text-center text-sm'>
				Don't have an account?{" "}
				<Link to='/signup' className='font-medium text-primary hover:underline'>
					Sign up
				</Link>
			</p>
		</div>
	);
}
</file>

<file path="apps/web/src/hooks/useAppWebSocket.ts">
import { useState, useEffect, useRef, useCallback } from 'react';
// Assuming your job status updates will have a structure.
// This should align with what the backend sends over WebSocket.
// Let's use the VideoJob type from our API types for now, or a subset.
import type { VideoJobSchema as VideoJob, ProcessingStatus } from '../types/api';
import { useAuth } from './useAuth'; // Assuming useAuth provides user and session

export interface WebSocketJobUpdate extends Partial<VideoJob> {
  job_id: number; // Ensure job_id is always present for identification
  // any other specific fields expected in a WebSocket message for job updates
  // e.g., status, progress_percentage, error_message
}

const WS_BASE_URL = import.meta.env.VITE_WS_BASE_URL || 'ws://localhost:8000'; // Example, ensure this is in your .env

export type WebSocketStatus = 'connecting' | 'open' | 'closing' | 'closed' | 'uninstantiated';

interface UseAppWebSocketOptions {
  onOpen?: (event: Event) => void;
  onMessage?: (event: MessageEvent) => void;
  onError?: (event: Event) => void;
  onClose?: (event: CloseEvent) => void;
  reconnectLimit?: number;
  reconnectIntervalMs?: number;
  // Path to append after WS_BASE_URL, e.g., /ws/jobs/status/
  // If it needs dynamic parts like user_id, the hook will append it.
  path?: string;
}

interface UseAppWebSocketReturn {
  sendJsonMessage: (data: any) => void;
  lastJsonMessage: any | null;
  connectionStatus: WebSocketStatus;
  isConnected: boolean;
  socketRef: React.MutableRefObject<WebSocket | null>;
}

export function useAppWebSocket(options?: UseAppWebSocketOptions): UseAppWebSocketReturn {
  const { session, user } = useAuth();
  const [lastJsonMessage, setLastJsonMessage] = useState<any | null>(null);
  const [connectionStatus, setConnectionStatus] = useState<WebSocketStatus>('uninstantiated');
  const socketRef = useRef<WebSocket | null>(null);
  const reconnectAttemptsRef = useRef<number>(0);

  const {
    onOpen,
    onMessage,
    onError,
    onClose,
    reconnectLimit = 5,
    reconnectIntervalMs = 3000,
    path = '/ws/jobs/status/' // Default path, user_id will be appended
  } = options || {};

  const connect = useCallback(async () => {
    if (socketRef.current && socketRef.current.readyState === WebSocket.OPEN) {
      return; // Already connected
    }
    if (!session?.access_token || !user?.id) {
      console.log('WebSocket: No session or user ID, not connecting.');
      setConnectionStatus('closed'); // Or some other appropriate status
      return;
    }

    setConnectionStatus('connecting');

    const wsUrl = `${WS_BASE_URL.replace(/^http/, 'ws')}${path}${user.id}?token=${session.access_token}`;

    try {
      const socket = new WebSocket(wsUrl);
      socketRef.current = socket;

      socket.onopen = (event) => {
        console.log('WebSocket: Connection opened');
        setConnectionStatus('open');
        reconnectAttemptsRef.current = 0; // Reset reconnect attempts on successful open
        if (onOpen) onOpen(event);
      };

      socket.onmessage = (event) => {
        try {
          const message = JSON.parse(event.data);
          setLastJsonMessage(message);
          if (onMessage) onMessage(event); // Pass the raw event too
        } catch (e) {
          console.error('WebSocket: Error parsing JSON message', e);
          // Handle non-JSON messages or pass raw data if needed
          if (onMessage) onMessage(event);
        }
      };

      socket.onerror = (event) => {
        console.error('WebSocket: Error', event);
        setConnectionStatus('closed'); // Or a specific error status
        if (onError) onError(event);
        // Reconnect logic will be triggered by onclose
      };

      socket.onclose = (event) => {
        console.log(`WebSocket: Connection closed (code: ${event.code}, reason: ${event.reason})`);
        setConnectionStatus('closed');
        if (onClose) onClose(event);

        // Reconnect logic
        if (reconnectAttemptsRef.current < reconnectLimit) {
          reconnectAttemptsRef.current++;
          console.log(`WebSocket: Attempting to reconnect (${reconnectAttemptsRef.current}/${reconnectLimit})...`);
          setTimeout(connect, reconnectIntervalMs);
        } else {
          console.log('WebSocket: Reconnect limit reached.');
        }
      };
    } catch (err) {
      console.error('WebSocket: Instantiation failed', err);
      setConnectionStatus('closed'); // Or a specific error status
    }
  }, [session, user, onOpen, onMessage, onError, onClose, reconnectLimit, reconnectIntervalMs, path]);

  useEffect(() => {
    if (session?.access_token && user?.id) {
      connect();
    } else {
      // If no session, ensure any existing socket is closed
      if (socketRef.current) {
        socketRef.current.close(1000, 'User logged out or session expired');
        socketRef.current = null;
      }
      setConnectionStatus('closed');
    }

    return () => {
      if (socketRef.current) {
        console.log('WebSocket: Cleaning up connection.');
        // Prevent reconnect attempts on component unmount
        reconnectAttemptsRef.current = reconnectLimit + 1;
        socketRef.current.close(1000, 'Component unmounted');
        socketRef.current = null;
      }
    };
  }, [session, user, connect, reconnectLimit]); // connect is stable due to useCallback with its own deps

  const sendJsonMessage = useCallback((data: any) => {
    if (socketRef.current && socketRef.current.readyState === WebSocket.OPEN) {
      try {
        socketRef.current.send(JSON.stringify(data));
      } catch (e) {
        console.error("WebSocket: Error sending JSON message", e);
      }
    } else {
      console.warn("WebSocket: Connection not open. Message not sent.", data);
    }
  }, []);

  return {
    sendJsonMessage,
    lastJsonMessage,
    connectionStatus,
    isConnected: connectionStatus === 'open',
    socketRef,
  };
}
</file>

<file path="apps/web/src/hooks/useJobStatus.ts">
import { useState, useEffect, useRef } from 'react';
import { useQueryClient } from '@tanstack/react-query';
import { useAppWebSocket } from './useAppWebSocket';
import { supabase } from '@echo/db';
import type { Session, AuthChangeEvent } from '@supabase/supabase-js';
import type { VideoJobSchema as VideoJob, ProcessingStatus, VideoSummary, WebSocketJobUpdate } from '../types/api';

export function useJobStatusManager() {
  const queryClient = useQueryClient();
  const [currentSession, setCurrentSession] = useState<Session | null>(null);
  const prevUserIdRef = useRef<string | null>(null);

  useEffect(() => {
    supabase.auth.getSession().then(({ data }: { data: { session: Session | null } }) => {
      setCurrentSession(data.session);
      const currentUserId = data.session?.user?.id ?? null;
      if (currentUserId) {
        console.log("JobStatusManager: Initial session found, user ID:", currentUserId);
      } else {
        console.log("JobStatusManager: No initial session found.");
      }
      prevUserIdRef.current = currentUserId;
    });

    const { data: authListenerData } = supabase.auth.onAuthStateChange(
      (_event: AuthChangeEvent, session: Session | null) => {
        setCurrentSession(session);
        const newUserId = session?.user?.id ?? null;
        if (newUserId) {
          console.log("JobStatusManager: Auth state changed, new user ID:", newUserId);
        } else {
          console.log("JobStatusManager: Auth state changed, user logged out or no session.");
        }
        prevUserIdRef.current = newUserId;
      }
    );

    return () => {
      authListenerData.subscription.unsubscribe();
    };
  }, []);

  const handleTypedWebSocketMessage = (wsUpdateData: WebSocketJobUpdate) => {
    console.log("JobStatusManager: Received parsed job update via WebSocket", wsUpdateData);

    if (!wsUpdateData.job_id) {
        console.warn("JobStatusManager: WebSocket update missing job_id, skipping cache update.", wsUpdateData);
        return;
    }

    const jobDetailsQueryKey = ['jobDetails', String(wsUpdateData.job_id)];
    queryClient.setQueryData<VideoJob | undefined>(
      jobDetailsQueryKey,
      (oldData) => {
        if (oldData) {
          const updatedData = { ...oldData, ...wsUpdateData };
          return updatedData as VideoJob;
        }
        return oldData;
      }
    );

    const myVideosQueryKey = ['myVideos'];
    queryClient.setQueryData<VideoSummary[] | undefined>(
      myVideosQueryKey,
      (oldVideoList) => {
        if (!oldVideoList) return undefined;
        return oldVideoList.map(videoSummary => {
          if (wsUpdateData.video_id && videoSummary.id === wsUpdateData.video_id) {
            const newStatus = wsUpdateData.status as ProcessingStatus | undefined;
            return {
              ...videoSummary,
              status: newStatus || videoSummary.status,
              title: wsUpdateData.title ?? videoSummary.title,
            };
          }
          return videoSummary;
        });
      }
    );
    console.log(`JobStatusManager: Updated cache for job ${wsUpdateData.job_id} and potentially related lists.`);
  };

  const { isConnected, lastJsonMessage } = useAppWebSocket({
    onOpen: () => console.log("JobStatusManager: WebSocket connection established."),
    onClose: (event) => console.log("JobStatusManager: WebSocket connection closed.", event),
    onError: (event) => console.error("JobStatusManager: WebSocket error.", event),
  });

  useEffect(() => {
    if (lastJsonMessage) {
        if (typeof lastJsonMessage === 'object' && lastJsonMessage !== null && ('job_id' in lastJsonMessage || 'video_id' in lastJsonMessage)) {
            const updateData = lastJsonMessage as WebSocketJobUpdate; 
            handleTypedWebSocketMessage(updateData);
        } else {
            console.warn("JobStatusManager: Received WebSocket message of unexpected shape:", lastJsonMessage);
        }
    }
  }, [lastJsonMessage]);

  useEffect(() => {
    const currentUserId = prevUserIdRef.current;
    if (currentUserId) {
      console.log(`JobStatusManager: Active for user ${currentUserId}. WebSocket connected: ${isConnected}`);
    } else {
      console.log("JobStatusManager: Waiting for user ID to be determined. WebSocket connected: ${isConnected}");
    }
  }, [isConnected]);

  return { isWebSocketConnected: isConnected, currentUserId: prevUserIdRef.current, session: currentSession };
}
</file>

<file path="apps/web/src/routes/_authed/video/$videoId.tsx">
import { createFileRoute, Link, useParams } from "@tanstack/react-router";
import { useQuery, useMutation } from "@tanstack/react-query";
import { getVideoDetails, updateVideoMetadata } from "@/lib/api";
import { Button } from "@/components/ui/button";
import { Card, CardContent, CardHeader, CardTitle, CardDescription } from "@/components/ui/card";
import { Textarea } from "@/components/ui/textarea";
import { Input } from "@/components/ui/input";
import {
  Form,
  FormControl,
  FormField,
  FormItem,
  FormLabel,
  FormMessage,
} from "@/components/ui/form";
import { Alert, AlertDescription, AlertTitle } from "@/components/ui/alert";
import { Skeleton } from "@/components/ui/skeleton";
import { toast } from "sonner";
import { useForm, ControllerRenderProps, FieldValues } from "react-hook-form";
import { zodResolver } from "@hookform/resolvers/zod";
import * as z from "zod";
import type { VideoDetailsResponse, VideoMetadataUpdateRequest } from "@/types/api";
import { useEffect } from 'react';

// Placeholder for MediaPlayer component - to be created later
const MediaPlayer = ({ src, title, subtitleFilesUrls }: { src: string; title?: string; subtitleFilesUrls?: { [key: string]: string } | null }) => {
  return (
    <div className="aspect-video bg-slate-900 flex items-center justify-center text-slate-100 rounded-lg overflow-hidden">
      {src ? (
        <video controls src={src} title={title || 'Video player'} className="w-full h-full" crossOrigin="anonymous">
          Your browser does not support the video tag.
          {subtitleFilesUrls && Object.entries(subtitleFilesUrls).map(([lang, subtitleSrc], index) => (
            <track
              key={lang}
              kind="subtitles"
              srcLang={lang}
              src={subtitleSrc}
              label={lang.toUpperCase()} // Simple label, could be more descriptive
              default={index === 0} // Set the first subtitle track as default
            />
          ))}
        </video>
      ) : (
        <p>Video playback URL not available. Please ensure backend provides it.</p>
      )}
    </div>
  );
};

const metadataFormSchema = z.object({
  title: z.string().min(1, "Title is required.").max(255),
  description: z.string().max(5000).optional(),
  tags: z.string().optional(), // Represent tags as a comma-separated string for simplicity in form
});
type MetadataFormValues = z.infer<typeof metadataFormSchema>;

export const Route = createFileRoute("/_authed/video/$videoId")({
  component: VideoDetailPage,
});

function VideoDetailPage() {
  const { videoId } = useParams({ from: Route.id });
  const { data: videoDetails, isLoading, error, isError, refetch } = useQuery<VideoDetailsResponse, Error>({
    queryKey: ["videoDetails", videoId],
    queryFn: () => getVideoDetails(videoId),
  });

  const form = useForm<MetadataFormValues>({
    resolver: zodResolver(metadataFormSchema),
    // Default values will be set in useEffect when videoDetails load
  });

  useEffect(() => {
    if (videoDetails?.metadata) {
      form.reset({
        title: videoDetails.metadata.title || "",
        description: videoDetails.metadata.description || "",
        tags: videoDetails.metadata.tags?.join(", ") || "",
      });
    }
  }, [videoDetails, form.reset]);

  const { mutate: submitMetadata, isPending: isUpdatingMetadata } = useMutation({
    mutationFn: async (data: VideoMetadataUpdateRequest) => updateVideoMetadata(videoId, data),
    onSuccess: () => {
      toast.success("Metadata updated successfully!");
      refetch(); // Refetch video details to show updated data
    },
    onError: (err: Error) => {
      toast.error(`Failed to update metadata: ${err.message}`);
    },
  });

  const onSubmitMetadata = (values: MetadataFormValues) => {
    const updateRequest: VideoMetadataUpdateRequest = {
      title: values.title,
      description: values.description || null,
      tags: values.tags ? values.tags.split(",").map(tag => tag.trim()).filter(tag => tag) : [],
    };
    submitMetadata(updateRequest);
  };

  if (isLoading) {
    return <VideoDetailSkeleton />;
  }

  if (isError || !videoDetails) {
    return (
      <div className="container mx-auto p-4">
        <Alert variant="destructive">
          <AlertTitle>Error Loading Video</AlertTitle>
          <AlertDescription>
            {error?.message || "Failed to load video details. The video may not exist or an error occurred."}
          </AlertDescription>
        </Alert>
      </div>
    );
  }

  // Playback URL needs to be provided by the backend in VideoDetailsResponse, ideally as a direct field
  // like `video.playback_url` (e.g., a GCS signed URL).
  // The `storage_path` (e.g., gs://bucket/path) is generally not directly playable.
  // Waiting for backend to add a dedicated playback URL field to VideoSchema or VideoDetailsResponse.
  const playbackUrl = (videoDetails.video as any)?.playback_url || videoDetails.video?.storage_path || "";
  const videoTitle = videoDetails.metadata?.title || videoDetails.video?.original_filename || "Video";
  const subtitles = videoDetails.metadata?.subtitle_files_urls as { [key: string]: string } | undefined;

  return (
    <div className="container mx-auto p-4 space-y-6">
      <Card>
        <CardHeader>
          <CardTitle>{videoTitle}</CardTitle>
          {videoDetails.video?.original_filename && <CardDescription>Original file: {videoDetails.video.original_filename}</CardDescription>}
        </CardHeader>
        <CardContent>
          <MediaPlayer src={playbackUrl} title={videoTitle} subtitleFilesUrls={subtitles} />
        </CardContent>
      </Card>

      <div className="grid md:grid-cols-3 gap-6">
        <div className="md:col-span-2 space-y-4">
          <Card>
            <CardHeader><CardTitle>Details & Metadata</CardTitle></CardHeader>
            <CardContent className="space-y-2">
              <p><strong>Status:</strong> {videoDetails.status}</p>
              <p><strong>Uploaded:</strong> {videoDetails.video?.created_at ? new Date(videoDetails.video.created_at).toLocaleString() : 'N/A'}</p>
              <p><strong>Last Updated:</strong> {videoDetails.updated_at ? new Date(videoDetails.updated_at).toLocaleString() : 'N/A'}</p>
              {videoDetails.metadata?.description && <p><strong>Description:</strong> {videoDetails.metadata.description}</p>}
              {videoDetails.metadata?.tags && videoDetails.metadata.tags.length > 0 && (
                <p><strong>Tags:</strong> {videoDetails.metadata.tags.join(", ")}</p>
              )}
            </CardContent>
          </Card>

          {videoDetails.metadata?.transcript_text && (
            <Card>
              <CardHeader><CardTitle>Transcript</CardTitle></CardHeader>
              <CardContent>
                <Textarea readOnly value={videoDetails.metadata.transcript_text} rows={10} className="font-mono text-sm" />
              </CardContent>
            </Card>
          )}
        </div>

        <div className="md:col-span-1">
          <Card>
            <CardHeader><CardTitle>Edit Metadata</CardTitle></CardHeader>
            <CardContent>
              <Form {...form}>
                <form onSubmit={form.handleSubmit(onSubmitMetadata)} className="space-y-4">
                  <FormField
                    control={form.control}
                    name="title"
                    render={({ field }: { field: ControllerRenderProps<MetadataFormValues, 'title'> }) => (
                      <FormItem>
                        <FormLabel>Title</FormLabel>
                        <FormControl><Input {...field} /></FormControl>
                        <FormMessage />
                      </FormItem>
                    )}
                  />
                  <FormField
                    control={form.control}
                    name="description"
                    render={({ field }: { field: ControllerRenderProps<MetadataFormValues, 'description'> }) => (
                      <FormItem>
                        <FormLabel>Description</FormLabel>
                        <FormControl><Textarea {...field} rows={4} /></FormControl>
                        <FormMessage />
                      </FormItem>
                    )}
                  />
                  <FormField
                    control={form.control}
                    name="tags"
                    render={({ field }: { field: ControllerRenderProps<MetadataFormValues, 'tags'> }) => (
                      <FormItem>
                        <FormLabel>Tags (comma-separated)</FormLabel>
                        <FormControl><Input {...field} /></FormControl>
                        <FormMessage />
                      </FormItem>
                    )}
                  />
                  <Button type="submit" disabled={isUpdatingMetadata} className="w-full">
                    {isUpdatingMetadata ? "Updating..." : "Save Metadata"}
                  </Button>
                </form>
              </Form>
            </CardContent>
          </Card>
        </div>
      </div>
    </div>
  );
}

const VideoDetailSkeleton = () => (
  <div className="container mx-auto p-4 space-y-6">
    <Card>
      <CardHeader>
        <Skeleton className="h-8 w-3/4 mb-2" />
        <Skeleton className="h-4 w-1/2" />
      </CardHeader>
      <CardContent>
        <Skeleton className="aspect-video w-full" />
      </CardContent>
    </Card>
    <div className="grid md:grid-cols-3 gap-6">
      <div className="md:col-span-2 space-y-4">
        <Card>
          <CardHeader><Skeleton className="h-7 w-1/3" /></CardHeader>
          <CardContent className="space-y-3">
            <Skeleton className="h-4 w-full" />
            <Skeleton className="h-4 w-full" />
            <Skeleton className="h-4 w-3/4" />
            <Skeleton className="h-4 w-full" />
            <Skeleton className="h-4 w-2/3" />
          </CardContent>
        </Card>
        <Card>
          <CardHeader><Skeleton className="h-7 w-1/3" /></CardHeader>
          <CardContent>
            <Skeleton className="h-24 w-full" />
          </CardContent>
        </Card>
      </div>
      <div className="md:col-span-1">
        <Card>
          <CardHeader><Skeleton className="h-7 w-1/2" /></CardHeader>
          <CardContent className="space-y-4">
            <Skeleton className="h-10 w-full" />
            <Skeleton className="h-20 w-full" />
            <Skeleton className="h-10 w-full" />
            <Skeleton className="h-10 w-full" />
          </CardContent>
        </Card>
      </div>
    </div>
  </div>
);
</file>

<file path="apps/web/src/routes/_authed.jobs.[jobId].tsx">
import { createFileRoute, useParams } from "@tanstack/react-router";
import { useQuery } from "@tanstack/react-query";
import { api } from "@/lib/api"; // Assuming api client is here
import { Badge } from "@/components/ui/badge";
import { Card, CardContent, CardHeader, CardTitle, CardDescription } from "@/components/ui/card";
import { Progress } from "@/components/ui/progress";
import { Alert, AlertDescription, AlertTitle } from "@/components/ui/alert";
import { Skeleton } from "@/components/ui/skeleton";
import { formatDistanceToNow } from 'date-fns';

export const Route = createFileRoute("/_authed/jobs/[jobId]")({
  component: JobDetailsPage,
});

function JobDetailsPage() {
  const { jobId } = useParams({ from: Route.id });
  // Convert jobId to number if your API expects that, or ensure it's a string if not.
  // For now, assuming API client handles string/number conversion or API accepts string.
  const numericJobId = parseInt(jobId, 10);

  const { data: jobDetails, isLoading, error, isError } = useQuery({
    queryKey: ["jobDetails", jobId], // Use string jobId for queryKey consistency if API takes string
                                     // or numericJobId if API needs number and key should reflect that.
                                     // useJobStatusManager updates cache for ['jobDetails', String(job_id)]
    queryFn: () => api.getJobDetails(jobId), // Or numericJobId if needed by API
    // Real-time updates will come from useJobStatusManager updating this query's cache
    // Optional: Add polling as a fallback if WS is not connected or for robustness
    // refetchInterval: (query) => {
    //   const job = query.state.data;
    //   if (job && (job.status === 'PENDING' || job.status === 'PROCESSING')) {
    //     return 5000; // Poll every 5 seconds if job is active
    //   }
    //   return false; // Don't poll if job is completed or failed
    // },
  });

  if (isLoading) {
    return (
      <div className="container mx-auto p-4">
        <Card>
          <CardHeader>
            <Skeleton className="h-8 w-3/4" />
            <Skeleton className="h-4 w-1/2 mt-2" />
          </CardHeader>
          <CardContent className="space-y-4">
            <Skeleton className="h-6 w-1/4" />
            <Skeleton className="h-4 w-full" />
            <Skeleton className="h-4 w-3/4" />
            <Skeleton className="h-10 w-full" />
          </CardContent>
        </Card>
      </div>
    );
  }

  if (isError || !jobDetails) {
    return (
      <div className="container mx-auto p-4">
        <Alert variant="destructive">
          <AlertTitle>Error</AlertTitle>
          <AlertDescription>
            {error?.message || "Failed to load job details. The job may not exist or an error occurred."}
          </AlertDescription>
        </Alert>
      </div>
    );
  }
  
  // Assuming VideoJobSchema structure from apps/core/api/schemas/video_processing_schemas.py
  // (e.g., job_id, video_id, status, progress, created_at, updated_at, error_message)
  const {
    id, // Assuming jobDetails has an 'id' field for the job ID
    video_id,
    status,
    progress,
    created_at,
    updated_at,
    error_message,
    // video_filename, // This might come from video details, not job details itself.
  } = jobDetails;

  const getStatusVariant = (status: string) => {
    switch (status.toUpperCase()) {
      case 'COMPLETED': return 'success';
      case 'FAILED': return 'destructive';
      case 'PROCESSING': return 'default';
      case 'PENDING': return 'secondary';
      default: return 'outline';
    }
  };
  
  const videoFilename = jobDetails.video?.original_filename || 'N/A'; // Example: access related video info

  return (
    <div className="container mx-auto p-4">
      <Card>
        <CardHeader>
          <CardTitle className="flex justify-between items-center">
            <span>Job ID: {id}</span>
            <Badge variant={getStatusVariant(status)}>{status}</Badge>
          </CardTitle>
          <CardDescription>
            Details for video processing job. Last updated: {formatDistanceToNow(new Date(updated_at), { addSuffix: true })}.
          </CardDescription>
        </CardHeader>
        <CardContent className="space-y-6">
          <div>
            <h3 className="text-lg font-semibold mb-1">Video Information</h3>
            <p><strong>Video ID:</strong> {video_id || 'N/A'}</p>
            <p><strong>Original Filename:</strong> {videoFilename}</p>
          </div>

          <div>
            <h3 className="text-lg font-semibold mb-1">Job Progress</h3>
            {(status === 'PROCESSING' || status === 'COMPLETED') && progress !== null && progress !== undefined && (
              <div className="space-y-1">
                <Progress value={progress} className="w-full" />
                <p className="text-sm text-muted-foreground">{progress}% complete</p>
              </div>
            )}
            {status === 'PENDING' && <p>Waiting to be processed...</p>}
            {status === 'COMPLETED' && !progress && <p>Processing complete.</p>}
          </div>
          
          <div>
            <h3 className="text-lg font-semibold mb-1">Timestamps</h3>
            <p><strong>Created:</strong> {new Date(created_at).toLocaleString()}</p>
            <p><strong>Last Updated:</strong> {new Date(updated_at).toLocaleString()}</p>
          </div>

          {error_message && (
            <div>
              <h3 className="text-lg font-semibold mb-1 text-destructive">Error Details</h3>
              <Alert variant="destructive">
                <AlertDescription>{error_message}</AlertDescription>
              </Alert>
            </div>
          )}
          
          {/* TODO: Add link to the video page if processing is complete */}
          {/* {status === 'COMPLETED' && video_id && (
            <Button asChild>
              <Link to={`/video/${video_id}`}>View Video</Link>
            </Button>
          )} */}
        </CardContent>
      </Card>
    </div>
  );
}
</file>

<file path="apps/web/src/routes/_authed.jobs.$jobId.tsx">
import { createFileRoute, useParams } from "@tanstack/react-router";
import { useQuery } from "@tanstack/react-query";
import { getJobDetails } from "@/lib/api"; // Corrected: Import specific function
import { Badge } from "@/components/ui/badge";
import { Card, CardContent, CardHeader, CardTitle, CardDescription } from "@/components/ui/card";
import { Alert, AlertDescription, AlertTitle } from "@/components/ui/alert";
import { Skeleton } from "@/components/ui/skeleton";
import { formatDistanceToNow } from 'date-fns';

export const Route = createFileRoute("/_authed/jobs/$jobId")({
  component: JobDetailsPage,
});

function JobDetailsPage() {
  const { jobId } = useParams({ from: Route.id });

  const { data: jobDetails, isLoading, error, isError } = useQuery({
    queryKey: ["jobDetails", jobId], 
    queryFn: () => getJobDetails(jobId), // Corrected: Use imported function directly
    // ... (rest of useQuery options)
  });

  if (isLoading) {
    return (
      <div className="container mx-auto p-4">
        <Card>
          <CardHeader>
            <div className="flex justify-between items-center mb-2">
              <Skeleton className="h-8 w-1/2" /> {/* Job ID Title */}
              <Skeleton className="h-6 w-24" /> {/* Status Badge */}
            </div>
            <Skeleton className="h-4 w-3/4" /> {/* Description */}
          </CardHeader>
          <CardContent className="space-y-6">
            {/* Video Information Section Skeleton */}
            <div>
              <Skeleton className="h-6 w-1/3 mb-2" /> {/* Section Title */}
              <Skeleton className="h-4 w-full mb-1" /> {/* Video ID line */}
              <Skeleton className="h-4 w-3/4" />      {/* Filename line */}
            </div>
            {/* Job Progress/Stages Section Skeleton */}
            <div>
              <Skeleton className="h-6 w-1/3 mb-2" /> {/* Section Title */}
              <Skeleton className="h-16 w-full" />    {/* Placeholder for stages/status text */}
            </div>
            {/* Timestamps Section Skeleton */}
            <div>
              <Skeleton className="h-6 w-1/3 mb-2" /> {/* Section Title */}
              <Skeleton className="h-4 w-full mb-1" /> {/* Created at line */}
              <Skeleton className="h-4 w-3/4" />      {/* Updated at line */}
            </div>
            {/* Potential Error Details Section Skeleton (optional, shows if error part visible) */}
            {/* No explicit skeleton for error_message as it only appears if there is an error, 
                which wouldn't be the case during isLoading. But if it were a default visible section:
            <div>
              <Skeleton className="h-6 w-1/3 mb-2 text-destructive" /> 
              <Skeleton className="h-10 w-full" /> 
            </div> 
            */}
          </CardContent>
        </Card>
      </div>
    );
  }

  if (isError || !jobDetails) {
    return (
      <div className="container mx-auto p-4">
        <Alert variant="destructive">
          <AlertTitle>Error</AlertTitle>
          <AlertDescription>
            {error?.message || "Failed to load job details. The job may not exist or an error occurred."}
          </AlertDescription>
        </Alert>
      </div>
    );
  }
  
  const {
    id, 
    video_id,
    status,
    processing_stages,
    created_at,
    updated_at,
    error_message,
    video // Assuming 'video' object is part of VideoJobSchema and contains original_filename
  } = jobDetails;

  const getStatusVariant = (currentStatus: string) => {
    switch (currentStatus.toUpperCase()) {
      case 'COMPLETED': return 'default'; // Corrected: Use 'default' for success, can be styled green via CSS
      case 'FAILED': return 'destructive';
      case 'PROCESSING': return 'default'; // Or 'secondary' if 'default' is used for COMPLETED with specific styling
      case 'PENDING': return 'secondary';
      default: return 'outline';
    }
  };
  
  const videoFilename = video?.original_filename || 'N/A';
  const lastUpdatedText = updated_at ? formatDistanceToNow(new Date(updated_at), { addSuffix: true }) : 'N/A';
  const createdAtText = created_at ? new Date(created_at).toLocaleString() : 'N/A';

  return (
    <div className="container mx-auto p-4">
      <Card>
        <CardHeader>
          <CardTitle className="flex justify-between items-center">
            <span>Job ID: {id}</span>
            <Badge variant={getStatusVariant(status)}>{status}</Badge>
          </CardTitle>
          <CardDescription>
            Details for video processing job. Last updated: {lastUpdatedText}.
          </CardDescription>
        </CardHeader>
        <CardContent className="space-y-6">
          <div>
            <h3 className="text-lg font-semibold mb-1">Video Information</h3>
            <p><strong>Video ID:</strong> {video_id || 'N/A'}</p>
            <p><strong>Original Filename:</strong> {videoFilename}</p>
          </div>

          <div>
            <h3 className="text-lg font-semibold mb-1">Job Progress/Stages</h3>
            {/* Display processing_stages if available and meaningful */}
            {processing_stages && (
              <pre className="text-xs bg-muted p-2 rounded-md overflow-x-auto">
                {JSON.stringify(processing_stages, null, 2)}
              </pre>
            )}
            {status === 'PENDING' && !processing_stages && <p>Waiting to be processed...</p>}
            {status === 'PROCESSING' && !processing_stages && <p>Processing...</p>}
            {status === 'COMPLETED' && <p>Processing complete.</p>}
          </div>
          
          <div>
            <h3 className="text-lg font-semibold mb-1">Timestamps</h3>
            <p><strong>Created:</strong> {createdAtText}</p>
            <p><strong>Last Updated:</strong> {updated_at ? new Date(updated_at).toLocaleString() : 'N/A'}</p>
          </div>

          {error_message && (
            <div>
              <h3 className="text-lg font-semibold mb-1 text-destructive">Error Details</h3>
              <Alert variant="destructive">
                <AlertDescription>{error_message}</AlertDescription>
              </Alert>
            </div>
          )}
          
          {/* TODO: Add link to the video page if processing is complete */}
          {/* {status === 'COMPLETED' && video_id && (
            <Button asChild>
              <Link to={`/video/${video_id}`}>View Video</Link>
            </Button>
          )} */}
        </CardContent>
      </Card>
    </div>
  );
}
</file>

<file path="apps/web/src/routes/_authed.tsx">
import { createFileRoute, Outlet, redirect } from "@tanstack/react-router";
import { supabase } from "@echo/db/clients/client";
import React from "react";

// Define a simple loading component to be used as the pendingComponent
function AuthedLayoutPendingComponent() {
	return (
		<div className="flex items-center justify-center h-screen">
			<svg className="animate-spin h-10 w-10 text-primary" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
				<circle className="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" strokeWidth="4"></circle>
				<path className="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
			</svg>
			<span className="ml-3 text-xl">Loading authenticated session...</span>
		</div>
	);
}

export const Route = createFileRoute("/_authed")({
	beforeLoad: async ({ location }) => {
		const { data: { session }, error } = await supabase.auth.getSession();

		if (error) {
			console.error("Error getting session in _authed beforeLoad:", error);
			// Handle error appropriately, perhaps redirect to an error page or login
			throw redirect({
				to: "/login",
				search: {
					redirect: location.pathname, // Pass the original intended path
				},
			});
		}

		if (!session) {
			throw redirect({
				to: "/login",
				search: {
					// Store the attempted URL to redirect back after login
					redirect: location.pathname,
				},
			});
		}
		// If session exists, proceed to load the route component.
		// Optionally, you can pass user/session to context if needed by child routes directly from loader
		// return { user: session.user }; // Example: if you want to put user in context
		return {}; // Or an empty object if not passing anything specific to context here
	},
	component: AuthedLayoutComponent,
	pendingComponent: AuthedLayoutPendingComponent,
	// Removing the old errorComponent that rendered <Login />
	// Error handling for auth failure is now a redirect in beforeLoad.
	// Other errors can be handled by a more generic error component higher up or per-route.
});

function AuthedLayoutComponent() {
	// This component will render if beforeLoad successfully finds a session.
	// It should provide the Outlet for nested authenticated routes.
	// You could also include a shared layout for authenticated pages here (e.g., a navbar with user info).
	// const { user } = Route.useRouteContext(); // Example if context was populated in beforeLoad
	
	// For now, a simple Outlet.
	// Components within this layout can use the useAuth() hook to get user/session details.
	return (
		<>
			{/* Example: <AuthenticatedNavbar /> */}
			<Outlet />
		</>
	);
}
</file>

<file path="apps/web/src/routes/login.tsx">
import { createFileRoute, redirect } from "@tanstack/react-router";
import { Login } from "../components/login";
import { supabase } from "@echo/db/clients/client"; // Import supabase client

export const Route = createFileRoute("/login")({
	beforeLoad: async ({ cause }) => {
		// Redirect to dashboard if user is already logged in and tries to enter the login page.
		if (cause === 'enter') { 
			const { data: { session } } = await supabase.auth.getSession();
			if (session) {
				throw redirect({
					to: "/dashboard",
					replace: true,
				});
			}
		}
	},
	component: LoginComp,
});

function LoginComp() {
	return <Login />;
}
</file>

<file path="apps/web/src/types/api.ts">
/* tslint:disable */
/* eslint-disable */
/**
/* This file was automatically generated from pydantic models by running pydantic2ts.
/* Do not modify it by hand - just update the pydantic models and then re-run the script
*/

/**
 * Current status of the processing job.
 */
export type ProcessingStatus = "PENDING" | "PROCESSING" | "COMPLETED" | "FAILED";
/**
 * Enumeration of possible video processing job statuses.
 *
 * This enum inherits from str to allow for easy serialization to/from databases
 * and JSON, while still providing type safety and enumeration benefits.
 *
 * Attributes:
 *     PENDING: Job has been created but processing has not started.
 *     PROCESSING: Job is currently being processed.
 *     COMPLETED: Job has completed successfully.
 *     FAILED: Job processing failed with an error.
 */
export type ProcessingStatus1 = "PENDING" | "PROCESSING" | "COMPLETED" | "FAILED";
/**
 * Enumeration of possible video processing job statuses.
 *
 * This enum inherits from str to allow for easy serialization to/from databases
 * and JSON, while still providing type safety and enumeration benefits.
 *
 * Attributes:
 *     PENDING: Job has been created but processing has not started.
 *     PROCESSING: Job is currently being processed.
 *     COMPLETED: Job has completed successfully.
 *     FAILED: Job processing failed with an error.
 */
export type ProcessingStatus2 = "PENDING" | "PROCESSING" | "COMPLETED" | "FAILED";

/**
 * Individual error detail, often part of a list in ApiErrorResponse.
 */
export interface ApiErrorDetail {
  /**
   * Location of the error (e.g., field path)
   */
  loc?: (string | number)[] | null;
  /**
   * Error message
   */
  msg: string;
  /**
   * Type of error (e.g., 'value_error')
   */
  type: string;
  /**
   * Additional context for the error
   */
  ctx?: {
    [k: string]: unknown;
  } | null;
}
/**
 * Standard error response structure for API errors.
 */
export interface ApiErrorResponse {
  /**
   * Error message or list of error details
   */
  detail: string | ApiErrorDetail[];
}
/**
 * Request schema for obtaining a signed URL for video upload.
 */
export interface SignedUploadUrlRequest {
  /**
   * Original filename of the video.
   */
  filename: string;
  /**
   * MIME type of the video file.
   */
  content_type: string;
}
/**
 * Response schema after requesting a signed URL.
 */
export interface SignedUploadUrlResponse {
  /**
   * The GCS signed URL for direct PUT upload.
   */
  upload_url: string;
  /**
   * The unique ID assigned to this video upload attempt/record.
   */
  video_id: string;
}
/**
 * Request schema to notify the backend that a direct upload is complete.
 */
export interface UploadCompleteRequest {
  /**
   * The unique ID of the video upload.
   */
  video_id: string;
  /**
   * Original filename of the uploaded video.
   */
  original_filename: string;
  /**
   * MIME type of the video file.
   */
  content_type: string;
  /**
   * Size of the video file in bytes.
   */
  size_bytes: number;
  /**
   * Canonical path in GCS if known by uploader; backend may infer.
   */
  storage_path?: string | null;
}
/**
 * Comprehensive details for a specific video, including its job and metadata.
 * This often mirrors VideoJobSchema if that schema is the primary source of truth.
 * Alternatively, it can be a composition of VideoSchema, VideoMetadataSchema, and job details.
 * Let's make it closely related to VideoJobSchema for now.
 */
export interface VideoDetailsResponse {
  /**
   * Video Job ID. If this is for Video Details, this might be Video ID with Job details nested or vice-versa
   */
  id: number;
  /**
   * Associated Video ID
   */
  video_id: number;
  /**
   * ID of the user who uploaded the video. (Derived from video)
   */
  uploader_user_id?: string | null;
  /**
   * Original filename from the upload. (Derived from video)
   */
  original_filename?: string | null;
  status: ProcessingStatus;
  /**
   * Progress information.
   */
  processing_stages?:
    | string[]
    | {
        [k: string]: unknown;
      }
    | null;
  /**
   * Error details if the job failed.
   */
  error_message?: string | null;
  /**
   * When the job (or video) was created.
   */
  created_at?: string | null;
  /**
   * When the job (or video) was last updated.
   */
  updated_at?: string | null;
  /**
   * Associated video details.
   */
  video?: VideoSchema | null;
  /**
   * Associated metadata details.
   */
  metadata?: VideoMetadataSchema | null;
}
/**
 * Schema representing a video file in the system. (Existing)
 */
export interface VideoSchema {
  id: number;
  uploader_user_id: string;
  original_filename: string;
  storage_path: string;
  content_type: string;
  size_bytes: number;
  created_at?: string | null;
  updated_at?: string | null;
}
/**
 * Schema representing metadata extracted from a processed video. (Existing)
 */
export interface VideoMetadataSchema {
  id?: number | null;
  job_id?: number | null;
  title?: string | null;
  description?: string | null;
  tags?: string[] | null;
  transcript_text?: string | null;
  transcript_file_url?: string | null;
  subtitle_files_urls?: {
    [k: string]: unknown;
  } | null;
  thumbnail_file_url?: string | null;
  extracted_video_duration_seconds?: number | null;
  extracted_video_resolution?: string | null;
  extracted_video_format?: string | null;
  show_notes_text?: string | null;
  created_at?: string | null;
  updated_at?: string | null;
}
/**
 * Schema representing a video processing job. (Existing)
 */
export interface VideoJobSchema {
  id: number;
  video_id: number;
  status: ProcessingStatus1;
  processing_stages?:
    | string[]
    | {
        [k: string]: unknown;
      }
    | null;
  error_message?: string | null;
  created_at?: string | null;
  updated_at?: string | null;
  video?: VideoSchema | null;
  metadata?: VideoMetadataSchema | null;
}
/**
 * Request schema for updating video metadata.
 */
export interface VideoMetadataUpdateRequest {
  /**
   * New title for the video.
   */
  title?: string | null;
  /**
   * New description for the video.
   */
  description?: string | null;
  /**
   * New list of tags for the video.
   */
  tags?: string[] | null;
}
/**
 * Summarized video information, typically for lists.
 */
export interface VideoSummary {
  /**
   * Unique identifier for the video.
   */
  id: number;
  /**
   * Original filename of the video.
   */
  original_filename: string;
  /**
   * Title of the video.
   */
  title?: string | null;
  /**
   * Timestamp of video creation.
   */
  created_at?: string | null;
  /**
   * Current processing status of the video.
   */
  status?: ProcessingStatus1 | null;
  /**
   * URL to the video's thumbnail.
   */
  thumbnail_file_url?: string | null;
}
/**
 * Response schema for the video upload endpoint. (Existing)
 * NOTE: This might be for when upload starts processing, not the signed URL itself.
 * If getSignedUploadUrl returns SignedUploadUrlResponse, this might be for a different step.
 * For now, keeping as is, assuming it's used by an endpoint.
 */
export interface VideoUploadResponseSchema {
  /**
   * The ID of the created video processing job.
   */
  job_id: number;
  status: ProcessingStatus2;
}
</file>

<file path="apps/web/.gitignore">
# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
pnpm-debug.log*
lerna-debug.log*

node_modules
dist
dist-ssr
*.local

# Editor directories and files
.vscode/*
!.vscode/extensions.json
.idea
.DS_Store
*.suo
*.ntvs*
*.njsproj
*.sln
*.sw?

# Environment variables
.env
.env.*
!.env.example
!.env.sample
</file>

<file path="supabase/package.json">
{
  "name": "@echo/db",
  "version": "1.0.0",
  "type": "module",
  "main": "./index.ts",
  "exports": {
    ".": "./index.ts",
    "./clients/client": "./clients/client.ts",
    "./clients/server": "./clients/server.ts",
    "./types/db": "./types/db.ts"
  },
  "scripts": {
    "lint": "eslint ."
  },
  "keywords": [],
  "author": "",
  "license": "ISC",
  "description": "",
  "dependencies": {
    "@supabase/ssr": "0.6.1",
    "@supabase/supabase-js": "^2.49.4"
  },
  "devDependencies": {
    "typescript": "^5.5.3"
  }
}
</file>

<file path="pnpm-workspace.yaml">
packages:
  - "apps/*"
  - "packages/*"
  - "supabase"
</file>

<file path=".clinerules/self-improving-cline.md">
---
description: Defines a process for Cline to reflect on interactions and suggest improvements to active .clinerules.
author: https://github.com/nickbaumann98
version: 1.0
tags: ["meta", "self-improvement", "clinerules", "reflection", "core-behavior"]
globs: ["*"]
---
# Self-Improving Cline Reflection

**Objective:** Offer opportunities to continuously improve `.clinerules` based on user interactions and feedback.

**Trigger:** Before using the `attempt_completion` tool for any task that involved user feedback provided at any point during the conversation, or involved multiple non-trivial steps (e.g., multiple file edits, complex logic generation).

**Process:**

1.  **Offer Reflection:** Ask the user: "Before I complete the task, would you like me to reflect on our interaction and suggest potential improvements to the active `.clinerules`?"
2.  **Await User Confirmation:** Proceed to `attempt_completion` immediately if the user declines or doesn't respond affirmatively.
3.  **If User Confirms:**
    a.  **Review Interaction:** Synthesize all feedback provided by the user throughout the entire conversation history for the task. Analyze how this feedback relates to the active `.clinerules` and identify areas where modified instructions could have improved the outcome or better aligned with user preferences.
    b.  **Identify Active Rules:** List the specific global and workspace `.clinerules` files active during the task.
    c.  **Formulate & Propose Improvements:** Generate specific, actionable suggestions for improving the *content* of the relevant active rule files. Prioritize suggestions directly addressing user feedback. Use `replace_in_file` diff blocks when practical, otherwise describe changes clearly.
    d.  **Await User Action on Suggestions:** Ask the user if they agree with the proposed improvements and if they'd like me to apply them *now* using the appropriate tool (`replace_in_file` or `write_to_file`). Apply changes if approved, then proceed to `attempt_completion`.

**Constraint:** Do not offer reflection if:
*   No `.clinerules` were active.
*   The task was very simple and involved no feedback.
</file>

<file path="apps/core/operations/video_repository.py">
"""
Repository for VideoModel data access operations.

This module provides a repository class implementing the repository pattern for
VideoModel entities. It abstracts database access operations, providing methods
for creating and retrieving video records. The repository focuses solely on data
access with no business logic, accepting a SQLAlchemy Session for each operation.

Usage:
    from sqlalchemy.orm import Session
    from apps.core.operations.video_repository import VideoRepository

    # Create a new video
    video_repo = VideoRepository()
    new_video = video_repo.create(
        db=db_session,
        uploader_user_id="user123",
        original_filename="video.mp4",
        storage_path="uploads/video.mp4",
        content_type="video/mp4",
        size_bytes=1024000
    )

    # Get a video by ID
    video = video_repo.get_by_id(db=db_session, video_id=1)
"""

from typing import Optional

from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import Session

from apps.core.models.video_model import VideoModel


class VideoRepository:
    """
    Repository for VideoModel data access.
    """

    @staticmethod
    async def create(
        db: AsyncSession,
        uploader_user_id: str,
        original_filename: str,
        storage_path: str,
        content_type: str,
        size_bytes: int,
    ) -> VideoModel:
        """
        Create and persist a new VideoModel.

        Args:
            db (AsyncSession): SQLAlchemy async session.
            uploader_user_id (str): Supabase Auth user ID.
            original_filename (str): Original filename.
            storage_path (str): Path in storage backend.
            content_type (str): MIME type.
            size_bytes (int): File size in bytes.

        Returns:
            VideoModel: The created video model.
        """
        video = VideoModel(
            uploader_user_id=uploader_user_id,
            original_filename=original_filename,
            storage_path=storage_path,
            content_type=content_type,
            size_bytes=size_bytes,
        )
        db.add(video)
        await db.flush()  # Assigns ID
        return video

    @staticmethod
    async def get_by_id(db: AsyncSession, video_id: int) -> Optional[VideoModel]:
        """
        Retrieve a VideoModel by its ID.

        Args:
            db (AsyncSession): SQLAlchemy async session.
            video_id (int): Video ID.

        Returns:
            Optional[VideoModel]: The video model, or None if not found.
        """
        result = await db.execute(select(VideoModel).filter(VideoModel.id == video_id))
        return result.scalars().first()


async def get_video_repository() -> VideoRepository:
    """
    FastAPI dependency for getting a VideoRepository instance.
    Used for dependency injection in API endpoints.

    Returns:
        VideoRepository: An instance of the VideoRepository.
    """
    return VideoRepository()
</file>

<file path="apps/core/main.py">
import os

from fastapi import BackgroundTasks, FastAPI, File, HTTPException, UploadFile

import apps.core.models
from apps.core.api.endpoints import router as video_api_router
from apps.core.api.endpoints.jobs_endpoints import router as jobs_api_router

app = FastAPI()

# Register routers
app.include_router(video_api_router, prefix="/api/v1/videos", tags=["Video Processing"])
app.include_router(jobs_api_router, prefix="/api/v1/jobs", tags=["Jobs"])


# Add a simple startup message
@app.get("/")
async def root():
    return {"message": "Video Processing API is running"}


# Simple test endpoint for video upload without authentication
@app.post("/test/upload")
async def test_upload(file: UploadFile = File(...)):
    """Test endpoint for uploading videos without authentication"""
    print(f"Received file: {file.filename}, content_type: {file.content_type}")

    # For testing, we'll accept any file type
    # if not file.content_type or not file.content_type.startswith("video/"):
    #     return {"error": "File must be a video"}

    if not file.filename:
        raise HTTPException(status_code=400, detail="Missing filename")

    # Save the file to a temporary location using an absolute path
    content = await file.read()

    # Get the absolute path to the core directory - we're in apps/core/main.py
    core_dir = os.path.dirname(os.path.abspath(__file__))
    test_upload_dir = os.path.join(core_dir, "output_files", "uploads", "test")

    # Create directory if it doesn't exist
    os.makedirs(test_upload_dir, exist_ok=True)

    # Build full file path
    save_path = os.path.join(test_upload_dir, file.filename)

    # Save the file
    with open(save_path, "wb") as f:
        f.write(content)

    return {
        "message": "Video uploaded successfully for testing",
        "filename": file.filename,
        "content_type": file.content_type,
        "size": len(content),
        "saved_path": save_path,
    }


# Run the application if this script is executed directly
if __name__ == "__main__":
    import uvicorn

    print("Starting FastAPI server...")
    uvicorn.run(app, host="0.0.0.0", port=8000)
</file>

<file path="apps/core/pyproject.toml">
[project]
name = "echo"
version = "0.1.0"
description = "Video processing pipeline for Echo project"
readme = "README.md"
requires-python = ">=3.13"

dependencies = [
    # Core dependencies
    "aiosqlite>=0.20.0",
    "asyncpg>=0.29.0",
    "autoflake>=2.3.1",
    "email-validator>=2.2.0",
    "fastapi>=0.115.12",
    "flake8>=7.2.0",
    "httpx>=0.28.1",
    "isort>=6.0.1",
    "jinja2>=3.1.6",
    "mypy>=1.15.0",
    "openai>=1.75.0",
    "pillow>=11.2.1",
    "pydantic>=2.11.3",
    "pydantic-settings",
    "pyjwt>=2.10.1",
    "python-jose[cryptography]",
    "python-dotenv>=1.1.0",
    "python-multipart>=0.0.20",
    "psycopg2-binary",
    "redis>=5.2.1",
    "sqlalchemy>=2.0.40",
    "uvicorn>=0.34.2",
    "requests>=2.20.0",
    "ruff>=0.2.1",
    "google-cloud-secret-manager>=2.16.0",
    "google-cloud-storage==2.10.0",
    "google-generativeai",
    "vertexai",
    "ffmpeg-python",
    "google-cloud-aiplatform",
    "gunicorn",
    "alembic",
    "supabase",
    # YouTube API dependencies
    "google-api-python-client>=2.80.0",
    "google-auth-oauthlib>=1.0.0",
    "google-auth-httplib2>=0.1.0",
    # Cloud Functions dependencies
    "functions-framework",
    "google-auth>=2.40.1",
    "pydantic-to-typescript>=2",
]

[project.optional-dependencies]
dev = [
    # Testing dependencies
    "pytest>=8.3.5",
    "pytest-asyncio",
    "pytest-mock==3.11.1",
    "pytest-cov==4.1.0",
    
    # Development tools
    "black==24.1.1",
    "ruff==0.2.1",
    "mypy>=1.15.0",
    "pre-commit>=3.0.0",
]

[tool.mypy]
python_version = "3.13"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = false
namespace_packages = true
explicit_package_bases = true
exclude = ['venv', '.venv', '__pycache__', '.pytest_cache'] 

[tool.setuptools.packages.find]
where = ["."]
include = ["api*", "core*", "lib*", "models*", "operations*", "services*", "config*"]
exclude = ["tests*", "video_processor*"]

[dependency-groups]
dev = [
    "pytest>=8.3.5",
]
</file>

<file path="apps/web/src/components/video/VideoUploadDropzone.tsx">
import React, { useRef, useState } from "react";
import {
  getSignedUploadUrl,
  notifyUploadComplete,
} from "../../lib/api"; // Import new API functions
import type {
  SignedUploadUrlRequest,
  SignedUploadUrlResponse,
  UploadCompleteRequest,
} from "../../types/api"; // Import types
import { toast } from "sonner"; // Import toast

type UploadStatus = "idle" | "requesting" | "uploading" | "finalizing" | "done" | "error";

export function VideoUploadDropzone({
  onUploadComplete: onUploadSuccess, // Renamed prop for clarity
}: {
  onUploadComplete?: (videoId: string) => void; // Pass videoId back
}) {
  const [status, setStatus] = useState<UploadStatus>("idle");
  const [error, setError] = useState<string | null>(null);
  const [progress, setProgress] = useState<number>(0);
  const inputRef = useRef<HTMLInputElement>(null);

  async function handleFile(file: File) {
    setError(null);
    setStatus("requesting");
    setProgress(0);

    let signedUrlResponse: SignedUploadUrlResponse;
    try {
      const requestData: SignedUploadUrlRequest = {
        filename: file.name,
        content_type: file.type,
      };
      signedUrlResponse = await getSignedUploadUrl(requestData);
      if (!signedUrlResponse.upload_url || !signedUrlResponse.video_id) {
        throw new Error("Invalid response from signed URL endpoint");
      }
    } catch (err: any) {
      const errorMsg = err.message || "Failed to get upload URL";
      setError(errorMsg);
      toast.error(errorMsg);
      setStatus("error");
      return;
    }

    const { upload_url: uploadUrl, video_id: videoId } = signedUrlResponse;

    // 2. Upload file to signed URL
    setStatus("uploading");
    try {
      await new Promise<void>((resolve, reject) => {
        const xhr = new XMLHttpRequest();
        xhr.open("PUT", uploadUrl);
        xhr.setRequestHeader("Content-Type", file.type);
        xhr.upload.onprogress = (e) => {
          if (e.lengthComputable) {
            setProgress(Math.round((e.loaded / e.total) * 100));
          }
        };
        xhr.onload = () => {
          if (xhr.status >= 200 && xhr.status < 300) {
            resolve();
          } else {
            reject(new Error(`Upload failed with status ${xhr.status}: ${xhr.statusText}`));
          }
        };
        xhr.onerror = () => reject(new Error("Upload failed due to network error"));
        xhr.send(file);
      });
    } catch (err: any) {
      const errorMsg = err.message || "Upload failed";
      setError(errorMsg);
      toast.error(errorMsg);
      setStatus("error");
      return;
    }

    // 3. Notify backend upload is complete
    setStatus("finalizing");
    try {
      const completeRequestData: UploadCompleteRequest = {
        video_id: videoId,
        original_filename: file.name,
        content_type: file.type,
        size_bytes: file.size,
        // storage_path can be omitted if backend infers it
      };
      await notifyUploadComplete(completeRequestData);
    } catch (err: any) {
      const errorMsg = err.message || "Failed to finalize upload";
      setError(errorMsg);
      toast.error(errorMsg);
      setStatus("error");
      return;
    }

    setStatus("done");
    setProgress(100);
    if (onUploadSuccess) onUploadSuccess(videoId); // Pass videoId on success
  }

  function onDrop(e: React.DragEvent<HTMLDivElement>) {
    e.preventDefault();
    if (status === "uploading" || status === "finalizing" || status === "requesting") return;
    if (e.dataTransfer.files && e.dataTransfer.files.length > 0) {
      handleFile(e.dataTransfer.files[0]);
    }
  }

  function onFileChange(e: React.ChangeEvent<HTMLInputElement>) {
    if (status === "uploading" || status === "finalizing" || status === "requesting") return;
    if (e.target.files && e.target.files.length > 0) {
      handleFile(e.target.files[0]);
      // Reset file input to allow uploading the same file again
      if (inputRef.current) {
        inputRef.current.value = "";
      }
    }
  }

  const isUploading = status === "requesting" || status === "uploading" || status === "finalizing";

  return (
    <div
      onDrop={onDrop}
      onDragOver={(e) => e.preventDefault()}
      style={{
        border: "2px dashed #888",
        borderRadius: 8,
        padding: 32,
        textAlign: "center",
        background: isUploading ? "#f0f0f0" : "#fafbfc",
        cursor: isUploading ? "default" : "pointer",
        opacity: isUploading ? 0.7 : 1,
        margin: "16px 0",
      }}
      onClick={() => !isUploading && inputRef.current?.click()}
      tabIndex={0}
      role="button"
      aria-label="Upload video"
      aria-disabled={isUploading}
    >
      <input
        ref={inputRef}
        type="file"
        accept="video/*"
        style={{ display: "none" }}
        onChange={onFileChange}
        disabled={isUploading}
      />
      {status === "idle" && <span>Drag & drop a video file here, or click to select</span>}
      {status === "requesting" && <span>Requesting upload permissions</span>}
      {status === "uploading" && (
        <div>
          <span>Uploading {progress}%</span>
          <progress
            value={progress}
            max={100}
            style={{ width: "100%", marginTop: "8px" }}
          />
        </div>
      )}
      {status === "finalizing" && <span>Finalizing upload</span>}
      {status === "done" && <span style={{ color: "green" }}>Upload complete! Ready for processing.</span>}
      {status === "error" && (
        <span style={{ color: "red" }}>
          Error: {error}
        </span>
      )}
    </div>
  );
}

export default VideoUploadDropzone;
</file>

<file path="apps/web/src/components/GoogleLoginButton.tsx">
import { useAuth } from '../hooks/useAuth'; // Adjusted path assuming hooks are in ../hooks
import { Button } from '@/components/ui/button'; // Assuming shadcn/ui Button is available

export function GoogleLoginButton() {
  const { signInWithGoogle, isLoading, error } = useAuth();

  const handleLogin = async () => {
    const result = await signInWithGoogle();
    // signInWithGoogle from useAuth already handles redirection or returns error.
    // Error will be in the `error` state from useAuth if it occurs before redirect.
    if (result.error) {
      // Optionally, display a specific toast or alert here if not handled globally
      console.error("Google Sign-In error:", result.error.message);
    }
    // If successful, navigation to Google occurs, then to callback.
  };

  return (
    <Button
      type="button"
      onClick={handleLogin}
      className="w-full bg-red-600 hover:bg-red-700 text-white font-bold uppercase flex items-center justify-center gap-2"
      aria-label="Sign in with Google"
      disabled={isLoading}
    >
      {isLoading ? (
        <>
          <svg className="animate-spin -ml-1 mr-3 h-5 w-5 text-white" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
            <circle className="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" strokeWidth="4"></circle>
            <path className="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
          </svg>
          Signing in...
        </>
      ) : (
        <>
          <svg
            width="20"
            height="20"
            viewBox="0 0 48 48"
            fill="none"
            xmlns="http://www.w3.org/2000/svg"
            className="mr-2"
          >
            <g>
              <path
                d="M44.5 20H24V28.5H36.9C35.5 33.1 31.2 36 24 36C16.3 36 10 29.7 10 22C10 14.3 16.3 8 24 8C27.3 8 30.1 9.1 32.3 11L38.1 5.2C34.5 2.1 29.6 0 24 0C10.7 0 0 10.7 0 24C0 37.3 10.7 48 24 48C37.3 48 48 37.3 48 24C48 22.3 47.8 20.7 47.5 19.1L44.5 20Z"
                fill="#FFC107"
              />
              <path
                d="M6.3 14.7L13.7 20.1C15.7 15.7 19.5 12.5 24 12.5C26.5 12.5 28.7 13.4 30.4 14.8L36.2 9C33.1 6.3 28.8 4.5 24 4.5C15.9 4.5 8.8 10.1 6.3 14.7Z"
                fill="#FF3D00"
              />
              <path
                d="M24 43.5C31.1 43.5 37.1 39.1 39.6 33.1L32.7 27.7C31.1 30.7 27.8 32.5 24 32.5C19.5 32.5 15.7 29.3 13.7 24.9L6.3 30.3C8.8 34.9 15.9 43.5 24 43.5Z"
                fill="#4CAF50"
              />
              <path
                d="M47.5 19.1H44.5V20H24V28.5H36.9C36.2 31 34.5 33.1 32.7 34.7L39.6 40.1C43.1 36.9 45.5 31.9 45.5 24C45.5 22.3 45.3 20.7 44.5 19.1Z"
                fill="#1976D2"
              />
            </g>
          </svg>
          Sign in with Google
        </>
      )}
      {/* Display error from useAuth if needed, though might be better handled by a global toast */}
      {/* {error && <p className="text-xs text-red-500 mt-1">Error: {error.message}</p>} */}
    </Button>
  );
}
</file>

<file path="apps/web/src/routes/auth/callback.tsx">
import { useEffect } from 'react';
import { createFileRoute, useNavigate } from '@tanstack/react-router';
import { useAuth } from '../../hooks/useAuth'; // Assuming path to useAuth hook
import { toast } from 'sonner';

export const Route = createFileRoute('/auth/callback')({
  component: AuthCallbackComponent,
});

function AuthCallbackComponent() {
  const navigate = useNavigate();
  const { session, error: authError, isLoading, isInitialized } = useAuth();

  useEffect(() => {
    if (!isInitialized) {
      // Still waiting for useAuth to initialize and process the auth state
      return;
    }

    if (authError) {
      toast.error(authError.message || 'Authentication failed. Please try again.');
      navigate({ to: '/login', replace: true });
    } else if (session) {
      toast.success('Successfully signed in!');
      navigate({ to: '/dashboard', replace: true });
    } else {
      // This case might occur if the callback was invalid or already processed, 
      // and no session was established, and no explicit error was thrown by useAuth from the callback processing.
      toast.error('Could not establish a session. Please log in.');
      navigate({ to: '/login', replace: true });
    }
  }, [session, authError, isLoading, isInitialized, navigate]);

  // Display a user-friendly loading message
  return (
    <div className="flex flex-col items-center justify-center min-h-screen p-4 text-center">
      <svg className="animate-spin h-12 w-12 text-primary mb-4" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
        <circle className="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" strokeWidth="4"></circle>
        <path className="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
      </svg>
      <h2 className="text-2xl font-semibold mb-2">Processing Authentication...</h2>
      <p className="text-gray-600">Please wait while we securely sign you in. You will be redirected shortly.</p>
    </div>
  );
}
</file>

<file path="apps/web/src/routes/__root.tsx">
import {
	HeadContent,
	Link,
	Outlet,
	Scripts,
	createRootRoute,
  } from '@tanstack/react-router'
  import { TanStackRouterDevtools } from '@tanstack/react-router-devtools'
  import * as React from 'react'
  import { QueryClient, QueryClientProvider } from '@tanstack/react-query'
  import { DefaultCatchBoundary } from '@/components/default-catch-boundary'
  import { NotFound } from '@/components/not-found'
  import appCss from '@/styles/app.css?url'
  import { seo } from '@/utils/seo'
  import { useAuth } from '../hooks/useAuth'

  // Create a single QueryClient instance for the app
  const queryClient = new QueryClient()
  
  export const Route = createRootRoute({
	head: () => ({
	  meta: [
		{
		  charSet: 'utf-8',
		},
		{
		  name: 'viewport',
		  content: 'width=device-width, initial-scale=1',
		},
		...seo({
		  title:
			'TanStack Start | Type-Safe, Client-First, Full-Stack React Framework',
		  description: `TanStack Start is a type-safe, client-first, full-stack React framework. `,
		}),
	  ],
	  links: [
		{ rel: 'stylesheet', href: appCss },
		{
		  rel: 'apple-touch-icon',
		  sizes: '180x180',
		  href: '/apple-touch-icon.png',
		},
		{
		  rel: 'icon',
		  type: 'image/png',
		  sizes: '32x32',
		  href: '/favicon-32x32.png',
		},
		{
		  rel: 'icon',
		  type: 'image/png',
		  sizes: '16x16',
		  href: '/favicon-16x16.png',
		},
		{ rel: 'manifest', href: '/site.webmanifest', color: '#fffff' },
		{ rel: 'icon', href: '/favicon.ico' },
	  ],
	}),
	errorComponent: (props) => {
	  return (
		<RootDocument>
		  <DefaultCatchBoundary {...props} />
		</RootDocument>
	  )
	},
	notFoundComponent: () => <NotFound />,
	component: RootComponent,
  })
  
  function RootComponent() {
	return (
	  <RootDocument>
		<Outlet />
	  </RootDocument>
	)
  }
  
  function RootDocument({ children }: { children: React.ReactNode }) {
	const { session, signOut, isLoading: authIsLoading } = useAuth()

	return (
	  <html>
		<head>
		  <HeadContent />
		</head>
		<body>
		  <QueryClientProvider client={queryClient}>
			<div className="p-2 flex flex-wrap sm:flex-nowrap gap-2 text-base sm:text-lg items-center">
			  <Link
				to="/"
				activeProps={{
				  className: 'font-bold',
				}}
				activeOptions={{ exact: true }}
			  >
				Home
			  </Link>{' '}
			  {authIsLoading ? (
				<span className="text-sm text-gray-500">Loading auth...</span>
			  ) : session ? (
				<>
				  <Link
					to="/dashboard"
					activeProps={{
					  className: 'font-bold',
					}}
				  >
					Dashboard
				  </Link>{' '}
				  <button
					onClick={() => signOut()}
					className="text-blue-600 hover:text-blue-800 text-base sm:text-lg"
				  >
					Logout
				  </button>
				</>
			  ) : (
				<Link
				  to="/login"
				  activeProps={{
					className: 'font-bold',
				  }}
				>
				  Login
				</Link>
			  )}
			</div>
			<hr />
			{children}
			<TanStackRouterDevtools position="bottom-right" />
			<Scripts />
		  </QueryClientProvider>
		</body>
	  </html>
	)
  }
</file>

<file path="apps/web/src/routes/signup.tsx">
import { createFileRoute, Link, useRouter, redirect } from "@tanstack/react-router";
import { useAuth } from "../hooks/useAuth";
import { useForm } from "react-hook-form";
import { zodResolver } from "@hookform/resolvers/zod";
import * as z from "zod";
import { Button } from "@/components/ui/button";
import {
	Form,
	FormControl,
	FormField,
	FormItem,
	FormLabel,
	FormMessage,
} from "@/components/ui/form";
import { Input } from "@/components/ui/input";
import { toast } from "sonner";
import { supabase } from "@echo/db/clients/client"; // Import supabase client

// Define the schema for signup form validation
const signupSchema = z.object({
	email: z.string().email({ message: "Invalid email address." }),
	password: z.string().min(8, { message: "Password must be at least 8 characters." }),
	confirmPassword: z.string(),
}).refine((data) => data.password === data.confirmPassword, {
	message: "Passwords do not match.",
	path: ["confirmPassword"], // Path to field where the error message should be shown
});

type SignupFormValues = z.infer<typeof signupSchema>;

export const Route = createFileRoute("/signup")({
	beforeLoad: async ({ cause }) => {
		// Redirect to dashboard if user is already logged in and tries to enter the signup page.
		if (cause === 'enter') { 
			const { data: { session } } = await supabase.auth.getSession();
			if (session) {
				throw redirect({
					to: "/dashboard",
					replace: true,
				});
			}
		}
	},
	component: SignupComponent,
});

function SignupComponent() {
	const router = useRouter();
	const { signUpWithEmailPassword, isLoading, error: authError, isInitialized } = useAuth();

	const form = useForm<SignupFormValues>({
		resolver: zodResolver(signupSchema),
		defaultValues: {
			email: "",
			password: "",
			confirmPassword: "",
		},
	});

	const onSubmit = async (values: SignupFormValues) => {
		const { error } = await signUpWithEmailPassword({
			email: values.email,
			password: values.password,
		});

		if (error) {
			toast.error(error.message || "Signup failed. Please try again.");
		} else {
			// Supabase typically sends a confirmation email.
			toast.success("Signup successful! Please check your email to confirm your account.");
			// Optionally, clear form or redirect to a page indicating to check email
			form.reset();
			// router.navigate({ to: "/check-email" }); // Example: if you have such a page
			// For now, we can redirect to login or home after a delay, or let user click away.
			// router.navigate({ to: "/login" }); 
		}
	};

	return (
		<div className="max-w-md mx-auto mt-8 p-6 border rounded-lg shadow-md">
			<h2 className="text-2xl font-semibold text-center mb-6">Create Account</h2>
			<Form {...form}>
				<form onSubmit={form.handleSubmit(onSubmit)} className="space-y-4">
					<FormField
						control={form.control}
						name="email"
						render={({ field }) => (
							<FormItem>
								<FormLabel>Email</FormLabel>
								<FormControl>
									<Input placeholder="your@email.com" {...field} />
								</FormControl>
								<FormMessage />
							</FormItem>
						)}
					/>
					<FormField
						control={form.control}
						name="password"
						render={({ field }) => (
							<FormItem>
								<FormLabel>Password</FormLabel>
								<FormControl>
									<Input type="password" placeholder="" {...field} />
								</FormControl>
								<FormMessage />
							</FormItem>
						)}
					/>
					<FormField
						control={form.control}
						name="confirmPassword"
						render={({ field }) => (
							<FormItem>
								<FormLabel>Confirm Password</FormLabel>
								<FormControl>
									<Input type="password" placeholder="" {...field} />
								</FormControl>
								<FormMessage />
							</FormItem>
						)}
					/>

					{authError && (
						<p className="text-sm text-red-500 text-center">{authError.message}</p>
					)}

					<Button type="submit" className="w-full" disabled={isLoading}>
						{isLoading ? (
							<>
								<svg className="animate-spin -ml-1 mr-3 h-5 w-5 text-white" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
									<circle className="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" strokeWidth="4"></circle>
									<path className="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
								</svg>
								Creating account...
							</>
						) : "Create Account"}
					</Button>
				</form>
			</Form>
			<p className="mt-6 text-center text-sm">
				Already have an account?{" "}
				<Link to="/login" className="font-medium text-primary hover:underline">
					Log in
				</Link>
			</p>
		</div>
	);
}
</file>

<file path="apps/web/src/routeTree.gen.ts">
/* eslint-disable */

// @ts-nocheck

// noinspection JSUnusedGlobalSymbols

// This file was automatically generated by TanStack Router.
// You should NOT make any changes in this file as it will be overwritten.
// Additionally, you should also exclude this file from your linter and/or formatter to prevent it from being checked or modified.

// Import Routes

import { Route as rootRoute } from './routes/__root'
import { Route as SignupImport } from './routes/signup'
import { Route as SettingsImport } from './routes/settings'
import { Route as RedirectImport } from './routes/redirect'
import { Route as LogoutImport } from './routes/logout'
import { Route as LoginImport } from './routes/login'
import { Route as DeferredImport } from './routes/deferred'
import { Route as DashboardImport } from './routes/dashboard'
import { Route as PathlessLayoutImport } from './routes/_pathlessLayout'
import { Route as AuthedImport } from './routes/_authed'
import { Route as UsersRouteImport } from './routes/users.route'
import { Route as IndexImport } from './routes/index'
import { Route as AuthCallbackImport } from './routes/auth/callback'
import { Route as ApiUsersImport } from './routes/api/users'
import { Route as PathlessLayoutNestedLayoutImport } from './routes/_pathlessLayout/_nested-layout'
import { Route as AuthedPostsImport } from './routes/_authed/posts'
import { Route as AuthedPostsIndexImport } from './routes/_authed/posts.index'
import { Route as DashboardE2eTestImport } from './routes/dashboard.e2e.test'
import { Route as ApiUsersIdImport } from './routes/api/users.$id'
import { Route as PathlessLayoutNestedLayoutRouteBImport } from './routes/_pathlessLayout/_nested-layout/route-b'
import { Route as PathlessLayoutNestedLayoutRouteAImport } from './routes/_pathlessLayout/_nested-layout/route-a'
import { Route as AuthedVideoVideoIdImport } from './routes/_authed/video/$videoId'
import { Route as AuthedPostsPostIdImport } from './routes/_authed/posts.$postId'
import { Route as AuthedJobsjobIdImport } from './routes/_authed.jobs.[jobId]'
import { Route as AuthedJobsJobIdImport } from './routes/_authed.jobs.$jobId'

// Create/Update Routes

const SignupRoute = SignupImport.update({
  id: '/signup',
  path: '/signup',
  getParentRoute: () => rootRoute,
} as any)

const SettingsRoute = SettingsImport.update({
  id: '/settings',
  path: '/settings',
  getParentRoute: () => rootRoute,
} as any)

const RedirectRoute = RedirectImport.update({
  id: '/redirect',
  path: '/redirect',
  getParentRoute: () => rootRoute,
} as any)

const LogoutRoute = LogoutImport.update({
  id: '/logout',
  path: '/logout',
  getParentRoute: () => rootRoute,
} as any)

const LoginRoute = LoginImport.update({
  id: '/login',
  path: '/login',
  getParentRoute: () => rootRoute,
} as any)

const DeferredRoute = DeferredImport.update({
  id: '/deferred',
  path: '/deferred',
  getParentRoute: () => rootRoute,
} as any)

const DashboardRoute = DashboardImport.update({
  id: '/dashboard',
  path: '/dashboard',
  getParentRoute: () => rootRoute,
} as any)

const PathlessLayoutRoute = PathlessLayoutImport.update({
  id: '/_pathlessLayout',
  getParentRoute: () => rootRoute,
} as any)

const AuthedRoute = AuthedImport.update({
  id: '/_authed',
  getParentRoute: () => rootRoute,
} as any)

const UsersRouteRoute = UsersRouteImport.update({
  id: '/users',
  path: '/users',
  getParentRoute: () => rootRoute,
} as any)

const IndexRoute = IndexImport.update({
  id: '/',
  path: '/',
  getParentRoute: () => rootRoute,
} as any)

const AuthCallbackRoute = AuthCallbackImport.update({
  id: '/auth/callback',
  path: '/auth/callback',
  getParentRoute: () => rootRoute,
} as any)

const ApiUsersRoute = ApiUsersImport.update({
  id: '/api/users',
  path: '/api/users',
  getParentRoute: () => rootRoute,
} as any)

const PathlessLayoutNestedLayoutRoute = PathlessLayoutNestedLayoutImport.update(
  {
    id: '/_nested-layout',
    getParentRoute: () => PathlessLayoutRoute,
  } as any,
)

const AuthedPostsRoute = AuthedPostsImport.update({
  id: '/posts',
  path: '/posts',
  getParentRoute: () => AuthedRoute,
} as any)

const AuthedPostsIndexRoute = AuthedPostsIndexImport.update({
  id: '/',
  path: '/',
  getParentRoute: () => AuthedPostsRoute,
} as any)

const DashboardE2eTestRoute = DashboardE2eTestImport.update({
  id: '/e2e/test',
  path: '/e2e/test',
  getParentRoute: () => DashboardRoute,
} as any)

const ApiUsersIdRoute = ApiUsersIdImport.update({
  id: '/$id',
  path: '/$id',
  getParentRoute: () => ApiUsersRoute,
} as any)

const PathlessLayoutNestedLayoutRouteBRoute =
  PathlessLayoutNestedLayoutRouteBImport.update({
    id: '/route-b',
    path: '/route-b',
    getParentRoute: () => PathlessLayoutNestedLayoutRoute,
  } as any)

const PathlessLayoutNestedLayoutRouteARoute =
  PathlessLayoutNestedLayoutRouteAImport.update({
    id: '/route-a',
    path: '/route-a',
    getParentRoute: () => PathlessLayoutNestedLayoutRoute,
  } as any)

const AuthedVideoVideoIdRoute = AuthedVideoVideoIdImport.update({
  id: '/video/$videoId',
  path: '/video/$videoId',
  getParentRoute: () => AuthedRoute,
} as any)

const AuthedPostsPostIdRoute = AuthedPostsPostIdImport.update({
  id: '/$postId',
  path: '/$postId',
  getParentRoute: () => AuthedPostsRoute,
} as any)

const AuthedJobsjobIdRoute = AuthedJobsjobIdImport.update({
  id: '/jobs/[jobId]',
  path: '/jobs/[jobId]',
  getParentRoute: () => AuthedRoute,
} as any)

const AuthedJobsJobIdRoute = AuthedJobsJobIdImport.update({
  id: '/jobs/$jobId',
  path: '/jobs/$jobId',
  getParentRoute: () => AuthedRoute,
} as any)

// Populate the FileRoutesByPath interface

declare module '@tanstack/react-router' {
  interface FileRoutesByPath {
    '/': {
      id: '/'
      path: '/'
      fullPath: '/'
      preLoaderRoute: typeof IndexImport
      parentRoute: typeof rootRoute
    }
    '/users': {
      id: '/users'
      path: '/users'
      fullPath: '/users'
      preLoaderRoute: typeof UsersRouteImport
      parentRoute: typeof rootRoute
    }
    '/_authed': {
      id: '/_authed'
      path: ''
      fullPath: ''
      preLoaderRoute: typeof AuthedImport
      parentRoute: typeof rootRoute
    }
    '/_pathlessLayout': {
      id: '/_pathlessLayout'
      path: ''
      fullPath: ''
      preLoaderRoute: typeof PathlessLayoutImport
      parentRoute: typeof rootRoute
    }
    '/dashboard': {
      id: '/dashboard'
      path: '/dashboard'
      fullPath: '/dashboard'
      preLoaderRoute: typeof DashboardImport
      parentRoute: typeof rootRoute
    }
    '/deferred': {
      id: '/deferred'
      path: '/deferred'
      fullPath: '/deferred'
      preLoaderRoute: typeof DeferredImport
      parentRoute: typeof rootRoute
    }
    '/login': {
      id: '/login'
      path: '/login'
      fullPath: '/login'
      preLoaderRoute: typeof LoginImport
      parentRoute: typeof rootRoute
    }
    '/logout': {
      id: '/logout'
      path: '/logout'
      fullPath: '/logout'
      preLoaderRoute: typeof LogoutImport
      parentRoute: typeof rootRoute
    }
    '/redirect': {
      id: '/redirect'
      path: '/redirect'
      fullPath: '/redirect'
      preLoaderRoute: typeof RedirectImport
      parentRoute: typeof rootRoute
    }
    '/settings': {
      id: '/settings'
      path: '/settings'
      fullPath: '/settings'
      preLoaderRoute: typeof SettingsImport
      parentRoute: typeof rootRoute
    }
    '/signup': {
      id: '/signup'
      path: '/signup'
      fullPath: '/signup'
      preLoaderRoute: typeof SignupImport
      parentRoute: typeof rootRoute
    }
    '/_authed/posts': {
      id: '/_authed/posts'
      path: '/posts'
      fullPath: '/posts'
      preLoaderRoute: typeof AuthedPostsImport
      parentRoute: typeof AuthedImport
    }
    '/_pathlessLayout/_nested-layout': {
      id: '/_pathlessLayout/_nested-layout'
      path: ''
      fullPath: ''
      preLoaderRoute: typeof PathlessLayoutNestedLayoutImport
      parentRoute: typeof PathlessLayoutImport
    }
    '/api/users': {
      id: '/api/users'
      path: '/api/users'
      fullPath: '/api/users'
      preLoaderRoute: typeof ApiUsersImport
      parentRoute: typeof rootRoute
    }
    '/auth/callback': {
      id: '/auth/callback'
      path: '/auth/callback'
      fullPath: '/auth/callback'
      preLoaderRoute: typeof AuthCallbackImport
      parentRoute: typeof rootRoute
    }
    '/_authed/jobs/$jobId': {
      id: '/_authed/jobs/$jobId'
      path: '/jobs/$jobId'
      fullPath: '/jobs/$jobId'
      preLoaderRoute: typeof AuthedJobsJobIdImport
      parentRoute: typeof AuthedImport
    }
    '/_authed/jobs/[jobId]': {
      id: '/_authed/jobs/[jobId]'
      path: '/jobs/[jobId]'
      fullPath: '/jobs/[jobId]'
      preLoaderRoute: typeof AuthedJobsjobIdImport
      parentRoute: typeof AuthedImport
    }
    '/_authed/posts/$postId': {
      id: '/_authed/posts/$postId'
      path: '/$postId'
      fullPath: '/posts/$postId'
      preLoaderRoute: typeof AuthedPostsPostIdImport
      parentRoute: typeof AuthedPostsImport
    }
    '/_authed/video/$videoId': {
      id: '/_authed/video/$videoId'
      path: '/video/$videoId'
      fullPath: '/video/$videoId'
      preLoaderRoute: typeof AuthedVideoVideoIdImport
      parentRoute: typeof AuthedImport
    }
    '/_pathlessLayout/_nested-layout/route-a': {
      id: '/_pathlessLayout/_nested-layout/route-a'
      path: '/route-a'
      fullPath: '/route-a'
      preLoaderRoute: typeof PathlessLayoutNestedLayoutRouteAImport
      parentRoute: typeof PathlessLayoutNestedLayoutImport
    }
    '/_pathlessLayout/_nested-layout/route-b': {
      id: '/_pathlessLayout/_nested-layout/route-b'
      path: '/route-b'
      fullPath: '/route-b'
      preLoaderRoute: typeof PathlessLayoutNestedLayoutRouteBImport
      parentRoute: typeof PathlessLayoutNestedLayoutImport
    }
    '/api/users/$id': {
      id: '/api/users/$id'
      path: '/$id'
      fullPath: '/api/users/$id'
      preLoaderRoute: typeof ApiUsersIdImport
      parentRoute: typeof ApiUsersImport
    }
    '/dashboard/e2e/test': {
      id: '/dashboard/e2e/test'
      path: '/e2e/test'
      fullPath: '/dashboard/e2e/test'
      preLoaderRoute: typeof DashboardE2eTestImport
      parentRoute: typeof DashboardImport
    }
    '/_authed/posts/': {
      id: '/_authed/posts/'
      path: '/'
      fullPath: '/posts/'
      preLoaderRoute: typeof AuthedPostsIndexImport
      parentRoute: typeof AuthedPostsImport
    }
  }
}

// Create and export the route tree

interface AuthedPostsRouteChildren {
  AuthedPostsPostIdRoute: typeof AuthedPostsPostIdRoute
  AuthedPostsIndexRoute: typeof AuthedPostsIndexRoute
}

const AuthedPostsRouteChildren: AuthedPostsRouteChildren = {
  AuthedPostsPostIdRoute: AuthedPostsPostIdRoute,
  AuthedPostsIndexRoute: AuthedPostsIndexRoute,
}

const AuthedPostsRouteWithChildren = AuthedPostsRoute._addFileChildren(
  AuthedPostsRouteChildren,
)

interface AuthedRouteChildren {
  AuthedPostsRoute: typeof AuthedPostsRouteWithChildren
  AuthedJobsJobIdRoute: typeof AuthedJobsJobIdRoute
  AuthedJobsjobIdRoute: typeof AuthedJobsjobIdRoute
  AuthedVideoVideoIdRoute: typeof AuthedVideoVideoIdRoute
}

const AuthedRouteChildren: AuthedRouteChildren = {
  AuthedPostsRoute: AuthedPostsRouteWithChildren,
  AuthedJobsJobIdRoute: AuthedJobsJobIdRoute,
  AuthedJobsjobIdRoute: AuthedJobsjobIdRoute,
  AuthedVideoVideoIdRoute: AuthedVideoVideoIdRoute,
}

const AuthedRouteWithChildren =
  AuthedRoute._addFileChildren(AuthedRouteChildren)

interface PathlessLayoutNestedLayoutRouteChildren {
  PathlessLayoutNestedLayoutRouteARoute: typeof PathlessLayoutNestedLayoutRouteARoute
  PathlessLayoutNestedLayoutRouteBRoute: typeof PathlessLayoutNestedLayoutRouteBRoute
}

const PathlessLayoutNestedLayoutRouteChildren: PathlessLayoutNestedLayoutRouteChildren =
  {
    PathlessLayoutNestedLayoutRouteARoute:
      PathlessLayoutNestedLayoutRouteARoute,
    PathlessLayoutNestedLayoutRouteBRoute:
      PathlessLayoutNestedLayoutRouteBRoute,
  }

const PathlessLayoutNestedLayoutRouteWithChildren =
  PathlessLayoutNestedLayoutRoute._addFileChildren(
    PathlessLayoutNestedLayoutRouteChildren,
  )

interface PathlessLayoutRouteChildren {
  PathlessLayoutNestedLayoutRoute: typeof PathlessLayoutNestedLayoutRouteWithChildren
}

const PathlessLayoutRouteChildren: PathlessLayoutRouteChildren = {
  PathlessLayoutNestedLayoutRoute: PathlessLayoutNestedLayoutRouteWithChildren,
}

const PathlessLayoutRouteWithChildren = PathlessLayoutRoute._addFileChildren(
  PathlessLayoutRouteChildren,
)

interface DashboardRouteChildren {
  DashboardE2eTestRoute: typeof DashboardE2eTestRoute
}

const DashboardRouteChildren: DashboardRouteChildren = {
  DashboardE2eTestRoute: DashboardE2eTestRoute,
}

const DashboardRouteWithChildren = DashboardRoute._addFileChildren(
  DashboardRouteChildren,
)

interface ApiUsersRouteChildren {
  ApiUsersIdRoute: typeof ApiUsersIdRoute
}

const ApiUsersRouteChildren: ApiUsersRouteChildren = {
  ApiUsersIdRoute: ApiUsersIdRoute,
}

const ApiUsersRouteWithChildren = ApiUsersRoute._addFileChildren(
  ApiUsersRouteChildren,
)

export interface FileRoutesByFullPath {
  '/': typeof IndexRoute
  '/users': typeof UsersRouteRoute
  '': typeof PathlessLayoutNestedLayoutRouteWithChildren
  '/dashboard': typeof DashboardRouteWithChildren
  '/deferred': typeof DeferredRoute
  '/login': typeof LoginRoute
  '/logout': typeof LogoutRoute
  '/redirect': typeof RedirectRoute
  '/settings': typeof SettingsRoute
  '/signup': typeof SignupRoute
  '/posts': typeof AuthedPostsRouteWithChildren
  '/api/users': typeof ApiUsersRouteWithChildren
  '/auth/callback': typeof AuthCallbackRoute
  '/jobs/$jobId': typeof AuthedJobsJobIdRoute
  '/jobs/[jobId]': typeof AuthedJobsjobIdRoute
  '/posts/$postId': typeof AuthedPostsPostIdRoute
  '/video/$videoId': typeof AuthedVideoVideoIdRoute
  '/route-a': typeof PathlessLayoutNestedLayoutRouteARoute
  '/route-b': typeof PathlessLayoutNestedLayoutRouteBRoute
  '/api/users/$id': typeof ApiUsersIdRoute
  '/dashboard/e2e/test': typeof DashboardE2eTestRoute
  '/posts/': typeof AuthedPostsIndexRoute
}

export interface FileRoutesByTo {
  '/': typeof IndexRoute
  '/users': typeof UsersRouteRoute
  '': typeof PathlessLayoutNestedLayoutRouteWithChildren
  '/dashboard': typeof DashboardRouteWithChildren
  '/deferred': typeof DeferredRoute
  '/login': typeof LoginRoute
  '/logout': typeof LogoutRoute
  '/redirect': typeof RedirectRoute
  '/settings': typeof SettingsRoute
  '/signup': typeof SignupRoute
  '/api/users': typeof ApiUsersRouteWithChildren
  '/auth/callback': typeof AuthCallbackRoute
  '/jobs/$jobId': typeof AuthedJobsJobIdRoute
  '/jobs/[jobId]': typeof AuthedJobsjobIdRoute
  '/posts/$postId': typeof AuthedPostsPostIdRoute
  '/video/$videoId': typeof AuthedVideoVideoIdRoute
  '/route-a': typeof PathlessLayoutNestedLayoutRouteARoute
  '/route-b': typeof PathlessLayoutNestedLayoutRouteBRoute
  '/api/users/$id': typeof ApiUsersIdRoute
  '/dashboard/e2e/test': typeof DashboardE2eTestRoute
  '/posts': typeof AuthedPostsIndexRoute
}

export interface FileRoutesById {
  __root__: typeof rootRoute
  '/': typeof IndexRoute
  '/users': typeof UsersRouteRoute
  '/_authed': typeof AuthedRouteWithChildren
  '/_pathlessLayout': typeof PathlessLayoutRouteWithChildren
  '/dashboard': typeof DashboardRouteWithChildren
  '/deferred': typeof DeferredRoute
  '/login': typeof LoginRoute
  '/logout': typeof LogoutRoute
  '/redirect': typeof RedirectRoute
  '/settings': typeof SettingsRoute
  '/signup': typeof SignupRoute
  '/_authed/posts': typeof AuthedPostsRouteWithChildren
  '/_pathlessLayout/_nested-layout': typeof PathlessLayoutNestedLayoutRouteWithChildren
  '/api/users': typeof ApiUsersRouteWithChildren
  '/auth/callback': typeof AuthCallbackRoute
  '/_authed/jobs/$jobId': typeof AuthedJobsJobIdRoute
  '/_authed/jobs/[jobId]': typeof AuthedJobsjobIdRoute
  '/_authed/posts/$postId': typeof AuthedPostsPostIdRoute
  '/_authed/video/$videoId': typeof AuthedVideoVideoIdRoute
  '/_pathlessLayout/_nested-layout/route-a': typeof PathlessLayoutNestedLayoutRouteARoute
  '/_pathlessLayout/_nested-layout/route-b': typeof PathlessLayoutNestedLayoutRouteBRoute
  '/api/users/$id': typeof ApiUsersIdRoute
  '/dashboard/e2e/test': typeof DashboardE2eTestRoute
  '/_authed/posts/': typeof AuthedPostsIndexRoute
}

export interface FileRouteTypes {
  fileRoutesByFullPath: FileRoutesByFullPath
  fullPaths:
    | '/'
    | '/users'
    | ''
    | '/dashboard'
    | '/deferred'
    | '/login'
    | '/logout'
    | '/redirect'
    | '/settings'
    | '/signup'
    | '/posts'
    | '/api/users'
    | '/auth/callback'
    | '/jobs/$jobId'
    | '/jobs/[jobId]'
    | '/posts/$postId'
    | '/video/$videoId'
    | '/route-a'
    | '/route-b'
    | '/api/users/$id'
    | '/dashboard/e2e/test'
    | '/posts/'
  fileRoutesByTo: FileRoutesByTo
  to:
    | '/'
    | '/users'
    | ''
    | '/dashboard'
    | '/deferred'
    | '/login'
    | '/logout'
    | '/redirect'
    | '/settings'
    | '/signup'
    | '/api/users'
    | '/auth/callback'
    | '/jobs/$jobId'
    | '/jobs/[jobId]'
    | '/posts/$postId'
    | '/video/$videoId'
    | '/route-a'
    | '/route-b'
    | '/api/users/$id'
    | '/dashboard/e2e/test'
    | '/posts'
  id:
    | '__root__'
    | '/'
    | '/users'
    | '/_authed'
    | '/_pathlessLayout'
    | '/dashboard'
    | '/deferred'
    | '/login'
    | '/logout'
    | '/redirect'
    | '/settings'
    | '/signup'
    | '/_authed/posts'
    | '/_pathlessLayout/_nested-layout'
    | '/api/users'
    | '/auth/callback'
    | '/_authed/jobs/$jobId'
    | '/_authed/jobs/[jobId]'
    | '/_authed/posts/$postId'
    | '/_authed/video/$videoId'
    | '/_pathlessLayout/_nested-layout/route-a'
    | '/_pathlessLayout/_nested-layout/route-b'
    | '/api/users/$id'
    | '/dashboard/e2e/test'
    | '/_authed/posts/'
  fileRoutesById: FileRoutesById
}

export interface RootRouteChildren {
  IndexRoute: typeof IndexRoute
  UsersRouteRoute: typeof UsersRouteRoute
  AuthedRoute: typeof AuthedRouteWithChildren
  PathlessLayoutRoute: typeof PathlessLayoutRouteWithChildren
  DashboardRoute: typeof DashboardRouteWithChildren
  DeferredRoute: typeof DeferredRoute
  LoginRoute: typeof LoginRoute
  LogoutRoute: typeof LogoutRoute
  RedirectRoute: typeof RedirectRoute
  SettingsRoute: typeof SettingsRoute
  SignupRoute: typeof SignupRoute
  ApiUsersRoute: typeof ApiUsersRouteWithChildren
  AuthCallbackRoute: typeof AuthCallbackRoute
}

const rootRouteChildren: RootRouteChildren = {
  IndexRoute: IndexRoute,
  UsersRouteRoute: UsersRouteRoute,
  AuthedRoute: AuthedRouteWithChildren,
  PathlessLayoutRoute: PathlessLayoutRouteWithChildren,
  DashboardRoute: DashboardRouteWithChildren,
  DeferredRoute: DeferredRoute,
  LoginRoute: LoginRoute,
  LogoutRoute: LogoutRoute,
  RedirectRoute: RedirectRoute,
  SettingsRoute: SettingsRoute,
  SignupRoute: SignupRoute,
  ApiUsersRoute: ApiUsersRouteWithChildren,
  AuthCallbackRoute: AuthCallbackRoute,
}

export const routeTree = rootRoute
  ._addFileChildren(rootRouteChildren)
  ._addFileTypes<FileRouteTypes>()

/* ROUTE_MANIFEST_START
{
  "routes": {
    "__root__": {
      "filePath": "__root.tsx",
      "children": [
        "/",
        "/users",
        "/_authed",
        "/_pathlessLayout",
        "/dashboard",
        "/deferred",
        "/login",
        "/logout",
        "/redirect",
        "/settings",
        "/signup",
        "/api/users",
        "/auth/callback"
      ]
    },
    "/": {
      "filePath": "index.tsx"
    },
    "/users": {
      "filePath": "users.route.tsx"
    },
    "/_authed": {
      "filePath": "_authed.tsx",
      "children": [
        "/_authed/posts",
        "/_authed/jobs/$jobId",
        "/_authed/jobs/[jobId]",
        "/_authed/video/$videoId"
      ]
    },
    "/_pathlessLayout": {
      "filePath": "_pathlessLayout.tsx",
      "children": [
        "/_pathlessLayout/_nested-layout"
      ]
    },
    "/dashboard": {
      "filePath": "dashboard.tsx",
      "children": [
        "/dashboard/e2e/test"
      ]
    },
    "/deferred": {
      "filePath": "deferred.tsx"
    },
    "/login": {
      "filePath": "login.tsx"
    },
    "/logout": {
      "filePath": "logout.tsx"
    },
    "/redirect": {
      "filePath": "redirect.tsx"
    },
    "/settings": {
      "filePath": "settings.tsx"
    },
    "/signup": {
      "filePath": "signup.tsx"
    },
    "/_authed/posts": {
      "filePath": "_authed/posts.tsx",
      "parent": "/_authed",
      "children": [
        "/_authed/posts/$postId",
        "/_authed/posts/"
      ]
    },
    "/_pathlessLayout/_nested-layout": {
      "filePath": "_pathlessLayout/_nested-layout.tsx",
      "parent": "/_pathlessLayout",
      "children": [
        "/_pathlessLayout/_nested-layout/route-a",
        "/_pathlessLayout/_nested-layout/route-b"
      ]
    },
    "/api/users": {
      "filePath": "api/users.ts",
      "children": [
        "/api/users/$id"
      ]
    },
    "/auth/callback": {
      "filePath": "auth/callback.tsx"
    },
    "/_authed/jobs/$jobId": {
      "filePath": "_authed.jobs.$jobId.tsx",
      "parent": "/_authed"
    },
    "/_authed/jobs/[jobId]": {
      "filePath": "_authed.jobs.[jobId].tsx",
      "parent": "/_authed"
    },
    "/_authed/posts/$postId": {
      "filePath": "_authed/posts.$postId.tsx",
      "parent": "/_authed/posts"
    },
    "/_authed/video/$videoId": {
      "filePath": "_authed/video/$videoId.tsx",
      "parent": "/_authed"
    },
    "/_pathlessLayout/_nested-layout/route-a": {
      "filePath": "_pathlessLayout/_nested-layout/route-a.tsx",
      "parent": "/_pathlessLayout/_nested-layout"
    },
    "/_pathlessLayout/_nested-layout/route-b": {
      "filePath": "_pathlessLayout/_nested-layout/route-b.tsx",
      "parent": "/_pathlessLayout/_nested-layout"
    },
    "/api/users/$id": {
      "filePath": "api/users.$id.ts",
      "parent": "/api/users"
    },
    "/dashboard/e2e/test": {
      "filePath": "dashboard.e2e.test.tsx",
      "parent": "/dashboard"
    },
    "/_authed/posts/": {
      "filePath": "_authed/posts.index.tsx",
      "parent": "/_authed/posts"
    }
  }
}
ROUTE_MANIFEST_END */
</file>

<file path="apps/web/tsconfig.json">
{
	"include": [
		"**/*.ts",
		"**/*.tsx",
		"../../supabase/**/*.ts"
	],
	"compilerOptions": {
		"strict": true,
		"esModuleInterop": true,
		"jsx": "react-jsx",
		"module": "ESNext",
		"moduleResolution": "Bundler",
		"lib": [
			"DOM",
			"DOM.Iterable",
			"ES2022"
		],
		"isolatedModules": true,
		"resolveJsonModule": true,
		"skipLibCheck": true,
		"target": "ES2022",
		"allowJs": true,
		"forceConsistentCasingInFileNames": true,
		"baseUrl": "./src",
		"paths": {
			"@/*": [
				"./*"
			],
			"@echo/db": [
				"../../supabase"
			],
			"@echo/db/*": [
				"../../supabase/*"
			]
		},
		"noEmit": true
	}
}
</file>

<file path="apps/web/vite.config.ts">
// vite.config.ts
import { defineConfig } from 'vite'
import react from '@vitejs/plugin-react'
import { TanStackRouterVite } from '@tanstack/router-plugin/vite'
import tsconfigPaths from 'vite-tsconfig-paths'
import { fileURLToPath, URL } from 'url'

// https://vitejs.dev/config/
export default defineConfig({
  plugins: [
    // Please make sure that '@tanstack/router-plugin' is passed before '@vitejs/plugin-react'
    TanStackRouterVite({ target: 'react', autoCodeSplitting: true }),
    react(),
    tsconfigPaths(),
    // ...,
  ],
  resolve: {
    alias: {
      '@': fileURLToPath(new URL('./src', import.meta.url)),
    }
  }
})
</file>

<file path="echo.code-workspace">
{
    "folders": [
        {
            "path": "."
        }
    ],
    "settings": {
        "python.testing.pytestEnabled": true,
        "python.testing.unittestEnabled": false,
        "python.testing.nosetestsEnabled": false,
        "python.testing.pytestArgs": [
            "."
        ],
        "python.testing.autoTestDiscoverOnSaveEnabled": true,
        "python.terminal.launchArgs": [
            "-m",
            "IPython"
        ],
        "javascript.updateImportsOnFileMove.enabled": "always",
        "typescript.updateImportsOnFileMove.enabled": "always",
        "markdown.updateLinksOnFileMove.enabled": "always",
        "python.analysis.autoImportCompletions": true,
        "python.terminal.activateEnvInCurrentTerminal": true,
        // "python.analysis.extraPaths": [
        //     "." // This is broad; .vscode/settings.json has a more specific one for apps/core
        // ],
        // "terminal.integrated.env.osx": {
        //     "PYTHONPATH": "${workspaceFolder}" // Prefer venv activation for PYTHONPATH
        // },
        // "terminal.integrated.env.linux": {
        //     "PYTHONPATH": "${workspaceFolder}"
        // },
        // "terminal.integrated.env.windows": {
        //     "PYTHONPATH": "${workspaceFolder}"
        // }
    }
}
</file>

<file path="apps/core/operations/video_job_repository.py">
"""
Repository for VideoJobModel data access operations.

This module provides a repository class implementing the repository pattern for
VideoJobModel entities. It abstracts database operations for creating, retrieving,
and updating video processing jobs. The repository handles the persistence details
while keeping business logic separate, accepting a SQLAlchemy Session for all operations.

Usage:
    from sqlalchemy.orm import Session
    from apps.core.models.enums import ProcessingStatus
    from apps.core.operations.video_job_repository import VideoJobRepository

    # Create a new video processing job
    job_repo = VideoJobRepository()
    new_job = job_repo.create(
        db=db_session,
        video_id=1,
        status=ProcessingStatus.PENDING,
        processing_stages={"transcription": False}
    )

    # Update job status
    updated_job = job_repo.update_status(
        db=db_session,
        job_id=new_job.id,
        status=ProcessingStatus.PROCESSING
    )

    # Record a processing stage
    job_repo.add_processing_stage(
        db=db_session,
        job_id=new_job.id,
        stage="transcription_complete"
    )
"""

from typing import Any, List, Optional

from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import Session, joinedload

from apps.core.models.enums import ProcessingStatus
from apps.core.models.video_job_model import VideoJobModel
from apps.core.models.video_model import VideoModel


class VideoJobRepository:
    """
    Repository for VideoJobModel data access.
    """

    @staticmethod
    async def create(
        db: AsyncSession,
        video_id: int,
        status: ProcessingStatus = ProcessingStatus.PENDING,
        processing_stages: Optional[Any] = None,
        error_message: Optional[str] = None,
    ) -> VideoJobModel:
        """
        Create and persist a new VideoJobModel.

        Args:
            db (AsyncSession): SQLAlchemy async session.
            video_id (int): Associated video ID.
            status (ProcessingStatus): Initial processing status.
            processing_stages (Optional[Any]): Initial processing stages (JSON/text).
            error_message (Optional[str]): Initial error message.

        Returns:
            VideoJobModel: The created job model.
        """
        job = VideoJobModel(
            video_id=video_id,
            status=status,
            processing_stages=processing_stages,
            error_message=error_message,
        )
        db.add(job)
        await db.flush()
        return job

    @staticmethod
    async def get_by_id(db: AsyncSession, job_id: int) -> Optional[VideoJobModel]:
        """
        Retrieve a VideoJobModel by its ID.

        Args:
            db (AsyncSession): SQLAlchemy async session.
            job_id (int): Job ID.

        Returns:
            Optional[VideoJobModel]: The job model, or None if not found.
        """
        result = await db.execute(
            select(VideoJobModel).filter(VideoJobModel.id == job_id)
        )
        return result.scalars().first()

    @staticmethod
    async def update_status(
        db: AsyncSession,
        job_id: int,
        status: ProcessingStatus,
        error_message: Optional[str] = None,
    ) -> Optional[VideoJobModel]:
        """
        Update the status (and optionally error message) of a VideoJobModel.

        Args:
            db (AsyncSession): SQLAlchemy async session.
            job_id (int): Job ID.
            status (ProcessingStatus): New status.
            error_message (Optional[str]): Error message.

        Returns:
            Optional[VideoJobModel]: The updated job model, or None if not found.
        """
        result = await db.execute(
            select(VideoJobModel).filter(VideoJobModel.id == job_id)
        )
        job = result.scalars().first()
        if job is not None:
            job.status = status  # type: ignore
            if error_message is not None:
                job.error_message = error_message  # type: ignore
            await db.flush()
        return job

    @staticmethod
    async def add_processing_stage(
        db: AsyncSession, job_id: int, stage: str
    ) -> Optional[VideoJobModel]:
        """
        Add a processing stage to the job's processing_stages list (assumes JSON/text).

        Args:
            db (AsyncSession): SQLAlchemy async session.
            job_id (int): Job ID.
            stage (str): Stage to add.

        Returns:
            Optional[VideoJobModel]: The updated job model, or None if not found.
        """
        result = await db.execute(
            select(VideoJobModel).filter(VideoJobModel.id == job_id)
        )
        job = result.scalars().first()
        if job is not None:
            # Explicitly handle None to avoid potential issues with linter and SQLAlchemy attributes
            current_processing_stages = job.processing_stages
            if current_processing_stages is None:
                stages = []
            else:
                stages = current_processing_stages

            if isinstance(stages, str):
                import json

                try:
                    stages = json.loads(stages)
                except Exception:
                    stages = []
            if not isinstance(stages, list):
                stages = []
            stages.append(stage)
            job.processing_stages = stages  # type: ignore
            await db.flush()
        return job

    @staticmethod
    async def get_by_user_id_and_statuses(
        db: AsyncSession,
        user_id: str,
        statuses: Optional[List[ProcessingStatus]] = None,
        limit: int = 100,
        offset: int = 0,
    ) -> List[VideoJobModel]:
        """
        Retrieve VideoJobModels for a specific user, filtered by statuses, with pagination.

        Args:
            db (AsyncSession): SQLAlchemy async session.
            user_id (str): The uploader_user_id (Supabase string UUID) from VideoModel.
            statuses (Optional[List[ProcessingStatus]]): List of statuses to filter by.
                                                      If None or empty, default statuses might be applied
                                                      (e.g., PENDING, PROCESSING) or no status filter.
            limit (int): Maximum number of jobs to return.
            offset (int): Number of jobs to skip for pagination.

        Returns:
            List[VideoJobModel]: A list of job models.
        """
        stmt = (
            select(VideoJobModel)
            .join(VideoModel, VideoJobModel.video_id == VideoModel.id)
            .filter(VideoModel.uploader_user_id == user_id)
        )

        if statuses:
            stmt = stmt.filter(VideoJobModel.status.in_(statuses))

        # Eager load related video and metadata to prevent N+1 queries if accessed later
        # This is useful if the VideoJobSchema nests VideoSchema and VideoMetadataSchema
        stmt = stmt.options(
            joinedload(VideoJobModel.video), joinedload(VideoJobModel.video_metadata)
        )

        stmt = (
            stmt.order_by(VideoJobModel.created_at.desc()).offset(offset).limit(limit)
        )

        result = await db.execute(stmt)
        return list(result.scalars().all())


async def get_video_job_repository() -> VideoJobRepository:
    """
    FastAPI dependency for getting a VideoJobRepository instance.
    Used for dependency injection in API endpoints.

    Returns:
        VideoJobRepository: An instance of the VideoJobRepository.
    """
    return VideoJobRepository()
</file>

<file path="apps/web/src/lib/api.ts">
// API fetchers for the web app

import type {
  VideoSummary,
  SignedUploadUrlRequest,
  SignedUploadUrlResponse,
  UploadCompleteRequest,
  VideoDetailsResponse,
  VideoMetadataUpdateRequest,
  ApiErrorResponse,
} from '../types/api';
import { supabase } from '@echo/db/clients/client'; // Import Supabase client

const API_BASE_URL = import.meta.env.VITE_API_URL || "http://localhost:8000";
const API_V1_PREFIX = "/api/v1";

// Helper to get Supabase token
async function getAuthHeaders(): Promise<HeadersInit> {
  const { data: { session } } = await supabase.auth.getSession();
  const headers: HeadersInit = {
    'Content-Type': 'application/json',
  };
  if (session?.access_token) {
    headers['Authorization'] = `Bearer ${session.access_token}`;
  }
  return headers;
}

async function handleApiResponse<T>(response: Response): Promise<T> {
  if (!response.ok) {
    let errorPayload: ApiErrorResponse | string;
    try {
      errorPayload = await response.json();
    } catch (e) {
      errorPayload = await response.text();
    }
    const errorMessage = typeof errorPayload === 'string' ? errorPayload : (errorPayload as ApiErrorResponse).detail || `HTTP error ${response.status}`;
    throw new Error(`API Error: ${response.status} - ${errorMessage}`);
  }
  if (response.status === 204) { // No Content
    return undefined as T; // Or handle as appropriate for your use case
  }
  return response.json() as Promise<T>;
}

// Define PaginationParams interface
export interface PaginationParams {
  limit?: number;
  offset?: number;
  // page?: number; // Alternative, if backend uses page numbers
}

// Fetches the current user's videos from the backend API.
export async function fetchMyVideos(params?: PaginationParams): Promise<VideoSummary[]> {
  const queryParams = new URLSearchParams();
  if (params?.limit) queryParams.append('limit', String(params.limit));
  if (params?.offset) queryParams.append('offset', String(params.offset));
  // if (params?.page) queryParams.append('page', String(params.page));

  const endpoint = `${API_BASE_URL}${API_V1_PREFIX}/users/me/videos${queryParams.toString() ? '?' + queryParams.toString() : ''}`;
  const headers = await getAuthHeaders();

  const res = await fetch(endpoint, {
    method: "GET",
    headers: headers,
  });

  return handleApiResponse<VideoSummary[]>(res);
}

// Requests a pre-signed URL to upload a video directly to cloud storage.
export async function getSignedUploadUrl(
  data: SignedUploadUrlRequest
): Promise<SignedUploadUrlResponse> {
  const endpoint = `${API_BASE_URL}${API_V1_PREFIX}/videos/signed-upload-url`;
  const headers = await getAuthHeaders(); // Assuming this might need auth too
  const res = await fetch(endpoint, {
    method: "POST",
    headers: headers,
    body: JSON.stringify(data),
  });
  return handleApiResponse<SignedUploadUrlResponse>(res);
}

// Notifies the backend that a direct video upload is complete.
export async function notifyUploadComplete(
  data: UploadCompleteRequest
): Promise<void> {
  const endpoint = `${API_BASE_URL}${API_V1_PREFIX}/videos/upload-complete`;
  const headers = await getAuthHeaders();
  const res = await fetch(endpoint, {
    method: "POST",
    headers: headers,
    body: JSON.stringify(data),
  });
  return handleApiResponse<void>(res);
}

// Fetches comprehensive details for a specific video.
export async function getVideoDetails(
  videoId: string
): Promise<VideoDetailsResponse> {
  const endpoint = `${API_BASE_URL}${API_V1_PREFIX}/videos/${videoId}/details`;
  const headers = await getAuthHeaders();
  const res = await fetch(endpoint, {
    method: "GET",
    headers: headers,
  });
  return handleApiResponse<VideoDetailsResponse>(res);
}

// Updates metadata for a specific video.
export async function updateVideoMetadata(
  videoId: string,
  metadata: VideoMetadataUpdateRequest
): Promise<void> {
  const endpoint = `${API_BASE_URL}${API_V1_PREFIX}/videos/${videoId}/metadata`;
  const headers = await getAuthHeaders();
  const res = await fetch(endpoint, {
    method: "PUT",
    headers: headers,
    body: JSON.stringify(metadata),
  });
  return handleApiResponse<void>(res);
}

// Fetches the status and results of a specific processing job.
// Note: WebSocket is preferred for real-time updates, but this can be a fallback.
// Assuming VideoJob type would be defined in types/api.ts similar to VideoDetailsResponse
// For now, let's assume it returns a similar structure to VideoJob from types/api.ts
import type { VideoJobSchema as VideoJob } from '../types/api';
export async function getJobDetails(jobId: string): Promise<VideoJob> {
  const endpoint = `${API_BASE_URL}${API_V1_PREFIX}/videos/jobs/${jobId}`;
  const headers = await getAuthHeaders();
  const res = await fetch(endpoint, {
    method: "GET",
    headers: headers,
  });
  return handleApiResponse<VideoJob>(res);
}

// Fetches processing jobs for the authenticated user, optionally filtered by status.
export async function getProcessingJobs(statuses?: string[]): Promise<VideoJob[]> {
  const queryParams = new URLSearchParams();
  if (statuses && statuses.length > 0) {
    statuses.forEach(status => queryParams.append('status', status));
  }
  
  const endpoint = `${API_BASE_URL}${API_V1_PREFIX}/users/me/jobs${queryParams.toString() ? '?' + queryParams.toString() : ''}`;
  const headers = await getAuthHeaders();
  const res = await fetch(endpoint, {
    method: "GET",
    headers: headers,
  });
  return handleApiResponse<VideoJob[]>(res);
}
</file>

<file path="apps/web/src/routes/dashboard.tsx">
import { Button } from "@/components/ui/button";
import { VideoUploadDropzone } from "@/components/video/VideoUploadDropzone";
import { Tabs, TabsContent, TabsList, TabsTrigger } from "@/components/ui/tabs";
import { ProcessingDashboard } from "@/components/video/processing-dashboard";
import { Link, createFileRoute, useNavigate } from "@tanstack/react-router";
import { fetchMyVideos, type PaginationParams } from "@/lib/api";
import type { VideoSummary } from "@/types/api";
import { useInfiniteQuery, type InfiniteData, type QueryKey } from "@tanstack/react-query";
import { ExternalLink, UploadIcon } from "lucide-react";
import { useEffect, useState, Fragment } from "react";
import {
	Dialog,
	DialogContent,
	DialogHeader,
	DialogTitle,
	DialogTrigger,
} from "@/components/ui/dialog";
import { VideoList } from "@/components/video/VideoList";

function DashboardComponent() {
	const [activeTab, setActiveTab] = useState("library");
	const [uploadDialogOpen, setUploadDialogOpen] = useState(false);

	const navigate = useNavigate();

	const { 
		data,
		fetchNextPage,
		hasNextPage,
		isFetchingNextPage,
		isLoading: videosLoading,
		error: videosError 
	} = useInfiniteQuery<
		VideoSummary[],
		Error,
		InfiniteData<VideoSummary[], PaginationParams | undefined>,
		QueryKey,
		PaginationParams | undefined
	>(
		{
			queryKey: ['myVideos'],
			queryFn: ({ pageParam = { offset: 0, limit: 10 } }) => fetchMyVideos(pageParam),
			initialPageParam: { offset: 0, limit: 10 } as PaginationParams | undefined,
			getNextPageParam: (lastPage: VideoSummary[], allPages: VideoSummary[][], lastPageParam: PaginationParams | undefined) => {
				if (!lastPageParam || lastPage.length < (lastPageParam.limit ?? 10)) {
					return undefined;
				}
				const currentTotal = allPages.reduce((acc, page) => acc + page.length, 0);
				return { offset: currentTotal, limit: lastPageParam.limit ?? 10 };
			},
		}
	);

	const videos: VideoSummary[] = data?.pages.flatMap((page: VideoSummary[]) => page) ?? [];

	return (
		<div className="container py-10">
			<div className="flex flex-col sm:flex-row sm:items-center sm:justify-between mb-8 gap-4">
				<div>
					<h1 className="text-xl sm:text-2xl font-bold tracking-tight">Video Dashboard</h1>
					<p className="text-muted-foreground text-sm mt-1">
						Manage and monitor your video processing status
					</p>
				</div>
				<Dialog open={uploadDialogOpen} onOpenChange={setUploadDialogOpen}>
					<DialogTrigger asChild>
						<Button>
							<UploadIcon className="h-4 w-4 md:mr-2" /> 
							<span className="hidden md:inline">Upload New Video</span>
							<span className="sr-only md:hidden">Upload New Video</span>
						</Button>
					</DialogTrigger>
					<DialogContent className="sm:max-w-md">
						<DialogHeader>
							<DialogTitle>Upload Video</DialogTitle>
						</DialogHeader>
						<div className="py-4">
							<VideoUploadDropzone
								onUploadComplete={() => setUploadDialogOpen(false)}
							/>
						</div>
					</DialogContent>
				</Dialog>
			</div>

			<Tabs value={activeTab} onValueChange={setActiveTab}>
				<TabsList className="w-full grid grid-cols-2 max-w-md mb-6">
					<TabsTrigger value="processing" className="flex-1">
						Processing Status
					</TabsTrigger>
					<TabsTrigger value="library" className="flex-1">
						Video Library
					</TabsTrigger>
				</TabsList>

				<TabsContent value="processing" className="space-y-4 animate-in">
					<ProcessingDashboard />
				</TabsContent>

				<TabsContent value="library" className="space-y-4 animate-in">
					<VideoList
						videos={videos}
						isLoading={videosLoading && !data}
						error={videosError}
						hasNextPage={hasNextPage}
						isFetchingNextPage={isFetchingNextPage}
						fetchNextPage={fetchNextPage}
						showUploadButton={true}
						onTriggerUpload={() => setUploadDialogOpen(true)}
					/>
				</TabsContent>
			</Tabs>
		</div>
	);
}

export const Route = createFileRoute("/dashboard")({
	component: DashboardComponent,
});
</file>

<file path=".cursor/rules/cursor_rules.mdc">
---
description: 
globs: 
alwaysApply: false
---
---
description: Guidelines for creating and maintaining Cursor rules to ensure consistency and effectiveness.
globs: .cursor/rules/*.mdc
alwaysApply: true
---

- **Required Rule Structure:**
  ```markdown
  ---
  description: Clear, one-line description of what the rule enforces
  globs: path/to/files/*.ext, other/path/**/*
  alwaysApply: boolean
  ---

  - **Main Points in Bold**
    - Sub-points with details
    - Examples and explanations
  ```

- **File References:**
  - Use `[filename](mdc:path/to/file)` ([filename](mdc:filename)) to reference files
  - Example: [prisma.mdc](mdc:.cursor/rules/prisma.mdc) for rule references
  - Example: [schema.prisma](mdc:prisma/schema.prisma) for code references

- **Code Examples:**
  - Use language-specific code blocks
  ```typescript
  //  DO: Show good examples
  const goodExample = true;
  
  //  DON'T: Show anti-patterns
  const badExample = false;
  ```

- **Rule Content Guidelines:**
  - Start with high-level overview
  - Include specific, actionable requirements
  - Show examples of correct implementation
  - Reference existing code when possible
  - Keep rules DRY by referencing other rules

- **Rule Maintenance:**
  - Update rules when new patterns emerge
  - Add examples from actual codebase
  - Remove outdated patterns
  - Cross-reference related rules

- **Best Practices:**
  - Use bullet points for clarity
  - Keep descriptions concise
  - Include both DO and DON'T examples
  - Reference actual code over theoretical examples
  - Use consistent formatting across rules
</file>

<file path=".cursor/rules/self_improve.mdc">
---
description: Guidelines for continuously improving Cursor rules based on emerging code patterns and best practices.
globs: **/*
alwaysApply: true
---

- **Rule Improvement Triggers:**
  - New code patterns not covered by existing rules
  - Repeated similar implementations across files
  - Common error patterns that could be prevented
  - New libraries or tools being used consistently
  - Emerging best practices in the codebase

- **Analysis Process:**
  - Compare new code with existing rules
  - Identify patterns that should be standardized
  - Look for references to external documentation
  - Check for consistent error handling patterns
  - Monitor test patterns and coverage

- **Rule Updates:**
  - **Add New Rules When:**
    - A new technology/pattern is used in 3+ files
    - Common bugs could be prevented by a rule
    - Code reviews repeatedly mention the same feedback
    - New security or performance patterns emerge

  - **Modify Existing Rules When:**
    - Better examples exist in the codebase
    - Additional edge cases are discovered
    - Related rules have been updated
    - Implementation details have changed

- **Example Pattern Recognition:**
  ```typescript
  // If you see repeated patterns like:
  const data = await prisma.user.findMany({
    select: { id: true, email: true },
    where: { status: 'ACTIVE' }
  });
  
  // Consider adding to [prisma.mdc](mdc:.cursor/rules/prisma.mdc):
  // - Standard select fields
  // - Common where conditions
  // - Performance optimization patterns
  ```

- **Rule Quality Checks:**
  - Rules should be actionable and specific
  - Examples should come from actual code
  - References should be up to date
  - Patterns should be consistently enforced

- **Continuous Improvement:**
  - Monitor code review comments
  - Track common development questions
  - Update rules after major refactors
  - Add links to relevant documentation
  - Cross-reference related rules

- **Rule Deprecation:**
  - Mark outdated patterns as deprecated
  - Remove rules that no longer apply
  - Update references to deprecated rules
  - Document migration paths for old patterns

- **Documentation Updates:**
  - Keep examples synchronized with code
  - Update references to external docs
  - Maintain links between related rules
  - Document breaking changes
Follow [cursor_rules.mdc](mdc:.cursor/rules/cursor_rules.mdc) for proper rule formatting and structure.
</file>

<file path="apps/web/package.json">
{
	"name": "@echo/web",
	"version": "1.0.0",
	"main": "index.js",
	"type": "module",
	"sideEffects": false,
	"scripts": {
		"test": "vitest run",
		"dev": "vinxi dev",
		"build": "vinxi build",
		"start": "vinxi start",
		"lint": "biome check --write --unsafe",
		"format": "biome format --write --unsafe",
		"check": "biome check --write",
		"generate:api-types": "cd ../core && uv run pydantic2ts --module api.schemas.video_processing_schemas --output ../web/src/types/api.ts --json2ts-cmd ../web/node_modules/.bin/json2ts && cd ../web"
	},
	"keywords": [],
	"author": "",
	"license": "ISC",
	"description": "",
	"dependencies": {
		"@echo/db": "workspace:*",
		"@hookform/resolvers": "^5.0.1",
		"@radix-ui/react-accordion": "^1.2.8",
		"@radix-ui/react-alert-dialog": "^1.1.11",
		"@radix-ui/react-aspect-ratio": "^1.1.4",
		"@radix-ui/react-avatar": "^1.1.7",
		"@radix-ui/react-checkbox": "^1.2.3",
		"@radix-ui/react-collapsible": "^1.1.8",
		"@radix-ui/react-context-menu": "^2.2.12",
		"@radix-ui/react-dialog": "^1.1.11",
		"@radix-ui/react-dropdown-menu": "^2.1.12",
		"@radix-ui/react-hover-card": "^1.1.11",
		"@radix-ui/react-label": "^2.1.2",
		"@radix-ui/react-menubar": "^1.1.12",
		"@radix-ui/react-navigation-menu": "^1.2.10",
		"@radix-ui/react-popover": "^1.1.11",
		"@radix-ui/react-progress": "^1.1.4",
		"@radix-ui/react-radio-group": "^1.3.4",
		"@radix-ui/react-scroll-area": "^1.2.6",
		"@radix-ui/react-select": "^2.1.6",
		"@radix-ui/react-separator": "^1.1.4",
		"@radix-ui/react-slider": "^1.2.3",
		"@radix-ui/react-slot": "^1.1.2",
		"@radix-ui/react-switch": "^1.1.3",
		"@radix-ui/react-tabs": "^1.1.8",
		"@radix-ui/react-toast": "^1.2.11",
		"@radix-ui/react-toggle": "^1.1.6",
		"@radix-ui/react-toggle-group": "^1.1.7",
		"@radix-ui/react-tooltip": "^1.2.4",
		"@supabase/ssr": "^0.5.2",
		"@supabase/supabase-js": "^2.48.1",
		"@tailwindcss/postcss": "^4.1.4",
		"@tailwindcss/vite": "^4.0.6",
		"@tanstack/react-form": "^1.0.0",
		"@tanstack/react-query": "^5.76.1",
		"@tanstack/react-router": "^1.116.0",
		"@tanstack/react-router-devtools": "^1.116.0",
		"@tanstack/react-router-with-query": "^1.116.0",
		"@tanstack/react-start": "^1.116.0",
		"@tanstack/react-table": "^8.21.3",
		"@tanstack/router-plugin": "^1.114.3",
		"boxen": "^8.0.1",
		"chalk": "^4.1.2",
		"class-variance-authority": "^0.7.1",
		"cli-table3": "^0.6.5",
		"clsx": "^2.1.1",
		"cmdk": "^1.1.1",
		"commander": "^11.1.0",
		"cors": "^2.8.5",
		"date-fns": "^4.1.0",
		"dotenv": "^16.5.0",
		"embla-carousel-react": "^8.6.0",
		"express": "^4.21.2",
		"fastmcp": "^1.23.2",
		"figlet": "^1.8.1",
		"fuse.js": "^7.1.0",
		"googleapis": "^148.0.0",
		"gradient-string": "^3.0.0",
		"helmet": "^8.1.0",
		"input-otp": "^1.4.2",
		"inquirer": "^12.6.0",
		"jsonwebtoken": "^9.0.2",
		"lru-cache": "^10.4.3",
		"lucide-react": "^0.488.0",
		"next-themes": "^0.4.6",
		"openai": "^4.96.2",
		"ora": "^8.2.0",
		"react": "^19.1.0",
		"react-day-picker": "8.10.1",
		"react-dom": "^19.1.0",
		"react-hook-form": "^7.56.1",
		"react-resizable-panels": "^3.0.0",
		"recharts": "^2.15.3",
		"sonner": "^2.0.3",
		"tailwind-merge": "3.2.0",
		"tailwindcss-animate": "^1.0.7",
		"vaul": "^1.1.2",
		"vinxi": "^0.5.6",
		"zod": "^3.24.3"
	},
	"devDependencies": {
		"@biomejs/biome": "1.9.4",
		"@testing-library/dom": "^10.4.0",
		"@testing-library/react": "^16.3.0",
		"@types/node": "^22.5.4",
		"@types/react": "^19.0.8",
		"@types/react-dom": "^19.0.3",
		"@vitejs/plugin-react": "^4.4.1",
		"@vitest/ui": "^3.1.2",
		"autoprefixer": "^10.4.20",
		"jsdom": "^26.1.0",
		"json-schema-to-typescript": "^15.0.4",
		"postcss": "^8.5.3",
		"tailwindcss": "^4.1.4",
		"typescript": "^5.7.2",
		"vite": "^6.3.5",
		"vite-tsconfig-paths": "^5.1.4",
		"vitest": "^3.1.2",
		"web-vitals": "^4.2.4"
	},
	"overrides": {
		"react-is": "^19.0.0-rc-69d4b800-20241021"
	}
}
</file>

<file path=".gitignore">
# See https://help.github.com/articles/ignoring-files/ for more about ignoring files.

# dependencies
**/node_modules/**
# Additions for monorepo structure
/apps/web/node_modules
/.pnp
.pnp.js

# testing
/coverage
.pytest_cache

# next.js
/.next/
/out/
# Additions for monorepo structure
/apps/web/.next/
/apps/web/out/

# python
venv/
.venv/
.env
__pycache__

# production
/build

# misc
.DS_Store
*.pem

# debug
npm-debug.log*
yarn-debug.log*
yarn-error.log*

# local env files
.env*.local

# typescript
*.tsbuildinfo
next-env.d.ts
# Additions for monorepo structure
/apps/web/next-env.d.ts
*.swp


api/tests/requests/

# Sensitive test files
api/src/tests/sensitive/*
api/src/tests/requests/*

# playwright
playwright-report/
test-results/

# docker
docker*.log

# Added by Claude Task Master
# Logs
logs
*.log
dev-debug.log
# Dependency directories
node_modules/
# Environment variables
# Editor directories and files
.idea
.vscode
*.suo
*.ntvs*
*.njsproj
*.sln
*.sw?
# OS specific
# Task files
tasks.json
tasks/ 
api/.env
apps/web/.env
api/config/secrets/*.json
</file>

<file path="package.json">
{
  "name": "echo",
  "version": "0.1.0",
  "private": true,
  "scripts": {
    "install:all": "pnpm install && (cd apps/core && uv venv core-api && uv sync)",
    "dev": "concurrently --kill-others-on-fail --names \"WEB,SUPA,API\" -c \"bgBlue.bold,bgGreen.bold,bgMagenta.bold\" \"pnpm dev:web\" \"pnpm dev:supabase\" \"pnpm dev:backend\"",
    "dev:web": "pnpm --filter @echo/web dev",
    "dev:supabase": "cd supabase && supabase start",
    "dev:backend": "cd apps/core && ./bin/dev.sh",
    "build": "npm-run-all --parallel build:web build:backend",
    "build:web": "pnpm --filter @echo/web build",
    "build:backend": "cd apps/core && ./bin/build.sh",
    "clean": "npm-run-all --parallel clean:ws clean:backend clean:supabase",
    "clean:ws": "pnpm --recursive exec -- rm -rf node_modules dist .turbo .next .svelte-kit .vite && rm -f pnpm-lock.yaml",
    "clean:backend": "cd apps/core && ./bin/clean_env.sh",
    "clean:supabase": "cd supabase && supabase stop --no-backup",
    "lint": "npm-run-all --continue-on-error --parallel lint:*",
    "lint:web": "pnpm --filter @echo/web lint",
    "lint:supabase": "pnpm --filter @echo/db lint",
    "lint:backend": "cd apps/core && ruff check . && black --check .",
    "format": "npm-run-all --continue-on-error --parallel format:*",
    "format:web": "pnpm --filter @echo/web format",
    "format:supabase": "pnpm --filter @echo/db format",
    "format:backend": "cd apps/core && ruff format . && black .",
    "reinstall": "pnpm clean:ws && pnpm install:all",
    "fastapi-dev": "cd apps/core && ./bin/dev.sh"
  },
  "dependencies": {
    "@tanstack/react-router": "^1.120.3",
    "@tanstack/react-router-devtools": "^1.120.3",
    "@tanstack/router-plugin": "^1.120.3",
    "react": "^19.0.0",
    "react-dom": "^19.0.0",
    "redaxios": "^0.5.1"
  },
  "devDependencies": {
    "@types/node": "^22.15.3",
    "@types/react": "^19",
    "@types/react-dom": "^19",
    "@vitejs/plugin-react": "^4.3.4",
    "concurrently": "^9.1.2",
    "eslint": "^9.25.1",
    "npm-run-all": "^4.1.5",
    "typescript": "^5.8.3",
    "vite": "^6.1.0",
    "vite-plugin-dts": "^4.5.0"
  }
}
</file>

<file path="README.md">
# AI YouTube Video Metadata Generator  PRD

##  Overview

This app helps users automatically generate metadata for YouTube videos (titles, subtitles, chapters, descriptions) using Google Gemini.

It uses a polyglot monorepo architecture with:

* Supabase for auth, database, and real-time features
* FastAPI for AI workflows and video processing
* GCS for video storage
* React + TanStack for frontend UI and routing

---

##  Stack Architecture

```mermaid
graph TD
  A[React + TanStack Router] -->|calls| B(Supabase Auth)
  A -->|calls| C[FastAPI Backend]
  A -->|calls| D[Supabase DB]
  C -->|generates| E[Google Cloud Storage Signed URLs]
  C -->|processes with| F[Gemini (via Vertex AI)]
  C -->|writes metadata to| D
  A -->|fetches metadata from| D
```

---

##  Key Use Cases

### 1. User Onboarding

* Login/signup via Supabase
* Connect YouTube account (OAuth)

### 2. Upload Video

* Get signed GCS upload URL from FastAPI
* Upload video directly to GCS
* Trigger FastAPI to process video

### 3. Generate Metadata

* FastAPI extracts audio
* Sends audio to Gemini
* Saves results (transcript, title, description, chapters) to Supabase

### 4. View Results

* React frontend fetches metadata via Supabase
* User sees structured AI-generated content

---

##  API Endpoints

| Endpoint         | Method   | Description                               |
| ---------------- | -------- | ----------------------------------------- |
| `/auth/login`    | POST     | Custom auth endpoint if needed            |
| `/upload-url`    | POST     | Returns signed GCS upload URL             |
| `/videos`        | POST     | Logs video metadata + triggers processing |
| `/videos/:id`    | GET      | Returns generated metadata                |
| `/youtube/oauth` | GET/POST | Handles YouTube OAuth                     |

---

##  Data Models

### `users`

* `id`
* `email`
* `youtube_access_token`
* `created_at`

### `videos`

* `id`
* `user_id`
* `gcs_path`
* `processing_status`
* `created_at`

### `video_metadata`

* `id`
* `video_id`
* `title`
* `description`
* `transcript`
* `chapters`
* `updated_at`

---

##  Frontend Responsibilities

* Supabase SDK handles auth and session
* React fetches signed URLs from FastAPI
* Video uploaded directly to GCS
* API call to FastAPI to trigger metadata processing
* Polls FastAPI for metadata or queries Supabase

---

##  Backend Responsibilities

* FastAPI routes for file handling, auth, AI logic
* Generates signed URLs
* Integrates with Gemini via Vertex AI
* Persists results in Supabase
* Background jobs for long-running tasks (via asyncio or Celery)

---

##  DevOps

* Docker Compose for local dev + prod
* Traefik reverse proxy for HTTPS
* GitHub Actions for CI/CD

---

##  Security

* Supabase handles RLS and auth
* GCS uploads via signed URLs
* JWT auth between frontend and FastAPI
* OAuth token refresh support

---

##  Testing

* Pytest for FastAPI endpoints
* Playwright for frontend E2E tests
* Supabase seeding for test environments

---

##  Workflow Diagram

```mermaid
sequenceDiagram
  participant U as User
  participant W as Web App
  participant F as FastAPI
  participant S as Supabase
  participant G as GCS
  participant M as Gemini

  U->>W: Logs in via Supabase
  W->>S: Validates session
  W->>F: GET /upload-url
  F->>G: Generate signed GCS URL
  F-->>W: Return URL
  W->>G: Upload .mp4
  W->>F: POST /videos
  F->>G: Fetch video
  F->>M: Send audio to Gemini
  M-->>F: Metadata
  F->>S: Store results
  W->>S: Fetch metadata
  W->>U: Show content
```

---

##  Development Setup

### Generating API Types

To ensure type consistency between the Python backend (`apps/core`) Pydantic models and the TypeScript frontend (`apps/web`), we use `pydantic-to-typescript`.

-   **Pydantic Models Location**: `apps/core/api/schemas/video_processing_schemas.py`
-   **Generated TypeScript Types Location**: `apps/web/src/types/api.ts`

To regenerate the TypeScript types after making changes to the Pydantic models, run the following command from the `apps/web` directory:

```bash
pnpm run generate:api-types
```

This script executes `pydantic2ts` using the `uv` environment from `apps/core`.

Make sure you have installed the necessary dependencies:
-   In `apps/core` (Python environment, managed by `uv`):
    ```bash
    # If not already installed (should be via pyproject.toml or uv.lock)
    # cd apps/core && uv add "pydantic-to-typescript>=2"
    ```
-   In `apps/web` (Node.js environment, managed by `pnpm`):
    ```bash
    # If not already installed (should be via package.json)
    # pnpm --filter @echo/web add -D json-schema-to-typescript 
    ```

---

This structure lets you move fast, use Supabase where it shines, and lean into Python where AI or Google SDKs are best. Let me know if you want to split this PRD into feature cards or set up tracking in Linear/Notion.
</file>

</files>
